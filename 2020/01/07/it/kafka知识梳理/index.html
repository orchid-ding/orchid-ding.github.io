<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="天行健、君子以自强不息；地势坤，君子以厚德载物。">
    <meta name="keyword"  content="兰草">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        kafka知识梳理 - Kaffir Lily的博客 | Kaffir Lily&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_1598291_q3el2wqimj.css" type="text/css">
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>kfly</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont iconhome"></i>
                    <span>主页</span>
                </a>
            </li>
 	   <li >
                <a href="/spec/">
                    <i class="iconfont iconzhuanti"></i>
                    <span>专题</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>简历</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#kafka知识梳理"><span class="toc-text">kafka知识梳理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Kafka概述"><span class="toc-text">1. Kafka概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-为什么有消息系统"><span class="toc-text">1.1 为什么有消息系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Kafka核心概念"><span class="toc-text">1.2 Kafka核心概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Kafka集群架构"><span class="toc-text">1.3 Kafka集群架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-kafka基本命令行"><span class="toc-text">1.4 kafka基本命令行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-Java开发"><span class="toc-text">1.5 Java开发</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、kafka的分区策略"><span class="toc-text">2、kafka的分区策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、指定具体分区"><span class="toc-text">1、指定具体分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、key-hashCode"><span class="toc-text">2、key.hashCode()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、轮询方式"><span class="toc-text">3、轮询方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、自定义分区类"><span class="toc-text">4、自定义分区类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5、源代码解析"><span class="toc-text">5、源代码解析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、-kafka的文件存储机制"><span class="toc-text">3、 kafka的文件存储机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1、文件结构"><span class="toc-text">3.1、文件结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2、-message消息结构"><span class="toc-text">3.2、 message消息结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3、-kafka优秀设计"><span class="toc-text">3.3、 kafka优秀设计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-1-顺序写"><span class="toc-text">3.3.1 顺序写</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-PageCache"><span class="toc-text">3.3.2 PageCache</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-3-跳-表"><span class="toc-text">3.3.3 跳 表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-4-稀疏索引"><span class="toc-text">3.3.4 稀疏索引</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-5-零拷贝"><span class="toc-text">3.3.5 零拷贝</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-kafka内核原理"><span class="toc-text">4. kafka内核原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-ISR机制"><span class="toc-text">4.1  ISR机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-HW-amp-LEO原理"><span class="toc-text">4.2 HW&amp;LEO原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-producer消息发送原理"><span class="toc-text">4.3 producer消息发送原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-producer核心流程概览"><span class="toc-text">4.3.1  producer核心流程概览</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-producer核心参数"><span class="toc-text">4.4 producer核心参数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1-常见异常处理"><span class="toc-text">4.4.1 常见异常处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-2-提升消息吞吐量"><span class="toc-text">4.4.2 提升消息吞吐量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-3-请求超时"><span class="toc-text">4.4.3 请求超时</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-4-ACK参数"><span class="toc-text">4.4.4 ACK参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-5-重试乱序"><span class="toc-text">4.4.5 重试乱序</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-broker核心参数"><span class="toc-text">4.5 broker核心参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-consumer消费原理"><span class="toc-text">4.6. consumer消费原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-1-Offset管理"><span class="toc-text">4.6.1 Offset管理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-2-Coordinator"><span class="toc-text">4.6.2 Coordinator</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-consumer消费者Rebalance策略"><span class="toc-text">4.7. consumer消费者Rebalance策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-7-1-range策略"><span class="toc-text">4.7.1 range策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-7-2-round-robin策略"><span class="toc-text">4.7.2 round-robin策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-7-3-sticky策略"><span class="toc-text">4.7.3 sticky策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-consumer核心参数"><span class="toc-text">4.8. consumer核心参数</span></a></li></ol></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        kafka知识梳理
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2020-01-07 12:28:55</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#-kafka" title="-kafka">-kafka</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h1 id="kafka知识梳理"><a href="#kafka知识梳理" class="headerlink" title="kafka知识梳理"></a>kafka知识梳理</h1><p><img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/Kafka架构原理图.png" alt="Kafka架构原理图"></p>
<h2 id="1-Kafka概述"><a href="#1-Kafka概述" class="headerlink" title="1. Kafka概述"></a>1. Kafka概述</h2><h3 id="1-1-为什么有消息系统"><a href="#1-1-为什么有消息系统" class="headerlink" title="1.1 为什么有消息系统"></a>1.1 为什么有消息系统</h3><p><strong>解耦</strong><br>允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p>
<p><strong>冗余</strong><br>消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。</p>
<p><strong>扩展性</strong><br>因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。</p>
<p><strong>灵活性 &amp; 峰值处理能力</strong><br>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</p>
<p><strong>可恢复性</strong><br>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</p>
<p><strong>顺序保证</strong><br>在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka 保证一个 Partition 内的消息的有序性）</p>
<p><strong>缓冲</strong><br>有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p>
<p><strong>异步通信</strong><br>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p>
<h3 id="1-2-Kafka核心概念"><a href="#1-2-Kafka核心概念" class="headerlink" title="1.2 Kafka核心概念"></a>1.2 Kafka核心概念</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做MQ系统），常见可以用于web/nginx日志、访问日志，消息服务等等，Linkedin于<span class="number">2010</span>年贡献给了Apache基金会并成为顶级开源项目。</span><br><span class="line"></span><br><span class="line">kafka是一个分布式消息队列。具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。Kafka就是一种发布-订阅模式。将消息保存在磁盘中，以顺序读写方式访问磁盘，避免随机读写导致性能瓶颈。</span><br></pre></td></tr></table></figure>
<h3 id="1-3-Kafka集群架构"><a href="#1-3-Kafka集群架构" class="headerlink" title="1.3 Kafka集群架构"></a>1.3 Kafka集群架构</h3><p><img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/kafka集群架构 .png" alt="kafka集群架构 "></p>
<ul>
<li><p>producer</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">消息生产者，发布消息到Kafka集群的终端或服务</span><br></pre></td></tr></table></figure>
</li>
<li><p>broker</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Kafka集群中包含的服务器，一个borker表示kafka集群中的一个节点</span><br></pre></td></tr></table></figure>
</li>
<li><p>topic</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">每条发布到Kafka集群的消息属于的类别，即Kafka是面向 topic 的。</span><br><span class="line">更通俗的说Topic就像一个消息队列，生产者可以向其写入消息，消费者可以从中读取消息，一个Topic支持多个生产者或消费者同时订阅它，所以其扩展性很好。</span><br></pre></td></tr></table></figure>
</li>
<li><p>partition</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">每个 topic 包含一个或多个<span class="built_in">partition</span>。Kafka分配的单位是<span class="built_in">partition</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>replica</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">partition</span> 的副本，保障 <span class="built_in">partition</span> 的高可用。</span><br></pre></td></tr></table></figure>
</li>
<li><p>consumer</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">从Kafka集群中消费消息的终端或服务</span><br></pre></td></tr></table></figure>
</li>
<li><p>consumer group</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">每个 consumer 都属于一个 consumer group，每条消息只能被 consumer<span class="built_in"> group </span>中的一个 Consumer 消费，但可以被多个 consumer<span class="built_in"> group </span>消费。</span><br></pre></td></tr></table></figure>
</li>
<li><p>leader</p>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">每个partition有多个副本，其中有且仅有一个作为<span class="built_in">Leader</span>，<span class="built_in">Leader</span>是当前负责数据的读写的partition。 producer 和 consumer 只跟 <span class="built_in">leader</span> 交互</span><br></pre></td></tr></table></figure>
</li>
<li><p>follower</p>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Follower跟随<span class="built_in">Leader</span>，所有写请求都通过<span class="built_in">Leader</span>路由，数据变更会广播给所有Follower，Follower与<span class="built_in">Leader</span>保持数据同步。如果<span class="built_in">Leader</span>失效，则从Follower中选举出一个新的<span class="built_in">Leader</span>。</span><br></pre></td></tr></table></figure>
</li>
<li><p>controller</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">知道大家有没有思考过一个问题，就是Kafka集群中某个broker宕机之后，是谁负责感知到他的宕机，以及负责进行Leader Partition的选举？如果你在Kafka集群里新加入了一些机器，此时谁来负责把集群里的数据进行负载均衡的迁移？包括你的Kafka集群的各种元数据，比如说每台机器上有哪些<span class="built_in">partition</span>，谁是leader，谁是follower，是谁来管理的？如果你要删除一个topic，那么背后的各种<span class="built_in">partition</span>如何删除，是谁来控制？还有就是比如Kafka集群扩容加入一个新的broker，是谁负责监听这个broker的加入？如果某个broker崩溃了，是谁负责监听这个broker崩溃？这里就需要一个Kafka集群的总控组件，Controller。他负责管理整个Kafka集群范围内的各种东西。</span><br></pre></td></tr></table></figure>
</li>
<li><p>zookeeper</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>)	Kafka 通过 zookeeper 来存储集群的meta元数据信息</span><br><span class="line">(<span class="number">2</span>)一旦controller所在broker宕机了，此时临时节点消失，集群里其他broker会一直监听这个临时节点，发现临时节点消失了，就争抢再次创建临时节点，保证有一台新的broker会成为controller角色。</span><br></pre></td></tr></table></figure>
</li>
<li><p>offset</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">消费者在对应分区上已经消费的消息数（位置），<span class="built_in">offset</span>保存的地方跟kafka版本有一定的关系。</span><br><span class="line">kafka0<span class="number">.8</span> 版本之前<span class="built_in">offset</span>保存在zookeeper上。</span><br><span class="line">kafka0<span class="number">.8</span> 版本之后<span class="built_in">offset</span>保存在kafka集群上。</span><br></pre></td></tr></table></figure>
</li>
<li><p>ISR机制</p>
<figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">光是依靠多副本机制能保证Kafka的高可用性，但是能保证数据不丢失吗？不行，因为如果<span class="built_in">leader</span>宕机，但是<span class="built_in">leader</span>的数据还没同步到follower上去，此时即使选举了follower作为新的<span class="built_in">leader</span>，当时刚才的数据已经丢失了。</span><br><span class="line">  </span><br><span class="line">ISR是：<span class="built_in">in</span>-sync replica，就是跟<span class="built_in">leader</span> partition保持同步的follower partition的数量，只有处于ISR列表中的follower才可以在<span class="built_in">leader</span>宕机之后被选举为新的<span class="built_in">leader</span>，因为在这个ISR列表里代表他的数据跟<span class="built_in">leader</span>是同步的。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="1-4-kafka基本命令行"><a href="#1-4-kafka基本命令行" class="headerlink" title="1.4 kafka基本命令行"></a>1.4 kafka基本命令行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1、启动，停止</span></span><br><span class="line">bin/kafka-server-start.sh config/server.properties</span><br><span class="line">bin/kafka-server-stop.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2、创建topic</span></span><br><span class="line">kafka-topics.sh --create --partitions 3 --replication-factor 2 --topic test --zookeeper node01:2181,node02:2181,node03:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3、查询所有topic</span></span><br><span class="line">kafka-topics.sh --list --zookeeper node01:2181,node02:2181,node03:2181 </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4、查看topic描述信息</span></span><br><span class="line">kafka-topics.sh --describe --topic test --zookeeper node01:2181,node02:2181,node03:2181  </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 5、删除topic</span></span><br><span class="line">kafka-topics.sh --delete --topic test --zookeeper node01:2181,node02:2181,node03:2181 </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 6、模拟生产者写入数据</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 9092是 kafka中 conf/producer.properties  bootstrap.servers=localhost:9092地址</span></span><br><span class="line">kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 7、模拟消费者消费数据</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  conf/consumer.properties  bootstrap.servers=localhost:9092地址</span></span><br><span class="line">kafka-console-consumer.sh --bootstrap-server node01:9092,node02:9092,node03:9092 --topic test --from-beginning</span><br></pre></td></tr></table></figure>
<h3 id="1-5-Java开发"><a href="#1-5-Java开发" class="headerlink" title="1.5 Java开发"></a>1.5 Java开发</h3><ul>
<li>生产者开发</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//准备配置属性</span></span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"><span class="comment">//kafka集群地址</span></span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"node01:9092,node02:9092,node03:9092"</span>);</span><br><span class="line"><span class="comment">//acks它代表消息确认机制</span></span><br><span class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line"><span class="comment">//重试的次数</span></span><br><span class="line">props.put(<span class="string">"retries"</span>, <span class="number">0</span>);</span><br><span class="line"><span class="comment">//批处理数据的大小，每次写入多少数据到topic</span></span><br><span class="line">props.put(<span class="string">"batch.size"</span>, <span class="number">16384</span>);</span><br><span class="line"><span class="comment">//可以延长多久发送数据</span></span><br><span class="line">props.put(<span class="string">"linger.ms"</span>, <span class="number">1</span>);</span><br><span class="line"><span class="comment">//缓冲区的大小</span></span><br><span class="line">props.put(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);</span><br><span class="line">props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"><span class="comment">//这里需要三个参数，第一个：topic的名称，第二个参数：表示消息的key,第三个参数：消息具体内容</span></span><br><span class="line">producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"test"</span>, Integer.toString(i), <span class="string">"hello-kafka-"</span>+i));</span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure>
<ul>
<li>消费者开发</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//准备配置属性</span></span><br><span class="line">    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    <span class="comment">//kafka集群地址</span></span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"node01:9092,node02:9092,node03:9092"</span>);</span><br><span class="line">    <span class="comment">//消费者组id</span></span><br><span class="line">    props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">    <span class="comment">//自动提交偏移量</span></span><br><span class="line">    props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">    <span class="comment">//自动提交偏移量的时间间隔</span></span><br><span class="line">    props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">    <span class="comment">//默认是latest</span></span><br><span class="line">    <span class="comment">//earliest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费</span></span><br><span class="line">    <span class="comment">//latest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据</span></span><br><span class="line">    <span class="comment">//none : topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常</span></span><br><span class="line">    props.put(<span class="string">"auto.offset.reset"</span>,<span class="string">"earliest"</span>);</span><br><span class="line">    props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, String&gt;(props);</span><br><span class="line">    <span class="comment">//指定消费哪些topic</span></span><br><span class="line">    consumer.subscribe(Arrays.asList(<span class="string">"test"</span>));</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">      <span class="comment">//指定每个多久拉取一次数据</span></span><br><span class="line">      ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>手动提交偏移量</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//关闭自动提交，改为手动提交偏移量</span></span><br><span class="line"> props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//定义一个数字，表示消息达到多少后手动提交偏移量</span></span><br><span class="line"> <span class="keyword">final</span> <span class="keyword">int</span> minBatchSize = <span class="number">20</span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">   <span class="comment">//insertIntoDb(buffer); todo  拿到数据之后，进行消费</span></span><br><span class="line">   consumer.commitSync();</span><br><span class="line">   buffer.clear();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h2 id="2、kafka的分区策略"><a href="#2、kafka的分区策略" class="headerlink" title="2、kafka的分区策略"></a>2、kafka的分区策略</h2><h3 id="1、指定具体分区"><a href="#1、指定具体分区" class="headerlink" title="1、指定具体分区"></a>1、指定具体分区</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// partition(指定存储在哪个分区)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, Integer partition, K key, V value)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>(topic, partition, <span class="keyword">null</span>, key, value, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2、key-hashCode"><a href="#2、key-hashCode" class="headerlink" title="2、key.hashCode()"></a>2、key.hashCode()</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 不指定分区，给定key值，通过key.hashCode() 分配到指定分区</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, K key, V value)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, key, value, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3、轮询方式"><a href="#3、轮询方式" class="headerlink" title="3、轮询方式"></a>3、轮询方式</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 不指定分区，也不指定key的值，使用轮询方式依次存储到某个分区</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, V value)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>, value, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4、自定义分区类"><a href="#4、自定义分区类" class="headerlink" title="4、自定义分区类"></a>4、自定义分区类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 4. 类似于DefaultPartitioner，自定义分区类，实现Partitioner</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitions</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>; <span class="comment">// 返回放入哪个分区</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5、源代码解析"><a href="#5、源代码解析" class="headerlink" title="5、源代码解析"></a>5、源代码解析</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 下方是KafkaProducer&lt;K, V&gt;和DefaultPartitioner的源代码</span></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">     * computes partition for given record.</span></span><br><span class="line"><span class="comment">     * if the record has partition returns the value otherwise</span></span><br><span class="line"><span class="comment">     * calls configured partitioner class to compute the partition.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="keyword">private</span> int partition(<span class="type">ProducerRecord</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; record, byte[] serializedKey, byte[] serializedValue, <span class="type">Cluster</span> cluster) &#123;</span><br><span class="line">  <span class="type">Integer</span> partition = record.partition();</span><br><span class="line">  <span class="comment">// 1. 如果指定了partition则为partition指定值</span></span><br><span class="line">  <span class="keyword">return</span> partition != <span class="literal">null</span> ?</span><br><span class="line">  partition :</span><br><span class="line">  <span class="comment">// 2. 没指定，则是调用以下方法，方法在下方</span></span><br><span class="line">  partitioner.partition(</span><br><span class="line">    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Compute the partition for the given record.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * @param topic The topic name</span></span><br><span class="line"><span class="comment">     * @param key The key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * @param keyBytes serialized key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * @param value The value to partition on or null</span></span><br><span class="line"><span class="comment">     * @param valueBytes serialized value to partition on or null</span></span><br><span class="line"><span class="comment">     * @param cluster The current cluster metadata</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    public int partition(<span class="type">String</span> topic, <span class="type">Object</span> key, byte[] keyBytes, <span class="type">Object</span> value, byte[] valueBytes, <span class="type">Cluster</span> cluster) &#123;</span><br><span class="line">      <span class="comment">// 1.获取分区数量</span></span><br><span class="line">        <span class="type">List</span>&lt;<span class="type">PartitionInfo</span>&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        int numPartitions = partitions.size();</span><br><span class="line">      <span class="comment">// 2. key 为空时，采取轮询</span></span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="literal">null</span>) &#123;</span><br><span class="line">            int nextValue = nextValue(topic);</span><br><span class="line">            <span class="type">List</span>&lt;<span class="type">PartitionInfo</span>&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                int part = <span class="type">Utils</span>.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">                <span class="keyword">return</span> <span class="type">Utils</span>.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">          	<span class="comment">// 使用key的hash值选择一个partition</span></span><br><span class="line">            <span class="keyword">return</span> <span class="type">Utils</span>.toPositive(<span class="type">Utils</span>.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="3、-kafka的文件存储机制"><a href="#3、-kafka的文件存储机制" class="headerlink" title="3、 kafka的文件存储机制"></a>3、 kafka的文件存储机制</h2><h3 id="3-1、文件结构"><a href="#3-1、文件结构" class="headerlink" title="3.1、文件结构"></a>3.1、文件结构</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1. Kafka文件存储在Topic中</span><br><span class="line">2. 一个Topic中包含多个分区（对应文件目录下Topic名称 + 0-partitions.size()）</span><br><span class="line">	1） 每个partition下的文件被等分成多个数据文件，默认1G，每一个数据文件被分为一个段（segment file）</span><br><span class="line">	2）每个segment file分为 .log 和.index timeindex两个文件</span><br><span class="line">		1. log文件名称是当前数据的序号，存储数据信息</span><br><span class="line">		2. index文件名称同上，记录消息的offset和所在log文件的position稀疏索引。</span><br><span class="line">		3. timeindex 存储消息 timestrap和稀疏索引</span><br><span class="line">3. 一个partition有多个副本replication，分布在相同或不同的kafka节点中</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结论：一个partition中的数据是有序的吗？回答：间隔有序，不连续。</span><br><span class="line"></span><br><span class="line">针对一个topic里面的数据，只能做到partition内部有序，不能做到全局有序。特别是加入消费者的场景后，如何保证消费者的消费的消息的全局有序性，</span><br><span class="line">这是一个伪命题，只有在一种情况下才能保证消费的消息的全局有序性，那就是只有一个partition。</span><br></pre></td></tr></table></figure>
<p><img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/image-20200104154450871.png" alt="image-20200104154450871"></p>
<h3 id="3-2、-message消息结构"><a href="#3-2、-message消息结构" class="headerlink" title="3.2、 message消息结构"></a>3.2、 message消息结构</h3><p>参数说明：</p>
<table>
<thead>
<tr>
<th>关键字</th>
<th>解释说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>8 byte offset</td>
<td>在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message</td>
</tr>
<tr>
<td>4 byte message size</td>
<td>message大小</td>
</tr>
<tr>
<td>4 byte CRC32</td>
<td>用crc32校验message</td>
</tr>
<tr>
<td>1 byte “magic”</td>
<td>表示本次发布Kafka服务程序协议版本号</td>
</tr>
<tr>
<td>1 byte “attributes”</td>
<td>表示为独立版本、或标识压缩类型、或编码类型。</td>
</tr>
<tr>
<td>4 byte key length</td>
<td>表示key的长度,当key为-1时，K byte key字段不填</td>
</tr>
<tr>
<td>K byte key</td>
<td>可选</td>
</tr>
<tr>
<td>value bytes payload</td>
<td>表示实际消息数据。</td>
</tr>
</tbody>
</table>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个就需要涉及到消息的物理结构了，消息都具有固定的物理结构，包括：offset（<span class="number">8</span> Bytes）、消息体的大小（<span class="number">4</span> Bytes）、crc32（<span class="number">4</span> Bytes）、magic（<span class="number">1</span> Byte）、attributes（<span class="number">1</span> Byte）、<span class="type">key</span> length（<span class="number">4</span> Bytes）、<span class="type">key</span>（K Bytes）、payload(N Bytes)等等字段，可以确定一条消息的大小，即读取到哪里截止。</span><br></pre></td></tr></table></figure>
<h3 id="3-3、-kafka优秀设计"><a href="#3-3、-kafka优秀设计" class="headerlink" title="3.3、 kafka优秀设计"></a>3.3、 kafka优秀设计</h3><h4 id="3-3-1-顺序写"><a href="#3-3-1-顺序写" class="headerlink" title="3.3.1 顺序写"></a>3.3.1 顺序写</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一开始很多人质疑 kafka，大家认为一个架构在磁盘之上的系统，性能是如何保证的。这点需要跟大家解释一下，客户端写入到 Kafka 的数据首先是写入到操作系统缓存的（所以很快），然后缓存里的数据根据一定的策略再写入到磁盘，并且写入到磁盘的时候是顺序写，顺序写如果磁盘的个数和转数跟得上的话，都快赶上写内存的速度了！</span><br></pre></td></tr></table></figure>
<h4 id="3-3-2-PageCache"><a href="#3-3-2-PageCache" class="headerlink" title="3.3.2 PageCache"></a>3.3.2 PageCache</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">	为了优化读写性能，Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存。这样做的好处有：</span><br><span class="line"></span><br><span class="line">（1）避免Object消耗：如果是使用Java堆，Java对象的内存消耗比较大，通常是所存储数据的两倍甚至更多。</span><br><span class="line">（2）避免GC问题：随着JVM中数据不断增多，垃圾回收将会变得复杂与缓慢，使用系统缓存就不会存在GC问题。</span><br></pre></td></tr></table></figure>
<h4 id="3-3-3-跳-表"><a href="#3-3-3-跳-表" class="headerlink" title="3.3.3 跳 表"></a>3.3.3 跳 表</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在 kafka 的代码里，我们一个的 log 文件是存储是 ConcurrentSkipListMap 里的，是一个 map 结构，key 用的是文件名（也就是 offset），value 就是 log 文件内容。而 ConcurrentSkipListMap 是基于跳表的数据结构设计的。想要消费某个大小的 offset，可以根据跳表快速的定位到这个 log 文件了。</span><br></pre></td></tr></table></figure>
<h4 id="3-3-4-稀疏索引"><a href="#3-3-4-稀疏索引" class="headerlink" title="3.3.4 稀疏索引"></a>3.3.4 稀疏索引</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.index 存储稀疏索引</span><br><span class="line">假设刚刚我们定位要消费的偏移量是在 00000000000000368769.log 文件里。</span><br><span class="line">如何根据 index 文件定位呢？</span><br><span class="line">（1）首先在 index 文件里找，index 文件存储的数据都是成对出现的，比如我们到的 1，0 代表的意思是，offset=368769+1=368770 这条信息存储的物理位置是 0 这个位置。那现在我们现在想要定位的消息是 368776 这条消息，368776 减去 368769 等于 7，我们就在 index 文件里找 offset 等于 7 对应的物理位置，但是因为是稀松索引，我们没找到，不过我们找到了 offset 等于 6 的物理值 1407。</span><br><span class="line">（2）接下来就到 log 文件里读取文件的 1407 的位置，然后遍历后面的 offset，很快就可以遍历到 offset 等于 7(368776)的数据了，然后从这儿开始消费即可</span><br></pre></td></tr></table></figure>
<h4 id="3-3-5-零拷贝"><a href="#3-3-5-零拷贝" class="headerlink" title="3.3.5 零拷贝"></a>3.3.5 零拷贝</h4><ul>
<li>零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在IO读写过程中。</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Kafka利用linux操作系统的 "零拷贝（zero-copy）" 机制在消费端做的优化。</span><br></pre></td></tr></table></figure>
<ul>
<li>首先来了解下数据从文件发送到socket网络连接中的常规传输路径</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">比如：读取文件，再用socket发送出去</span><br><span class="line">传统方式实现：</span><br><span class="line">先读取、再发送，实际经过1~4四次copy。</span><br><span class="line">buffer = File.read </span><br><span class="line">Socket.send(buffer)</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* 第一步：操作系统从磁盘读取数据到内核空间（kernel space）的Page Cache缓冲区</span><br><span class="line">* 第二步：应用程序读取内核缓冲区的数据copy到用户空间（user space）的缓冲区</span><br><span class="line">* 第三步：应用程序将用户空间缓冲区的数据copy回内核空间到socket缓冲区</span><br><span class="line">* 第四步：操作系统将数据从socket缓冲区copy到网卡，由网卡进行网络传输</span><br></pre></td></tr></table></figure>
<p><img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/传统IO.png" alt="传统IO"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">传统方式，读取磁盘文件并进行网络发送，经过的四次数据copy是非常繁琐的。实际IO读写，需要进行IO中断，需要CPU响应中断(带来上下文切换)，尽管后来引入DMA来接管CPU的中断请求，但四次copy是存在“不必要的拷贝”的。</span><br><span class="line"></span><br><span class="line">重新思考传统IO方式，会注意到实际上并不需要第二个和第三个数据副本。应用程序除了缓存数据并将其传输回套接字缓冲区之外什么都不做。相反，数据可以直接从读缓冲区传输到套接字缓冲区。</span><br><span class="line"></span><br><span class="line">显然，第二次和第三次数据copy 其实在这种场景下没有什么帮助反而带来开销，这也正是零拷贝出现的意义。</span><br><span class="line"></span><br><span class="line">这种场景：是指读取磁盘文件后，不需要做其他处理，直接用网络发送出去。试想，如果读取磁盘的数据需要用程序进一步处理的话，必须要经过第二次和第三次数据copy，让应用程序在内存缓冲区处理。</span><br></pre></td></tr></table></figure>
<p><img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/sendfile.png" alt="sendfile"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">	此时我们会发现用户态“空空如也”。数据没有来到用户态，而是直接在核心态就进行了传输，但这样依然还是有多次复制。首先数据被读取到read buffer中，然后发到socket buffer，最后才发到网卡。虽然减少了用户态和核心态的切换，但依然存在多次数据复制。</span><br><span class="line"></span><br><span class="line">如果可以进一步减少数据复制的次数，甚至没有数据复制是不是就会做到最快呢？</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>DMA</strong></p>
<ul>
<li>DMA，全称叫Direct Memory Access，一种可让某些硬件子系统去直接访问系统主内存，而不用依赖CPU的计算机系统的功能。听着是不是很厉害，跳过CPU，直接访问主内存。传统的内存访问都需要通过CPU的调度来完成。如下图：</li>
</ul>
<p><img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/access memory.png" alt="access memory"></p>
<ul>
<li>DMA，则可以绕过CPU，硬件自己去直接访问系统主内存。如下图</li>
</ul>
<p><img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/1577687862577.png" alt="1577687862577"></p>
<ul>
<li>回到本文中的文件传输，有了DMA后，就可以实现绝对的零拷贝了，因为网卡是直接去访问系统主内存的。如下图：    </li>
</ul>
</li>
</ul>
<p>  <img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/零拷贝.png" alt="零拷贝"></p>
<ul>
<li><p>总结</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">	Kafka采用顺序读写、Page Cache、零拷贝以及分区分段等这些设计，再加上在索引方面做的优化，另外Kafka数据读写也是批量的而不是单条的，使得Kafka具有了高性能、高吞吐、低延时的特点。这样Kafka提供大容量的磁盘存储也变成了一种优点</span><br><span class="line"></span><br><span class="line">Java的NIO提供了FileChannle，它的transferTo、transferFrom方法就是Zero <span class="keyword">Copy</span><span class="bash">。</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="4-kafka内核原理"><a href="#4-kafka内核原理" class="headerlink" title="4. kafka内核原理"></a>4. kafka内核原理</h2><h3 id="4-1-ISR机制"><a href="#4-1-ISR机制" class="headerlink" title="4.1  ISR机制"></a>4.1  ISR机制</h3><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">光是依靠多副本机制能保证Kafka的高可用性，但是能保证数据不丢失吗？</span><br><span class="line">不行，因为如果<span class="built_in">leader</span>宕机，但是<span class="built_in">leader</span>的数据还没同步到follower上去，此时即使选举了follower作为新的<span class="built_in">leader</span>，当时刚才的数据已经丢失了。</span><br><span class="line"></span><br><span class="line">ISR是：<span class="built_in">in</span>-sync replica，就是跟<span class="built_in">leader</span> partition保持同步的follower partition的数量，只有处于ISR列表中的follower才可以在<span class="built_in">leader</span>宕机之后被选举为新的<span class="built_in">leader</span>，因为在这个ISR列表里代表他的数据跟<span class="built_in">leader</span>是同步的。</span><br><span class="line"></span><br><span class="line">如果要保证写入kafka的数据不丢失，首先需要保证ISR中至少有一个follower，其次就是在一条数据写入了<span class="built_in">leader</span> partition之后，要求必须复制给ISR中所有的follower partition，才能说代表这条数据已提交，绝对不会丢失，这是Kafka给出的承诺</span><br></pre></td></tr></table></figure>
<h3 id="4-2-HW-amp-LEO原理"><a href="#4-2-HW-amp-LEO原理" class="headerlink" title="4.2 HW&amp;LEO原理"></a>4.2 HW&amp;LEO原理</h3><ul>
<li><p><strong>LEO</strong></p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">last</span> <span class="keyword">end</span> <span class="built_in">offset</span>，日志末端偏移量，标识当前日志文件中下一条待写入的消息的<span class="built_in">offset</span>。举一个例子，若LEO=<span class="number">10</span>，那么表示在该副本日志上已经保存了<span class="number">10</span>条消息，位移范围是[<span class="number">0</span>，<span class="number">9</span>]。</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>HW</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Highwatermark，俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息。任何一个副本对象的HW值一定不大于其LEO值。</span><br><span class="line">小于或等于HW值的所有消息被认为是“已提交的”或“已备份的”。HW它的作用主要是用来判断副本的备份进度.</span><br><span class="line"></span><br><span class="line">下图表示一个日志文件，这个日志文件中只有9条消息，第一条消息的offset（LogStartOffset）为0，最有一条消息的offset为8，offset为9的消息使用虚线表示的，代表下一条待写入的消息。日志文件的 HW 为6，表示消费者只能拉取offset在 0 到 5 之间的消息，offset为6的消息对消费者而言是不可见的。</span><br></pre></td></tr></table></figure>
<p><img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/691225277.png" alt="img"></p>
</li>
</ul>
  <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">leader持有的HW即为分区的HW,同时leader所在broker还保存了所有follower副本的leo</span><br><span class="line"></span><br><span class="line">（1）关系：leader的leo &gt;= follower的leo &gt;= leader保存的follower的leo &gt;= leader的hw &gt;= follower的hw</span><br><span class="line">（2）原理：上面关系反应出各个值的更新逻辑的先后</span><br></pre></td></tr></table></figure>
<ul>
<li><p>==<strong>更新LEO的机制</strong>==</p>
<ul>
<li>注意<ul>
<li>follower副本的LEO保存在2个地方</li>
</ul>
</li>
</ul>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（<span class="number">1</span>）follower副本所在的broker缓存里。</span><br><span class="line">（<span class="number">2</span>）leader所在broker的缓存里，也就是leader所在broker的缓存上保存了该分区所有副本的LEO。</span><br></pre></td></tr></table></figure>
<ul>
<li><p>更新LEO的时机</p>
<ul>
<li>follower更新LEO</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">（1）follower的leo更新时间</span><br><span class="line">	每当follower副本写入一条消息时，leo值会被更新</span><br><span class="line">	</span><br><span class="line">（2）leader端的follower副本的leo更新时间</span><br><span class="line">	当follower从leader处fetch消息时，leader获取follower的fetch请求中offset参数，更新保存在leader端follower的leo。</span><br></pre></td></tr></table></figure>
<ul>
<li>leader更新LEO</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（1）leader本身的leo的更新时间：leader向log写消息时</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>==<strong>更新HW的机制</strong>==</p>
<ul>
<li><p>follower更新HW</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">follower更新HW发生在其更新完LEO后，即follower向log写完数据，它就会尝试更新HW值。具体算法就是比较当前LEO(已更新)与fetch响应中leader的HW值，取两者的小者作为新的HW值。</span><br></pre></td></tr></table></figure>
</li>
<li><p>leader更新HW</p>
<ul>
<li>leader更新HW的时机</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（1）producer 向 leader 写消息时</span><br><span class="line">（2）leader 处理 follower 的 fetch 请求时</span><br><span class="line">（3）某副本成为leader时</span><br><span class="line">（4）broker 崩溃导致副本被踢出ISR时</span><br></pre></td></tr></table></figure>
<ul>
<li>leader更新HW的方式</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">当尝试确定分区HW时，它会选出所有满足条件的副本，比较它们的LEO（当然也包括leader自己的LEO），并选择最小的LEO值作为HW值。</span><br><span class="line">这里的满足条件主要是指副本要满足以下两个条件之一：</span><br><span class="line">（1）处于ISR中</span><br><span class="line">（2）副本LEO落后于leader LEO的时长不大于replica.lag.time.max.ms参数值（默认值是10秒）</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<p><img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/HW和LEO的更新.png" alt="HW和LEO的更新"></p>
<h3 id="4-3-producer消息发送原理"><a href="#4-3-producer消息发送原理" class="headerlink" title="4.3 producer消息发送原理"></a>4.3 producer消息发送原理</h3><h4 id="4-3-1-producer核心流程概览"><a href="#4-3-1-producer核心流程概览" class="headerlink" title="4.3.1  producer核心流程概览"></a>4.3.1  producer核心流程概览</h4><p><img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/Producer流程分析.png" alt="Producer流程分析"></p>
<ul>
<li><p>1、ProducerInterceptors是一个拦截器，对发送的数据进行拦截</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps：说实话这个功能其实没啥用，我们即使真的要过滤，拦截一些消息，也不考虑使用它，我们直接发送数据之前自己用代码过滤即可</span><br></pre></td></tr></table></figure>
</li>
<li><p>2、Serializer 对消息的key和value进行序列化</p>
</li>
<li><p>3、通过使用分区器作用在每一条消息上，实现数据分发进行入到topic不同的分区中</p>
</li>
<li><p>4、RecordAccumulator收集消息，实现批量发送</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">它是一个缓冲区，可以缓存一批数据，把topic的每一个分区数据存在一个队列中，然后封装消息成一个一个的batch批次，最后实现数据分批次批量发送。</span><br></pre></td></tr></table></figure>
</li>
<li><p>5、Sender线程从RecordAccumulator获取消息</p>
</li>
<li><p>6、构建ClientRequest对象</p>
</li>
<li><p>7、将ClientRequest交给 NetWorkClient准备发送</p>
</li>
<li><p>8、NetWorkClient 将请求放入到KafkaChannel的缓存</p>
</li>
<li><p>9、发送请求到kafka集群</p>
</li>
<li><p>10、调用回调函数，接受到响应</p>
</li>
</ul>
<h3 id="4-4-producer核心参数"><a href="#4-4-producer核心参数" class="headerlink" title="4.4 producer核心参数"></a>4.4 producer核心参数</h3><h4 id="4-4-1-常见异常处理"><a href="#4-4-1-常见异常处理" class="headerlink" title="4.4.1 常见异常处理"></a>4.4.1 常见异常处理</h4><ul>
<li><p>不管是异步还是同步，都可能让你处理异常，常见的异常如下：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1）LeaderNotAvailableException：这个就是如果某台机器挂了，此时leader副本不可用，会导致你写入失败，要等待其他follower副本切换为leader副本之后，才能继续写入，此时可以重试发送即可。如果说你平时重启kafka的broker进程，肯定会导致leader切换，一定会导致你写入报错，是LeaderNotAvailableException</span><br><span class="line"></span><br><span class="line">2）NotControllerException：这个也是同理，如果说Controller所在Broker挂了，那么此时会有问题，需要等待Controller重新选举，此时也是一样就是重试即可</span><br><span class="line"></span><br><span class="line">3）NetworkException：网络异常，重试即可</span><br><span class="line">我们之前配置了一个参数，retries，他会自动重试的，但是如果重试几次之后还是不行，就会提供Exception给我们来处理了。</span><br></pre></td></tr></table></figure>
</li>
<li><p>retries</p>
<ul>
<li>重新发送数据的次数</li>
</ul>
</li>
<li><p>retry.backoff.ms</p>
<ul>
<li>两次重试之间的时间间隔</li>
</ul>
</li>
</ul>
<h4 id="4-4-2-提升消息吞吐量"><a href="#4-4-2-提升消息吞吐量" class="headerlink" title="4.4.2 提升消息吞吐量"></a>4.4.2 提升消息吞吐量</h4><ul>
<li><p>buffer.memory</p>
<ul>
<li>设置发送消息的缓冲区，默认值是33554432，就是32MB</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果发送消息出去的速度小于写入消息进去的速度，就会导致缓冲区写满，此时生产消息就会阻塞住，所以说这里就应该多做一些压测，尽可能保证说这块缓冲区不会被写满导致生产行为被阻塞住</span><br></pre></td></tr></table></figure>
</li>
<li><p>compression.type</p>
<ul>
<li>producer用于压缩数据的压缩类型。默认是none表示无压缩。可以指定gzip、snappy</li>
<li>压缩最好用于批量处理，批量处理消息越多，压缩性能越好。</li>
</ul>
</li>
<li><p>batch.size</p>
<ul>
<li>producer将试图批处理消息记录，以减少请求次数。这将改善client与server之间的性能。</li>
<li>默认是16384Bytes，即16kB，也就是一个batch满了16kB就发送出去</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果batch太小，会导致频繁网络请求，吞吐量下降；如果batch太大，会导致一条消息需要等待很久才能被发送出去，而且会让内存缓冲区有很大压力，过多数据缓冲在内存里。</span><br></pre></td></tr></table></figure>
</li>
<li><p>linger.ms</p>
<ul>
<li>这个值默认是0，就是消息必须立即被发送</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">一般设置一个100毫秒之类的，这样的话就是说，这个消息被发送出去后进入一个batch，如果100毫秒内，这个batch满了16kB，自然就会发送出去。</span><br><span class="line">但是如果100毫秒内，batch没满，那么也必须把消息发送出去了，不能让消息的发送延迟时间太长，也避免给内存造成过大的一个压力。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="4-4-3-请求超时"><a href="#4-4-3-请求超时" class="headerlink" title="4.4.3 请求超时"></a>4.4.3 请求超时</h4><ul>
<li>==max.request.size==<ul>
<li>这个参数用来控制发送出去的消息的大小，默认是1048576字节，也就1mb</li>
<li>这个一般太小了，很多消息可能都会超过1mb的大小，所以需要自己优化调整，把他设置更大一些（企业一般设置成10M）</li>
</ul>
</li>
<li>==request.timeout.ms==<ul>
<li>这个就是说发送一个请求出去之后，他有一个超时的时间限制，默认是30秒</li>
<li>如果30秒都收不到响应，那么就会认为异常，会抛出一个TimeoutException来让我们进行处理</li>
</ul>
</li>
</ul>
<h4 id="4-4-4-ACK参数"><a href="#4-4-4-ACK参数" class="headerlink" title="4.4.4 ACK参数"></a>4.4.4 ACK参数</h4><p>acks参数，其实是控制发送出去的消息的持久化机制的。</p>
<ul>
<li><p>==acks=0==</p>
<ul>
<li>生产者只管发数据，不管消息是否写入成功到broker中，数据丢失的风险最高</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">	producer根本不管写入broker的消息到底成功没有，发送一条消息出去，立马就可以发送下一条消息，这是吞吐量最高的方式，但是可能消息都丢失了。</span><br><span class="line">你也不知道的，但是说实话，你如果真是那种实时数据流分析的业务和场景，就是仅仅分析一些数据报表，丢几条数据影响不大的。会让你的发送吞吐量会提升很多，你发送弄一个batch出去，不需要等待人家leader写成功，直接就可以发送下一个batch了，吞吐量很大的，哪怕是偶尔丢一点点数据，实时报表，折线图，饼图。</span><br></pre></td></tr></table></figure>
</li>
<li><p>==acks=1==</p>
<ul>
<li>只要leader写入成功，就认为消息成功了.</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">默认给这个其实就比较合适的，还是可能会导致数据丢失的，如果刚写入leader，leader就挂了，此时数据必然丢了，其他的follower没收到数据副本，变成leader.</span><br></pre></td></tr></table></figure>
</li>
<li><p>==acks=all，或者 acks=-1==</p>
<ul>
<li>这个leader写入成功以后，必须等待其他ISR中的副本都写入成功，才可以返回响应说这条消息写入成功了，此时你会收到一个回调通知.</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这种方式数据最安全，但是性能最差。</span><br></pre></td></tr></table></figure>
</li>
<li><p>==如果要想保证数据不丢失，得如下设置==</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">（1）min.insync.replicas = 2</span><br><span class="line">	ISR里必须有2个副本，一个leader和一个follower，最最起码的一个，不能只有一个leader存活，连一个follower都没有了。</span><br><span class="line"></span><br><span class="line">（2）acks = -1</span><br><span class="line">	每次写成功一定是leader和follower都成功才可以算做成功，这样leader挂了，follower上是一定有这条数据，不会丢失。</span><br><span class="line">	</span><br><span class="line">（3）retries = Integer.MAX_VALUE</span><br><span class="line">	无限重试，如果上述两个条件不满足，写入一直失败，就会无限次重试，保证说数据必须成功的发送给两个副本，如果做不到，就不停的重试。</span><br><span class="line">	除非是面向金融级的场景，面向企业大客户，或者是广告计费，跟钱的计算相关的场景下，才会通过严格配置保证数据绝对不丢失</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="4-4-5-重试乱序"><a href="#4-4-5-重试乱序" class="headerlink" title="4.4.5 重试乱序"></a>4.4.5 重试乱序</h4><ul>
<li>max.in.flight.requests.per.connection<ul>
<li>每个网络连接可以忍受 producer端发送给broker 消息然后消息没有响应的个数</li>
</ul>
</li>
</ul>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">消息重试是可能导致消息的乱序的，因为可能排在你后面的消息都发送出去了，你现在收到回调失败了才在重试，此时消息就会乱序，所以可以使用“max<span class="selector-class">.in</span><span class="selector-class">.flight</span><span class="selector-class">.requests</span><span class="selector-class">.per</span><span class="selector-class">.connection</span>”参数设置为<span class="number">1</span>，这样可以保证producer同一时间只能发送一条消息</span><br></pre></td></tr></table></figure>
<h3 id="4-5-broker核心参数"><a href="#4-5-broker核心参数" class="headerlink" title="4.5 broker核心参数"></a>4.5 broker核心参数</h3><ul>
<li><p>server.properties配置文件核心参数</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">【broker.id】</span><br><span class="line">每个broker都必须自己设置的一个唯一id</span><br><span class="line"></span><br><span class="line">【log.dirs】</span><br><span class="line">这个极为重要，kafka的所有数据就是写入这个目录下的磁盘文件中的，如果说机器上有多块物理硬盘，那么可以把多个目录挂载到不同的物理硬盘上，然后这里可以设置多个目录，这样kafka可以数据分散到多块物理硬盘，多个硬盘的磁头可以并行写，这样可以提升吞吐量。</span><br><span class="line"></span><br><span class="line">【zookeeper.connect】</span><br><span class="line">连接kafka底层的zookeeper集群的</span><br><span class="line"></span><br><span class="line">【Listeners】</span><br><span class="line">broker监听客户端发起请求的端口号，默认是9092</span><br><span class="line"></span><br><span class="line">【unclean.leader.election.enable】</span><br><span class="line">默认是false，意思就是只能选举ISR列表里的follower成为新的leader，1.0版本后才设为false，之前都是true，允许非ISR列表的follower选举为新的leader</span><br><span class="line"></span><br><span class="line">【delete.topic.enable】</span><br><span class="line">默认true，允许删除topic</span><br><span class="line"></span><br><span class="line">【log.retention.hours】</span><br><span class="line">可以设置一下，要保留数据多少个小时(默认168小时)，这个就是底层的磁盘文件，默认保留7天的数据，根据自己的需求来就行了</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="4-6-consumer消费原理"><a href="#4-6-consumer消费原理" class="headerlink" title="4.6. consumer消费原理"></a>4.6. consumer消费原理</h3><h4 id="4-6-1-Offset管理"><a href="#4-6-1-Offset管理" class="headerlink" title="4.6.1 Offset管理"></a>4.6.1 Offset管理</h4><p>​    每个consumer内存里数据结构保存对每个topic的每个分区的消费offset，定期会提交offset，老版本是写入zk，但是那样高并发请求zk是不合理的架构设计，zk是做分布式系统的协调的，轻量级的元数据存储，不能负责高并发读写，作为数据存储。所以后来就是提交offset发送给内部topic：<strong>consumer_offsets，提交过去的时候，key是group.id+topic+分区号，value就是当前offset的值，每隔一段时间，kafka内部会对这个topic进行compact。也就是每个group.id+topic+分区号就保留最新的那条数据即可。而且因为这个 </strong>consumer_offsets可能会接收高并发的请求，所以默认分区50个，这样如果你的kafka部署了一个大的集群，比如有50台机器，就可以用50台机器来抗offset提交的请求压力，就好很多。</p>
<h4 id="4-6-2-Coordinator"><a href="#4-6-2-Coordinator" class="headerlink" title="4.6.2 Coordinator"></a>4.6.2 Coordinator</h4><ul>
<li><p>Coordinator的作用</p>
<figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">每个consumer <span class="keyword">group</span>都会选择一个broker作为自己的coordinator，他是负责监控这个消费组里的各个消费者的心跳，以及判断是否宕机，然后开启rebalance.</span><br><span class="line">根据内部的一个选择机制，会挑选一个对应的Broker，Kafka总会把你的各个消费组均匀分配给各个Broker作为coordinator来进行管理的.</span><br><span class="line">consumer <span class="keyword">group</span>中的每个consumer刚刚启动就会跟选举出来的这个fconsumer <span class="keyword">group</span>对应的coordinator所在的broker进行通信，然后由coordinator分配分区给你的这个consumer来进行消费。coordinator会尽可能均匀的分配分区给各个consumer来消费。</span><br></pre></td></tr></table></figure>
</li>
<li><p>如何选择哪台是coordinator</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">首先对消费组的groupId进行hash，接着对consumer_offsets的分区数量取模，默认是50，可以通过offsets.topic.num.partitions来设置，找到你的这个consumer group的offset要提交到consumer_offsets的哪个分区。</span><br><span class="line">比如说：groupId，"membership-consumer-group" -&gt; hash值（数字）-&gt; 对50取模 -&gt; 就知道这个consumer group下的所有的消费者提交offset的时候是往哪个分区去提交offset，找到consumer_offsets的一个分区，consumer_offset的分区的副本数量默认来说1，只有一个leader，然后对这个分区找到对应的leader所在的broker，这个broker就是这个consumer group的coordinator了，consumer接着就会维护一个Socket连接跟这个Broker进行通信。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="http://kfly.top/picture/kfly-top/kafka知识梳理/assets/GroupCoordinator原理剖析.png" alt="39 GroupCoordinator原理剖析"></p>
<h3 id="4-7-consumer消费者Rebalance策略"><a href="#4-7-consumer消费者Rebalance策略" class="headerlink" title="4.7. consumer消费者Rebalance策略"></a>4.7. consumer消费者Rebalance策略</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">比如我们消费的一个topic主题有12个分区：p0,p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11</span><br><span class="line">假设我们的消费者组里面有三个消费者。</span><br></pre></td></tr></table></figure>
<h4 id="4-7-1-range策略"><a href="#4-7-1-range策略" class="headerlink" title="4.7.1 range策略"></a>4.7.1 range策略</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">range策略就是按照partiton的序号范围</span><br><span class="line">	p0~3             consumer1</span><br><span class="line">	p4~7             consumer2</span><br><span class="line">	p8~11            consumer3</span><br><span class="line">默认就是这个策略</span><br></pre></td></tr></table></figure>
<h4 id="4-7-2-round-robin策略"><a href="#4-7-2-round-robin策略" class="headerlink" title="4.7.2 round-robin策略"></a>4.7.2 round-robin策略</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">consumer1:	0,3,6,9</span><br><span class="line">consumer2:	1,4,7,10</span><br><span class="line">consumer3:	2,5,8,11</span><br><span class="line"></span><br><span class="line">但是前面的这两个方案有个问题：</span><br><span class="line">	假设consuemr1挂了:p0-5分配给consumer2,p6-11分配给consumer3</span><br><span class="line">	这样的话，原本在consumer2上的的p6,p7分区就被分配到了 consumer3上</span><br></pre></td></tr></table></figure>
<h4 id="4-7-3-sticky策略"><a href="#4-7-3-sticky策略" class="headerlink" title="4.7.3 sticky策略"></a>4.7.3 sticky策略</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">	最新的一个sticky策略，就是说尽可能保证在rebalance的时候，让原本属于这个consumer</span><br><span class="line">的分区还是属于他们，然后把多余的分区再均匀分配过去，这样尽可能维持原来的分区分配的策略</span><br><span class="line"></span><br><span class="line">consumer1： 0-3</span><br><span class="line">consumer2:  4-7</span><br><span class="line">consumer3:  8-11 </span><br><span class="line"></span><br><span class="line">假设consumer3挂了</span><br><span class="line">consumer1：0-3，+8,9</span><br><span class="line">consumer2: 4-7，+10,11</span><br></pre></td></tr></table></figure>
<h3 id="4-8-consumer核心参数"><a href="#4-8-consumer核心参数" class="headerlink" title="4.8. consumer核心参数"></a>4.8. consumer核心参数</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">【<span class="selector-tag">heartbeat</span><span class="selector-class">.interval</span><span class="selector-class">.ms</span>】</span><br><span class="line">默认值：3000</span><br><span class="line"><span class="selector-tag">consumer</span>心跳时间，必须得保持心跳才能知道<span class="selector-tag">consumer</span>是否故障了，然后如果故障之后，就会通过心跳下发<span class="selector-tag">rebalance</span>的指令给其他的<span class="selector-tag">consumer</span>通知他们进行<span class="selector-tag">rebalance</span>的操作</span><br><span class="line"></span><br><span class="line">【<span class="selector-tag">session</span><span class="selector-class">.timeout</span><span class="selector-class">.ms</span>】</span><br><span class="line">默认值：10000	</span><br><span class="line"><span class="selector-tag">kafka</span>多长时间感知不到一个<span class="selector-tag">consumer</span>就认为他故障了，默认是10秒</span><br><span class="line"></span><br><span class="line">【<span class="selector-tag">max</span><span class="selector-class">.poll</span><span class="selector-class">.interval</span><span class="selector-class">.ms</span>】</span><br><span class="line">默认值：300000</span><br><span class="line">如果在两次<span class="selector-tag">poll</span>操作之间，超过了这个时间，那么就会认为这个<span class="selector-tag">consume</span>处理能力太弱了，会被踢出消费组，分区分配给别人去消费，一遍来说结合你自己的业务处理的性能来设置就可以了</span><br><span class="line"></span><br><span class="line">【<span class="selector-tag">fetch</span><span class="selector-class">.max</span><span class="selector-class">.bytes</span>】</span><br><span class="line">默认值：1048576</span><br><span class="line">获取一条消息最大的字节数，一般建议设置大一些</span><br><span class="line"></span><br><span class="line">【<span class="selector-tag">max</span><span class="selector-class">.poll</span><span class="selector-class">.records</span>】</span><br><span class="line">默认值：500条</span><br><span class="line">一次<span class="selector-tag">poll</span>返回消息的最大条数，</span><br><span class="line"></span><br><span class="line">【<span class="selector-tag">connections</span><span class="selector-class">.max</span><span class="selector-class">.idle</span><span class="selector-class">.ms</span>】</span><br><span class="line">默认值：540000</span><br><span class="line"><span class="selector-tag">consumer</span>跟<span class="selector-tag">broker</span>的<span class="selector-tag">socket</span>连接如果空闲超过了一定的时间，此时就会自动回收连接，但是下次消费就要重新建立<span class="selector-tag">socket</span>连接，这个建议设置为<span class="selector-tag">-1</span>，不要去回收</span><br><span class="line"></span><br><span class="line">【<span class="selector-tag">auto</span><span class="selector-class">.offset</span><span class="selector-class">.reset</span>】</span><br><span class="line">  <span class="selector-tag">earliest</span></span><br><span class="line">		当各分区下有已提交的<span class="selector-tag">offset</span>时，从提交的<span class="selector-tag">offset</span>开始消费；无提交的<span class="selector-tag">offset</span>时，从头开始消费		  </span><br><span class="line">	<span class="selector-tag">latest</span></span><br><span class="line">		当各分区下有已提交的<span class="selector-tag">offset</span>时，从提交的<span class="selector-tag">offset</span>开始消费；无提交的<span class="selector-tag">offset</span>时，从当前位置开始消费</span><br><span class="line">	<span class="selector-tag">none</span></span><br><span class="line">		<span class="selector-tag">topic</span>各分区都存在已提交的<span class="selector-tag">offset</span>时，从<span class="selector-tag">offset</span>后开始消费；只要有一个分区不存在已提交的<span class="selector-tag">offset</span>，则抛出异常</span><br><span class="line">注：我们生产里面一般设置的是<span class="selector-tag">latest</span></span><br><span class="line"></span><br><span class="line">【<span class="selector-tag">enable</span><span class="selector-class">.auto</span><span class="selector-class">.commit</span>】</span><br><span class="line">默认值：<span class="selector-tag">true</span></span><br><span class="line">设置为自动提交<span class="selector-tag">offset</span></span><br><span class="line"></span><br><span class="line">【<span class="selector-tag">auto</span><span class="selector-class">.commit</span><span class="selector-class">.interval</span><span class="selector-class">.ms</span>】</span><br><span class="line">默认值：60 * 1000</span><br><span class="line">每隔多久更新一下偏移量</span><br></pre></td></tr></table></figure>
<p>官网查看kafka参数<a href="http://kafka.apache.org/10/documentation.html" target="_blank" rel="noopener">http://kafka.apache.org/10/documentation.html</a></p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="https://blog.sev7e0.site/">大数据施工现场</a></span>
        <span>/</span>
        
        <span><a href="https://wangchujiang.com/linux-command/">linux命令行工具</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="/js/gitment.js"></script>
<script>
    var gitment = new Gitment({
        id: 'kafka知识梳理',
        owner: 'orchid-ding',
        repo: 'kfly-blog-comment',
        oauth: {
            client_id: '0770cdab79393197b6f5',
            client_secret: '376fb6c7bcd5047718b356712f596b89e490360c',
        },
    })
    gitment.render('comment-container')
</script>




</html>
