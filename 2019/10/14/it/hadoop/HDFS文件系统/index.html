<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="天行健、君子以自强不息；地势坤，君子以厚德载物。">
    <meta name="keyword"  content="兰草">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        HDFS文件系统 - Kaffir Lily的博客 | Kaffir Lily&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_1598291_q3el2wqimj.css" type="text/css">
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>kfly</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont iconhome"></i>
                    <span>主页</span>
                </a>
            </li>
 	   <li >
                <a href="/spec/">
                    <i class="iconfont iconzhuanti"></i>
                    <span>专题</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>简历</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS分布式文件系统"><span class="toc-text">HDFS分布式文件系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-HDFS读写流程"><span class="toc-text">1. HDFS读写流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-数据写流程"><span class="toc-text">1.1 数据写流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-数据读流程"><span class="toc-text">1.2 数据读流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Hadoop-HA高可用"><span class="toc-text">2. Hadoop HA高可用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-HDFS高可用原理"><span class="toc-text">2.1 HDFS高可用原理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Hadoop联邦"><span class="toc-text">3. Hadoop联邦</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-为什么需要联邦"><span class="toc-text">3.1 为什么需要联邦</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-联邦"><span class="toc-text">3.2 联邦</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-扩展"><span class="toc-text">3.3 扩展</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-文件压缩"><span class="toc-text">4. 文件压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-压缩算法"><span class="toc-text">4.1 压缩算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-编程实践"><span class="toc-text">4.2 编程实践</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-小文件治理"><span class="toc-text">5. 小文件治理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-有没有问题"><span class="toc-text">5.1 有没有问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-HAR文件方案（10分钟）"><span class="toc-text">5.2 HAR文件方案（10分钟）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-Sequence-Files方案（-）"><span class="toc-text">5.3 Sequence Files方案（*）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-文件快照"><span class="toc-text">6. 文件快照</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-什么是快照"><span class="toc-text">6.1 什么是快照</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-快照操作"><span class="toc-text">6.2 快照操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7、拓展点、未来计划、行业趋势"><span class="toc-text">7、拓展点、未来计划、行业趋势</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        HDFS文件系统
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-10-14 16:56:16</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#hadoop" title="hadoop">hadoop</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#hdfs" title="hdfs">hdfs</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h1 id="HDFS分布式文件系统"><a href="#HDFS分布式文件系统" class="headerlink" title="HDFS分布式文件系统"></a>HDFS分布式文件系统</h1><h3 id="1-HDFS读写流程"><a href="#1-HDFS读写流程" class="headerlink" title="1. HDFS读写流程"></a>1. HDFS读写流程</h3><h4 id="1-1-数据写流程"><a href="#1-1-数据写流程" class="headerlink" title="1.1 数据写流程"></a>1.1 数据写流程</h4><p><img src="img/1557999856839.png" alt="1557999856839"></p>
<p><img src="img/HDFS写入文件流程.png" alt="HDFS写入文件流程"></p>
<p><strong>1.1 详细流程</strong></p>
<ul>
<li><p>创建文件：    </p>
<ul>
<li>HDFS客户端向HDFS写数据，先调用DistributedFileSystem.create()方法，在HDFS创建新的空文件</li>
<li>RPC（ClientProtocol.create()）远程过程调用NameNode（NameNodeRpcServer）的create()，首先在HDFS目录树指定路径添加新文件</li>
<li>然后将创建新文件的操作记录在editslog中</li>
<li>NameNode.create方法执行完后，DistributedFileSystem.create()返回FSDataOutputStream，它本质是封装了一个DFSOutputStream对象</li>
</ul>
</li>
<li><p>建立数据流管道：</p>
<ul>
<li>客户端调用DFSOutputStream.write()写数据</li>
<li>DFSOutputStream调用ClientProtocol.addBlock()，首先向NameNode申请一个空的数据块</li>
<li>addBlock()返回LocatedBlock对象，对象包含当前数据块的所有datanode的位置信息</li>
<li>根据位置信息，建立数据流管道</li>
</ul>
</li>
<li><p>向数据流管道pipeline中写当前块的数据：</p>
<ul>
<li>客户端向流管道中写数据，先将数据写入一个检验块chunk中，大小512Byte，写满后，计算chunk的检验和checksum值（4Byte）</li>
<li>然后将chunk数据本身加上checksum，形成一个带checksum值的chunk（516Byte）</li>
<li>保存到一个更大一些的结构<strong>packet数据包</strong>中，packet为64kB大小</li>
</ul>
</li>
<li>packet写满后，先被写入一个<strong>dataQueue</strong>队列中<ul>
<li>packet被从队列中取出，向pipeline中写入，先写入datanode1，再从datanoe1传到datanode2，再从datanode2传到datanode3中</li>
</ul>
</li>
<li>一个packet数据取完后，后被放入到<strong>ackQueue</strong>中等待pipeline关于该packet的ack的反馈<ul>
<li>每个packet都会有ack确认包，逆pipeline（dn3 -&gt; dn2 -&gt; dn1）传回输出流</li>
</ul>
</li>
<li>若packet的ack是SUCCESS成功的，则从ackQueue中，将packet删除；否则，将packet从ackQueue中取出，重新放入dataQueue，重新发送<ul>
<li>如果当前块写完后，文件还有其它块要写，那么再调用addBlock方法（<strong>流程同上</strong>）</li>
</ul>
</li>
<li>文件最后一个block块数据写完后，会再发送一个空的packet，表示当前block写完了，然后关闭pipeline<ul>
<li>所有块写完，close()关闭流</li>
</ul>
</li>
<li>ClientProtocol.complete()通知namenode当前文件所有块写完了</li>
</ul>
<p><strong>6.1.2 容错</strong></p>
<ul>
<li>在写的过程中，pipeline中的datanode出现故障（如网络不通），输出流如何恢复<ul>
<li>输出流中ackQueue缓存的所有packet会被重新加入dataQueue</li>
<li>输出流调用ClientProtocol.updateBlockForPipeline()，为block申请一个新的时间戳，namenode会记录新时间戳</li>
<li>确保故障datanode即使恢复，但由于其上的block时间戳与namenode记录的新的时间戳不一致，故障datanode上的block进而被删除</li>
<li>故障的datanode从pipeline中删除</li>
<li>输出流调用ClientProtocol.getAdditionalDatanode()通知namenode分配新的datanode到数据流pipeline中，并使用新的时间戳建立pipeline</li>
<li>新添加到pipeline中的datanode，目前还没有存储这个新的block，HDFS客户端通过DataTransferProtocol通知pipeline中的一个datanode复制这个block到新的datanode中</li>
<li>pipeline重建后，输出流调用ClientProtocol.updatePipeline()，更新namenode中的元数据</li>
<li>故障恢复完毕，完成后续的写入流程</li>
</ul>
</li>
</ul>
<h4 id="1-2-数据读流程"><a href="#1-2-数据读流程" class="headerlink" title="1.2 数据读流程"></a>1.2 数据读流程</h4><p><strong>1.2.1 基本流程</strong></p>
<p><img src="img/HDFS文件读取流程.png" alt="HDFS文件读取流程"></p>
<ul>
<li>1、client端读取HDFS文件，client调用文件系统对象DistributedFileSystem的open方法</li>
<li>2、返回FSDataInputStream对象（对DFSInputStream的包装）</li>
<li>3、构造DFSInputStream对象时，调用namenode的getBlockLocations方法，获得file的开始若干block（如blk1, blk2, blk3, blk4）的存储datanode（以下简称dn）列表；针对每个block的dn列表，会根据网络拓扑做排序，离client近的排在前；</li>
<li>4、调用DFSInputStream的read方法，先读取blk1的数据，与client最近的datanode建立连接，读取数据</li>
<li>5、读取完后，关闭与dn建立的流</li>
<li>6、读取下一个block，如blk2的数据（重复步骤4、5、6）</li>
<li>7、这一批block读取完后，再读取下一批block的数据（重复3、4、5、6、7）</li>
<li>8、完成文件数据读取后，调用FSDataInputStream的close方法</li>
</ul>
<p><strong>1.2.2 容错</strong></p>
<ul>
<li><p>情况一：读取block过程中，client与datanode通信中断</p>
<ul>
<li>client与存储此block的第二个datandoe建立连接，读取数据</li>
<li>记录此有问题的datanode，不会再从它上读取数据</li>
</ul>
</li>
<li><p>情况二：client读取block，发现block数据有问题</p>
<ul>
<li>client读取block数据时，同时会读取到block的校验和，若client针对读取过来的block数据，计算检验和，其值与读取过来的校验和不一样，说明block数据损坏</li>
<li>client从存储此block副本的其它datanode上读取block数据（也会计算校验和）</li>
<li>同时，client会告知namenode此情况；</li>
</ul>
</li>
</ul>
<h3 id="2-Hadoop-HA高可用"><a href="#2-Hadoop-HA高可用" class="headerlink" title="2. Hadoop HA高可用"></a>2. Hadoop HA高可用</h3><h4 id="2-1-HDFS高可用原理"><a href="#2-1-HDFS高可用原理" class="headerlink" title="2.1 HDFS高可用原理"></a>2.1 HDFS高可用原理</h4><p><img src="img/Image201905211519.png" alt=""></p>
<ul>
<li>对于HDFS ，NN存储元数据在内存中，并负责管理文件系统的命名空间和客户端对HDFS的读写请求。但是，如果只存在一个NN，一旦发生“单点故障”，会使整个系统失效。</li>
<li>虽然有个SNN，但是它并不是NN的热备份</li>
<li>因为SNN无法提供“热备份”功能，在NN故障时，无法立即切换到SNN对外提供服务，即HDFS处于停服状态。</li>
<li>HDFS2.x采用了HA（High Availability高可用）架构。<ul>
<li>在HA集群中，可设置两个NN，一个处于“活跃（Active）”状态，另一个处于“待命（Standby）”状态。</li>
<li>由zookeeper确保一主一备（讲zookeeper时具体展开）</li>
<li>处于Active状态的NN负责响应所有客户端的请求，处于Standby状态的NN作为热备份节点，保证与active的NN的元数据同步</li>
<li>Active节点发生故障时，zookeeper集群会发现此情况，通知Standby节点立即切换到活跃状态对外提供服务</li>
<li>确保集群一直处于可用状态</li>
</ul>
</li>
<li>如何热备份元数据：<ul>
<li>Standby NN是Active NN的“热备份”，因此Active NN的状态信息必须实时同步到StandbyNN。</li>
<li>可借助一个共享存储系统来实现状态同步，如NFS(NetworkFile System)、QJM(Quorum Journal Manager)或者Zookeeper。</li>
<li>Active NN将更新数据写入到共享存储系统，Standby NN一直监听该系统，一旦发现有新的数据写入，就立即从公共存储系统中读取这些数据并加载到Standby NN自己内存中，从而保证元数据与Active NN状态一致。</li>
</ul>
</li>
<li>块报告：<ul>
<li>NN保存了数据块到实际存储位置的映射信息，为了实现故障时的快速切换，必须保证StandbyNN中也包含最新的块映射信息</li>
<li>因此需要给所有DN配置Active和Standby两个NN的地址，把块的位置和心跳信息同时发送到两个NN上。</li>
</ul>
</li>
</ul>
<h3 id="3-Hadoop联邦"><a href="#3-Hadoop联邦" class="headerlink" title="3. Hadoop联邦"></a>3. Hadoop联邦</h3><h4 id="3-1-为什么需要联邦"><a href="#3-1-为什么需要联邦" class="headerlink" title="3.1 为什么需要联邦"></a>3.1 为什么需要联邦</h4><ul>
<li>虽然HDFS HA解决了“单点故障”问题，但HDFS在扩展性、整体性能和隔离性方面仍有问题<ul>
<li>系统扩展性方面，元数据存储在NN内存中，受限于内存上限（每个文件、目录、block占用约150字节）</li>
<li>整体性能方面，吞吐量受单个NN的影响</li>
<li>隔离性方面，一个程序可能会影响其他程序的运行，如果一个程序消耗过多资源会导致其他程序无法顺利运行</li>
<li>HDFS HA本质上还是单名称节点</li>
</ul>
</li>
</ul>
<h4 id="3-2-联邦"><a href="#3-2-联邦" class="headerlink" title="3.2 联邦"></a>3.2 联邦</h4><p><img src="img/Image201909041239.png" alt=""></p>
<ul>
<li>HDFS联邦可以解决以上三个问题<ul>
<li>HDFS联邦中，设计了多个命名空间；每个命名空间有一个NN或一主一备两个NN，使得HDFS的命名服务能够水平扩展</li>
<li>这些NN分别进行各自命名空间namespace和块的管理，相互独立，不需要彼此协调</li>
<li>每个DN要向集群中所有的NN注册，并周期性的向所有NN发送心跳信息和块信息，报告自己的状态</li>
<li>HDFS联邦每个相互独立的NN对应一个独立的命名空间</li>
<li>每一个命名空间管理属于自己的一组块，这些属于同一命名空间的块对应一个“块池”的概念。</li>
<li>每个DN会为所有块池提供块的存储，块池中的各个块实际上是存储在不同DN中的</li>
</ul>
</li>
</ul>
<h4 id="3-3-扩展"><a href="#3-3-扩展" class="headerlink" title="3.3 扩展"></a>3.3 扩展</h4><p><a href="https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="noopener">联邦-官网</a></p>
<h3 id="4-文件压缩"><a href="#4-文件压缩" class="headerlink" title="4. 文件压缩"></a>4. 文件压缩</h3><h4 id="4-1-压缩算法"><a href="#4-1-压缩算法" class="headerlink" title="4.1 压缩算法"></a>4.1 压缩算法</h4><ul>
<li><p>文件压缩好处：</p>
<ul>
<li>减少数据所占用的磁盘空间</li>
<li>加快数据在磁盘、网络上的IO</li>
</ul>
</li>
<li><p>常用压缩格式</p>
<p>| 压缩格式 | UNIX工具 | 算      法 | 文件扩展名 | 可分割 |<br>| ——– | ——– | ———- | ———- | —— |<br>| DEFLATE  | 无       | DEFLATE    | .deflate   | No     |<br>| gzip     | gzip     | DEFLATE    | .gz        | No     |<br>| zip      | zip      | DEFLATE    | .zip       | YES    |<br>| bzip     | bzip2    | bzip2      | .bz2       | YES    |<br>| LZO      | lzop     | LZO        | .lzo       | No     |<br>| Snappy   | 无       | Snappy     | .snappy    | No     |</p>
</li>
<li><p>Hadoop的压缩实现类；均实现CompressionCodec接口</p>
<p>| 压缩格式 | 对应的编码/解码器                          |<br>| ——– | —————————————— |<br>| DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |<br>| gzip     | org.apache.hadoop.io.compress.GzipCodec    |<br>| bzip2    | org.apache.hadoop.io.compress.BZip2Codec   |<br>| LZO      | com.hadoop.compression.lzo.LzopCodec       |<br>| Snappy   | org.apache.hadoop.io.compress.SnappyCodec  |</p>
</li>
<li><p>查看集群是否支持本地压缩（所有节点都要确认）</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop<span class="variable">@node01</span> ~]<span class="variable">$ </span>hadoop checknative</span><br></pre></td></tr></table></figure>
<p><img src="img/Image201910111114.png" alt=""></p>
</li>
</ul>
<h4 id="4-2-编程实践"><a href="#4-2-编程实践" class="headerlink" title="4.2 编程实践"></a>4.2 编程实践</h4><ul>
<li>编程：上传压缩过的文件到HDFS</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 上传压缩文件到服务器</span></span><br><span class="line"><span class="comment"> *  传递参数</span></span><br><span class="line"><span class="comment"> *  args[0] 本地文件路径</span></span><br><span class="line"><span class="comment"> *  args[1] hdoop文件系统 路径</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">uploadFileZipToFileSystem</span><span class="params">(String source,String targetUrl)</span></span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"文件地址："</span> + source);</span><br><span class="line">    System.out.println(<span class="string">"目标服务器："</span> + targetUrl);</span><br><span class="line">    InputStream inputStreamSourceFile = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 获取文件输入流</span></span><br><span class="line">        inputStreamSourceFile = <span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(source));</span><br><span class="line">        <span class="comment">// HDFS 读写配置文件</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 压缩类型</span></span><br><span class="line">        BZip2Codec codec = <span class="keyword">new</span> BZip2Codec();</span><br><span class="line">        codec.setConf(configuration);</span><br><span class="line">        <span class="comment">// 通过url 返回文件系统实例</span></span><br><span class="line">        FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);</span><br><span class="line">        <span class="comment">//调用Filesystem的create方法返回的是FSDataOutputStream对象</span></span><br><span class="line">        <span class="comment">//该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加</span></span><br><span class="line">        <span class="comment">// 获取文件系用的输出流</span></span><br><span class="line">        OutputStream outputStreamTarget = fileSystem.create(<span class="keyword">new</span> Path(targetUrl));</span><br><span class="line">        <span class="comment">// 对输出流进行压缩</span></span><br><span class="line">        CompressionOutputStream compressionOut = codec.createOutputStream(outputStreamTarget);</span><br><span class="line">        <span class="comment">// 将文件输入流，写入输入流</span></span><br><span class="line">        IOUtils.copyBytes(inputStreamSourceFile,compressionOut,<span class="number">4069</span>,<span class="keyword">true</span>);</span><br><span class="line">        System.out.println(<span class="string">"上传成功"</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (FileNotFoundException e) &#123;</span><br><span class="line">        System.err.println(e.getMessage());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>扩展阅读<ul>
<li>《Hadoop权威指南》 5.2章节 压缩</li>
<li><a href="https://blog.csdn.net/qq_38262266/article/details/79171524" target="_blank" rel="noopener">HDFS文件压缩</a></li>
</ul>
</li>
</ul>
<h3 id="5-小文件治理"><a href="#5-小文件治理" class="headerlink" title="5. 小文件治理"></a>5. 小文件治理</h3><h4 id="5-1-有没有问题"><a href="#5-1-有没有问题" class="headerlink" title="5.1 有没有问题"></a>5.1 有没有问题</h4><ul>
<li>NameNode存储着文件系统的元数据，每个文件、目录、块大概有150字节的元数据；</li>
<li>因此文件数量的限制也由NN内存大小决定，如果小文件过多则会造成NN的压力过大</li>
<li>且HDFS能存储的数据总量也会变小</li>
</ul>
<h4 id="5-2-HAR文件方案（10分钟）"><a href="#5-2-HAR文件方案（10分钟）" class="headerlink" title="5.2 HAR文件方案（10分钟）"></a>5.2 HAR文件方案（10分钟）</h4><ul>
<li>本质启动mr程序，所以需要启动yarn</li>
</ul>
<p><img src="img/1558004541101.png" alt="1558004541101"></p>
<p>用法：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">archive -archiveName &lt;NAME&gt;.har -p &lt;parent path&gt; [-r &lt;replication factor&gt;]&lt;src&gt;* &lt;dest&gt;</span><br></pre></td></tr></table></figure>
<p><img src="img/Image201909041408.png" alt=""></p>
<p><img src="img/Image201906210960.png" alt="alt"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建archive文件；/testhar有两个子目录th1、th2；两个子目录中有若干文件</span></span><br><span class="line">hadoop archive -archiveName test.har -p /testhar -r 3 th1 th2 /outhar # 原文件还存在，需手动删除</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看archive文件</span></span><br><span class="line">hdfs dfs -ls -R har:///outhar/test.har</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压archive文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 方式一</span></span><br><span class="line">hdfs dfs -cp har:///outhar/test.har/th1 hdfs:/unarchivef # 顺序</span><br><span class="line">hadoop fs -ls /unarchivef	</span><br><span class="line"><span class="meta">#</span><span class="bash"> 方式二</span></span><br><span class="line">hadoop distcp har:///outhar/test.har/th1 hdfs:/unarchivef2 # 并行，启动MR</span><br></pre></td></tr></table></figure>
<h4 id="5-3-Sequence-Files方案（-）"><a href="#5-3-Sequence-Files方案（-）" class="headerlink" title="5.3 Sequence Files方案（*）"></a>5.3 Sequence Files方案（*）</h4><ul>
<li>SequenceFile文件，主要由一条条record记录组成；每个record是键值对形式的</li>
<li>SequenceFile文件可以作为小文件的存储容器；<ul>
<li>每条record保存一个小文件的内容</li>
<li>小文件名作为当前record的键；</li>
<li>小文件的内容作为当前record的值；</li>
<li>如10000个100KB的小文件，可以编写程序将这些文件放到一个SequenceFile文件。</li>
</ul>
</li>
<li>一个SequenceFile是<strong>可分割</strong>的，所以MapReduce可将文件切分成块，每一块独立操作。</li>
<li>具体结构（如下图）：<ul>
<li>一个SequenceFile首先有一个4字节的header（文件版本号）</li>
<li>接着是若干record记录</li>
<li>记录间会随机的插入一些同步点sync marker，用于方便定位到记录边界</li>
</ul>
</li>
<li>不像HAR，SequenceFile<strong>支持压缩</strong>。记录的结构取决于是否启动压缩<ul>
<li>支持两类压缩：<ul>
<li>不压缩NONE，如下图</li>
<li>压缩RECORD，如下图</li>
<li>压缩BLOCK，①一次性压缩多条记录；②每一个新块Block开始处都需要插入同步点；如下图</li>
</ul>
</li>
<li>在大多数情况下，以block（注意：指的是SequenceFile中的block）为单位进行压缩是最好的选择</li>
<li>因为一个block包含多条记录，利用record间的相似性进行压缩，压缩效率更高</li>
<li>把已有的数据转存为SequenceFile比较慢。比起先写小文件，再将小文件写入SequenceFile，一个更好的选择是直接将数据写入一个SequenceFile文件，省去小文件作为中间媒介.</li>
</ul>
</li>
</ul>
<p><img src="img/Image201907101934.png" alt=""></p>
<p><img src="img/Image201907101935.png" alt=""></p>
<ul>
<li>向SequenceFile写入数据</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba.hadoop.sequencefile;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.SequenceFile;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileWriteNewVersion</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//模拟数据源</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String[] DATA = &#123;</span><br><span class="line">            <span class="string">"The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models."</span>,</span><br><span class="line">            <span class="string">"It is designed to scale up from single servers to thousands of machines, each offering local computation and storage."</span>,</span><br><span class="line">            <span class="string">"Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer"</span>,</span><br><span class="line">            <span class="string">"o delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures."</span>,</span><br><span class="line">            <span class="string">"Hadoop Common: The common utilities that support the other Hadoop modules."</span></span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">//输出路径：要生成的SequenceFile文件名</span></span><br><span class="line">        String uri = <span class="string">"hdfs://node01:9000/writeSequenceFile"</span>;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(URI.create(uri), conf);</span><br><span class="line">        <span class="comment">//向HDFS上的此SequenceFile文件写数据</span></span><br><span class="line">        Path path = <span class="keyword">new</span> Path(uri);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//因为SequenceFile每个record是键值对的</span></span><br><span class="line">        <span class="comment">//指定key类型</span></span><br><span class="line">        IntWritable key = <span class="keyword">new</span> IntWritable();</span><br><span class="line">        <span class="comment">//指定value类型</span></span><br><span class="line">        Text value = <span class="keyword">new</span> Text();</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//            FileContext fileContext = FileContext.getFileContext(URI.create(uri));</span></span><br><span class="line"><span class="comment">//            Class&lt;?&gt; codecClass = Class.forName("org.apache.hadoop.io.compress.SnappyCodec");</span></span><br><span class="line"><span class="comment">//            CompressionCodec SnappyCodec = (CompressionCodec)ReflectionUtils.newInstance(codecClass, conf);</span></span><br><span class="line"><span class="comment">//            SequenceFile.Metadata metadata = new SequenceFile.Metadata();</span></span><br><span class="line"><span class="comment">//            //writer = SequenceFile.createWriter(fs, conf, path, key.getClass(), value.getClass());</span></span><br><span class="line"><span class="comment">//            writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(path), SequenceFile.Writer.keyClass(IntWritable.class),</span></span><br><span class="line"><span class="comment">//                                        SequenceFile.Writer.valueClass(Text.class));</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建向SequenceFile文件写入数据时的一些选项</span></span><br><span class="line">        <span class="comment">//要写入的SequenceFile的路径</span></span><br><span class="line">        SequenceFile.Writer.Option pathOption       = SequenceFile.Writer.file(path);</span><br><span class="line">        <span class="comment">//record的key类型选项</span></span><br><span class="line">        SequenceFile.Writer.Option keyOption        = SequenceFile.Writer.keyClass(IntWritable.class);</span><br><span class="line">        <span class="comment">//record的value类型选项</span></span><br><span class="line">        SequenceFile.Writer.Option valueOption      = SequenceFile.Writer.valueClass(Text.class);</span><br><span class="line">        <span class="comment">//SequenceFile压缩方式：NONE | RECORD | BLOCK三选一</span></span><br><span class="line">        <span class="comment">//方案一：RECORD、不指定压缩算法</span></span><br><span class="line">        SequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD);</span><br><span class="line">        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//方案二：BLOCK、不指定压缩算法</span></span><br><span class="line"><span class="comment">//        SequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.BLOCK);</span></span><br><span class="line"><span class="comment">//        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//方案三：使用BLOCK、压缩算法BZip2Codec；压缩耗时间</span></span><br><span class="line">        <span class="comment">//再加压缩算法</span></span><br><span class="line"><span class="comment">//        BZip2Codec codec = new BZip2Codec();</span></span><br><span class="line"><span class="comment">//        codec.setConf(conf);</span></span><br><span class="line"><span class="comment">//        SequenceFile.Writer.Option compressAlgorithm = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD, codec);</span></span><br><span class="line"><span class="comment">//        //创建写数据的Writer实例</span></span><br><span class="line"><span class="comment">//        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressAlgorithm);</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100000</span>; i++) &#123;</span><br><span class="line">            <span class="comment">//分别设置key、value值</span></span><br><span class="line">            key.set(<span class="number">100</span> - i);</span><br><span class="line">            value.set(DATA[i % DATA.length]);</span><br><span class="line">            System.out.printf(<span class="string">"[%s]\t%s\t%s\n"</span>, writer.getLength(), key, value);</span><br><span class="line">            <span class="comment">//在SequenceFile末尾追加内容</span></span><br><span class="line">            writer.append(key, value);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//关闭流</span></span><br><span class="line">        IOUtils.closeStream(writer);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>命令查看SequenceFile内容</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -text /writeSequenceFile</span><br></pre></td></tr></table></figure>
<ul>
<li>读取SequenceFile文件</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba.hadoop.sequencefile;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.SequenceFile;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ReflectionUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileReadNewVersion</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">//要读的SequenceFile</span></span><br><span class="line">        String uri = <span class="string">"hdfs://node01:9000/writeSequenceFile"</span>;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Path path = <span class="keyword">new</span> Path(uri);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Reader对象</span></span><br><span class="line">        SequenceFile.Reader reader = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//读取SequenceFile的Reader的路径选项</span></span><br><span class="line">            SequenceFile.Reader.Option pathOption = SequenceFile.Reader.file(path);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//实例化Reader对象</span></span><br><span class="line">            reader = <span class="keyword">new</span> SequenceFile.Reader(conf, pathOption);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//根据反射，求出key类型</span></span><br><span class="line">            Writable key = (Writable)</span><br><span class="line">                    ReflectionUtils.newInstance(reader.getKeyClass(), conf);</span><br><span class="line">            <span class="comment">//根据反射，求出value类型</span></span><br><span class="line">            Writable value = (Writable)</span><br><span class="line">                    ReflectionUtils.newInstance(reader.getValueClass(), conf);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">long</span> position = reader.getPosition();</span><br><span class="line">            System.out.println(position);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (reader.next(key, value)) &#123;</span><br><span class="line">                String syncSeen = reader.syncSeen() ? <span class="string">"*"</span> : <span class="string">""</span>;</span><br><span class="line">                System.out.printf(<span class="string">"[%s%s]\t%s\t%s\n"</span>, position, syncSeen, key, value);</span><br><span class="line">                position = reader.getPosition(); <span class="comment">// beginning of next record</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            IOUtils.closeStream(reader);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="6-文件快照"><a href="#6-文件快照" class="headerlink" title="6. 文件快照"></a>6. 文件快照</h3><h4 id="6-1-什么是快照"><a href="#6-1-什么是快照" class="headerlink" title="6.1 什么是快照"></a>6.1 什么是快照</h4><ul>
<li>快照比较常见的应用场景是数据备份，以防一些用户错误或灾难恢复</li>
<li>快照snapshots是HDFS文件系统的，只读的、某时间点的拷贝</li>
<li>可以针对某个目录，或者整个文件系统做快照</li>
<li>创建快照时，block块并不会被拷贝。快照文件中只是记录了block列表和文件大小，<strong>不会做任何数据拷贝</strong></li>
</ul>
<h4 id="6-2-快照操作"><a href="#6-2-快照操作" class="headerlink" title="6.2 快照操作"></a>6.2 快照操作</h4><ul>
<li><p>允许快照</p>
<p>允许一个快照目录被创建。如果这个操作成功完成，这个目录就变成snapshottable</p>
<p>用法：hdfs dfsadmin -allowSnapshot <snapshotdir></snapshotdir></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -allowSnapshot /wordcount</span><br></pre></td></tr></table></figure>
</li>
<li><p>禁用快照</p>
<p>用法：hdfs dfsadmin -disallowSnapshot <snapshotdir></snapshotdir></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -disallowSnapshot /wordcount</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建快照</p>
<p>用法：hdfs dfs -createSnapshot <snapshotdir> [<snapshotname>]</snapshotname></snapshotdir></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">注意：先将/wordcount目录变成允许快照的</span></span><br><span class="line">hdfs dfs -createSnapshot /wordcount wcSnapshot</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看快照</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -ls /wordcount/.snapshot</span><br></pre></td></tr></table></figure>
<p><img src="img/Image201909041346.png" alt=""></p>
</li>
<li><p>重命名快照</p>
<p>这个操作需要拥有snapshottabl目录所有者权限</p>
<p>用法：hdfs dfs -renameSnapshot <snapshotdir> <oldname> <newname></newname></oldname></snapshotdir></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -renameSnapshot /wordcount wcSnapshot newWCSnapshot</span><br></pre></td></tr></table></figure>
</li>
<li><p>用快照恢复误删除数据</p>
<p>HFDS的/wordcount目录，文件列表如下</p>
<p><img src="img/Image201909041356.png" alt=""></p>
<p>误删除/wordcount/edit.xml文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm /wordcount/edit.xml</span><br></pre></td></tr></table></figure>
<p><img src="img/Image201909041400.png" alt=""></p>
<p>恢复数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp /wordcount/.snapshot/newWCSnapshot/edit.xml /wordcount</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除快照</p>
<p>这个操作需要拥有snapshottabl目录所有者权限</p>
<p>用法：hdfs dfs -deleteSnapshot <snapshotdir> <snapshotname></snapshotname></snapshotdir></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -deleteSnapshot /wordcount newWCSnapshot</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="7、拓展点、未来计划、行业趋势"><a href="#7、拓展点、未来计划、行业趋势" class="headerlink" title="7、拓展点、未来计划、行业趋势"></a>7、拓展点、未来计划、行业趋势</h2><ol>
<li><p>HDFS存储地位</p>
</li>
<li><p><strong>block块为什么设置的比较大</strong></p>
</li>
</ol>
<ul>
<li><p><a href="https://www.cnblogs.com/jswang/p/9071847.html" target="_blank" rel="noopener">磁盘基础知识</a>    </p>
<ul>
<li>盘片platter、磁头head、磁道track、扇区sector、柱面cylinder</li>
<li>为了最小化寻址开销；从磁盘传输数据的时间明显大于定位这个块开始位置所需的时间</li>
</ul>
</li>
<li><p>问：块的大小是不是设置的越大越好呢？</p>
<p>1、 不是，寻址的时间大概是 100ms，设计一般设置为寻址时间占用十分之一，也就是一秒。 硬盘的传输速录大概是100m/s 一秒大概为100M，最接近100的大小为128M。 </p>
</li>
</ul>
<p><img src="img/Image201906211143.png" alt=""></p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="https://blog.sev7e0.site/">大数据施工现场</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
