<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="天行健、君子以自强不息；地势坤，君子以厚德载物。">
    <meta name="keyword"  content="兰草">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        MapRedecer编程（一） - Kaffir Lily的博客 | Kaffir Lily&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_1598291_q3el2wqimj.css" type="text/css">
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>kfly</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont iconhome"></i>
                    <span>主页</span>
                </a>
            </li>
 	   <li >
                <a href="/"></a>
                    <i class="iconfont iconzhuanti"></i>
                    <span>专题</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>简历</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MapReduce编程模型"><span class="toc-text">MapReduce编程模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#一、知识要点"><span class="toc-text">一、知识要点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-MapReduce编程模型"><span class="toc-text">1. MapReduce编程模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-Map阶段"><span class="toc-text">1.1 Map阶段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-Reduce阶段"><span class="toc-text">1.2 Reduce阶段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-Map-amp-Reduce"><span class="toc-text">1.3 Map&amp;Reduce</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-MapReduce编程示例"><span class="toc-text">2. MapReduce编程示例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-MapReduce原理图"><span class="toc-text">2.1 MapReduce原理图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-MR中key的作用"><span class="toc-text">2.2 MR中key的作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-MR参考代码"><span class="toc-text">2.4 MR参考代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-集群运行"><span class="toc-text">2.5 集群运行</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6-总结"><span class="toc-text">2.6 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-WEB-UI查看结果"><span class="toc-text">3. WEB UI查看结果</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-Yarn"><span class="toc-text">3.1 Yarn</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-HDFS结果"><span class="toc-text">3.2 HDFS结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Shuffle"><span class="toc-text">4. Shuffle</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-shuffle简图"><span class="toc-text">4.1 shuffle简图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-shuffle细节图"><span class="toc-text">4.2 shuffle细节图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-map端"><span class="toc-text">4.3 map端</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-reduce端"><span class="toc-text">4.4 reduce端</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-总结"><span class="toc-text">4.5 总结</span></a></li></ol></li></ol></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        MapRedecer编程（一）
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-10-15 23:11:32</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#hadoop" title="hadoop">hadoop</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#MapReduce" title="MapReduce">MapReduce</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h1 id="MapReduce编程模型"><a href="#MapReduce编程模型" class="headerlink" title="MapReduce编程模型"></a>MapReduce编程模型</h1><h2 id="一、知识要点"><a href="#一、知识要点" class="headerlink" title="一、知识要点"></a>一、知识要点</h2><h3 id="1-MapReduce编程模型"><a href="#1-MapReduce编程模型" class="headerlink" title="1. MapReduce编程模型"></a>1. MapReduce编程模型</h3><ul>
<li><p>Hadoop架构图</p>
<p>Hadoop由HDFS分布式存储、<strong>MapReduce分布式计算</strong>、Yarn资源调度三部分组成</p>
</li>
</ul>
<p><img src="assets/Image201906191834-1562922704761.png" alt=""></p>
<ul>
<li>MapReduce是采用一种<strong>分而治之</strong>的思想设计出来的分布式计算框架</li>
<li>MapReduce由两个阶段组成：<ul>
<li>Map阶段（切分成一个个小的任务）</li>
<li>Reduce阶段（汇总小任务的结果）</li>
</ul>
</li>
<li>那什么是分而治之呢？<ul>
<li>比如一复杂、计算量大、耗时长的的任务，暂且称为“大任务”；</li>
<li>此时使用单台服务器无法计算或较短时间内计算出结果时，可将此大任务切分成一个个小的任务，小任务分别在不同的服务器上<strong>并行</strong>的执行</li>
<li>最终再汇总每个小任务的结果</li>
</ul>
</li>
</ul>
<p><img src="assets/Image201906251747.png" alt=""></p>
<h4 id="1-1-Map阶段"><a href="#1-1-Map阶段" class="headerlink" title="1.1 Map阶段"></a>1.1 Map阶段</h4><ul>
<li>map阶段有一个关键的map()函数；</li>
<li>此函数的输入是<strong>键值对</strong></li>
<li>输出是一系列<strong>键值对</strong>，输出写入<strong>本地磁盘</strong>。</li>
</ul>
<h4 id="1-2-Reduce阶段"><a href="#1-2-Reduce阶段" class="headerlink" title="1.2 Reduce阶段"></a>1.2 Reduce阶段</h4><ul>
<li><p>reduce阶段有一个关键的函数reduce()函数</p>
</li>
<li><p>此函数的输入也是键值对（即map的输出（kv对））</p>
</li>
<li><p>输出也是一系列键值对，结果最终写入HDFS</p>
</li>
</ul>
<h4 id="1-3-Map-amp-Reduce"><a href="#1-3-Map-amp-Reduce" class="headerlink" title="1.3 Map&amp;Reduce"></a>1.3 Map&amp;Reduce</h4><p><img src="assets/Image201906251807.png" alt=""></p>
<h3 id="2-MapReduce编程示例"><a href="#2-MapReduce编程示例" class="headerlink" title="2. MapReduce编程示例"></a>2. MapReduce编程示例</h3><ul>
<li>以<strong>MapReduce的词频统计</strong>为例：统计一批英文文章当中，每个单词出现的总次数</li>
</ul>
<h4 id="2-1-MapReduce原理图"><a href="#2-1-MapReduce原理图" class="headerlink" title="2.1 MapReduce原理图"></a>2.1 MapReduce原理图</h4><p><img src="assets/Image201906271715.png" alt=""></p>
<ul>
<li>Map阶段<ul>
<li>假设MR的输入文件“<strong>Gone With The Wind</strong>”有三个block；block1、block2、block3 </li>
<li>MR编程时，每个block对应一个分片split</li>
<li>每一个split对应一个map任务（map task）</li>
<li>如图共3个map任务（map1、map2、map3）；这3个任务的逻辑一样，所以以第一个map任务（map1）为例分析 </li>
<li>map1读取block1的数据；一次读取block1的一行数据；<ul>
<li>产生键值对(key/value)，作为map()的参数传入，调用map()；</li>
<li>假设当前所读行是第一行</li>
<li>将当前所读行的行首相对于当前block开始处的字节偏移量作为key（0）</li>
<li>当前行的内容作为value（Dear Bear River）</li>
</ul>
</li>
<li>map()内<ul>
<li>(按需求，写业务代码)，将value当前行内容按空格切分，得到三个单词Dear | Bear | River</li>
<li>将每个单词变成键值对，输出出去(Dear, 1) | (Bear, 1) | (River, 1)；最终结果写入map任务所在节点的本地磁盘中（内里还有细节，讲到shuffle时，再细细展开）</li>
<li>block的第一行的数据被处理完后，接着处理第二行；逻辑同上</li>
<li>当map任务将当前block中所有的数据全部处理完后，此map任务即运行结束</li>
</ul>
</li>
<li>其它的每一个map任务都是如上逻辑，不再赘述</li>
</ul>
</li>
<li>Reduce阶段<ul>
<li>reduce任务（reduce task）的个数由自己写的程序编程指定，main()内的job.setNumReduceTasks(4)指定reduce任务是4个（reduce1、reduce2、reduce3、reduce4）</li>
<li>每一个reduce任务的逻辑一下，所以以第一个reduce任务（reduce1）为例分析</li>
<li>map1任务完成后，reduce1通过网络，连接到map1，将map1输出结果中属于reduce1的分区的数据，通过网络获取到reduce1端（拷贝阶段）</li>
<li>同样也如此连接到map2、map3获取结果</li>
<li>最终reduce1端获得4个(Dear, 1)键值对；由于key键相同，它们分到同一组；</li>
<li>4个(Dear, 1)键值对，转换成[Dear, Iterable(1, 1, 1, )]，作为两个参数传入reduce()</li>
<li>在reduce()内部，计算Dear的总数为4，并将(Dear, 4)作为键值对输出</li>
<li>每个reduce任务最终输出文件（内里还有细节，讲到shuffle时，再细细展开），文件写入到HDFS</li>
</ul>
</li>
</ul>
<h4 id="2-2-MR中key的作用"><a href="#2-2-MR中key的作用" class="headerlink" title="2.2 MR中key的作用"></a>2.2 MR中key的作用</h4><ul>
<li><font color="red"><strong>MapReduce编程中，key有特殊的作用</strong></font>

<ul>
<li><p><strong>①数据中，若要针对某个值进行分组、聚合时，需将此值作为MR中的reduce的输入的key</strong></p>
</li>
<li><p><strong>如当前的词频统计例子，按单词进行分组，每组中对出现次数做聚合（计算总和）；所以需要将每个单词作为reduce输入的key，MapReduce框架自动按照单词分组，进而求出每组即每个单词的总次数</strong></p>
<p><img src="/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201910141101.png" alt=""></p>
</li>
<li><p><strong>②另外，key还具有可排序的特性，因为MR中的key类需要实现WritableComparable接口；而此接口又继承Comparable接口（可查看源码）</strong></p>
</li>
<li><p><strong>MR编程时，要充分利用以上两点；结合实际业务需求，设置合适的key</strong></p>
<p><img src="/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201908221717.png" alt=""></p>
<p><img src="/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201908221718.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<h4 id="2-4-MR参考代码"><a href="#2-4-MR参考代码" class="headerlink" title="2.4 MR参考代码"></a>2.4 MR参考代码</h4><p><strong>2.4.1 Mapper代码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba.hadoop.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 类Mapper&lt;LongWritable, Text, Text, IntWritable&gt;的四个泛型分别表示</span></span><br><span class="line"><span class="comment"> * map方法的输入的键的类型kin、值的类型vin；输出的键的类型kout、输出的值的类型vout</span></span><br><span class="line"><span class="comment"> * kin指的是当前所读行行首相对于split分片开头的字节偏移量,所以是long类型，对应序列化类型LongWritable</span></span><br><span class="line"><span class="comment"> * vin指的是当前所读行，类型是String，对应序列化类型Text</span></span><br><span class="line"><span class="comment"> * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text</span></span><br><span class="line"><span class="comment"> * vout根据需求，输出值指的是单词的个数，1，类型是int，对应序列化类型是IntWritable</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMap</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 处理分片split中的每一行的数据；针对每行数据，会调用一次map方法</span></span><br><span class="line"><span class="comment">     * 在一次map方法调用时，从一行数据中，获得一个个单词word，再将每个单词word变成键值对形式(word, 1)输出出去</span></span><br><span class="line"><span class="comment">     * 输出的值最终写到本地磁盘中</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key 当前所读行行首相对于split分片开头的字节偏移量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value  当前所读行</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//当前行的示例数据(单词间空格分割)：Dear Bear River</span></span><br><span class="line">        <span class="comment">//取得当前行的数据</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">//按照\t进行分割，得到当前行所有单词</span></span><br><span class="line">        String[] words = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            <span class="comment">//将每个单词word变成键值对形式(word, 1)输出出去</span></span><br><span class="line">            <span class="comment">//同样，输出前，要将kout, vout包装成对应的可序列化类型，如String对应Text，int对应IntWritable</span></span><br><span class="line">            context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>2.4.2 Reducer代码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba.hadoop.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Reducer&lt;Text, IntWritable, Text, IntWritable&gt;的四个泛型分别表示</span></span><br><span class="line"><span class="comment"> * reduce方法的输入的键的类型kin、输入值的类型vin；输出的键的类型kout、输出的值的类型vout</span></span><br><span class="line"><span class="comment"> * 注意：因为map的输出作为reduce的输入，所以此处的kin、vin类型分别与map的输出的键类型、值类型相同</span></span><br><span class="line"><span class="comment"> * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text</span></span><br><span class="line"><span class="comment"> * vout根据需求，输出值指的是每个单词的总个数，类型是int，对应序列化类型是IntWritable</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReduce</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * key相同的一组kv对，会调用一次reduce方法</span></span><br><span class="line"><span class="comment">     * 如reduce task汇聚了众多的键值对，有key是hello的键值对，也有key是spark的键值对，如下</span></span><br><span class="line"><span class="comment">     * (hello, 1)</span></span><br><span class="line"><span class="comment">     * (hello, 1)</span></span><br><span class="line"><span class="comment">     * (hello, 1)</span></span><br><span class="line"><span class="comment">     * (hello, 1)</span></span><br><span class="line"><span class="comment">     * ...</span></span><br><span class="line"><span class="comment">     * (spark, 1)</span></span><br><span class="line"><span class="comment">     * (spark, 1)</span></span><br><span class="line"><span class="comment">     * (spark, 1)</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 其中，key是hello的键值对被分成一组；merge成[hello, Iterable(1,1,1,1)]，调用一次reduce方法</span></span><br><span class="line"><span class="comment">     * 同样，key是spark的键值对被分成一组；merge成[spark, Iterable(1,1,1)]，再调用一次reduce方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key 当前组的key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> values 当前组中，所有value组成的可迭代集和</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context reduce上下文环境对象</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,</span></span></span><br><span class="line"><span class="function"><span class="params">                          Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//定义变量，用于累计当前单词出现的次数</span></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (IntWritable count : values) &#123;</span><br><span class="line">            <span class="comment">//从count中获得值，累加到sum中</span></span><br><span class="line">            sum += count.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将单词、单词次数，分别作为键值对，输出</span></span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(sum));<span class="comment">// 输出最终结果</span></span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>2.4.3 Main程序入口</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba.hadoop.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * MapReduce程序入口</span></span><br><span class="line"><span class="comment"> * 注意：</span></span><br><span class="line"><span class="comment"> *  导包时，不要导错了；</span></span><br><span class="line"><span class="comment"> *  另外，map\reduce相关的类，使用mapreduce包下的，是新API，如org.apache.hadoop.mapreduce.Job;；</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMain</span> </span>&#123;</span><br><span class="line">    <span class="comment">//若在IDEA中本地执行MR程序，需要将mapred-site.xml中的mapreduce.framework.name值修改成local</span></span><br><span class="line">    <span class="comment">//参数 c:/test/README.txt c:/test/wc</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException,</span></span><br><span class="line"><span class="function">            ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//判断一下，输入参数是否是两个，分别表示输入路径、输出路径</span></span><br><span class="line">       <span class="keyword">if</span> (args.length != <span class="number">2</span> || args == <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"please input Path!"</span>);</span><br><span class="line">            System.exit(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">//configuration.set("mapreduce.framework.name","local");</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//告诉程序，要运行的jar包在哪</span></span><br><span class="line">        <span class="comment">//configuration.set("mapreduce.job.jar","/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar");</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//调用getInstance方法，生成job实例</span></span><br><span class="line">        Job job = Job.getInstance(configuration, WordCountMain.class.getSimpleName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置job的jar包，如果参数指定的类包含在一个jar包中，则此jar包作为job的jar包； 参数class跟主类在一个工程即可；一般设置成主类</span></span><br><span class="line"><span class="comment">//        job.setJarByClass(WordCountMain.class);</span></span><br><span class="line">        job.setJarByClass(WordCountMain.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过job设置输入/输出格式</span></span><br><span class="line">        <span class="comment">//MR的默认输入格式是TextInputFormat，输出格式是TextOutputFormat；所以下两行可以注释掉</span></span><br><span class="line"><span class="comment">//        job.setInputFormatClass(TextInputFormat.class);</span></span><br><span class="line"><span class="comment">//        job.setOutputFormatClass(TextOutputFormat.class);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置输入/输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置处理Map阶段的自定义的类</span></span><br><span class="line">        job.setMapperClass(WordCountMap.class);</span><br><span class="line">        <span class="comment">//设置map combine类，减少网路传出量</span></span><br><span class="line">        job.setCombinerClass(WordCountReduce.class);</span><br><span class="line">        <span class="comment">//设置处理Reduce阶段的自定义的类</span></span><br><span class="line">        job.setReducerClass(WordCountReduce.class);</span><br><span class="line">        <span class="comment">//注意：如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型</span></span><br><span class="line">        <span class="comment">//注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错</span></span><br><span class="line"><span class="comment">//        job.setMapOutputKeyClass(Text.class);</span></span><br><span class="line"><span class="comment">//        job.setMapOutputValueClass(IntWritable.class);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置reduce task最终输出key/value的类型</span></span><br><span class="line">        <span class="comment">//注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交作业</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>程序运行有两种方式，分别是windows本地运行、集群运行，依次演示</p>
</blockquote>
<h4 id="2-5-集群运行"><a href="#2-5-集群运行" class="headerlink" title="2.5 集群运行"></a>2.5 集群运行</h4><ul>
<li>用maven插件打jar包；①点击Maven，②双击package打包</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@node01 ~]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.wordcount.WordCountMain /README.txt /wordcount01</span><br></pre></td></tr></table></figure>
<blockquote>
<p>说明：</p>
<p>com.kaikeba.hadoop-1.0-SNAPSHOT.jar是jar包名</p>
<p>com.kaikeba.hadoop.wordcount.WordCountMain是包含main方法的类的全限定名</p>
<p>/NOTICE.txt和/wordcount是main方法的两个参数，表示输入路径、输出路径</p>
</blockquote>
<p><img src="assets/hadoop jar.gif" alt=""></p>
<ul>
<li>确认结果</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@node01 ~]$ hadoop fs -ls /wordcount01</span><br></pre></td></tr></table></figure>
<p><img src="assets/Image201908221620.png" alt=""></p>
<h4 id="2-6-总结"><a href="#2-6-总结" class="headerlink" title="2.6 总结"></a>2.6 总结</h4><ul>
<li>MR分为两个阶段：map阶段、reduce阶段</li>
<li>MR输入的文件有几个block，就会生成几个map任务</li>
<li>MR的reduce任务的个数，由程序中编程指定：job.setNumReduceTasks(4)</li>
<li>map任务<ul>
<li>map任务中map()一次读取block的一行数据，以kv对的形式输入map()</li>
<li>map()的输出作为reduce()的输入</li>
</ul>
</li>
<li>reduce任务<ul>
<li>reduce任务通过网络将各执行完成的map任务输出结果中，属于自己的数据取过来</li>
<li>key相同的键值对作为一组，调用一次reduce()</li>
<li>reduce任务生成一个结果文件</li>
<li>文件写入HDFS</li>
</ul>
</li>
</ul>
<h3 id="3-WEB-UI查看结果"><a href="#3-WEB-UI查看结果" class="headerlink" title="3. WEB UI查看结果"></a>3. WEB UI查看结果</h3><h4 id="3-1-Yarn"><a href="#3-1-Yarn" class="headerlink" title="3.1 Yarn"></a>3.1 Yarn</h4><blockquote>
<p>node01是resourcemanager所在节点主机名，根据自己的实际情况修改主机名</p>
</blockquote>
<p>浏览器访问url地址：<a href="http://node01:8088" target="_blank" rel="noopener">http://node01:8088</a></p>
<p><img src="assets/Image201908221638.png" alt=""></p>
<h4 id="3-2-HDFS结果"><a href="#3-2-HDFS结果" class="headerlink" title="3.2 HDFS结果"></a>3.2 HDFS结果</h4><p>浏览器输入URL：<a href="http://node01:50070" target="_blank" rel="noopener">http://node01:50070</a></p>
<p>①点击下拉框；②浏览文件系统；③输入根目录，查看hdfs根路径中的内容</p>
<p><img src="assets/Image201908221639.png" alt=""></p>
<h3 id="4-Shuffle"><a href="#4-Shuffle" class="headerlink" title="4. Shuffle"></a>4. Shuffle</h3><ul>
<li>shuffle主要指的是map端的输出作为reduce端输入的过程</li>
</ul>
<h4 id="4-1-shuffle简图"><a href="#4-1-shuffle简图" class="headerlink" title="4.1 shuffle简图"></a>4.1 shuffle简图</h4><p><img src="assets/Image201905231409.png" alt=""></p>
<h4 id="4-2-shuffle细节图"><a href="#4-2-shuffle细节图" class="headerlink" title="4.2 shuffle细节图"></a>4.2 shuffle细节图</h4><p><img src="assets/Image201906280906.png" alt=""></p>
<ul>
<li><p>分区用到了分区器，默认分区器是HashPartitioner</p>
<p>源码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>&gt; <span class="keyword">implements</span> <span class="title">Partitioner</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(JobConf job)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K2 key, V2 value,</span></span></span><br><span class="line"><span class="function"><span class="params">                          <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="4-3-map端"><a href="#4-3-map端" class="headerlink" title="4.3 map端"></a>4.3 map端</h4><ul>
<li>每个map任务都有一个对应的环形内存缓冲区；输出是kv对，先写入到环形缓冲区（默认大小100M），当内容占据80%缓冲区空间后，由一个后台线程将缓冲区中的数据溢出写到一个磁盘文件</li>
<li>在溢出写的过程中，map任务可以继续向环形缓冲区写入数据；但是若写入速度大于溢出写的速度，最终造成100m占满后，map任务会暂停向环形缓冲区中写数据的过程；只执行溢出写的过程；直到环形缓冲区的数据全部溢出写到磁盘，才恢复向缓冲区写入</li>
<li>后台线程溢写磁盘过程，有以下几个步骤：<ul>
<li>先对每个溢写的kv对做分区；分区的个数由MR程序的reduce任务数决定；默认使用HashPartitioner计算当前kv对属于哪个分区；计算公式：(key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks</li>
<li>每个分区中，根据kv对的key做内存中排序；</li>
<li>若设置了map端本地聚合combiner，则对每个分区中，排好序的数据做combine操作；</li>
<li>若设置了对map输出压缩的功能，会对溢写数据压缩</li>
</ul>
</li>
<li>随着不断的向环形缓冲区中写入数据，会多次触发溢写（每当环形缓冲区写满100m），本地磁盘最终会生成多个溢出文件</li>
<li>合并溢写文件：在map task完成之前，所有溢出文件会被合并成一个大的溢出文件；且是已分区、已排序的输出文件</li>
<li>小细节：<ul>
<li>在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作；</li>
<li>但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销）</li>
</ul>
</li>
</ul>
<h4 id="4-4-reduce端"><a href="#4-4-reduce端" class="headerlink" title="4.4 reduce端"></a>4.4 reduce端</h4><ul>
<li><p>reduce task会在每个map task运行完成后，通过HTTP获得map task输出中，属于自己的分区数据（许多kv对）</p>
</li>
<li><p>如果map输出数据比较小，先保存在reduce的jvm内存中，否则直接写入reduce磁盘</p>
</li>
<li><p>一旦内存缓冲区达到阈值（默认0.66）或map输出数的阈值（默认1000），则触发<strong>归并merge</strong>，结果写到本地磁盘</p>
</li>
<li><p>若MR编程指定了combine，在归并过程中会执行combine操作</p>
</li>
<li><p>随着溢出写的文件的增多，后台线程会将它们合并大的、排好序的文件</p>
</li>
<li><p>reduce task将所有map task复制完后，将合并磁盘上所有的溢出文件</p>
</li>
<li><p>默认一次合并10个</p>
</li>
<li><p>最后一批合并，部分数据来自内存，部分来自磁盘上的文件</p>
</li>
<li><p>进入“归并、排序、分组阶段”</p>
</li>
<li><p>每组数据调用一次reduce方法</p>
</li>
<li><p>参考文件《<strong>reduce端merge 排序 分组.txt</strong>》</p>
</li>
</ul>
<h4 id="4-5-总结"><a href="#4-5-总结" class="headerlink" title="4.5 总结"></a>4.5 总结</h4><ul>
<li>map端<ul>
<li>map()输出结果先写入环形缓冲区</li>
<li>缓冲区100M；写满80M后，开始溢出写磁盘文件</li>
<li>此过程中，会进行分区、排序、combine（可选）、压缩（可选）</li>
<li>map任务完成前，会将多个小的溢出文件，合并成一个大的溢出文件（已分区、排序）</li>
</ul>
</li>
<li>reduce端<ul>
<li>拷贝阶段：reduce任务通过http将map任务属于自己的分区数据拉取过来</li>
<li>开始merge及溢出写磁盘文件</li>
<li>所有map任务的分区全部拷贝过来后，进行阶段合并、排序、分组阶段</li>
<li>每组数据调用一次reduce()</li>
<li>结果写入HDFS</li>
</ul>
</li>
</ul>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="https://blog.sev7e0.site/">大数据施工现场</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
