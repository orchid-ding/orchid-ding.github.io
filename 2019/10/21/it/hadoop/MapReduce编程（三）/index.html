<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="天行健、君子以自强不息；地势坤，君子以厚德载物。">
    <meta name="keyword"  content="兰草">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        MapReduce编程（三） - kfly的博客 | kfly&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_1598291_q3el2wqimj.css" type="text/css">
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>kfly</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont iconhome"></i>
                    <span>主页</span>
                </a>
            </li>
 	   <li >
                <a href="/spec/">
                    <i class="iconfont iconzhuanti"></i>
                    <span>专题</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>简历</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MapReduce编程模型"><span class="toc-text">MapReduce编程模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-自定义OutputFormat"><span class="toc-text">1. 自定义OutputFormat</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-需求"><span class="toc-text">1.1 需求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-逻辑分析"><span class="toc-text">1.2 逻辑分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-实现要点"><span class="toc-text">1.3 实现要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-MR代码"><span class="toc-text">1.4 MR代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-总结"><span class="toc-text">1.5 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-二次排序"><span class="toc-text">2. 二次排序</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-需求"><span class="toc-text">2.1 需求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-逻辑分析"><span class="toc-text">2.2 逻辑分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-MR代码"><span class="toc-text">2.3 MR代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-总结"><span class="toc-text">2.4 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-自定义分组求topN"><span class="toc-text">3. 自定义分组求topN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-需求"><span class="toc-text">3.1 需求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-逻辑分析"><span class="toc-text">3.2 逻辑分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-MR代码"><span class="toc-text">3.3 MR代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-总结"><span class="toc-text">3.4 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-MapReduce数据倾斜-20分钟"><span class="toc-text">4. MapReduce数据倾斜(20分钟)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-如何诊断是否存在数据倾斜（10分钟）"><span class="toc-text">4.1 如何诊断是否存在数据倾斜（10分钟）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-减缓数据倾斜"><span class="toc-text">4.2 减缓数据倾斜</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-MR调优"><span class="toc-text">5. MR调优</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-抽样、范围分区"><span class="toc-text">6. 抽样、范围分区</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-数据"><span class="toc-text">6.1 数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-需求"><span class="toc-text">6.2 需求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-实现方案"><span class="toc-text">6.3 实现方案</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-4-MR代码"><span class="toc-text">6.4 MR代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-5-总结"><span class="toc-text">6.5 总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#注意"><span class="toc-text">注意</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        MapReduce编程（三）
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-10-21 23:06:43</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#hadoop" title="hadoop">hadoop</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#MapReduce" title="MapReduce">MapReduce</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h1 id="MapReduce编程模型"><a href="#MapReduce编程模型" class="headerlink" title="MapReduce编程模型"></a>MapReduce编程模型</h1><h3 id="1-自定义OutputFormat"><a href="#1-自定义OutputFormat" class="headerlink" title="1. 自定义OutputFormat"></a>1. 自定义OutputFormat</h3><h4 id="1-1-需求"><a href="#1-1-需求" class="headerlink" title="1.1 需求"></a>1.1 需求</h4><ul>
<li><p>现在有一些订单的评论数据，要将订单的好评与其它级别的评论（中评、差评）进行区分开来，将最终的数据分开到不同的文件夹下面去</p>
</li>
<li><p>数据第九个字段表示评分等级：0 好评，1 中评，2 差评</p>
<p><img src="assets/Image201909111129.png" alt=""></p>
</li>
</ul>
<h4 id="1-2-逻辑分析"><a href="#1-2-逻辑分析" class="headerlink" title="1.2 逻辑分析"></a>1.2 逻辑分析</h4><ul>
<li>程序的关键点是在一个mapreduce程序中，根据数据的不同(好评的评级不同)，输出两类结果到不同<strong>目录</strong></li>
<li>这类灵活的输出，需求通过自定义OutputFormat来实现</li>
</ul>
<h4 id="1-3-实现要点"><a href="#1-3-实现要点" class="headerlink" title="1.3 实现要点"></a>1.3 实现要点</h4><ul>
<li>在mapreduce中访问外部资源</li>
<li>自定义OutputFormat类，覆写getRecordWriter()方法</li>
<li>自定义RecordWriter类，覆写具体输出数据的方法write()</li>
</ul>
<h4 id="1-4-MR代码"><a href="#1-4-MR代码" class="headerlink" title="1.4 MR代码"></a>1.4 MR代码</h4><ul>
<li>自定义OutputFormat</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.outputformat;

import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

/**
 *
 * 本例使用框架默认的Reducer，它将Mapper输入的kv对，原样输出；所以reduce输出的kv类型分别是Text, NullWritable
 * 自定义OutputFormat的类，泛型表示reduce输出的键值对类型；要保持一致;
 * map--(kv)--&gt;reduce--(kv)--&gt;OutputFormat
 */
public class MyOutPutFormat extends FileOutputFormat&lt;Text, NullWritable&gt; {

    /**
     * 两个输出文件;
     * good用于保存好评文件；其它评级保存到bad中
     * 根据实际情况修改path;node01及端口号8020
     */
    String bad = &quot;hdfs://node01:8020/outputformat/bad/r.txt&quot;;
    String good = &quot;hdfs://node01:8020/outputformat/good/r.txt&quot;;

    /**
     *
     * @param context
     * @return
     * @throws IOException
     * @throws InterruptedException
     */
    @Override
    public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException {
        //获得文件系统对象
        FileSystem fs = FileSystem.get(context.getConfiguration());
        //两个输出文件路径
        Path badPath = new Path(bad);
        Path goodPath = new Path(good);
        FSDataOutputStream badOut = fs.create(badPath);
        FSDataOutputStream goodOut = fs.create(goodPath);
        return new MyRecordWriter(badOut,goodOut);
    }

    /**
     * 泛型表示reduce输出的键值对类型；要保持一致
     */
    static class MyRecordWriter extends RecordWriter&lt;Text, NullWritable&gt;{

        FSDataOutputStream badOut = null;
        FSDataOutputStream goodOut = null;

        public MyRecordWriter(FSDataOutputStream badOut, FSDataOutputStream goodOut) {
            this.badOut = badOut;
            this.goodOut = goodOut;
        }

        /**
         * 自定义输出kv对逻辑
         * @param key
         * @param value
         * @throws IOException
         * @throws InterruptedException
         */
        @Override
        public void write(Text key, NullWritable value) throws IOException, InterruptedException {
            if (key.toString().split(&quot;\t&quot;)[9].equals(&quot;0&quot;)){//好评
                goodOut.write(key.toString().getBytes());
                goodOut.write(&quot;\r\n&quot;.getBytes());
            }else{//其它评级
                badOut.write(key.toString().getBytes());
                badOut.write(&quot;\r\n&quot;.getBytes());
            }
        }

        /**
         * 关闭流
         * @param context
         * @throws IOException
         * @throws InterruptedException
         */
        @Override
        public void close(TaskAttemptContext context) throws IOException, InterruptedException {
            if(goodOut !=null){
                goodOut.close();
            }
            if(badOut !=null){
                badOut.close();
            }
        }
    }
}
</code></pre>
<ul>
<li>main方法</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.outputformat;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import java.io.IOException;

public class MyOwnOutputFormatMain extends Configured implements Tool {

    public int run(String[] args) throws Exception {
        Configuration conf = super.getConf();
        Job job = Job.getInstance(conf, MyOwnOutputFormatMain.class.getSimpleName());
        job.setJarByClass(MyOwnOutputFormatMain.class);

        //默认项，可以省略或者写出也可以
        //job.setInputFormatClass(TextInputFormat.class);
        //设置输入文件
        TextInputFormat.addInputPath(job, new Path(args[0]));
        job.setMapperClass(MyMapper.class);
        //job.setMapOutputKeyClass(Text.class);
        //job.setMapOutputValueClass(NullWritable.class);

        //设置自定义的输出类
        job.setOutputFormatClass(MyOutPutFormat.class);
        //设置一个输出目录，这个目录会输出一个success的成功标志的文件
        MyOutPutFormat.setOutputPath(job, new Path(args[1]));
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(NullWritable.class);

        //默认项，即默认有一个reduce任务，所以可以省略
//        job.setNumReduceTasks(1);
//        //Reducer将输入的键值对原样输出
//        job.setReducerClass(Reducer.class);

        boolean b = job.waitForCompletion(true);
        return b ? 0: 1;
    }

    /**
     *
     * Mapper输出的key、value类型
     * 文件每行的内容作为输出的key，对应Text类型
     * 输出的value为null，对应NullWritable
     */
    public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; {
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            //把当前行内容作为key输出；value为null
            context.write(value, NullWritable.get());
        }
    }

    /**
     *
     * @param args /ordercomment.csv /ofo
     * @throws Exception
     */
    public static void main(String[] args) throws Exception {
        Configuration configuration = new Configuration();
        ToolRunner.run(configuration, new MyOwnOutputFormatMain(), args);
    }
}
</code></pre>
<h4 id="1-5-总结"><a href="#1-5-总结" class="headerlink" title="1.5 总结"></a>1.5 总结</h4><ul>
<li>自定义outputformat<ul>
<li>泛型与reduce输出的键值对类型保持一致</li>
<li>覆写getRecordWriter()方法</li>
</ul>
</li>
<li><p>自定义RecordWriter</p>
<ul>
<li>泛型与reduce输出的键值对类型保持一致</li>
<li>覆写具体输出数据的方法write()、close()</li>
</ul>
</li>
<li><p>main方法</p>
<ul>
<li>job.setOutputFormatClass使用自定义在输出类</li>
</ul>
</li>
</ul>
<h3 id="2-二次排序"><a href="#2-二次排序" class="headerlink" title="2. 二次排序"></a>2. 二次排序</h3><h4 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1 需求"></a>2.1 需求</h4><ul>
<li><p>数据：有一个简单的关于员工工资的记录文件salary.txt</p>
<ul>
<li><p>每条记录如下，有3个字段，分别表示name、age、salary</p>
</li>
<li><p>nancy    22    8000</p>
<p><img src="assets/Image201910181039.png" alt=""></p>
</li>
</ul>
</li>
<li><p>使用MR处理记录，实现结果中</p>
<ul>
<li>按照工资从高到低的降序排序</li>
<li>若工资相同，则按年龄升序排序</li>
</ul>
</li>
</ul>
<h4 id="2-2-逻辑分析"><a href="#2-2-逻辑分析" class="headerlink" title="2.2 逻辑分析"></a>2.2 逻辑分析</h4><ul>
<li><p>利用MR中key具有可比较的特点</p>
</li>
<li><p>MapReduce中，根据key进行分区、排序、分组</p>
</li>
<li><p>有些MR的输出的key可以直接使用hadoop框架的可序列化可比较类型表示，如Text、IntWritable等等，而这些类型本身是可比较的；如IntWritable默认升序排序</p>
<p><img src="assets/Image201909111209.png" alt=""></p>
</li>
<li><p>但有时，使用MR编程，输出的key，若使用hadoop自带的key类型无法满足需求</p>
<ul>
<li>此时，需要自定义的key类型（包含的是非单一信息，如此例包含工资、年龄）；</li>
<li>并且也得是<strong><font color="red">可序列化、可比较的</font></strong></li>
</ul>
</li>
<li><p>需要自定义key，定义排序规则</p>
<ul>
<li>实现：按照人的salary降序排序，若相同，则再按age升序排序；若salary、age相同，则放入同一组</li>
</ul>
</li>
</ul>
<h4 id="2-3-MR代码"><a href="#2-3-MR代码" class="headerlink" title="2.3 MR代码"></a>2.3 MR代码</h4><ul>
<li>详见工程代码</li>
<li>自定义key类型Person类</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.secondarysort;

import org.apache.hadoop.io.WritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

//根据输入文件格式，定义JavaBean，作为MR时，Map的输出key类型；要求此类可序列化、可比较
public class Person implements WritableComparable&lt;Person&gt; {
    private String name;
    private int age;
    private int salary;

    public Person() {
    }

    public Person(String name, int age, int salary) {
        //super();
        this.name = name;
        this.age = age;
        this.salary = salary;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public int getAge() {
        return age;
    }

    public void setAge(int age) {
        this.age = age;
    }

    public int getSalary() {
        return salary;
    }

    public void setSalary(int salary) {
        this.salary = salary;
    }

    @Override
    public String toString() {
        return this.salary + &quot;  &quot; + this.age + &quot;    &quot; + this.name;
    }

    //两个Person对象的比较规则：①先比较salary，高的排序在前；②若相同，age小的在前
    public int compareTo(Person other) {
        int compareResult= this.salary - other.salary;
        if(compareResult != 0) {//若两个人工资不同
            //工资降序排序；即工资高的排在前边
            return -compareResult;
        } else {//若工资相同
            //年龄升序排序；即年龄小的排在前边
            return this.age - other.age;
        }
    }

    //序列化，将NewKey转化成使用流传送的二进制
    public void write(DataOutput dataOutput) throws IOException {
        //注意：①使用正确的write方法；②记住此时的序列化的顺序，name、age、salary
        dataOutput.writeUTF(name);
        dataOutput.writeInt(age);
        dataOutput.writeInt(salary);
    }

    //使用in读字段的顺序，要与write方法中写的顺序保持一致：name、age、salary
    public void readFields(DataInput dataInput) throws IOException {
        //read string
        //注意：①使用正确的read方法；②读取顺序与write()中序列化的顺序保持一致
        this.name = dataInput.readUTF();
        this.age = dataInput.readInt();
        this.salary = dataInput.readInt();
    }
}
</code></pre>
<ul>
<li>main类、mapper、reducer</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.secondarysort;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;
import java.net.URI;

public class SecondarySort {

    /**
     *
     * @param args /salary.txt /secondarysort
     * @throws Exception
     */
    public static void main(String[] args) throws Exception {
        Configuration configuration = new Configuration();
        //configuration.set(&quot;mapreduce.job.jar&quot;,&quot;/home/bruce/project/kkbhdp01/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar&quot;);
        Job job = Job.getInstance(configuration, SecondarySort.class.getSimpleName());

        FileSystem fileSystem = FileSystem.get(URI.create(args[1]), configuration);
        //生产中慎用
        if (fileSystem.exists(new Path(args[1]))) {
            fileSystem.delete(new Path(args[1]), true);
        }

        FileInputFormat.setInputPaths(job, new Path(args[0]));
        job.setMapperClass(MyMap.class);
        //由于mapper与reducer输出的kv类型分别相同，所以，下两行可以省略
//        job.setMapOutputKeyClass(Person.class);
//        job.setMapOutputValueClass(NullWritable.class);

        //设置reduce的个数;默认为1
        //job.setNumReduceTasks(1);

        job.setReducerClass(MyReduce.class);
        job.setOutputKeyClass(Person.class);
        job.setOutputValueClass(NullWritable.class);
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.waitForCompletion(true);

    }

    //MyMap的输出key用自定义的Person类型；输出的value为null
    public static class MyMap extends Mapper&lt;LongWritable, Text, Person, NullWritable&gt; {
        @Override
        protected void map(LongWritable key, Text value, Context context)
                throws IOException, InterruptedException {

            String[] fields = value.toString().split(&quot;\t&quot;);
            String name = fields[0];
            int age = Integer.parseInt(fields[1]);
            int salary = Integer.parseInt(fields[2]);
            //在自定义类中进行比较
            Person person = new Person(name, age, salary);
            //person对象作为输出的key
            context.write(person, NullWritable.get());
        }
    }

    public static class MyReduce extends Reducer&lt;Person, NullWritable, Person, NullWritable&gt; {
        @Override
        protected void reduce(Person key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {
            //输入的kv对，原样输出
            context.write(key, NullWritable.get());
        }
    }
}
</code></pre>
<h4 id="2-4-总结"><a href="#2-4-总结" class="headerlink" title="2.4 总结"></a>2.4 总结</h4><ul>
<li>如果MR时，key的排序规则比较复杂，比如需要根据字段1排序，若字段1相同，则需要根据字段2排序…，此时，可以使用自定义key实现</li>
<li>将自定义的key作为MR中，map输出的key的类型（reduce输入的类型）</li>
<li>自定义的key<ul>
<li>实现WritableComparable接口</li>
<li>实现compareTo比较方法</li>
<li>实现write序列化方法</li>
<li>实现readFields反序列化方法</li>
</ul>
</li>
</ul>
<h3 id="3-自定义分组求topN"><a href="#3-自定义分组求topN" class="headerlink" title="3. 自定义分组求topN"></a>3. 自定义分组求topN</h3><h4 id="3-1-需求"><a href="#3-1-需求" class="headerlink" title="3.1 需求"></a>3.1 需求</h4><ul>
<li><p>现有一个淘宝用户订单历史记录文件；每条记录有6个字段，分别表示</p>
<ul>
<li>userid、datetime、title商品标题、unitPrice商品单价、purchaseNum购买量、productId商品ID</li>
</ul>
<p><img src="assets/Image201909111241.png" alt=""></p>
</li>
<li><p>现使用MR编程，求出每个用户、每个月消费金额最多的两笔订单，花了多少钱</p>
<ul>
<li>所以得相同用户、同一个年月的数据，分到同一组</li>
</ul>
</li>
</ul>
<h4 id="3-2-逻辑分析"><a href="#3-2-逻辑分析" class="headerlink" title="3.2 逻辑分析"></a>3.2 逻辑分析</h4><ul>
<li>根据文件格式，自定义JavaBean类OrderBean<ul>
<li>实现WritableComparable接口</li>
<li>包含6个字段分别对应文件中的6个字段</li>
<li>重点实现compareTo方法<ul>
<li>先比较userid是否相等；若不相等，则userid升序排序</li>
<li>若相等，比较两个Bean的日期是否相等；若不相等，则日期升序排序</li>
<li>若相等，再比较总开销，降序排序</li>
</ul>
</li>
<li>实现序列化方法write()</li>
<li>实现反序列化方法readFields()</li>
</ul>
</li>
<li>自定义分区类<ul>
<li>继承Partitioner类</li>
<li>getPartiton()实现，userid相同的，处于同一个分区</li>
</ul>
</li>
<li>自定义Mapper类<ul>
<li>输出key是当前记录对应的Bean对象</li>
<li>输出的value对应当前下单的总开销</li>
</ul>
</li>
<li>自定义分组类<ul>
<li>决定userid相同、日期（年月）相同的记录，分到同一组中，调用一次reduce()</li>
</ul>
</li>
<li>自定义Reduce类<ul>
<li>reduce()中求出当前一组数据中，开销头两笔的信息</li>
</ul>
</li>
<li>main方法<ul>
<li>job.setMapperClass</li>
<li>job.setPartitionerClass</li>
<li>job.setReducerClass</li>
<li>job.setGroupingComparatorClass</li>
</ul>
</li>
</ul>
<h4 id="3-3-MR代码"><a href="#3-3-MR代码" class="headerlink" title="3.3 MR代码"></a>3.3 MR代码</h4><blockquote>
<p>详细代码见代码工程</p>
</blockquote>
<ul>
<li>OrderBean</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.grouping;

import org.apache.hadoop.io.WritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

//实现WritableComparable接口
public class OrderBean implements WritableComparable&lt;OrderBean&gt; {

    //用户ID
    private String userid;
    //年月
    //year+month -&gt; 201408
    private String datetime;
    //标题
    private String title;
    //单价
    private double unitPrice;
    //购买量
    private int purchaseNum;
    //商品ID
    private String produceId;

    public OrderBean() {
    }

    public OrderBean(String userid, String datetime, String title, double unitPrice, int purchaseNum, String produceId) {
        super();
        this.userid = userid;
        this.datetime = datetime;
        this.title = title;
        this.unitPrice = unitPrice;
        this.purchaseNum = purchaseNum;
        this.produceId = produceId;
    }

    //key的比较规则
    public int compareTo(OrderBean other) {
        //OrderBean作为MR中的key；如果对象中的userid相同，即ret1为0；就表示两个对象是同一个用户
        int ret1 = this.userid.compareTo(other.userid);

        if (ret1 == 0) {
            //如果userid相同，比较年月
            String thisYearMonth = this.getDatetime();
            String otherYearMonth = other.getDatetime();
            int ret2 = thisYearMonth.compareTo(otherYearMonth);

            if(ret2 == 0) {//若datetime相同
                //如果userid、年月都相同，比较单笔订单的总开销
                Double thisTotalPrice = this.getPurchaseNum()*this.getUnitPrice();
                Double oTotalPrice = other.getPurchaseNum()*other.getUnitPrice();
                //总花销降序排序；即总花销高的排在前边
                return -thisTotalPrice.compareTo(oTotalPrice);
            } else {
                //若datatime不同，按照datetime升序排序
                return ret2;
            }
        } else {
            //按照userid升序排序
            return ret1;
        }
    }

    /**
     * 序列化
     * @param dataOutput
     * @throws IOException
     */
    public void write(DataOutput dataOutput) throws IOException {
        dataOutput.writeUTF(userid);
        dataOutput.writeUTF(datetime);
        dataOutput.writeUTF(title);
        dataOutput.writeDouble(unitPrice);
        dataOutput.writeInt(purchaseNum);
        dataOutput.writeUTF(produceId);
    }

    /**
     * 反序列化
     * @param dataInput
     * @throws IOException
     */
    public void readFields(DataInput dataInput) throws IOException {
        this.userid = dataInput.readUTF();
        this.datetime = dataInput.readUTF();
        this.title = dataInput.readUTF();
        this.unitPrice = dataInput.readDouble();
        this.purchaseNum = dataInput.readInt();
        this.produceId = dataInput.readUTF();
    }

    /**
     * 使用默认分区器，那么userid相同的，落入同一分区；
     * 另外一个方案：此处不覆写hashCode方法，而是自定义分区器，getPartition方法中，对OrderBean的userid求hashCode值%reduce任务数
     * @return
     */
//    @Override
//    public int hashCode() {
//        return this.userid.hashCode();
//    }

    @Override
    public String toString() {
        return &quot;OrderBean{&quot; +
                &quot;userid=&#39;&quot; + userid + &#39;\&#39;&#39; +
                &quot;, datetime=&#39;&quot; + datetime + &#39;\&#39;&#39; +
                &quot;, title=&#39;&quot; + title + &#39;\&#39;&#39; +
                &quot;, unitPrice=&quot; + unitPrice +
                &quot;, purchaseNum=&quot; + purchaseNum +
                &quot;, produceId=&#39;&quot; + produceId + &#39;\&#39;&#39; +
                &#39;}&#39;;
    }

    public String getUserid() {
        return userid;
    }

    public void setUserid(String userid) {
        this.userid = userid;
    }

    public String getDatetime() {
        return datetime;
    }

    public void setDatetime(String datetime) {
        this.datetime = datetime;
    }

    public String getTitle() {
        return title;
    }

    public void setTitle(String title) {
        this.title = title;
    }

    public double getUnitPrice() {
        return unitPrice;
    }

    public void setUnitPrice(double unitPrice) {
        this.unitPrice = unitPrice;
    }

    public int getPurchaseNum() {
        return purchaseNum;
    }

    public void setPurchaseNum(int purchaseNum) {
        this.purchaseNum = purchaseNum;
    }

    public String getProduceId() {
        return produceId;
    }

    public void setProduceId(String produceId) {
        this.produceId = produceId;
    }
}

</code></pre>
<ul>
<li>MyPartitioner</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.grouping;

import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.mapreduce.Partitioner;

//mapper的输出key类型是自定义的key类型OrderBean；输出value类型是单笔订单的总开销double -&gt; DoubleWritable
public class MyPartitioner extends Partitioner&lt;OrderBean, DoubleWritable&gt; {
    @Override
    public int getPartition(OrderBean orderBean, DoubleWritable doubleWritable, int numReduceTasks) {
        //userid相同的，落入同一分区
        return (orderBean.getUserid().hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;
    }
}

</code></pre>
<ul>
<li>MyMapper</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.grouping;

import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * 输出kv，分别是OrderBean、用户每次下单的总开销
 */
public class MyMapper extends Mapper&lt;LongWritable, Text, OrderBean, DoubleWritable&gt; {
    DoubleWritable valueOut = new DoubleWritable();
    DateUtils dateUtils = new DateUtils();

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        //13764633023     2014-12-01 02:20:42.000 全视目Allseelook 原宿风暴显色美瞳彩色隐形艺术眼镜1片 拍2包邮    33.6    2       18067781305
        String record = value.toString();
        String[] fields = record.split(&quot;\t&quot;);
        if(fields.length == 6) {
            String userid = fields[0];
            String datetime = fields[1];
            String yearMonth = dateUtils.getYearMonthString(datetime);
            String title = fields[2];
            double unitPrice = Double.parseDouble(fields[3]);
            int purchaseNum = Integer.parseInt(fields[4]);
            String produceId = fields[5];

            //生成OrderBean对象
            OrderBean orderBean = new OrderBean(userid, yearMonth, title, unitPrice, purchaseNum, produceId);

            //此订单的总开销
            double totalPrice = unitPrice * purchaseNum;
            valueOut.set(totalPrice);

            context.write(orderBean, valueOut);
        }
    }
}

</code></pre>
<ul>
<li>MyReducer</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.grouping;

import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

//输出的key为userid拼接上年月的字符串，对应Text；输出的value对应单笔订单的金额
public class MyReducer extends Reducer&lt;OrderBean, DoubleWritable, Text, DoubleWritable&gt; {
    /**
     * ①由于自定义分组逻辑，相同用户、相同年月的订单是一组，调用一次reduce()；
     * ②由于自定义的key类OrderBean中，比较规则compareTo规定，相同用户、相同年月的订单，按总金额降序排序
     * 所以取出头两笔，就实现需求
     * @param key
     * @param values
     * @param context
     * @throws IOException
     * @throws InterruptedException
     */
    @Override
    protected void reduce(OrderBean key, Iterable&lt;DoubleWritable&gt; values, Context context) throws IOException, InterruptedException {
        //求每个用户、每个月、消费金额最多的两笔多少钱
        int num = 0;
        for(DoubleWritable value: values) {
            if(num &lt; 2) {
                String keyOut = key.getUserid() + &quot;  &quot; + key.getDatetime();
                context.write(new Text(keyOut), value);
                num++;
            } else {
                break;
            }
        }

    }
}
</code></pre>
<ul>
<li>MyGroup</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.grouping;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

//自定义分组类：reduce端调用reduce()前，对数据做分组；每组数据调用一次reduce()
public class MyGroup extends WritableComparator {

    public MyGroup() {
        //第一个参数表示key class
        super(OrderBean.class, true);
    }

      // 注意： 分组实现的方法是这个
      // compare（Object a,Object b） 这个方法不可以
    //分组逻辑
    @Override
    public int compare(WritableComparable a, WritableComparable b) {
        //userid相同，且同一月的分成一组
        OrderBean aOrderBean = (OrderBean)a;
        OrderBean bOrderBean = (OrderBean)b;

        String aUserId = aOrderBean.getUserid();
        String bUserId = bOrderBean.getUserid();

        //userid、年、月相同的，作为一组
        int ret1 = aUserId.compareTo(bUserId);
        if(ret1 == 0) {//同一用户
            //年月也相同返回0，在同一组；
            return aOrderBean.getDatetime().compareTo(bOrderBean.getDatetime());
        } else {
            return ret1;
        }
    }
}

</code></pre>
<ul>
<li>CustomGroupingMain</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.grouping;

import com.kaikeba.hadoop.wordcount.WordCountMain;
import com.kaikeba.hadoop.wordcount.WordCountMap;
import com.kaikeba.hadoop.wordcount.WordCountReduce;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import java.io.IOException;

public class CustomGroupingMain extends Configured implements Tool {

    ///tmall-201412-test.csv /cgo
    public static void main(String[] args) throws Exception {
        int exitCode = ToolRunner.run(new CustomGroupingMain(), args);
        System.exit(exitCode);
    }

    @Override
    public int run(String[] args) throws Exception {
        //判断以下，输入参数是否是两个，分别表示输入路径、输出路径
        if (args.length != 2 || args == null) {
            System.out.println(&quot;please input Path!&quot;);
            System.exit(0);
        }

        Configuration configuration = new Configuration();
        //告诉程序，要运行的jar包在哪
        //configuration.set(&quot;mapreduce.job.jar&quot;,&quot;/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar&quot;);

        //调用getInstance方法，生成job实例
        Job job = Job.getInstance(configuration, CustomGroupingMain.class.getSimpleName());

        //设置jar包，参数是包含main方法的类
        job.setJarByClass(CustomGroupingMain.class);

        //通过job设置输入/输出格式
        //MR的默认输入格式是TextInputFormat，所以下两行可以注释掉
//        job.setInputFormatClass(TextInputFormat.class);
//        job.setOutputFormatClass(TextOutputFormat.class);

        //设置输入/输出路径
        FileInputFormat.setInputPaths(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        //设置处理Map阶段的自定义的类
        job.setMapperClass(MyMapper.class);
        //设置map combine类，减少网路传出量
        //job.setCombinerClass(MyReducer.class);
        job.setPartitionerClass(MyPartitioner.class);
        //设置处理Reduce阶段的自定义的类
        job.setReducerClass(MyReducer.class);
        job.setGroupingComparatorClass(MyGroup.class);

        //如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型
        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错
        job.setMapOutputKeyClass(OrderBean.class);
        job.setMapOutputValueClass(DoubleWritable.class);

        //设置reduce task最终输出key/value的类型
        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(DoubleWritable.class);

        // 提交作业
        return job.waitForCompletion(true) ? 0 : 1;
    }
}

</code></pre>
<ul>
<li>DateUtils</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.grouping;

import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;

public class DateUtils {

    public static void main(String[] args) {
        //test1
//        String str1 = &quot;13764633024  2014-10-01 02:20:42.000&quot;;
//        String str2 = &quot;13764633023  2014-11-01 02:20:42.000&quot;;
//        System.out.println(str1.compareTo(str2));

        //test2
//        String datetime = &quot;2014-12-01 02:20:42.000&quot;;
//        LocalDateTime localDateTime = parseDateTime(datetime);
//        int year = localDateTime.getYear();
//        int month = localDateTime.getMonthValue();
//        int day = localDateTime.getDayOfMonth();
//        System.out.println(&quot;year-&gt; &quot; + year + &quot;; month -&gt; &quot; + month + &quot;; day -&gt; &quot; + day);

        //test3
//        String datetime = &quot;2014-12-01 02:20:42.000&quot;;
//        System.out.println(getYearMonthString(datetime));
    }

    public LocalDateTime parseDateTime(String dateTime) {
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;);
        LocalDateTime localDateTime = LocalDateTime.parse(dateTime, formatter);
        return localDateTime;
    }

    //日期格式转换工具类：将2014-12-14 20:42:14.000转换成201412
    public String getYearMonthString(String dateTime) {
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;);
        LocalDateTime localDateTime = LocalDateTime.parse(dateTime, formatter);
        int year = localDateTime.getYear();
        int month = localDateTime.getMonthValue();
        return year + &quot;&quot; + month;
    }



}

</code></pre>
<h4 id="3-4-总结"><a href="#3-4-总结" class="headerlink" title="3.4 总结"></a>3.4 总结</h4><ul>
<li>要实现自定义分组逻辑<ul>
<li>一般会自定义JavaBean，作为map输出的key<ul>
<li>实现其中的compareTo方法，设置key的比较逻辑</li>
<li>实现序列化方法write()</li>
<li>实现反序列化方法readFields()</li>
</ul>
</li>
<li>自定义mapper类、reducer类</li>
<li>自定义partition类，getPartition方法，决定哪些key落入哪些分区</li>
<li>自定义group分组类，决定reduce阶段，哪些kv对，落入同一组，调用一次reduce()</li>
<li>写main方法，设置自定义的类<ul>
<li>job.setMapperClass</li>
<li>job.setPartitionerClass</li>
<li>job.setReducerClass</li>
<li>job.setGroupingComparatorClass</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4-MapReduce数据倾斜-20分钟"><a href="#4-MapReduce数据倾斜-20分钟" class="headerlink" title="4. MapReduce数据倾斜(20分钟)"></a>4. MapReduce数据倾斜(20分钟)</h3><ul>
<li>什么是数据倾斜？<ul>
<li>数据中不可避免地会出现离群值（outlier），并导致数据倾斜。这些离群值会显著地拖慢MapReduce的执行。</li>
</ul>
</li>
<li><p>常见的数据倾斜有以下几类：</p>
<ul>
<li>数据频率倾斜——某一个区域的数据量要远远大于其他区域。比如某一个key对应的键值对远远大于其他键的键值对。</li>
<li>数据大小倾斜——部分记录的大小远远大于平均值。</li>
</ul>
</li>
<li><p>在map端和reduce端都有可能发生数据倾斜</p>
<ul>
<li>在map端的数据倾斜可以考虑使用combine</li>
<li>在reduce端的数据倾斜常常来源于MapReduce的默认分区器</li>
</ul>
</li>
<li><p>数据倾斜会导致map和reduce的任务执行时间大为延长，也会让需要缓存数据集的操作消耗更多的内存资源</p>
</li>
</ul>
<h4 id="4-1-如何诊断是否存在数据倾斜（10分钟）"><a href="#4-1-如何诊断是否存在数据倾斜（10分钟）" class="headerlink" title="4.1 如何诊断是否存在数据倾斜（10分钟）"></a>4.1 如何诊断是否存在数据倾斜（10分钟）</h4><ol start="2">
<li>如何诊断哪些键存在数据倾斜？<ul>
<li>发现倾斜数据之后，有必要诊断造成数据倾斜的那些键。有一个简便方法就是在代码里实现追踪每个键的<strong>最大值</strong>。</li>
<li>为了减少追踪量，可以设置数据量阀值，只追踪那些数据量大于阀值的键，并输出到日志中。实现代码如下</li>
<li>运行作业后就可以从日志中判断发生倾斜的键以及倾斜程度；跟踪倾斜数据是了解数据的重要一步，也是设计MapReduce作业的重要基础</li>
</ul>
</li>
</ol>
<pre><code class="java">   package com.kaikeba.hadoop.dataskew;

   import org.apache.hadoop.io.IntWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Reducer;
   import org.apache.log4j.Logger;

   import java.io.IOException;

   public class WordCountReduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {

       private int maxValueThreshold;

       //日志类
       private static final Logger LOGGER = Logger.getLogger(WordCountReduce.class);

       @Override
       protected void setup(Context context) throws IOException, InterruptedException {

           //一个键达到多少后，会做数据倾斜记录
           maxValueThreshold = 10000;
       }

       /*
               (hello, 1)
               (hello, 1)
               (hello, 1)
               ...
               (spark, 1)

               key: hello
               value: List(1, 1, 1)
           */
       public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                             Context context) throws IOException, InterruptedException {
           int sum = 0;
           //用于记录键出现的次数
           int i = 0;

           for (IntWritable count : values) {
               sum += count.get();
               i++;
           }

           //如果当前键超过10000个，则打印日志
           if(i &gt; maxValueThreshold) {
               LOGGER.info(&quot;Received &quot; + i + &quot; values for key &quot; + key);
           }

           context.write(key, new IntWritable(sum));// 输出最终结果
       };
   }
</code></pre>
<h4 id="4-2-减缓数据倾斜"><a href="#4-2-减缓数据倾斜" class="headerlink" title="4.2 减缓数据倾斜"></a>4.2 减缓数据倾斜</h4><ul>
<li><p>Reduce数据倾斜一般是指map的输出数据中存在数据频率倾斜的状况，即部分输出键的数据量远远大于其它的输出键</p>
</li>
<li><p>如何减小reduce端数据倾斜的性能损失？常用方式有：</p>
<ul>
<li><p>一、自定义分区</p>
<ul>
<li><p>基于输出键的背景知识进行自定义分区。</p>
</li>
<li><p>例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的一部分reduce实例。而将其他的都发送给剩余的reduce实例。</p>
</li>
</ul>
</li>
<li><p>二、Combine</p>
<ul>
<li>使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。</li>
<li>combine的目的就是聚合并精简数据。</li>
</ul>
</li>
<li><p>三、抽样和范围分区</p>
<ul>
<li><p>Hadoop默认的分区器是HashPartitioner，基于map输出键的哈希值分区。这仅在数据分布比较均匀时比较好。<strong>在有数据倾斜时就很有问题</strong>。</p>
</li>
<li><p>使用分区器需要首先了解数据的特性。<strong>TotalOrderPartitioner</strong>中，可以通过对原始数据进行抽样得到的结果集来<strong>预设分区边界值</strong>。</p>
</li>
<li>TotalOrderPartitioner中的范围分区器可以通过预设的分区边界值进行分区。因此它也可以很好地用在矫正数据中的部分键的数据倾斜问题。</li>
</ul>
</li>
<li><p>四、数据大小倾斜的自定义策略</p>
<ul>
<li><p>在map端或reduce端的数据大小倾斜都会对缓存造成较大的影响，乃至导致OutOfMemoryError异常。处理这种情况并不容易。可以参考以下方法。</p>
</li>
<li><p>设置mapreduce.input.linerecordreader.line.maxlength来限制RecordReader读取的最大长度。</p>
</li>
<li>RecordReader在TextInputFormat和KeyValueTextInputFormat类中使用。默认长度没有上限。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="5-MR调优"><a href="#5-MR调优" class="headerlink" title="5. MR调优"></a>5. MR调优</h3><ul>
<li>见后续文章</li>
</ul>
<h3 id="6-抽样、范围分区"><a href="#6-抽样、范围分区" class="headerlink" title="6. 抽样、范围分区"></a>6. 抽样、范围分区</h3><h4 id="6-1-数据"><a href="#6-1-数据" class="headerlink" title="6.1 数据"></a>6.1 数据</h4><ul>
<li><p>数据：气象站气象数据，来源美国国家气候数据中心（NCDC）（1900-2000年数据，每年一个文件）</p>
<ul>
<li>气候数据record的格式如下</li>
</ul>
</li>
</ul>
<p><img src="assets/Image201907151554.png" alt=""></p>
<h4 id="6-2-需求"><a href="#6-2-需求" class="headerlink" title="6.2 需求"></a>6.2 需求</h4><ul>
<li>对气象数据，按照气温进行排序（气温符合正太分布）</li>
</ul>
<h4 id="6-3-实现方案"><a href="#6-3-实现方案" class="headerlink" title="6.3 实现方案"></a>6.3 实现方案</h4><ul>
<li><p>三种实现思路</p>
<ul>
<li>方案一：<ul>
<li>设置一个分区，即一个reduce任务；在一个reduce中对结果进行排序；</li>
<li>失去了MR框架并行计算的优势</li>
</ul>
</li>
<li>方案二：<ul>
<li>自定义分区，人为指定各温度区间的记录，落入哪个分区；如分区温度边界值分别是-15、0、20，共4个分区</li>
<li>但由于对整个数据集的气温分布不了解，可能某些分区的数据量大，其它的分区小，数据倾斜</li>
</ul>
</li>
<li>方案三：<ul>
<li>通过对键空间采样</li>
<li>只查看一小部分键，获得键的近似分布（好温度的近似分布）</li>
<li>进而据此结果创建分区，实现尽可能的均匀的划分数据集；</li>
<li>Hadoop内置了采样器；InputSampler</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="6-4-MR代码"><a href="#6-4-MR代码" class="headerlink" title="6.4 MR代码"></a>6.4 MR代码</h4><blockquote>
<p>分两大步</p>
</blockquote>
<ul>
<li><p>一、先将数据按气温对天气数据集排序。结果存储为sequencefile文件，气温作为输出键，数据行作为输出值</p>
</li>
<li><p>代码</p>
<blockquote>
<p>此代码处理原始日志文件</p>
<p>结果用SequenceFile格式存储；</p>
<p>温度作为SequenceFile的key；记录作为value</p>
</blockquote>
</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.totalorder;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;

import java.io.IOException;

/**
 * 此代码处理原始日志文件 1901
 * 结果用SequenceFile格式存储；
 * 温度作为SequenceFile的key；记录作为value
 */
public class SortDataPreprocessor {

  //输出的key\value分别是气温、记录
  static class CleanerMapper extends Mapper&lt;LongWritable, Text, IntWritable, Text&gt; {

    private NcdcRecordParser parser = new NcdcRecordParser();
    private IntWritable temperature = new IntWritable();

    @Override
    protected void map(LongWritable key, Text value, Context context)
        throws IOException, InterruptedException {
      //0029029070999991901010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999
      parser.parse(value);
      if (parser.isValidTemperature()) {//是否是有效的记录
        temperature.set(parser.getAirTemperature());
        context.write(temperature, value);
      }
    }
  }


  //两个参数：/ncdc/input /ncdc/sfoutput
  public static void main(String[] args) throws Exception {

    if (args.length != 2) {
      System.out.println(&quot;&lt;input&gt; &lt;output&gt;&quot;);
    }

    Configuration conf = new Configuration();

    Job job = Job.getInstance(conf, SortDataPreprocessor.class.getSimpleName());
    job.setJarByClass(SortDataPreprocessor.class);
    //
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));

    job.setMapperClass(CleanerMapper.class);
    //最终输出的键、值类型
    job.setOutputKeyClass(IntWritable.class);
    job.setOutputValueClass(Text.class);
    //reduce个数为0
    job.setNumReduceTasks(0);
    //以sequencefile的格式输出
    job.setOutputFormatClass(SequenceFileOutputFormat.class);

    //开启job输出压缩功能
    //方案一
    conf.set(&quot;mapreduce.output.fileoutputformat.compress&quot;, &quot;true&quot;);
    conf.set(&quot;mapreduce.output.fileoutputformat.compress.type&quot;,&quot;RECORD&quot;);
    //指定job输出使用的压缩算法
    conf.set(&quot;mapreduce.output.fileoutputformat.compress.codec&quot;, &quot;org.apache.hadoop.io.compress.BZip2Codec&quot;);

    //方案二
    //设置sequencefile的压缩、压缩算法、sequencefile文件压缩格式block
    //SequenceFileOutputFormat.setCompressOutput(job, true);
    //SequenceFileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
    //SequenceFileOutputFormat.setOutputCompressorClass(job, SnappyCodec.class);
    //SequenceFileOutputFormat.setOutputCompressionType(job, SequenceFile.CompressionType.BLOCK);

    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
</code></pre>
<ul>
<li><p>二、全局排序</p>
<blockquote>
<p>使用全排序分区器TotalOrderPartitioner</p>
</blockquote>
</li>
</ul>
<pre><code class="java">package com.kaikeba.hadoop.totalorder;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
import org.apache.hadoop.mapreduce.lib.partition.InputSampler;
import org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner;

import java.net.URI;

/**
 * 使用TotalOrderPartitioner全局排序一个SequenceFile文件的内容；
 * 此文件是SortDataPreprocessor的输出文件；
 * key是IntWritble，气象记录中的温度
 */
public class SortByTemperatureUsingTotalOrderPartitioner{

  /**
   * 两个参数：/ncdc/sfoutput /ncdc/totalorder
   * 第一个参数是SortDataPreprocessor的输出文件
   */
  public static void main(String[] args) throws Exception {
    if (args.length != 2) {
      System.out.println(&quot;&lt;input&gt; &lt;output&gt;&quot;);
    }

    Configuration conf = new Configuration();

    Job job = Job.getInstance(conf, SortByTemperatureUsingTotalOrderPartitioner.class.getSimpleName());
    job.setJarByClass(SortByTemperatureUsingTotalOrderPartitioner.class);

    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));

    //输入文件是SequenceFile
    job.setInputFormatClass(SequenceFileInputFormat.class);

    //Hadoop提供的方法来实现全局排序，要求Mapper的输入、输出的key必须保持类型一致
    job.setOutputKeyClass(IntWritable.class);
    job.setOutputFormatClass(SequenceFileOutputFormat.class);

    //分区器：全局排序分区器
    job.setPartitionerClass(TotalOrderPartitioner.class);

    //分了3个区；且分区i-1中的key小于i分区中所有的键
    job.setNumReduceTasks(3);

    /**
     * 随机采样器从所有的分片中采样
     * 每一个参数：采样率；
     * 第二个参数：总的采样数
     * 第三个参数：采样的最大分区数；
     * 只要numSamples和maxSplitSampled（第二、第三参数）任一条件满足，则停止采样
     */
    InputSampler.Sampler&lt;IntWritable, Text&gt; sampler =
            new InputSampler.RandomSampler&lt;IntWritable, Text&gt;(0.1, 5000, 10);
//    TotalOrderPartitioner.setPartitionFile();
    /**
     * 存储定义分区的键；即整个数据集中温度的大致分布情况；
     * 由TotalOrderPartitioner读取，作为全排序的分区依据，让每个分区中的数据量近似
     */
    InputSampler.writePartitionFile(job, sampler);

    //根据上边的SequenceFile文件（包含键的近似分布情况），创建分区
    String partitionFile = TotalOrderPartitioner.getPartitionFile(job.getConfiguration());
    URI partitionUri = new URI(partitionFile);

//    JobConf jobConf = new JobConf();

    //与所有map任务共享此文件，添加到分布式缓存中
    DistributedCache.addCacheFile(partitionUri, job.getConfiguration());
//    job.addCacheFile(partitionUri);

    //方案一：输出的文件RECORD级别，使用BZip2Codec进行压缩
    conf.set(&quot;mapreduce.output.fileoutputformat.compress&quot;, &quot;true&quot;);
    conf.set(&quot;mapreduce.output.fileoutputformat.compress.type&quot;,&quot;RECORD&quot;);
    //指定job输出使用的压缩算法
    conf.set(&quot;mapreduce.output.fileoutputformat.compress.codec&quot;, &quot;org.apache.hadoop.io.compress.BZip2Codec&quot;);

    //方案二
    //SequenceFileOutputFormat.setCompressOutput(job, true);
    //SequenceFileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
    //SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);

    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}

</code></pre>
<h4 id="6-5-总结"><a href="#6-5-总结" class="headerlink" title="6.5 总结"></a>6.5 总结</h4><ul>
<li><p>对大量数据进行全局排序</p>
<ul>
<li><p>先使用InputSampler.Sampler采样器，对整个key空间进行采样，得到key的近似分布</p>
</li>
<li><p>保存到key分布情况文件中</p>
</li>
<li><p>使用TotalOrderPartitioner，利用上边的key分布情况文件，进行分区；每个分区的数据量近似，从而防止数据倾斜</p>
</li>
</ul>
</li>
</ul>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><ol>
<li>描述MR的shuffle全流程（面试）</li>
<li>搭建MAVEN工程，统计词频，并提交集群运行，查看结果</li>
<li>利用搜狗数据，找出所有独立的uid并写入HDFS</li>
<li>利用搜狗数据，找出所有独立的uid出现次数，并写入HDFS，并要求使用Map端的Combine操作</li>
<li>谈谈什么是数据倾斜，什么情况会造成数据倾斜？（面试）</li>
<li>对MR数据倾斜，如何解决？（面试）</li>
</ol>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="https://blog.sev7e0.site/">大数据施工现场</a></span>
        <span>/</span>
        
        <span><a href="https://wangchujiang.com/linux-command/">linux命令行工具</a></span>
        <span>/</span>
        
        <span><a href="https://github.com/orchid-ding">github</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="/js/gitment.js"></script>
<script>
    var gitment = new Gitment({
        id: 'MapReduce编程（三）',
        owner: 'orchid-ding',
        repo: 'kfly-blog-comment',
        oauth: {
            client_id: '0770cdab79393197b6f5',
            client_secret: '376fb6c7bcd5047718b356712f596b89e490360c',
        },
    })
    gitment.render('comment-container')
</script>




</html>
