<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="天行健、君子以自强不息；地势坤，君子以厚德载物。">
    <meta name="keyword"  content="兰草">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        SparkCore/sql知识梳理 - kfly的博客 | kfly&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_1598291_q3el2wqimj.css" type="text/css">
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>kfly</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont iconhome"></i>
                    <span>主页</span>
                </a>
            </li>
 	   <li >
                <a href="/spec/">
                    <i class="iconfont iconzhuanti"></i>
                    <span>专题</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>简历</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一、Spark-Code"><span class="toc-text">一、Spark Code</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark简介"><span class="toc-text">Spark简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-spark是什么"><span class="toc-text">1. spark是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-spark的四大特性"><span class="toc-text">2. spark的四大特性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-速度快"><span class="toc-text">2.1 速度快</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-易用性"><span class="toc-text">2.2 易用性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-通用性"><span class="toc-text">2.3 通用性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-兼容性"><span class="toc-text">2.4 兼容性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-spark集群架构"><span class="toc-text">3. spark集群架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Spark启动停止"><span class="toc-text">4. Spark启动停止</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-初识spark程序"><span class="toc-text">5. 初识spark程序</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、SparkRDD"><span class="toc-text">二、SparkRDD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-RDD是什么"><span class="toc-text">1. RDD是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-RDD的算子分类"><span class="toc-text">2. RDD的算子分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-RDD的依赖关系"><span class="toc-text">4. RDD的依赖关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-lineage（血统）"><span class="toc-text">5. lineage（血统）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-RDD的缓存机制"><span class="toc-text">6. RDD的缓存机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-什么是rdd的缓存"><span class="toc-text">6.1 什么是rdd的缓存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-如何对rdd设置缓存"><span class="toc-text">6.2 如何对rdd设置缓存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-cache和persist区别"><span class="toc-text">6.3 cache和persist区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-4-什么时候设置缓存"><span class="toc-text">6.4 什么时候设置缓存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-5-清除缓存数据"><span class="toc-text">6.5  清除缓存数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-RDD的checkpoint机制"><span class="toc-text">7. RDD的checkpoint机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-1-checkpoint概念"><span class="toc-text">7.1 checkpoint概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-如何设置checkpoint"><span class="toc-text">7.2 如何设置checkpoint</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-3-cache、persist、checkpoint三者区别"><span class="toc-text">7.3 cache、persist、checkpoint三者区别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-DAG有向无环图生成"><span class="toc-text">8. DAG有向无环图生成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-DAG是什么"><span class="toc-text">8.1 DAG是什么</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-DAG划分stage"><span class="toc-text">9. DAG划分stage</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#9-1-stage是什么"><span class="toc-text">9.1 stage是什么</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-2-为什么要划分stage"><span class="toc-text">9.2 为什么要划分stage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-3-如何划分stage"><span class="toc-text">9.3 如何划分stage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-4-stage与stage之间的关系"><span class="toc-text">9.4 stage与stage之间的关系</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#三、Spark任务调度"><span class="toc-text">三、Spark任务调度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-spark的任务调度"><span class="toc-text">1. spark的任务调度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-spark的运行架构"><span class="toc-text">2. spark的运行架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-基于wordcount程序剖析spark任务的提交、划分、调度流程"><span class="toc-text">3.  基于wordcount程序剖析spark任务的提交、划分、调度流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-spark自定义分区"><span class="toc-text">4. spark自定义分区</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-自定义分区说明"><span class="toc-text">4.1 自定义分区说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-自定义partitioner"><span class="toc-text">4.2 自定义partitioner</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-spark的共享变量"><span class="toc-text">5. spark的共享变量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-spark的广播变量-broadcast-variable"><span class="toc-text">5.1 spark的广播变量(broadcast variable)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#5-1-1-广播变量原理"><span class="toc-text">5.1.1 广播变量原理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-1-2-广播变量使用"><span class="toc-text">5.1.2 广播变量使用</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-1-3-广播变量使用注意事项"><span class="toc-text">5.1.3 广播变量使用注意事项</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-spark的累加器-accumulator"><span class="toc-text">5.2 spark的累加器(accumulator)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-spark程序的序列化问题"><span class="toc-text">6. spark程序的序列化问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-transformation操作为什么需要序列化"><span class="toc-text">6.1 transformation操作为什么需要序列化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-spark的任务序列化异常"><span class="toc-text">6.2 spark的任务序列化异常</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-spark中解决序列化的办法"><span class="toc-text">6.3 spark中解决序列化的办法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-application、job、stage、task之间的关系"><span class="toc-text">7. application、job、stage、task之间的关系</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#四、Spark内存计算框架"><span class="toc-text">四、Spark内存计算框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-两种计算模型"><span class="toc-text">1. 两种计算模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-collect-算子操作剖析"><span class="toc-text">2. collect 算子操作剖析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-spark任务中资源参数剖析"><span class="toc-text">3. spark任务中资源参数剖析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-spark的shuffle原理分析"><span class="toc-text">6. spark的shuffle原理分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-shuffle概述"><span class="toc-text">6.1 shuffle概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-spark中的shuffle介绍"><span class="toc-text">6.2 spark中的shuffle介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-HashShuffle机制"><span class="toc-text">6.3 HashShuffle机制</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#6-3-1-HashShuffle概述"><span class="toc-text">6.3.1 HashShuffle概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#6-3-2-普通机制的Hash-shuffle"><span class="toc-text">6.3.2 普通机制的Hash shuffle</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#6-3-3-合并机制的Hash-shuffle"><span class="toc-text">6.3.3 合并机制的Hash shuffle</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-4-Sort-shuffle"><span class="toc-text">6.4 Sort shuffle</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#6-4-1-Sort-shuffle的普通机制"><span class="toc-text">6.4.1 Sort shuffle的普通机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#6-4-2-bypass模式的sortShuffle"><span class="toc-text">6.4.2 bypass模式的sortShuffle</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#五、Spark-SQL"><span class="toc-text">五、Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-sparksql概述"><span class="toc-text">1.sparksql概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-sparksql的前世今生"><span class="toc-text">1.1 sparksql的前世今生</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-什么是sparksql"><span class="toc-text">1.2 什么是sparksql</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-sparksql的四大特性"><span class="toc-text">2. sparksql的四大特性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-DataFrame概述"><span class="toc-text">3. DataFrame概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-DataFrame发展"><span class="toc-text">3.1 DataFrame发展</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-DataFrame是什么"><span class="toc-text">3.2 DataFrame是什么</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-DataFrame和RDD的优缺点"><span class="toc-text">3.3 DataFrame和RDD的优缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-DataFrame使用"><span class="toc-text">4. DataFrame使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-DataSet概述"><span class="toc-text">5. DataSet概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-DataSet是什么"><span class="toc-text">5.1 DataSet是什么</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-DataSet的区别"><span class="toc-text">5.2 DataSet的区别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Spark-IDEA开发"><span class="toc-text">6. Spark IDEA开发</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Spark自定义函数"><span class="toc-text">7 Spark自定义函数</span></a></li></ol></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        SparkCore/sql知识梳理
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-12-27 10:26:52</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#spark sql" title="spark sql">spark sql</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#spark core" title="spark core">spark core</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h1 id="一、Spark-Code"><a href="#一、Spark-Code" class="headerlink" title="一、Spark Code"></a>一、Spark Code</h1><h2 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h2><h3 id="1-spark是什么"><a href="#1-spark是什么" class="headerlink" title="1. spark是什么"></a>1. spark是什么</h3><ul>
<li><p><strong>Apache Spark™</strong> is a unified analytics engine for large-scale data processing.</p>
</li>
<li><p>spark是针对于大规模数据处理的统一分析引擎</p>
<pre><code>    spark是在Hadoop基础上的改进，是UC Berkeley AMP lab所开源的类Hadoop MapReduce的通用的并行计算框架，Spark基于map reduce算法实现的分布式计算，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是Job中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的map reduce的算法。

    spark是基于内存计算框架，计算速度非常之快，但是它仅仅只是涉及到计算，并没有涉及到数据的存储，后期需要使用spark对接外部的数据源，比如hdfs。
</code></pre></li>
</ul>
<h3 id="2-spark的四大特性"><a href="#2-spark的四大特性" class="headerlink" title="2. spark的四大特性"></a>2. spark的四大特性</h3><h4 id="2-1-速度快"><a href="#2-1-速度快" class="headerlink" title="2.1 速度快"></a>2.1 速度快</h4><ul>
<li><p>运行速度提高100倍</p>
<ul>
<li>Apache Spark使用最先进的DAG调度程序，查询优化程序和物理执行引擎，实现批量和流式数据的高性能。</li>
</ul>
</li>
<li><p>spark比mapreduce快的2个主要原因</p>
<ul>
<li><p>1、==基于内存==</p>
<p>~~~<br>（1）mapreduce任务后期再计算的时候，每一个job的输出结果会落地到磁盘，后续有其他的job需要依赖于前面job的输出结果，这个时候就需要进行大量的磁盘io操作。性能就比较低。</p>
</li>
</ul>
</li>
</ul>
<pre><code>（2）spark任务后期再计算的时候，job的输出结果可以保存在内存中，后续有其他的job需要依赖于前面job的输出结果，这个时候就直接从内存中获取得到，避免了磁盘io操作，性能比较高

~~~
</code></pre><ul>
<li><p>2、==进程与线程==</p>
<pre><code>（1）mapreduce任务以进程的方式运行在yarn集群中，比如程序中有100个MapTask，一个task就需要一个进程，这些task要运行就需要开启100个进程。

（2）spark任务以线程的方式运行在进程中，比如程序中有100个MapTask，后期一个task就对应一个线程，这里就不在是进程，这些task需要运行，这里可以极端一点：
只需要开启1个进程，在这个进程中启动100个线程就可以了。
进程中可以启动很多个线程，而开启一个进程与开启一个线程需要的时间和调度代价是不一样。 开启一个进程需要的时间远远大于开启一个线程。
</code></pre></li>
</ul>
<h4 id="2-2-易用性"><a href="#2-2-易用性" class="headerlink" title="2.2 易用性"></a>2.2 易用性</h4><ul>
<li>可以快速去编写spark程序通过 java/scala/python/R/SQL等不同语言</li>
</ul>
<h4 id="2-3-通用性"><a href="#2-3-通用性" class="headerlink" title="2.3 通用性"></a>2.3 通用性</h4><ul>
<li>spark框架不在是一个简单的框架，可以把spark理解成一个==<strong>生态系统</strong>==，它内部是包含了很多模块，基于不同的应用场景可以选择对应的模块去使用<ul>
<li>==<strong>sparksql</strong>==<ul>
<li>通过sql去开发spark程序做一些离线分析</li>
</ul>
</li>
<li>==<strong>sparkStreaming</strong>==<ul>
<li>主要是用来解决公司有实时计算的这种场景</li>
</ul>
</li>
<li>==<strong>Mlib</strong>==<ul>
<li>它封装了一些机器学习的算法库</li>
</ul>
</li>
<li>==<strong>Graphx</strong>==<ul>
<li>图计算</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2-4-兼容性"><a href="#2-4-兼容性" class="headerlink" title="2.4 兼容性"></a>2.4 兼容性</h4><p>spark程序就是一个计算逻辑程序，这个任务要运行就需要计算资源（内存、cpu、磁盘），哪里可以给当前这个任务提供计算资源，就可以把spark程序提交到哪里去运行</p>
<ul>
<li>==<strong>standAlone</strong>==<ul>
<li>它是spark自带的独立运行模式，整个任务的资源分配由spark集群的老大Master负责</li>
</ul>
</li>
<li>==<strong>yarn</strong>==<ul>
<li>可以把spark程序提交到yarn中运行，整个任务的资源分配由yarn中的老大ResourceManager负责</li>
</ul>
</li>
<li><strong>mesos</strong><ul>
<li>它也是apache开源的一个类似于yarn的资源调度平台</li>
</ul>
</li>
</ul>
<h3 id="3-spark集群架构"><a href="#3-spark集群架构" class="headerlink" title="3. spark集群架构"></a>3. spark集群架构</h3><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/spark.png" alt="spark"></p>
<ul>
<li><p><strong>==Driver==</strong></p>
<ul>
<li>它会执行客户端写好的main方法，它会构建一个名叫SparkContext对象<ul>
<li>该对象是所有spark程序的执行入口</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>==Application==</strong></p>
<ul>
<li>就是一个spark的应用程序，它是包含了客户端的代码和任务运行的资源信息</li>
</ul>
</li>
<li><p><strong>==ClusterManager==</strong></p>
<ul>
<li>它是给程序提供计算资源的外部服务<ul>
<li><strong>standAlone</strong><ul>
<li>它是spark自带的集群模式，整个任务的资源分配由spark集群的老大Master负责</li>
</ul>
</li>
<li><strong>yarn</strong><ul>
<li>可以把spark程序提交到yarn中运行，整个任务的资源分配由yarn中的老大ResourceManager负责</li>
</ul>
</li>
<li><strong>mesos</strong><ul>
<li>它也是apache开源的一个类似于yarn的资源调度平台。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>==Master==</strong></p>
<ul>
<li>它是整个spark集群的主节点，负责任务资源的分配</li>
</ul>
</li>
<li><p><strong>==Worker==</strong></p>
<ul>
<li>它是整个spark集群的从节点，负责任务计算的节点</li>
</ul>
</li>
<li><p><strong>==Executor==</strong></p>
<ul>
<li>它是一个进程，它会在worker节点启动该进程（计算资源）</li>
</ul>
</li>
<li><p><strong>==Task==</strong></p>
<ul>
<li>spark任务是以task线程的方式运行在worker节点对应的executor进程中</li>
</ul>
</li>
</ul>
<h3 id="4-Spark启动停止"><a href="#4-Spark启动停止" class="headerlink" title="4. Spark启动停止"></a>4. Spark启动停止</h3><pre><code class="reStructuredText">(1) 如何恢复到上一次活着master挂掉之前的状态?
    在高可用模式下，整个spark集群就有很多个master，其中只有一个master被zk选举成活着的master，其他的多个master都处于standby，同时把整个spark集群的元数据信息通过zk中节点进行保存。

    后期如果活着的master挂掉。首先zk会感知到活着的master挂掉，下面开始在多个处于standby中的master进行选举，再次产生一个活着的master，这个活着的master会读取保存在zk节点中的spark集群元数据信息，恢复到上一次master的状态。整个过程在恢复的时候经历过了很多个不同的阶段，每个阶段都需要一定时间，最终恢复到上个活着的master的转态，整个恢复过程一般需要1-2分钟。

(2) 在master的恢复阶段对任务的影响?
   a）对已经运行的任务是没有任何影响
         由于该任务正在运行，说明它已经拿到了计算资源，这个时候就不需要master。
   b) 对即将要提交的任务是有影响
         由于该任务需要有计算资源，这个时候会找活着的master去申请计算资源，由于没有一个活着的master,该任务是获取不到计算资源，也就是任务无法运行。
</code></pre>
<h3 id="5-初识spark程序"><a href="#5-初识spark程序" class="headerlink" title="5. 初识spark程序"></a>5. 初识spark程序</h3><pre><code class="shell">bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://node01:7077 \
--executor-memory 1G \
--total-executor-cores 2 \
examples/jars/spark-examples_2.11-2.3.3.jar \
10

####参数说明
--class：指定包含main方法的主类
--master：指定spark集群master地址
--executor-memory：指定任务在运行的时候需要的每一个executor内存大小
--total-executor-cores： 指定任务在运行的时候需要总的cpu核数

### 注意
spark集群中有很多个master，并不知道哪一个master是活着的master，即使你知道哪一个master是活着的master，它也有可能下一秒就挂掉，这里就可以把所有master都罗列出来
--master spark://node01:7077,node02:7077,node03:7077

后期程序会轮训整个master列表，最终找到活着的master，然后向它申请计算资源，最后运行程序。
</code></pre>
<h1 id="二、SparkRDD"><a href="#二、SparkRDD" class="headerlink" title="二、SparkRDD"></a>二、SparkRDD</h1><h3 id="1-RDD是什么"><a href="#1-RDD是什么" class="headerlink" title="1. RDD是什么"></a>1. RDD是什么</h3><ul>
<li><p>RDD（Resilient Distributed Dataset）叫做==弹性分布式数据集==，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合.</p>
<ul>
<li><strong>Dataset</strong>:          就是一个集合，存储很多数据.</li>
<li><strong>Distributed</strong>：它内部的元素进行了分布式存储，方便于后期进行分布式计算.</li>
<li><strong>Resilient</strong>：     表示弹性，rdd的数据是可以保存在内存或者是磁盘中.</li>
</ul>
</li>
<li><p>（1）A list of partitions</p>
<ul>
<li>==一个分区（Partition）列表，数据集的基本组成单位。==</li>
</ul>
</li>
</ul>
<pre><code>    这里表示一个rdd有很多分区，每一个分区内部是包含了该rdd的部分数据，
spark中任务是以task线程的方式运行， 一个分区就对应一个task线程。

    用户可以在创建RDD时指定RDD的分区个数，如果没有指定，那么就会采用默认值。
    val rdd=sparkContext.textFile(&quot;/words.txt&quot;)
    如果该文件的block块个数小于等于2，这里生产的RDD分区数就为2
    如果该文件的block块个数大于2，这里生产的RDD分区数就与block块个数保持一致
</code></pre><ul>
<li>（2）A function for computing each split<ul>
<li>==一个计算每个分区的函数==</li>
</ul>
</li>
</ul>
<pre><code>    Spark中RDD的计算是以分区为单位的，每个RDD都会实现compute计算函数以达到这个目的.
</code></pre><ul>
<li>（3）A list of dependencies on other RDDs<ul>
<li>==一个rdd会依赖于其他多个rdd==</li>
</ul>
</li>
</ul>
<pre><code>  这里就涉及到rdd与rdd之间的依赖关系，spark任务的容错机制就是根据这个特性（血统）而来。
</code></pre><ul>
<li>（4）Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<ul>
<li>==一个Partitioner，即RDD的分区函数（可选项）==</li>
</ul>
</li>
</ul>
<pre><code>当前Spark中实现了两种类型的分区函数，
一个是基于哈希的HashPartitioner，(key.hashcode % 分区数= 分区号)
另外一个是基于范围的RangePartitioner。
只有对于key-value的RDD,并且产生shuffle，才会有Partitioner，

非key-value的RDD的Parititioner的值是None。
</code></pre><ul>
<li>（5）Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)<ul>
<li>==一个列表，存储每个Partition的优先位置(可选项)==</li>
</ul>
</li>
</ul>
<pre><code>这里涉及到数据的本地性，数据块位置最优。
spark任务在调度的时候会优先考虑存有数据的节点开启计算任务，减少数据的网络传输，提升计算效率。
</code></pre><ul>
<li>流程分析</li>
</ul>
<p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/rdd的五大属性.png" alt="rdd的五大属性"></p>
<h3 id="2-RDD的算子分类"><a href="#2-RDD的算子分类" class="headerlink" title="2. RDD的算子分类"></a>2. RDD的算子分类</h3><ul>
<li>1、==transformation（转换）==<ul>
<li>根据已经存在的rdd转换生成一个新的rdd,  它是延迟加载，它不会立即执行</li>
<li>例如<ul>
<li>map / flatMap / reduceByKey 等</li>
</ul>
</li>
</ul>
</li>
<li>2、==action (动作)==<ul>
<li>它会真正触发任务的运行<ul>
<li>将rdd的计算的结果数据返回给Driver端，或者是保存结果数据到外部存储介质中</li>
</ul>
</li>
<li>例如<ul>
<li>collect / saveAsTextFile 等</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4-RDD的依赖关系"><a href="#4-RDD的依赖关系" class="headerlink" title="4. RDD的依赖关系"></a>4. RDD的依赖关系</h3><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/rdd-dependencies.png" alt="rdd-dependencies"></p>
<ul>
<li><p>RDD和它依赖的父RDD的关系有两种不同的类型</p>
</li>
<li><p>窄依赖（narrow dependency）和宽依赖（wide dependency）</p>
<ul>
<li><p>==窄依赖==</p>
<ul>
<li><p>窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用</p>
<ul>
<li>总结：窄依赖我们形象的比喻为独生子女</li>
</ul>
<pre><code>哪些算子操作是窄依赖：
    map/flatMap/filter/union等等

    所有的窄依赖不会产生shuffle
</code></pre></li>
</ul>
</li>
<li><p>==宽依赖==</p>
<ul>
<li><p>宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition</p>
<ul>
<li>总结：宽依赖我们形象的比喻为超生 </li>
</ul>
<pre><code>哪些算子操作是宽依赖：
    reduceByKey/sortByKey/groupBy/groupByKey/join等等

    所有的宽依赖会产生shuffle
</code></pre></li>
</ul>
</li>
<li><p>补充说明</p>
<pre><code>由上图可知，join分为宽依赖和窄依赖，如果RDD有相同的partitioner，那么将不会引起shuffle，这种join是窄依赖，反之就是宽依赖
</code></pre></li>
</ul>
</li>
</ul>
<h3 id="5-lineage（血统）"><a href="#5-lineage（血统）" class="headerlink" title="5. lineage（血统）"></a>5. lineage（血统）</h3><ul>
<li>RDD只支持粗粒度转换<ul>
<li>即只记录单个块上执行的单个操作。</li>
</ul>
</li>
<li>将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区</li>
<li>==RDD的Lineage会记录RDD的元数据信息和转换行为，lineage保存了RDD的依赖关系，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区==。</li>
</ul>
<h3 id="6-RDD的缓存机制"><a href="#6-RDD的缓存机制" class="headerlink" title="6. RDD的缓存机制"></a>6. RDD的缓存机制</h3><h4 id="6-1-什么是rdd的缓存"><a href="#6-1-什么是rdd的缓存" class="headerlink" title="6.1 什么是rdd的缓存"></a>6.1 什么是rdd的缓存</h4><pre><code>    可以把一个rdd的数据缓存起来，后续有其他的job需要用到该rdd的结果数据，可以直接从缓存中获取得到，避免了重复计算。缓存是加快后续对该数据的访问操作。
</code></pre><h4 id="6-2-如何对rdd设置缓存"><a href="#6-2-如何对rdd设置缓存" class="headerlink" title="6.2 如何对rdd设置缓存"></a>6.2 如何对rdd设置缓存</h4><ul>
<li>RDD通过==persist方法==或==cache方法==可以将前面的计算结果缓存。<ul>
<li>但是并不是这两个方法被调用时立即缓存，而是==触发后面的action==时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</li>
</ul>
</li>
</ul>
<p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569036662039.png" alt="1569036662039"></p>
<ul>
<li>通过查看源码发现==cache最终也是调用了persist方法==，默认的存储级别都是==仅在内存存储一份==，Spark的存储级别还有好多种，存储级别在==object StorageLevel==中定义的。</li>
</ul>
<p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569036703460.png" alt="1569036703460"></p>
<ul>
<li>使用演示</li>
</ul>
<pre><code class="scala">val rdd1=sc.textFile(&quot;/words.txt&quot;)
val rdd2=rdd1.flatMap(_.split(&quot; &quot;))
val rdd3=rdd2.cache
rdd3.collect

val rdd4=rdd3.map((_,1))
val rdd5=rdd4.persist(缓存级别)
rdd5.collect
</code></pre>
<h4 id="6-3-cache和persist区别"><a href="#6-3-cache和persist区别" class="headerlink" title="6.3 cache和persist区别"></a>6.3 cache和persist区别</h4><ul>
<li><p>==面试经常被问到==</p>
<ul>
<li>例如<ul>
<li>==简述下如何对RDD设置缓存，以及它们的区别是什么？==</li>
</ul>
</li>
</ul>
<pre><code>    对RDD设置缓存成可以调用rdd的2个方法： 一个是cache，一个是persist
调用上面2个方法都可以对rdd的数据设置缓存，但不是立即就触发缓存执行，后面需要有action，才会触发缓存的执行。

cache方法和persist方法区别：
    cache:   默认是把数据缓存在内存中，其本质就是调用persist方法；
    persist：可以把数据缓存在内存或者是磁盘，有丰富的缓存级别，这些缓存级别都被定义在StorageLevel这个object中。
</code></pre></li>
</ul>
<h4 id="6-4-什么时候设置缓存"><a href="#6-4-什么时候设置缓存" class="headerlink" title="6.4 什么时候设置缓存"></a>6.4 什么时候设置缓存</h4><ul>
<li>==1、某个rdd的数据后期被使用了多次==</li>
</ul>
<p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569037915592.png" alt="1569037915592"></p>
<pre><code>如上图所示的计算逻辑： 
（1）当第一次使用rdd2做相应的算子操作得到rdd3的时候，就会从rdd1开始计算，先读取HDFS上的文件，然后对rdd1 做对应的算子操作得到rdd2,再由rdd2计算之后得到rdd3。同样为了计算得到rdd4，前面的逻辑会被重新计算。

（2）默认情况下多次对一个rdd执行算子操作， rdd都会对这个rdd及之前的父rdd全部重新计算一次。 这种情况在实际开发代码的时候会经常遇到，但是我们一定要避免一个rdd重复计算多次，否则会导致性能急剧降低。   

总结：
可以把多次使用到的rdd，也就是公共rdd进行持久化，避免后续需要，再次重新计算，提升效率。
</code></pre><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569045749546.png" alt="1569045749546"></p>
<ul>
<li>2、为了获取得到一个rdd的结果数据，经过了大量的算子操作或者是计算逻辑比较复杂<ul>
<li>总之某个rdd的数据来之不易</li>
</ul>
</li>
</ul>
<pre><code>val rdd2=rdd1.flatMap(函数).map(函数).reduceByKey(函数).xxx.xxx.xxx.xxx.xxx
</code></pre><h4 id="6-5-清除缓存数据"><a href="#6-5-清除缓存数据" class="headerlink" title="6.5  清除缓存数据"></a>6.5  清除缓存数据</h4><ul>
<li><p>1、==自动清除==</p>
<pre><code>一个application应用程序结束之后，对应的缓存数据也就自动清除
</code></pre></li>
<li><p>2、==手动清除==</p>
<pre><code>调用rdd的unpersist方法
</code></pre></li>
</ul>
<h3 id="7-RDD的checkpoint机制"><a href="#7-RDD的checkpoint机制" class="headerlink" title="7. RDD的checkpoint机制"></a>7. RDD的checkpoint机制</h3><h4 id="7-1-checkpoint概念"><a href="#7-1-checkpoint概念" class="headerlink" title="7.1 checkpoint概念"></a>7.1 checkpoint概念</h4><ul>
<li><p>我们可以对rdd的数据进行缓存，保存在内存或者是磁盘中。</p>
<ul>
<li><p>后续就可以直接从内存或者磁盘中获取得到，但是它们不是特别安全。</p>
</li>
<li><p><strong>cache</strong></p>
<pre><code>它是直接把数据保存在内存中，后续操作起来速度比较快，直接从内存中获取得到。但这种方式很不安全，由于服务器挂掉或者是进程终止，会导致数据的丢失。
</code></pre></li>
<li><p><strong>persist</strong></p>
<pre><code>它可以把数据保存在本地磁盘中，后续可以从磁盘中获取得到该数据，但它也不是特别安全，由于系统管理员一些误操作删除了，或者是磁盘损坏，也有可能导致数据的丢失。
</code></pre></li>
</ul>
</li>
<li><p>==<strong>checkpoint</strong>（检查点）==</p>
<pre><code>它是提供了一种相对而言更加可靠的数据持久化方式。它是把数据保存在分布式文件系统，
比如HDFS上。这里就是利用了HDFS高可用性，高容错性（多副本）来最大程度保证数据的安全性。
</code></pre></li>
</ul>
<h4 id="7-2-如何设置checkpoint"><a href="#7-2-如何设置checkpoint" class="headerlink" title="7.2 如何设置checkpoint"></a>7.2 如何设置checkpoint</h4><ul>
<li><p>1、在hdfs上设置一个checkpoint目录</p>
<pre><code class="scala">sc.setCheckpointDir(&quot;hdfs://node01:8020/checkpoint&quot;) 
</code></pre>
</li>
<li><p>2、对需要做checkpoint操作的rdd调用checkpoint方法</p>
<pre><code class="scala">val rdd1=sc.textFile(&quot;/words.txt&quot;)
rdd1.checkpoint
val rdd2=rdd1.flatMap(_.split(&quot; &quot;)) 
</code></pre>
</li>
<li><p>3、最后需要有一个action操作去触发任务的运行</p>
<pre><code class="scala">rdd2.collect
</code></pre>
</li>
</ul>
<h4 id="7-3-cache、persist、checkpoint三者区别"><a href="#7-3-cache、persist、checkpoint三者区别" class="headerlink" title="7.3 cache、persist、checkpoint三者区别"></a>7.3 cache、persist、checkpoint三者区别</h4><ul>
<li><p>==cache和persist==</p>
<ul>
<li>cache默认数据缓存在内存中</li>
<li>persist可以把数据保存在内存或者磁盘中</li>
<li>后续要触发 cache 和 persist 持久化操作，需要有一个action操作</li>
<li>它不会开启其他新的任务，一个action操作就对应一个job </li>
<li>它不会改变rdd的依赖关系，程序运行完成后对应的缓存数据就自动消失</li>
</ul>
</li>
<li><p>==checkpoint==</p>
<ul>
<li>可以把数据持久化写入到hdfs上</li>
<li>后续要触发checkpoint持久化操作，需要有一个action操作，后续会开启新的job执行checkpoint操作</li>
<li>它会改变rdd的依赖关系，后续数据丢失了不能够在通过血统进行数据的恢复。</li>
<li>程序运行完成后对应的checkpoint数据就不会消失</li>
</ul>
</li>
</ul>
<pre><code class="scala">   sc.setCheckpointDir(&quot;/checkpoint&quot;)
   val rdd1=sc.textFile(&quot;/words.txt&quot;)
   val rdd2=rdd1.cache
   rdd2.checkpoint
   val rdd3=rdd2.flatMap(_.split(&quot; &quot;))
   rdd3.collect

   checkpoint操作要执行需要有一个action操作，一个action操作对应后续的一个job。该job执行完成之后，它会再次单独开启另外一个job来执行 rdd1.checkpoint操作。

   对checkpoint在使用的时候进行优化，在调用checkpoint操作之前，可以先来做一个cache操作，缓存对应rdd的结果数据，后续就可以直接从cache中获取到rdd的数据写入到指定checkpoint目录中
</code></pre>
<h3 id="8-DAG有向无环图生成"><a href="#8-DAG有向无环图生成" class="headerlink" title="8. DAG有向无环图生成"></a>8. DAG有向无环图生成</h3><h4 id="8-1-DAG是什么"><a href="#8-1-DAG是什么" class="headerlink" title="8.1 DAG是什么"></a>8.1 DAG是什么</h4><ul>
<li><p>==DAG(Directed Acyclic Graph)== 叫做有向无环图（有方向,无闭环,代表着数据的流向），原始的RDD通过一系列的转换就形成了DAG。</p>
</li>
<li><p>下图是基于单词统计逻辑得到的DAG有向无环图</p>
</li>
</ul>
<p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569047954944.png" alt="1569047954944"></p>
<h3 id="9-DAG划分stage"><a href="#9-DAG划分stage" class="headerlink" title="9. DAG划分stage"></a>9. DAG划分stage</h3><h4 id="9-1-stage是什么"><a href="#9-1-stage是什么" class="headerlink" title="9.1 stage是什么"></a>9.1 stage是什么</h4><ul>
<li>==一个Job会被拆分为多组Task，每组任务被称为一个stage==</li>
<li>stage表示不同的调度阶段，一个spark job会对应产生很多个stage<ul>
<li>stage类型一共有2种<ul>
<li>==ShuffleMapStage==<ul>
<li>最后一个shuffle之前的所有变换的Stage叫ShuffleMapStage<ul>
<li>它对应的task是shuffleMapTask</li>
</ul>
</li>
</ul>
</li>
<li>==ResultStage==<ul>
<li>最后一个shuffle之后操作的Stage叫ResultStage，它是最后一个Stage。<ul>
<li>它对应的task是ResultTask</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="9-2-为什么要划分stage"><a href="#9-2-为什么要划分stage" class="headerlink" title="9.2 为什么要划分stage"></a>9.2 为什么要划分stage</h4><pre><code>根据RDD之间依赖关系的不同将DAG划分成不同的Stage(调度阶段)
对于窄依赖，partition的转换处理在一个Stage中完成计算
对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，

由于划分完stage之后，在同一个stage中只有窄依赖，没有宽依赖，可以实现流水线计算，
stage中的每一个分区对应一个task，在同一个stage中就有很多可以并行运行的task。
</code></pre><h4 id="9-3-如何划分stage"><a href="#9-3-如何划分stage" class="headerlink" title="9.3 如何划分stage"></a>9.3 如何划分stage</h4><ul>
<li>==划分stage的依据就是宽依赖==</li>
</ul>
<pre><code>(1) 首先根据rdd的算子操作顺序生成DAG有向无环图，接下里从最后一个rdd往前推，创建一个新的stage，把该rdd加入到该stage中，它是最后一个stage。

(2) 在往前推的过程中运行遇到了窄依赖就把该rdd加入到本stage中，如果遇到了宽依赖，就从宽依赖切开，那么最后一个stage也就结束了。

(3) 重新创建一个新的stage，按照第二个步骤继续往前推，一直到最开始的rdd，整个划分stage也就结束了
</code></pre><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/划分stage.png" alt="划分stage"></p>
<h4 id="9-4-stage与stage之间的关系"><a href="#9-4-stage与stage之间的关系" class="headerlink" title="9.4 stage与stage之间的关系"></a>9.4 stage与stage之间的关系</h4><pre><code>    划分完stage之后，每一个stage中有很多可以并行运行的task，后期把每一个stage中的task封装在一个taskSet集合中，最后把一个一个的taskSet集合提交到worker节点上的executor进程中运行。

rdd与rdd之间存在依赖关系，stage与stage之前也存在依赖关系，前面stage中的task先运行，运行完成了再运行后面stage中的task，也就是说后面stage中的task输入数据是前面stage中task的输出结果数据。
</code></pre><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/stage.png" alt="stage"></p>
<h1 id="三、Spark任务调度"><a href="#三、Spark任务调度" class="headerlink" title="三、Spark任务调度"></a>三、Spark任务调度</h1><h3 id="1-spark的任务调度"><a href="#1-spark的任务调度" class="headerlink" title="1. spark的任务调度"></a>1. spark的任务调度</h3><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/spark任务调度.png" alt="spark任务调度"></p>
<pre><code>(1) Driver端运行客户端的main方法，构建SparkContext对象，在SparkContext对象内部依次构建DAGScheduler和TaskScheduler

(2) 按照rdd的一系列操作顺序，来生成DAG有向无环图

(3) DAGScheduler拿到DAG有向无环图之后，按照宽依赖进行stage的划分。每一个stage内部有很多可以并行运行的task，最后封装在一个一个的taskSet集合中，然后把taskSet发送给TaskScheduler

（4）TaskScheduler得到taskSet集合之后，依次遍历取出每一个task提交到worker节点上的executor进程中运行。

（5）所有task运行完成，整个任务也就结束了
</code></pre><h3 id="2-spark的运行架构"><a href="#2-spark的运行架构" class="headerlink" title="2. spark的运行架构"></a>2. spark的运行架构</h3><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/spark-7527224.png" alt="spark"></p>
<pre><code>(1) Driver端向资源管理器Master发送注册和申请计算资源的请求

(2) Master通知对应的worker节点启动executor进程(计算资源)

(3) executor进程向Driver端发送注册并且申请task请求

(4) Driver端运行客户端的main方法，构建SparkContext对象，在SparkContext对象内部依次构建DAGScheduler和TaskScheduler

(5) 按照客户端代码洪rdd的一系列操作顺序，生成DAG有向无环图

(6) DAGScheduler拿到DAG有向无环图之后，按照宽依赖进行stage的划分。每一个stage内部有很多可以并行运行的task，最后封装在一个一个的taskSet集合中，然后把taskSet发送给TaskScheduler

(7) TaskScheduler得到taskSet集合之后，依次遍历取出每一个task提交到worker节点上的executor进程中运行

(8) 所有task运行完成，Driver端向Master发送注销请求，Master通知Worker关闭executor进程，Worker上的计算资源得到释放，最后整个任务也就结束了。
</code></pre><h3 id="3-基于wordcount程序剖析spark任务的提交、划分、调度流程"><a href="#3-基于wordcount程序剖析spark任务的提交、划分、调度流程" class="headerlink" title="3.  基于wordcount程序剖析spark任务的提交、划分、调度流程"></a>3.  基于wordcount程序剖析spark任务的提交、划分、调度流程</h3><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/job-scheduler-running.png" alt="job-scheduler-running"></p>
<h3 id="4-spark自定义分区"><a href="#4-spark自定义分区" class="headerlink" title="4. spark自定义分区"></a>4. spark自定义分区</h3><h4 id="4-1-自定义分区说明"><a href="#4-1-自定义分区说明" class="headerlink" title="4.1 自定义分区说明"></a>4.1 自定义分区说明</h4><ul>
<li><p>在对RDD数据进行分区时，默认使用的是==HashPartitioner==</p>
</li>
<li><p>该函数对key进行哈希，然后对分区总数取模，取模结果相同的就会被分到同一个partition中</p>
<pre><code>HashPartitioner分区逻辑：
    key.hashcode % 分区总数 = 分区号
</code></pre></li>
<li><p>如果嫌HashPartitioner功能单一，可以自定义partitioner</p>
</li>
</ul>
<h4 id="4-2-自定义partitioner"><a href="#4-2-自定义partitioner" class="headerlink" title="4.2 自定义partitioner"></a>4.2 自定义partitioner</h4><ul>
<li>实现自定义partitioner大致分为3个步骤<ul>
<li>1、继承==org.apache.spark.Partitioner==</li>
<li>2、重写==numPartitions==方法</li>
<li>3、重写==getPartition==方法</li>
</ul>
</li>
</ul>
<pre><code class="scala">//1、对应上面的rdd数据进行自定义分区
 val result: RDD[(String, Int)] = wordLengthRDD.partitionBy(new MyPartitioner(3))

//2、自定义分区
class MyPartitioner(num:Int) extends Partitioner{
  //指定rdd的总的分区数
  override def numPartitions: Int = {
    num
  }
  //消息按照key的某种规则进入到指定的分区号中
  override def getPartition(key: Any): Int ={
    //这里的key就是单词
    val length: Int = key.toString.length
    length match {
      case 4 =&gt;0
      case 5 =&gt;1
    }
  }
}
</code></pre>
<h3 id="5-spark的共享变量"><a href="#5-spark的共享变量" class="headerlink" title="5. spark的共享变量"></a>5. spark的共享变量</h3><h4 id="5-1-spark的广播变量-broadcast-variable"><a href="#5-1-spark的广播变量-broadcast-variable" class="headerlink" title="5.1 spark的广播变量(broadcast variable)"></a>5.1 spark的广播变量(broadcast variable)</h4><ul>
<li>​    Spark中分布式执行的代码需要==传递到各个Executor的Task上运行==。对于一些只读、固定的数据(比如从DB中读出的数据),每次都需要Driver广播到各个Task上，这样效率低下。</li>
<li>​      广播变量允许==将变量只广播给各个Executor==。该Executor上的各个Task再从所在节点的BlockManager获取变量，而不是从Driver获取变量，以减少通信的成本，减少内存的占用，从而提升了效率。</li>
</ul>
<h5 id="5-1-1-广播变量原理"><a href="#5-1-1-广播变量原理" class="headerlink" title="5.1.1 广播变量原理"></a>5.1.1 广播变量原理</h5><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/广播变量.png" alt="广播变量"></p>
<h5 id="5-1-2-广播变量使用"><a href="#5-1-2-广播变量使用" class="headerlink" title="5.1.2 广播变量使用"></a>5.1.2 广播变量使用</h5><pre><code>(1) 通过对一个类型T的对象调用 SparkContext.broadcast创建出一个Broadcast[T]对象。
    任何可序列化的类型都可以这么实现
(2) 通过 value 属性访问该对象的值
(3) 变量只会被发到各个节点一次，应作为只读值处理（修改这个值不会影响到别的节点）
</code></pre><ul>
<li>使用广播变量代码示例</li>
</ul>
<pre><code class="scala">val word=&quot;spark&quot;
val rddData = rdd.collect
//通过调用sparkContext对象的broadcast方法把数据广播出去
val broadCast = sc.broadcast(word)
val broadRddData = sc.broadcast(rddData)

//在executor中通过调用广播变量的value属性获取广播变量的值,分布式环境下广播变量通过网络传输需要序列化
val rdd2=rdd1.flatMap(_.split(&quot; &quot;)).filter(x=&gt;x.equals(broadCast.value))
</code></pre>
<h5 id="5-1-3-广播变量使用注意事项"><a href="#5-1-3-广播变量使用注意事项" class="headerlink" title="5.1.3 广播变量使用注意事项"></a>5.1.3 广播变量使用注意事项</h5><pre><code>1、不能将一个RDD使用广播变量广播出去

2、广播变量只能在Driver端定义，不能在Executor端定义

3、在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值

4、如果executor端用到了Driver的变量，如果不使用广播变量在Executor有多少task就有多少Driver端的变量副本

5、如果Executor端用到了Driver的变量，如果使用广播变量在每个Executor中只有一份Driver端的变量副本
</code></pre><h4 id="5-2-spark的累加器-accumulator"><a href="#5-2-spark的累加器-accumulator" class="headerlink" title="5.2 spark的累加器(accumulator)"></a>5.2 spark的累加器(accumulator)</h4><ul>
<li>累加器（accumulator）是Spark中提供的一种分布式的变量机制，其原理类似于mapreduce，即分布式的改变，然后聚合这些改变</li>
<li>==累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数。可以使用累加器来进行全局的计数</li>
</ul>
<pre><code class="scala">val accumulator = sc.accumulator(0); 
    val result = linesRDD.map(s =&gt; {
      accumulator.add(1) //有一条数据就增加1
    })
</code></pre>
<h3 id="6-spark程序的序列化问题"><a href="#6-spark程序的序列化问题" class="headerlink" title="6. spark程序的序列化问题"></a>6. spark程序的序列化问题</h3><h4 id="6-1-transformation操作为什么需要序列化"><a href="#6-1-transformation操作为什么需要序列化" class="headerlink" title="6.1 transformation操作为什么需要序列化"></a>6.1 transformation操作为什么需要序列化</h4><ul>
<li>spark是分布式执行引擎，其核心抽象是弹性分布式数据集RDD，其代表了分布在不同节点的数据。Spark的计算是在executor上分布式执行的，故用户开发的关于RDD的map，flatMap，reduceByKey等transformation 操作（闭包）有如下执行过程：<ul>
<li>（1）代码中对象在driver本地序列化</li>
<li>（2）对象序列化后传输到远程executor节点</li>
<li>（3）远程executor节点反序列化对象</li>
<li>（4）最终远程节点执行</li>
</ul>
</li>
<li>故对象在执行中需要序列化通过网络传输，则必须经过序列化过程。</li>
</ul>
<h4 id="6-2-spark的任务序列化异常"><a href="#6-2-spark的任务序列化异常" class="headerlink" title="6.2 spark的任务序列化异常"></a>6.2 spark的任务序列化异常</h4><ul>
<li>在编写spark程序中，由于在map，foreachPartition等算子==内部使用了外部定义的变量和函数==，从而引发Task未序列化问题。</li>
<li>然而spark算子在计算过程中使用外部变量在许多情形下确实在所难免，比如在filter算子根据外部指定的条件进行过滤，map根据相应的配置进行变换。</li>
<li>经常会出现“==org.apache.spark.SparkException: Task not serializable==”这个错误<ul>
<li>其原因就在于这些算子使用了==外部的变量==，但是这个变量不能序列化。</li>
<li>当前类使用了“extends Serializable”声明支持序列化，但是由于某些字段==不支持序列化==，仍然会导致整个类序列化时出现问题，最终导致出现Task未序列化问题。</li>
</ul>
</li>
</ul>
<h4 id="6-3-spark中解决序列化的办法"><a href="#6-3-spark中解决序列化的办法" class="headerlink" title="6.3 spark中解决序列化的办法"></a>6.3 spark中解决序列化的办法</h4><ul>
<li>(1) 如果函数中使用了该类对象，该类要实现序列化<ul>
<li>==类  extends  Serializable==</li>
</ul>
</li>
<li>(2) 如果函数中使用了该类对象的成员变量，该类除了要实现序列化之外，所有的成员变量必须要实现序列化</li>
<li>(3) 对于不能序列化的成员变量使用==“@transient”==标注，告诉编译器不需要序列化</li>
<li>(4) 也可将依赖的变量独立放到一个小的class中，让这个class支持序列化，这样做可以减少网络传输量，提高效率。</li>
<li>(5) 可以把对象的创建直接在该函数中构建这样避免需要序列化</li>
</ul>
<h3 id="7-application、job、stage、task之间的关系"><a href="#7-application、job、stage、task之间的关系" class="headerlink" title="7. application、job、stage、task之间的关系"></a>7. application、job、stage、task之间的关系</h3><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/application.png" alt="application"></p>
<ul>
<li>一个application就是一个应用程序，包含了客户端所有的代码和计算资源</li>
<li>一个action操作对应一个DAG有向无环图，即一个action操作就是一个job </li>
<li>一个job中包含了大量的宽依赖，按照宽依赖进行stage划分，一个job产生了很多个stage</li>
<li>一个stage中有很多分区，一个分区就是一个task，即一个stage中有很多个task</li>
<li>==总结==<ul>
<li>一个application包含了很多个job</li>
<li>一个job包含了很多个stage</li>
<li>一个stage包含了很多个task</li>
</ul>
</li>
</ul>
<h1 id="四、Spark内存计算框架"><a href="#四、Spark内存计算框架" class="headerlink" title="四、Spark内存计算框架"></a>四、Spark内存计算框架</h1><h3 id="1-两种计算模型"><a href="#1-两种计算模型" class="headerlink" title="1. 两种计算模型"></a>1. 两种计算模型</h3><pre><code class="shell">spark-submit --class org.apache.spark.examples.SparkPi \
--master yarn \
# cluster / client
--deploy-mode cluster \
--driver-memory 1g \
--executor-memory 1g \
--executor-cores 1 \
/kfly/install/spark-2.3.3-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.3.jar \
10
</code></pre>
<ul>
<li><p>yarn-cluster模式</p>
<ul>
<li><p>spark程序的==Driver程序在YARN中运行==，运行结果不能在客户端显示，并且客户端可以在启动应用程序后消失应用的。</p>
</li>
<li><p>最好运行那些将结果最终保存在外部存储介质（如HDFS、Redis、Mysql），客户端的终端显示的仅是作为YARN的job的简单运行状况。</p>
</li>
</ul>
</li>
</ul>
<p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/yarn-cluster.png" alt="yarn-cluster" style="zoom:33%;"></p>
<ul>
<li>yarn-client模式<ul>
<li>spark程序的==Driver运行在Client上==，应用程序运行结果会在客户端显示，所有适合运行结果有输出的应用程序（如spark-shell）</li>
</ul>
</li>
</ul>
<p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/yarn-client.png" alt="yarn-client" style="zoom: 33%;"></p>
<pre><code>最大的区别就是Driver端的位置不一样。

yarn-cluster: Driver端运行在yarn集群中，与ApplicationMaster进程在一起。
yarn-client:  Driver端运行在提交任务的客户端,与ApplicationMaster进程没关系,经常用于进行测试
</code></pre><h3 id="2-collect-算子操作剖析"><a href="#2-collect-算子操作剖析" class="headerlink" title="2. collect 算子操作剖析"></a>2. collect 算子操作剖析</h3><ul>
<li><p>collect算子操作的作用</p>
<ul>
<li><p>1、它是一个action操作，会触发任务的运行</p>
</li>
<li><p>2、它会把RDD的数据进行收集之后，以数组的形式返回给Driver端</p>
</li>
<li><ul>
<li><p>==默认Driver端的内存大小为1G，由参数 spark.driver.memory 设置==</p>
</li>
<li><p>如果某个rdd的数据量超过了Driver端默认的1G内存，对rdd调用collect操作，这里会出现Driver端的内存溢出，所有这个collect操作存在一定的风险，实际开发代码一般不会使用。</p>
</li>
<li><p>==实际企业中一般都会把该参数调大，比如5G/10G等==</p>
<ul>
<li><p>可以在代码中修改该参数，如下</p>
<pre><code class="scala">new SparkConf().set(&quot;spark.driver.memory&quot;,&quot;5G&quot;)
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>比如说rdd的数据量达到了10G

rdd.collect这个操作非常危险，很有可能出现driver端的内存不足
</code></pre></li>
</ul>
<h3 id="3-spark任务中资源参数剖析"><a href="#3-spark任务中资源参数剖析" class="headerlink" title="3. spark任务中资源参数剖析"></a>3. spark任务中资源参数剖析</h3><ul>
<li><p>==–executor-memory==</p>
<ul>
<li>表示每一个executor进程需要的内存大小，它决定了后期操作数据的速度</li>
</ul>
<p><code>`</code><br>比如说一个rdd的数据量大小为5g,这里给定的executor-memory为2g, 在这种情况下，内存是存储不下，它会把一部分数据保存在内存中，还有一部分数据保存在磁盘，后续需要用到该rdd的结果数据，可以从内存和磁盘中获取得到，这里就涉及到一定的磁盘io操作。</p>
<p>,这里给定的executor-memory为10g，这里数据就可以完全在内存中存储下，后续需要用到该rdd的数据，就可以直接从内存中获取，这样一来，避免了大量的磁盘io操作。性能得到提升。</p>
</li>
</ul>
<p>  在实际的工作，这里 –executor-memory 需要设置的大一点。<br>  比如说10G/20G/30G等</p>
<pre><code>
- ==--total-executor-cores==

  - 表示任务运行需要总的cpu核数，它决定了任务并行运行的粒度

  ~~~
  比如说要处理100个task，注意一个cpu在同一时间只能处理一个task线程。

  如果给定的总的cpu核数是5个，这里就需要100/5=20个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行20分钟。

  如果给定的总的cpu核数是20个，这里就需要100/20=5个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行5分钟。

  如果如果给定的总的cpu核数是100个，这里就需要100/100=1个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行1分钟。


  在实际的生产环境中，--total-executor-cores 这个参数一般也会设置的大一点，
  比如说 30个/50个/100个
  ~~~


- ==总结==

</code></pre><pre><code>  后期对于spark程序的优化，可以从这2个参数入手，无论你把哪一个参数调大，对程序运行的效率来说都会达到一定程度的提升
  加大计算资源它是最直接、最有效果的优化手段。
  在计算资源有限的情况下，可以考虑其他方面，比如说代码层面，JVM层面等
</code></pre><pre><code>
### 4. spark任务的调度模式

* Spark中的调度模式主要有两种：==FIFO 和 FAIR==
  * ==FIFO（先进先出）==
    * 默认情况下Spark的调度模式是FIFO，谁先提交谁先执行，后面的任务需要等待前面的任务执行。
  * ==FAIR（公平调度）==
    * 支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。避免大任务运行时间长，占用了大量的资源，后面小任务无法提交运行。

### 5. spark任务的分配资源策略

* 给application分配资源选择worker（executor），现在有两种策略
  * ==尽量的打散==，即一个Application尽可能多的分配到不同的节点。这个可以通过设置spark.deploy.spreadOut来实现。默认值为true，即尽量的打散（默认）
    * 可以充分的发挥数据的本地性，提升执行效率

  * ==尽量的集中==，即一个Application尽量分配到尽可能少的节点。

  ```sh
  # 假如集群有两个节点，worker1,worker2。各 cores 4 memory 128G，需要分配 4cores。 32g
  # 1. 尽量的集中(尽可能分配更少的节点，worker1 4 32g)
  # 2. 尽量打散（尽可能多的分配，worker1 worker2按照顺序依次分配，不够再次循环）
</code></pre><h3 id="6-spark的shuffle原理分析"><a href="#6-spark的shuffle原理分析" class="headerlink" title="6. spark的shuffle原理分析"></a>6. spark的shuffle原理分析</h3><h4 id="6-1-shuffle概述"><a href="#6-1-shuffle概述" class="headerlink" title="6.1 shuffle概述"></a>6.1 shuffle概述</h4><pre><code>      Shuffle就是对数据进行重组，由于分布式计算的特性和要求，在实现细节上更加繁琐和复杂。
      在MapReduce框架，Shuffle是连接Map和Reduce之间的桥梁，Map阶段通过shuffle读取数据并输出到对应的Reduce；而Reduce阶段负责从Map端拉取数据并进行计算。在整个shuffle过程中，往往伴随着大量的磁盘和网络I/O。所以shuffle性能的高低也直接决定了整个程序的性能高低。Spark也会有自己的shuffle实现过程。 
</code></pre><h4 id="6-2-spark中的shuffle介绍"><a href="#6-2-spark中的shuffle介绍" class="headerlink" title="6.2 spark中的shuffle介绍"></a>6.2 spark中的shuffle介绍</h4><pre><code>      在DAG调度的过程中，Stage阶段的划分是根据是否有shuffle过程，也就是存在wide Dependency宽依赖的时候,需要进行shuffle,这时候会将作业job划分成多个Stage，每一个stage内部有很多可以并行运行的task。

      stage与stage之间的过程就是shuffle阶段，在Spark的中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。ShuffleManager随着Spark的发展有两种实现的方式，分别为HashShuffleManager和SortShuffleManager，因此spark的Shuffle有Hash Shuffle和Sort Shuffle两种。
</code></pre><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/Image201906272145.png" alt="img"></p>
<h4 id="6-3-HashShuffle机制"><a href="#6-3-HashShuffle机制" class="headerlink" title="6.3 HashShuffle机制"></a>6.3 HashShuffle机制</h4><h5 id="6-3-1-HashShuffle概述"><a href="#6-3-1-HashShuffle概述" class="headerlink" title="6.3.1 HashShuffle概述"></a>6.3.1 HashShuffle概述</h5><pre><code>      在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。
      该ShuffleManager-HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。
      SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并(merge)成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。
</code></pre><ul>
<li><p>==Hash shuffle==</p>
<ul>
<li>HashShuffleManager的运行机制主要分成两种<ul>
<li>一种是==普通运行机制==</li>
<li>另一种是==合并的运行机制==。</li>
</ul>
</li>
<li>==合并机制主要是通过复用buffer来优化Shuffle过程中产生的小文件的数量。==</li>
<li>==Hash shuffle是不具有排序的Shuffle。==</li>
</ul>
<h5 id="6-3-2-普通机制的Hash-shuffle"><a href="#6-3-2-普通机制的Hash-shuffle" class="headerlink" title="6.3.2 普通机制的Hash shuffle"></a>6.3.2 普通机制的Hash shuffle</h5></li>
</ul>
<p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/未优化的HashShuffle机制.png" alt="未优化的HashShuffle机制"></p>
<ul>
<li><p>==图解==</p>
<pre><code> 这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。

  图中有3个ReduceTask，从ShuffleMapTask 开始那边各自把自己进行 Hash 计算(分区器：hash/numreduce取模)，分类出3个不同的类别，每个 ShuffleMapTask 都分成3种类别的数据，想把不同的数据汇聚然后计算出最终的结果，所以ReduceTask 会在属于自己类别的数据收集过来，汇聚成一个同类别的大集合，每1个 ShuffleMapTask 输出3份本地文件，这里有4个 ShuffleMapTask，所以总共输出了4 x 3个分类文件 = 12个本地小文件。
</code></pre></li>
<li><p>==shuffle Write阶段==</p>
<pre><code>  主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子(比如reduceByKey，groupByKey)，而将每个task处理的数据按key进行“分区”。所谓“分区”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于reduce端的stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。

   那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢? 很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。
</code></pre></li>
<li><p>==shuffle Read阶段==</p>
<pre><code>  shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给Reduce端的stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。

shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。
</code></pre></li>
<li><p>==注意==</p>
<pre><code>（1）buffer起到的是缓存作用，缓存能够加速写磁盘，提高计算的效率,buffer的默认大小32k。

（2）分区器：根据hash/numRedcue取模决定数据由几个Reduce处理，也决定了写入几个buffer中

（3）block file：磁盘小文件，从图中我们可以知道磁盘小文件的个数计算公式：
               block file=M*R

(4) M为map task的数量，R为Reduce的数量，一般Reduce的数量等于buffer的数量，都是由分区器决定的
</code></pre></li>
<li><p>==Hash shuffle普通机制的问题==</p>
<pre><code>（1).Shuffle阶段在磁盘上会产生海量的小文件，建立通信和拉取数据的次数变多,此时会产生大量耗时低效的 IO 操作 (因为产生过多的小文件)

（2).可能导致OOM，大量耗时低效的 IO 操作 ，导致写磁盘时的对象过多，读磁盘时候的对象也过多，这些对象存储在堆内存中，会导致堆内存不足，相应会导致频繁的GC，GC会导致OOM。由于内存中需要保存海量文件操作句柄和临时信息，如果数据处理的规模比较庞大的话，内存不可承受，会出现 OOM 等问题
</code></pre><h5 id="6-3-3-合并机制的Hash-shuffle"><a href="#6-3-3-合并机制的Hash-shuffle" class="headerlink" title="6.3.3 合并机制的Hash shuffle"></a>6.3.3 合并机制的Hash shuffle</h5><pre><code>  合并机制就是复用buffer缓冲区，开启合并机制的配置是spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。
</code></pre></li>
</ul>
<p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/优化后的Shuffle机制.png" alt="优化后的Shuffle机制"></p>
<ul>
<li><p>==图解==</p>
<pre><code>  这里有6个这里有6个shuffleMapTask，数据类别还是分成3种类型，因为Hash算法会根据你的 Key 进行分类，在同一个进程中，无论是有多少过Task，都会把同样的Key放在同一个Buffer里，然后把Buffer中的数据写入以Core数量为单位的本地文件中，(一个Core只有一种类型的Key的数据)，每1个Task所在的进程中，分别写入共同进程中的3份本地文件，这里有6个shuffleMapTasks，所以总共输出是 2个Cores x 3个分类文件 = 6个本地小文件。
</code></pre></li>
<li><p>==注意==</p>
<pre><code>（1).启动HashShuffle的合并机制ConsolidatedShuffle的配置
 spark.shuffle.consolidateFiles=true

（2).block file=Core*R
  Core为CPU的核数，R为Reduce的数量
</code></pre></li>
<li><p>==Hash shuffle合并机制的问题==</p>
<pre><code>  如果 Reducer 端的并行任务或者是数据分片过多的话则 Core * Reducer Task 依旧过大，也会产生很多小文件。
</code></pre></li>
</ul>
<h4 id="6-4-Sort-shuffle"><a href="#6-4-Sort-shuffle" class="headerlink" title="6.4 Sort shuffle"></a>6.4 Sort shuffle</h4><ul>
<li>SortShuffleManager的运行机制主要分成两种，<ul>
<li>一种是==普通运行机制==</li>
<li>另一种是==bypass运行机制==</li>
</ul>
</li>
</ul>
<h5 id="6-4-1-Sort-shuffle的普通机制"><a href="#6-4-1-Sort-shuffle的普通机制" class="headerlink" title="6.4.1 Sort shuffle的普通机制"></a>6.4.1 Sort shuffle的普通机制</h5><p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/sortshuffle.png" alt="sortshuffle"></p>
<ul>
<li><p>==图解==</p>
<pre><code>  在该模式下，数据会先写入一个数据结构，聚合算子写入Map，一边通过Map局部聚合，一边写入内存。Join算子写入ArrayList直接写入内存中。然后需要判断是否达到阈值（5M），如果达到就会将内存数据结构的数据写入到磁盘，清空内存数据结构。

在溢写磁盘前，先根据key进行排序，排序过后的数据，会分批写入到磁盘文件中。默认批次为10000条，数据会以每批一万条写入到磁盘文件。写入磁盘文件通过缓冲区溢写的方式，每次溢写都会产生一个磁盘文件，也就是说一个task过程会产生多个临时文件
。

最后在每个task中，将所有的临时文件合并，这就是merge过程，此过程将所有临时文件读取出来，一次写入到最终文件。意味着一个task的所有数据都在这一个文件中。同时单独写一份索引文件，标识下游各个task的数据在文件中的索引start offset和end offset。

  这样算来如果第一个stage 50个task，每个Executor执行一个task，那么无论下游有几个task，就需要50*2=100个磁盘文件。
</code></pre></li>
<li><p>==好处==</p>
<p><code>`</code></p>
</li>
</ul>
<ol>
<li><p>小文件明显变少了，一个task只生成一个file文件</p>
</li>
<li><p>file文件整体有序，加上索引文件的辅助，查找变快，虽然排序浪费一些性能，但是查找变快很多<br><code>`</code></p>
</li>
</ol>
<h5 id="6-4-2-bypass模式的sortShuffle"><a href="#6-4-2-bypass模式的sortShuffle" class="headerlink" title="6.4.2 bypass模式的sortShuffle"></a>6.4.2 bypass模式的sortShuffle</h5><ul>
<li>bypass机制运行条件<ul>
<li>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值</li>
<li>不是聚合类的shuffle算子（比如reduceByKey）</li>
</ul>
</li>
</ul>
<p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/bypasssortshuffle.png" alt="bypasssortshuffle"></p>
<ul>
<li><p>==好处==</p>
<pre><code>  该机制与sortshuffle的普通机制相比，在shuffleMapTask不多的情况下，首先写的机制是不同，其次不会进行排序。这样就可以节约一部分性能开销。
</code></pre></li>
<li><p>==总结==</p>
<pre><code>在shuffleMapTask数量小于默认值200时，启用bypass模式的sortShuffle(原因是数据量本身比较少，没必要进行sort全排序，因为数据量少本身查询速度就快，正好省了sort的那部分性能开销。)

该机制与普通SortShuffleManager运行机制的不同在于：
第一: 磁盘写机制不同；
第二: 不会进行sort排序；
</code></pre></li>
</ul>
<h2 id="五、Spark-SQL"><a href="#五、Spark-SQL" class="headerlink" title="五、Spark SQL"></a>五、Spark SQL</h2><h3 id="1-sparksql概述"><a href="#1-sparksql概述" class="headerlink" title="1.sparksql概述"></a>1.sparksql概述</h3><h4 id="1-1-sparksql的前世今生"><a href="#1-1-sparksql的前世今生" class="headerlink" title="1.1 sparksql的前世今生"></a>1.1 sparksql的前世今生</h4><ul>
<li>==Shark是专门针对于spark的构建大规模数据仓库系统的一个框架==</li>
<li>Shark与Hive兼容、同时也依赖于Spark版本</li>
<li>Hivesql底层把sql解析成了mapreduce程序，Shark是把sql语句解析成了Spark任务</li>
<li>随着性能优化的上限，以及集成SQL的一些复杂的分析功能，发现Hive的MapReduce思想限制了Shark的发展。</li>
<li>最后Databricks公司终止对Shark的开发<ul>
<li>决定单独开发一个框架，不在依赖hive，把重点转移到了==sparksql==这个框架上。</li>
</ul>
</li>
</ul>
<h4 id="1-2-什么是sparksql"><a href="#1-2-什么是sparksql" class="headerlink" title="1.2 什么是sparksql"></a>1.2 什么是sparksql</h4><ul>
<li>Spark SQL is Apache Spark’s module for working with structured data.</li>
<li>SparkSQL是apache Spark用来处理结构化数据的一个模块</li>
</ul>
<h3 id="2-sparksql的四大特性"><a href="#2-sparksql的四大特性" class="headerlink" title="2. sparksql的四大特性"></a>2. sparksql的四大特性</h3><ul>
<li><p>==1、易整合(Integrated)==</p>
<ul>
<li><pre><code class="sql">results = spark.sql(&quot;SELECT * FROM people&quot;)
</code></pre>
</li>
</ul>
</li>
<li><p>==2、统一的数据源访问(Uniform Data Access)==</p>
<ul>
<li><pre><code class="scala">spark.read.json(&quot;s3n://...&quot;)
spark.read.text(&quot;s3n://...&quot;)
spark.read.parquet(&quot;s3n://...&quot;)
</code></pre>
</li>
</ul>
</li>
<li><p>==3、兼容hive(Hive Integration)==</p>
</li>
<li><p>==4、支持标准的数据库连接(Standard Connectivity)==</p>
<ul>
<li><pre><code class="scala">spark.read.jdbc(---)
</code></pre>
</li>
</ul>
</li>
</ul>
<h3 id="3-DataFrame概述"><a href="#3-DataFrame概述" class="headerlink" title="3. DataFrame概述"></a>3. DataFrame概述</h3><h4 id="3-1-DataFrame发展"><a href="#3-1-DataFrame发展" class="headerlink" title="3.1 DataFrame发展"></a>3.1 DataFrame发展</h4><ul>
<li>DataFrame前身是schemaRDD,这个schemaRDD是直接继承自RDD，它是RDD的一个实现类</li>
<li>在spark1.3.0之后把schemaRDD改名为DataFrame,它不在继承自RDD，而是自己实现RDD上的一些功能</li>
<li>也可以把dataFrame转换成一个rdd，调用rdd这个方法<ul>
<li>例如 val rdd1=dataFrame.rdd</li>
</ul>
</li>
</ul>
<h4 id="3-2-DataFrame是什么"><a href="#3-2-DataFrame是什么" class="headerlink" title="3.2 DataFrame是什么"></a>3.2 DataFrame是什么</h4><ul>
<li>在Spark中，DataFrame是一种==以RDD为基础的分布式数据集==，类似于==传统数据库的二维表格==</li>
<li>DataFrame带有==Schema元信息==，即DataFrame所表示的二维表数据集的每一列都带有名称和类型，但底层做了更多的优化</li>
<li>DataFrame可以从很多数据源构建<ul>
<li>比如：已经存在的RDD、结构化文件、外部数据库、Hive表。</li>
</ul>
</li>
<li>RDD可以把它内部元素看成是一个java对象</li>
<li>DataFrame可以把内部是一个Row对象，它表示一行一行的数据</li>
</ul>
<h4 id="3-3-DataFrame和RDD的优缺点"><a href="#3-3-DataFrame和RDD的优缺点" class="headerlink" title="3.3 DataFrame和RDD的优缺点"></a>3.3 DataFrame和RDD的优缺点</h4><ul>
<li><p>==1、RDD==</p>
<ul>
<li><p>==优点==</p>
<ul>
<li>1、编译时类型安全<ul>
<li>开发会进行类型检查，在编译的时候及时发现错误</li>
</ul>
</li>
<li>2、具有面向对象编程的风格</li>
</ul>
</li>
<li><p>==缺点==</p>
<ul>
<li><p>1、构建大量的java对象占用了大量heap堆空间，导致频繁的GC</p>
<pre><code>由于数据集RDD它的数据量比较大，后期都需要存储在heap堆中，这里有heap堆中的内存空间有限，出现频繁的垃圾回收（GC），程序在进行垃圾回收的过程中，所有的任务都是暂停。影响程序执行的效率
</code></pre></li>
<li><p>2、数据的序列化和反序列性能开销很大</p>
<pre><code>  在分布式程序中，对象(对象的内容和结构)是先进行序列化，发送到其他服务器，进行大量的网络传输，然后接受到这些序列化的数据之后，再进行反序列化来恢复该对象
</code></pre></li>
</ul>
</li>
</ul>
</li>
<li><p>==2、DataFrame==</p>
<ul>
<li>==DataFrame引入了schema元信息和off-heap(堆外)==</li>
<li>==优点==<ul>
<li>1、DataFrame引入off-heap，大量的对象构建直接使用操作系统层面上的内存，不在使用heap堆中的内存，这样一来heap堆中的内存空间就比较充足，不会导致频繁GC，程序的运行效率比较高，它是解决了RDD构建大量的java对象占用了大量heap堆空间，导致频繁的GC这个缺点。</li>
<li>2、DataFrame引入了schema元信息—就是数据结构的描述信息，后期spark程序中的大量对象在进行网络传输的时候，只需要把数据的内容本身进行序列化就可以，数据结构信息可以省略掉。这样一来数据网络传输的数据量是有所减少，数据的序列化和反序列性能开销就不是很大了。它是解决了RDD数据的序列化和反序列性能开销很大这个缺点</li>
<li>==缺点==<ul>
<li>DataFrame引入了schema元信息和off-heap(堆外)它是分别解决了RDD的缺点，同时它也丢失了RDD的优点<ul>
<li>1、编译时类型不安全<ul>
<li>编译时不会进行类型的检查，这里也就意味着前期是无法在编译的时候发现错误，只有在运行的时候才会发现</li>
</ul>
</li>
<li>2、不在具有面向对象编程的风格</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4-DataFrame使用"><a href="#4-DataFrame使用" class="headerlink" title="4. DataFrame使用"></a>4. DataFrame使用</h3><pre><code class="SCALA">// 1. 读取文本文件
val personDF=spark.read.text(&quot;/person.txt&quot;)
val peopleDF=spark.read.json(&quot;/people.json&quot;)
val usersDF=spark.read.parquet(&quot;/users.parquet&quot;)

// 2. 加载数据
val rdd1=sc.textFile(&quot;/person.txt&quot;).map(x=&gt;x.split(&quot; &quot;))
    //定义一个样例类
    case class Person(id:String,name:String,age:Int)
    //把rdd与样例类进行关联
    val personRDD=rdd1.map(x=&gt;Person(x(0),x(1),x(2).toInt))
    //把rdd转换成DataFrame
    val personDF=personRDD.toDF
// 3.语法风格
// 3.1 DSL
        personDF.select(&quot;name&quot;).show
        personDF.select($&quot;name&quot;).show
        personDF.select(col(&quot;name&quot;).show
        //实现age+1
         personDF.select($&quot;name&quot;,$&quot;age&quot;,$&quot;age&quot;+1)).show   
        //实现age大于30过滤
         personDF.filter($&quot;age&quot; &gt; 30).show
         //按照age分组统计次数
         personDF.groupBy(&quot;age&quot;).count.show 
        //按照age分组统计次数降序
         personDF.groupBy(&quot;age&quot;).count().sort($&quot;count&quot;.desc)show  
// 3.2 sql
        //DataFrame注册成表
        personDF.createTempView(&quot;person&quot;)

        //使用SparkSession调用sql方法统计查询
        spark.sql(&quot;select * from person&quot;).show
        spark.sql(&quot;select name from person&quot;).show
        spark.sql(&quot;select name,age from person&quot;).show
</code></pre>
<h3 id="5-DataSet概述"><a href="#5-DataSet概述" class="headerlink" title="5. DataSet概述"></a>5. DataSet概述</h3><h4 id="5-1-DataSet是什么"><a href="#5-1-DataSet是什么" class="headerlink" title="5.1 DataSet是什么"></a>5.1 DataSet是什么</h4><ul>
<li>DataSet是分布式的数据集合，Dataset提供了强类型支持，也是在RDD的每行数据加了类型约束。</li>
<li>DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型和可以用强大lambda函数）以及使用了Spark SQL优化的执行引擎。</li>
</ul>
<h4 id="5-2-DataSet的区别"><a href="#5-2-DataSet的区别" class="headerlink" title="5.2 DataSet的区别"></a>5.2 DataSet的区别</h4><pre><code class="properties">1. 假设RDD中的两行数据长这样
        1,张三,23
        2,李四,35
2.那么DataFrame中的数据长这样
                ID:String    Name:String    Age:int
                1                张三                23
                2                李四                35
3.Dataset中的数据长这样 
            value:String
        1,张三,23
        2,李四,35
  或者
  value:People(age:bigint,id:bigint,name:string)
        People(id=1,name=&quot;张三&quot;,age=23)
        People(id=2,name=&quot;李四&quot;,age=23)
</code></pre>
<pre><code class="properties">DataSet包含了DataFrame的功能，Spark2.0中两者统一，DataFrame表示为DataSet[Row]，即DataSet的子集。
（1）DataSet可以在编译时检查类型
（2）并且是面向对象的编程接口
</code></pre>
<p>5.3 DataFrame DataSet转换 构建dataset</p>
<pre><code class="scala">// 把一个DataFrame转换成DataSet
val dataSet=dataFrame.as[强类型]
//  2、把一个DataSet转换成DataFrame
val dataFrame=dataSet.toDF

// 补充说明,可以从dataFrame和dataSet获取得到rdd
val rdd1=dataFrame.rdd
val rdd2=dataSet.rdd

// 1、 通过sparkSession调用createDataset方法
  val ds=spark.createDataset(1 to 10) //scala集合
  val ds=spark.createDataset(sc.textFile(&quot;/person.txt&quot;))  //rdd

// 2、使用scala集合和rdd调用toDS方法
  sc.textFile(&quot;/person.txt&quot;).toDS
  List(1,2,3,4,5).toDS

// 3、把一个DataFrame转换成DataSet
  val dataSet=dataFrame.as[强类型]

// 4、通过一个DataSet转换生成一个新的DataSet
   List(1,2,3,4,5).toDS.map(x=&gt;x*10)

// 5、将rdd与Row对象进行关联
    val rowRDD: RDD[Row] = data.map(x=&gt;Row(x(0),x(1),x(2).toInt))
    //指定dataFrame的schema信息   
    //这里指定的字段个数和类型必须要跟Row对象保持一致
    val schema=StructType(
        StructField(&quot;id&quot;,StringType)::
        StructField(&quot;name&quot;,StringType)::
        StructField(&quot;age&quot;,IntegerType)::Nil
    )
    val dataFrame: DataFrame = spark.createDataFrame(rowRDD,schema)
</code></pre>
<h3 id="6-Spark-IDEA开发"><a href="#6-Spark-IDEA开发" class="headerlink" title="6. Spark IDEA开发"></a>6. Spark IDEA开发</h3><pre><code class="scala">// EG
 // 1、构建SparkSession对象,开启hive支持
    val spark: SparkSession = SparkSession.builder()
      .appName(&quot;HiveSupport&quot;)
      .master(&quot;local[2]&quot;)
      .enableHiveSupport() //开启对hive的支持
      .getOrCreate()

// 2. 读取mysql数据
        val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
        val url=&quot;jdbc:mysql://node03:3306/spark&quot;
        val tableName=&quot;user&quot;
        val properties = new Properties()
      properties.setProperty(&quot;user&quot;,&quot;root&quot;)
      properties.setProperty(&quot;password&quot;,&quot;123456&quot;)
   val mysqlDF: DataFrame = spark.read.jdbc(url,tableName,properties)

// 3. 保存数据到mysql表中
     //mode:指定数据的插入模式
        //overwrite: 表示覆盖，如果表不存在，事先帮我们创建
        //append   :表示追加， 如果表不存在，事先帮我们创建
        //ignore   :表示忽略，如果表事先存在，就不进行任何操作
        //error    :如果表事先存在就报错（默认选项）
    result.write.mode(&quot;append&quot;).jdbc(url,&quot;kaikeba&quot;,properties)
</code></pre>
<h3 id="7-Spark自定义函数"><a href="#7-Spark自定义函数" class="headerlink" title="7 Spark自定义函数"></a>7 Spark自定义函数</h3><pre><code class="scala">//小写转大写
sparkSession.udf.register(&quot;low2Up&quot;,new UDF1[String,String]() {
  override def call(t1: String): String = {
    t1.toUpperCase
  }
},StringType)
//大写转小写
sparkSession.udf.register(&quot;up2low&quot;,(x:String)=&gt;x.toLowerCase)
// 把数据文件中的单词统一转换成大小写
sparkSession.sql(&quot;select  value from t_udf&quot;).show()
sparkSession.sql(&quot;select  low2Up(value) from t_udf&quot;).show()
sparkSession.sql(&quot;select  up2low(value) from t_udf&quot;).show()
</code></pre>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="https://blog.sev7e0.site/">大数据施工现场</a></span>
        <span>/</span>
        
        <span><a href="https://wangchujiang.com/linux-command/">linux命令行工具</a></span>
        <span>/</span>
        
        <span><a href="https://github.com/orchid-ding">github</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="/js/gitment.js"></script>
<script>
    var gitment = new Gitment({
        id: 'SparkCore/sql知识梳理',
        owner: 'orchid-ding',
        repo: 'kfly-blog-comment',
        oauth: {
            client_id: '0770cdab79393197b6f5',
            client_secret: '376fb6c7bcd5047718b356712f596b89e490360c',
        },
    })
    gitment.render('comment-container')
</script>




</html>
