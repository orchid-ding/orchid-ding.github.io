<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="天行健、君子以自强不息；地势坤，君子以厚德载物。">
    <meta name="keyword"  content="兰草">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        大数据开发之HBase（二） - Kaffir Lily的博客 | Kaffir Lily&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>kfly</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="icon-font icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#大数据数据库之hbase"><span class="toc-text">大数据数据库之hbase</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-HBase的数据存储原理"><span class="toc-text">1. HBase的数据存储原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-HBase读数据流程"><span class="toc-text">2. HBase读数据流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-HBase写数据流程"><span class="toc-text">3. HBase写数据流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-HBase的flush、compact机制"><span class="toc-text">4. HBase的flush、compact机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Flush触发条件"><span class="toc-text">4.1 Flush触发条件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-memstore级别限制"><span class="toc-text">4.1.1 memstore级别限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-region级别限制"><span class="toc-text">4.1.2 region级别限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-3-Region-Server级别限制"><span class="toc-text">4.1.3 Region Server级别限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-4-HLog数量上限"><span class="toc-text">4.1.4 HLog数量上限</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-5-定期刷新Memstore"><span class="toc-text">4.1.5 定期刷新Memstore</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-6-手动flush"><span class="toc-text">4.1.6 手动flush</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-flush的流程"><span class="toc-text">4.2 flush的流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Compact合并机制"><span class="toc-text">4.3  Compact合并机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-minor-compaction-小合并"><span class="toc-text">4.3.1 minor compaction 小合并</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-major-compaction-大合并"><span class="toc-text">4.3.2 major compaction 大合并</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-region-拆分机制"><span class="toc-text">5. region 拆分机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-HBase表的预分区"><span class="toc-text">6. HBase表的预分区</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-为何要预分区？"><span class="toc-text">6.1 为何要预分区？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-预分区原理"><span class="toc-text">6.2 预分区原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-手动指定预分区"><span class="toc-text">6.3 手动指定预分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-2-HexStringSplit-算法"><span class="toc-text">6.2.2 HexStringSplit 算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-region-合并"><span class="toc-text">7. region 合并</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-region合并说明"><span class="toc-text">7.1 region合并说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-如何进行region合并"><span class="toc-text">7.2 如何进行region合并</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-1-通过Merge类冷合并Region"><span class="toc-text">7.2.1 通过Merge类冷合并Region</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-2-通过online-merge热合并Region"><span class="toc-text">7.2.2  通过online_merge热合并Region</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-HBase集成MapReduce"><span class="toc-text">8. HBase集成MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-实战一"><span class="toc-text">8.1 实战一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-实战二"><span class="toc-text">8.2 实战二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-实战三"><span class="toc-text">8.3 实战三</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-HBase集成Hive"><span class="toc-text">8. HBase集成Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-HBase与Hive的对比（"><span class="toc-text">8.1 HBase与Hive的对比（</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-1-Hive"><span class="toc-text">8.1.1 Hive</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-2-HBase"><span class="toc-text">8.1.2 HBase</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#8-1-3-总结：Hive与HBase"><span class="toc-text">8.1.3 总结：Hive与HBase</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-整合配置"><span class="toc-text">9.2 整合配置</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#9-2-1-拷贝jar包"><span class="toc-text">9.2.1 拷贝jar包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-2-2-修改hive的配置文件"><span class="toc-text">9.2.2 修改hive的配置文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-2-3-修改hive-env-sh配置文件"><span class="toc-text">9.2.3 修改hive-env.sh配置文件</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-需求一：将hive表当中分析的结果保存到hbase表当中去"><span class="toc-text">9.3 需求一：将hive表当中分析的结果保存到hbase表当中去</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#9-3-1-hive当中建表"><span class="toc-text">9.3.1 hive当中建表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-3-2-准备数据内容如下并加载到hive表"><span class="toc-text">9.3.2 准备数据内容如下并加载到hive表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-3-3-创建hive管理表与HBase进行映射"><span class="toc-text">9.3.3 创建hive管理表与HBase进行映射</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-3-4-hbase当中查看表hbase-score"><span class="toc-text">9.3.4 hbase当中查看表hbase_score</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-4-需求二：创建hive外部表，映射HBase当中已有的表模型（5分钟）"><span class="toc-text">9.4 需求二：创建hive外部表，映射HBase当中已有的表模型（5分钟）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#9-4-1-HBase当中创建表并手动插入加载一些数据"><span class="toc-text">9.4.1 HBase当中创建表并手动插入加载一些数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-4-2-建立hive的外部表，映射HBase当中的表以及字段"><span class="toc-text">9.4.2 建立hive的外部表，映射HBase当中的表以及字段</span></a></li></ol></li></ol></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        大数据开发之HBase（二）
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-11-13 14:46:15</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#hadoop" title="hadoop">hadoop</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#hbase" title="hbase">hbase</a>
        <span>/</span>
        
        <a class="tag" href="/tags/#hbase与hive整合" title="hbase与hive整合">hbase与hive整合</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h1 id="大数据数据库之hbase"><a href="#大数据数据库之hbase" class="headerlink" title="大数据数据库之hbase"></a>大数据数据库之hbase</h1><h2 id="1-HBase的数据存储原理"><a href="#1-HBase的数据存储原理" class="headerlink" title="1. HBase的数据存储原理"></a>1. HBase的数据存储原理</h2><p><img src="assets/hbase%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84.png?lastModify=1573631775" alt="hbase存储架构"></p>
<p><img src="assets/hbase_data_storage-1565601156263.png?lastModify=1573631775" alt="img"></p>
<ul>
<li>一个HRegionServer会负责管理很多个region</li>
<li>一个<strong>==region==</strong>包含很多个==store==<ul>
<li>一个<strong>==列族==</strong>就划分成一个<strong>==store==</strong></li>
<li>如果一个表中只有1个列族，那么每一个region中只有一个store</li>
<li>如果一个表中有N个列族，那么每一个region中有N个store</li>
</ul>
</li>
<li>==一个store==里面只有==一个memstore==<ul>
<li>memstore是一块<strong>内存区域</strong>，写入的数据会先写入memstore进行缓冲，然后再把数据刷到磁盘</li>
</ul>
</li>
<li>一个store里面有很多个<strong>==StoreFile==</strong>, 最后数据是以很多个<strong>==HFile==</strong>这种数据结构的文件保存在HDFS上<ul>
<li>StoreFile是HFile的抽象对象，如果说到StoreFile就等于HFile</li>
<li>==每次memstore刷写数据到磁盘，就生成对应的一个新的HFile文件出来==</li>
</ul>
</li>
</ul>
<p><img src="assets/region.png?lastModify=1573631775" alt="region"></p>
<h2 id="2-HBase读数据流程"><a href="#2-HBase读数据流程" class="headerlink" title="2. HBase读数据流程"></a>2. HBase读数据流程</h2><p><img src="assets/hbase%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png?lastModify=1573631775" alt="img"></p>
<blockquote>
<p>说明：HBase集群，只有一张meta表，此表只有一个region，该region数据保存在一个HRegionServer上</p>
</blockquote>
<ul>
<li>1、客户端首先与zk进行连接；从zk找到meta表的region位置，即meta表的数据存储在某一HRegionServer上；客户端与此HRegionServer建立连接，然后读取meta表中的数据；meta表中存储了所有用户表的region信息，我们可以通过<code>scan  &#39;hbase:meta&#39;</code>来查看meta表信息</li>
<li>2、根据要查询的namespace、表名和rowkey信息。找到写入数据对应的region信息</li>
<li>3、找到这个region对应的regionServer，然后发送请求</li>
<li>4、查找并定位到对应的region</li>
<li>5、先从memstore查找数据，如果没有，再从BlockCache上读取<ul>
<li>HBase上Regionserver的内存分为两个部分<ul>
<li>一部分作为Memstore，主要用来写；</li>
<li>另外一部分作为BlockCache，主要用于读数据；</li>
</ul>
</li>
</ul>
</li>
<li>6、如果BlockCache中也没有找到，再到StoreFile上进行读取<ul>
<li>从storeFile中读取到数据之后，不是直接把结果数据返回给客户端，而是把数据先写入到BlockCache中，目的是为了加快后续的查询；然后在返回结果给客户端。</li>
</ul>
</li>
</ul>
<h2 id="3-HBase写数据流程"><a href="#3-HBase写数据流程" class="headerlink" title="3. HBase写数据流程"></a>3. HBase写数据流程</h2><p><img src="assets/hbase%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png?lastModify=1573631775" alt="img"></p>
<ul>
<li>1、客户端首先从zk找到meta表的region位置，然后读取meta表中的数据，meta表中存储了用户表的region信息</li>
<li>2、根据namespace、表名和rowkey信息。找到写入数据对应的region信息</li>
<li>3、找到这个region对应的regionServer，然后发送请求</li>
<li>4、把数据分别写到HLog（write ahead log）和memstore各一份</li>
<li>5、memstore达到阈值后把数据刷到磁盘，生成storeFile文件</li>
<li>6、删除HLog中的历史数据</li>
</ul>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">补充：</span><br><span class="line">HLog（<span class="built_in">write</span> ahead <span class="built_in">log</span>）：</span><br><span class="line">  也称为WAL意为<span class="built_in">Write</span> ahead <span class="built_in">log</span>，类似mysql中的binlog,用来做灾难恢复时用，HLog记录数据的所有变更,一旦数据修改，就可以从<span class="built_in">log</span>中进行恢复。</span><br></pre></td></tr></table></figure>
<h2 id="4-HBase的flush、compact机制"><a href="#4-HBase的flush、compact机制" class="headerlink" title="4. HBase的flush、compact机制"></a>4. HBase的flush、compact机制</h2><p><img src="assets/hbase-split-compaction.png" alt=""></p>
<h3 id="4-1-Flush触发条件"><a href="#4-1-Flush触发条件" class="headerlink" title="4.1 Flush触发条件"></a>4.1 Flush触发条件</h3><h4 id="4-1-1-memstore级别限制"><a href="#4-1-1-memstore级别限制" class="headerlink" title="4.1.1 memstore级别限制"></a>4.1.1 memstore级别限制</h4><ul>
<li>当Region中任意一个MemStore的大小达到了上限（hbase.hregion.memstore.flush.size，默认128MB），会触发Memstore刷新。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.hregion.memstore.flush.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="4-1-2-region级别限制"><a href="#4-1-2-region级别限制" class="headerlink" title="4.1.2 region级别限制"></a>4.1.2 region级别限制</h4><ul>
<li>当Region中所有Memstore的大小总和达到了上限（hbase.hregion.memstore.block.multiplier <em> hbase.hregion.memstore.flush.size，默认 2</em> 128M = 256M），会触发memstore刷新。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.hregion.memstore.flush.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.hregion.memstore.block.multiplier<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="4-1-3-Region-Server级别限制"><a href="#4-1-3-Region-Server级别限制" class="headerlink" title="4.1.3 Region Server级别限制"></a>4.1.3 Region Server级别限制</h4><ul>
<li>当一个Region Server中所有Memstore的大小总和超过低水位阈值hbase.regionserver.global.memstore.size.lower.limit*hbase.regionserver.global.memstore.size（前者默认值0.95），RegionServer开始强制flush；</li>
<li>先Flush Memstore最大的Region，再执行次大的，依次执行；</li>
<li>如写入速度大于flush写出的速度，导致总MemStore大小超过高水位阈值hbase.regionserver.global.memstore.size（默认为JVM内存的40%），此时RegionServer会阻塞更新并强制执行flush，直到总MemStore大小低于低水位阈值</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.global.memstore.size.lower.limit<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>0.95<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.global.memstore.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>0.4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="4-1-4-HLog数量上限"><a href="#4-1-4-HLog数量上限" class="headerlink" title="4.1.4 HLog数量上限"></a>4.1.4 HLog数量上限</h4><ul>
<li>当一个Region Server中HLog数量达到上限（可通过参数hbase.regionserver.maxlogs配置）时，系统会选取最早的一个 HLog对应的一个或多个Region进行flush</li>
</ul>
<h4 id="4-1-5-定期刷新Memstore"><a href="#4-1-5-定期刷新Memstore" class="headerlink" title="4.1.5 定期刷新Memstore"></a>4.1.5 定期刷新Memstore</h4><ul>
<li>默认周期为1小时，确保Memstore不会长时间没有持久化。为避免所有的MemStore在同一时间都进行flush导致的问题，定期的flush操作有20000左右的随机延时。</li>
</ul>
<h4 id="4-1-6-手动flush"><a href="#4-1-6-手动flush" class="headerlink" title="4.1.6 手动flush"></a>4.1.6 手动flush</h4><ul>
<li>用户可以通过shell命令<code>flush ‘tablename’</code>或者<code>flush ‘region name’</code>分别对一个表或者一个Region进行flush。</li>
</ul>
<h3 id="4-2-flush的流程"><a href="#4-2-flush的流程" class="headerlink" title="4.2 flush的流程"></a>4.2 flush的流程</h3><ul>
<li><p>为了减少flush过程对读写的影响，将整个flush过程分为三个阶段：</p>
<ul>
<li><p>prepare阶段：遍历当前Region中所有的Memstore，将Memstore中当前数据集CellSkipListSet做一个<strong>快照snapshot</strong>；然后再新建一个CellSkipListSet。后期写入的数据都会写入新的CellSkipListSet中。prepare阶段需要加一把updateLock对<strong>写请求阻塞</strong>，结束之后会释放该锁。因为此阶段没有任何费时操作，因此持锁时间很短。</p>
</li>
<li><p>flush阶段：遍历所有Memstore，将prepare阶段生成的snapshot持久化为<strong>临时文件</strong>，临时文件会统一放到目录.tmp下。这个过程因为涉及到磁盘IO操作，因此相对比较耗时。</p>
</li>
<li>commit阶段：遍历所有Memstore，将flush阶段生成的临时文件移到指定的ColumnFamily目录下，针对HFile生成对应的storefile和Reader，把storefile添加到HStore的storefiles列表中，最后再<strong>清空</strong>prepare阶段生成的snapshot。</li>
</ul>
</li>
</ul>
<h3 id="4-3-Compact合并机制"><a href="#4-3-Compact合并机制" class="headerlink" title="4.3  Compact合并机制"></a>4.3  Compact合并机制</h3><ul>
<li><p>hbase为了==防止小文件过多==，以保证查询效率，hbase需要在必要的时候将这些小的store file合并成相对较大的store file，这个过程就称之为compaction。</p>
</li>
<li><p>在hbase中主要存在两种类型的compaction合并</p>
<ul>
<li><strong>==minor compaction 小合并==</strong></li>
<li><strong>==major compaction 大合并==</strong></li>
</ul>
</li>
</ul>
<h4 id="4-3-1-minor-compaction-小合并"><a href="#4-3-1-minor-compaction-小合并" class="headerlink" title="4.3.1 minor compaction 小合并"></a>4.3.1 minor compaction 小合并</h4><ul>
<li><p>在将Store中多个HFile合并为一个HFile</p>
<p>在这个过程中会选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，对于超过了TTL的数据、更新的数据、删除的数据仅仅只是做了标记。并没有进行物理删除，一次Minor Compaction的结果是更少并且更大的StoreFile。这种合并的触发频率很高。</p>
</li>
<li><p>minor compaction触发条件由以下几个参数共同决定：</p>
</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--表示至少需要三个满足条件的store file时，minor compaction才会启动--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.hstore.compactionThreshold<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--表示一次minor compaction中最多选取10个store file--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.hstore.compaction.max<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--默认值为128m,</span></span><br><span class="line"><span class="comment">表示文件大小小于该值的store file 一定会加入到minor compaction的store file中</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.hstore.compaction.min.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--默认值为LONG.MAX_VALUE，</span></span><br><span class="line"><span class="comment">表示文件大小大于该值的store file 一定会被minor compaction排除--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.hstore.compaction.max.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>9223372036854775807<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="4-3-2-major-compaction-大合并"><a href="#4-3-2-major-compaction-大合并" class="headerlink" title="4.3.2 major compaction 大合并"></a>4.3.2 major compaction 大合并</h4><ul>
<li><p>合并Store中所有的HFile为一个HFile</p>
<p>将所有的StoreFile合并成一个StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL过期数据、版本号超过设定版本号的数据。合并频率比较低，默认7天执行一次，并且性能消耗非常大，建议生产关闭(设置为0)，在应用空闲时间手动触发。一般可以是手动控制进行合并，防止出现在业务高峰期。</p>
</li>
<li><p>major compaction触发时间条件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--默认值为7天进行一次大合并，--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.hregion.majorcompaction<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>604800000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>手动触发</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##使用major_compact命令</span></span><br><span class="line">major_compact tableName</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="5-region-拆分机制"><a href="#5-region-拆分机制" class="headerlink" title="5. region 拆分机制"></a>5. region 拆分机制</h2><ul>
<li><p>region中存储的是大量的rowkey数据 ,当region中的数据条数过多的时候,直接影响查询效率.当region过大的时候.hbase会拆分region , 这也是Hbase的一个优点 .</p>
</li>
<li><p>HBase的region split策略一共有以下几种：</p>
</li>
</ul>
<ul>
<li><p>1、<strong>ConstantSizeRegionSplitPolicy</strong></p>
<ul>
<li>0.94版本前默认切分策略</li>
</ul>
</li>
<li><p>当region大小大于某个阈值(hbase.hregion.max.filesize=10G)之后就会触发切分，一个region等分为2个region。</p>
<ul>
<li>但是在生产线上这种切分策略却有相当大的弊端：切分策略对于大表和小表没有明显的区分。阈值(hbase.hregion.max.filesize)设置较大对大表比较友好，但是小表就有可能不会触发分裂，极端情况下可能就1个，这对业务来说并不是什么好事。如果设置较小则对小表友好，但一个大表就会在整个集群产生大量的region，这对于集群的管理、资源使用、failover来说都不是一件好事。</li>
</ul>
</li>
</ul>
<ul>
<li><p>2、<strong>IncreasingToUpperBoundRegionSplitPolicy</strong></p>
<ul>
<li>0.94版本~2.0版本默认切分策略</li>
</ul>
<ul>
<li><p>切分策略稍微有点复杂，总体看和ConstantSizeRegionSplitPolicy思路相同，一个region大小大于设置阈值就会触发切分。但是这个阈值并不像ConstantSizeRegionSplitPolicy是一个固定的值，而是会在一定条件下不断调整，调整规则和region所属表在当前regionserver上的region个数有关系.</p>
</li>
<li><p>region split的计算公式是：<br>regioncount^3 <em> 128M </em> 2，当region达到该size的时候进行split<br>例如：<br>第一次split：1^3 <em> 256 = 256MB<br>第二次split：2^3 </em> 256 = 2048MB<br>第三次split：3^3 <em> 256 = 6912MB<br>第四次split：4^3 </em> 256 = 16384MB &gt; 10GB，因此取较小的值10GB<br>后面每次split的size都是10GB了</p>
</li>
</ul>
</li>
<li><p>3、<strong>SteppingSplitPolicy</strong></p>
<ul>
<li>2.0版本默认切分策略</li>
</ul>
<ul>
<li>这种切分策略的切分阈值又发生了变化，相比 IncreasingToUpperBoundRegionSplitPolicy 简单了一些，依然和待分裂region所属表在当前regionserver上的region个数有关系，如果region个数等于1，<br>切分阈值为flush size * 2，否则为MaxRegionFileSize。这种切分策略对于大集群中的大表、小表会比 IncreasingToUpperBoundRegionSplitPolicy 更加友好，小表不会再产生大量的小region，而是适可而止。</li>
</ul>
</li>
<li><p>4、<strong>KeyPrefixRegionSplitPolicy</strong></p>
<ul>
<li>根据rowKey的前缀对数据进行分组，这里是指定rowKey的前多少位作为前缀，比如rowKey都是16位的，指定前5位是前缀，那么前5位相同的rowKey在进行region split的时候会分到相同的region中。</li>
</ul>
</li>
<li><p>5、<strong>DelimitedKeyPrefixRegionSplitPolicy</strong></p>
<ul>
<li>保证相同前缀的数据在同一个region中，例如rowKey的格式为：userid_eventtype_eventid，指定的delimiter为 _ ，则split的的时候会确保userid相同的数据在同一个region中。</li>
</ul>
</li>
</ul>
<ul>
<li>6、<strong>DisabledRegionSplitPolicy</strong><ul>
<li>不启用自动拆分, 需要指定手动拆分</li>
</ul>
</li>
</ul>
<h2 id="6-HBase表的预分区"><a href="#6-HBase表的预分区" class="headerlink" title="6. HBase表的预分区"></a>6. HBase表的预分区</h2><ul>
<li>当一个table刚被创建的时候，Hbase默认的分配一个region给table。也就是说这个时候，所有的读写请求都会访问到同一个regionServer的同一个region中，这个时候就达不到负载均衡的效果了，集群中的其他regionServer就可能会处于比较空闲的状态。</li>
<li>解决这个问题可以用<strong>pre-splitting</strong>,在创建table的时候就配置好，生成多个region。</li>
</ul>
<h3 id="6-1-为何要预分区？"><a href="#6-1-为何要预分区？" class="headerlink" title="6.1 为何要预分区？"></a>6.1 为何要预分区？</h3><ul>
<li>增加数据读写效率</li>
<li>负载均衡，防止数据倾斜</li>
<li>方便集群容灾调度region</li>
<li>优化Map数量</li>
</ul>
<h3 id="6-2-预分区原理"><a href="#6-2-预分区原理" class="headerlink" title="6.2 预分区原理"></a>6.2 预分区原理</h3><ul>
<li>每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。</li>
</ul>
<h3 id="6-3-手动指定预分区"><a href="#6-3-手动指定预分区" class="headerlink" title="6.3 手动指定预分区"></a>6.3 手动指定预分区</h3><ul>
<li><p>两种方式</p>
</li>
<li><p>方式一</p>
</li>
</ul>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create <span class="string">'person'</span>,<span class="string">'info1'</span>,<span class="string">'info2'</span>,SPLITS =&gt; [<span class="string">'1000'</span>,<span class="string">'2000'</span>,<span class="string">'3000'</span>,<span class="string">'4000'</span>]</span><br></pre></td></tr></table></figure>
<p><img src="assets/personSplit.png" alt="personSplit"></p>
<ul>
<li><p>方式二：也可以把分区规则创建于文件中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /kfly/doc</span><br><span class="line"></span><br><span class="line">vim split.txt</span><br></pre></td></tr></table></figure>
<ul>
<li>文件内容</li>
</ul>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">aaa</span></span><br><span class="line"><span class="keyword">bbb</span></span><br><span class="line"><span class="keyword">ccc</span></span><br><span class="line"><span class="keyword">ddd</span></span><br></pre></td></tr></table></figure>
<ul>
<li>hbase shell中，执行命令</li>
</ul>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create <span class="string">'student'</span>,<span class="string">'info'</span>,SPLITS_FILE =&gt; <span class="string">'/kfly/install/split.txt'</span></span><br></pre></td></tr></table></figure>
<ul>
<li>成功后查看web界面</li>
</ul>
<p><img src="assets/splitFile.png" alt="splitFile"></p>
</li>
</ul>
<h3 id="6-2-2-HexStringSplit-算法"><a href="#6-2-2-HexStringSplit-算法" class="headerlink" title="6.2.2 HexStringSplit 算法"></a>6.2.2 HexStringSplit 算法</h3><ul>
<li><p>HexStringSplit会将数据从“00000000”到“FFFFFFFF”之间的数据长度按照<strong>n等分</strong>之后算出每一段的其实rowkey和结束rowkey，以此作为拆分点。</p>
</li>
<li><p>例如：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create <span class="string">'mytable'</span>, <span class="string">'base_info'</span>,<span class="string">' extra_info'</span>, &#123;NUMREGIONS =&gt; <span class="number">15</span>, SPLITALGO =&gt; <span class="string">'HexStringSplit'</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="assets/hbasePreSplit.png" alt="hbasePreSplit"></p>
<h2 id="7-region-合并"><a href="#7-region-合并" class="headerlink" title="7. region 合并"></a>7. region 合并</h2><h3 id="7-1-region合并说明"><a href="#7-1-region合并说明" class="headerlink" title="7.1 region合并说明"></a>7.1 region合并说明</h3><ul>
<li>Region的合并不是为了性能,  而是出于维护的目的 .</li>
<li>比如删除了大量的数据 ,这个时候每个Region都变得很小 ,存储多个Region就浪费了 ,这个时候可以把Region合并起来，进而可以减少一些Region服务器节点 </li>
</ul>
<h3 id="7-2-如何进行region合并"><a href="#7-2-如何进行region合并" class="headerlink" title="7.2 如何进行region合并"></a>7.2 如何进行region合并</h3><h4 id="7-2-1-通过Merge类冷合并Region"><a href="#7-2-1-通过Merge类冷合并Region" class="headerlink" title="7.2.1 通过Merge类冷合并Region"></a>7.2.1 通过Merge类冷合并Region</h4><ul>
<li><p>执行合并前，==需要先关闭hbase集群==</p>
</li>
<li><p>创建一张hbase表：</p>
</li>
</ul>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create <span class="string">'test'</span>,<span class="string">'info1'</span>,SPLITS =&gt; [<span class="string">'1000'</span>,<span class="string">'2000'</span>,<span class="string">'3000'</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>查看表region</li>
</ul>
<p><img src="assets/testRegion.png" alt="testRegion"></p>
<ul>
<li><p>需求：</p>
<p>需要把test表中的2个region数据进行合并：<br>test,,1565940912661.62d28d7d20f18debd2e7dac093bc09d8.<br>test,1000,1565940912661.5b6f9e8dad3880bcc825826d12e81436.</p>
</li>
<li><p>这里通过org.apache.hadoop.hbase.util.Merge类来实现，不需要进入hbase shell，直接执行（==需要先关闭hbase集群==）：<br>hbase org.apache.hadoop.hbase.util.Merge test test,,1565940912661.62d28d7d20f18debd2e7dac093bc09d8. test,1000,1565940912661.5b6f9e8dad3880bcc825826d12e81436.</p>
</li>
<li><p>成功后界面观察</p>
</li>
</ul>
<p><img src="assets/testMerge.png" alt="testMerge"></p>
<h4 id="7-2-2-通过online-merge热合并Region"><a href="#7-2-2-通过online-merge热合并Region" class="headerlink" title="7.2.2  通过online_merge热合并Region"></a>7.2.2  通过online_merge热合并Region</h4><ul>
<li><p>==不需要关闭hbase集群==，在线进行合并</p>
</li>
<li><p>与冷合并不同的是，online_merge的传参是Region的hash值，而Region的hash值就是Region名称的最后那段在两个.之间的字符串部分。</p>
</li>
<li><p>需求：需要把test表中的2个region数据进行合并：<br>test,2000,1565940912661.c2212a3956b814a6f0d57a90983a8515.<br>test,3000,1565940912661.553dd4db667814cf2f050561167ca030.</p>
</li>
<li><p>需要进入hbase shell：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">merge_region <span class="string">'c2212a3956b814a6f0d57a90983a8515'</span>,<span class="string">'553dd4db667814cf2f050561167ca030'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>成功后观察界面</p>
</li>
</ul>
<p><img src="assets/online_merge.png" alt="online_merge"></p>
<h2 id="8-HBase集成MapReduce"><a href="#8-HBase集成MapReduce" class="headerlink" title="8. HBase集成MapReduce"></a>8. HBase集成MapReduce</h2><ul>
<li>HBase表中的数据最终都是存储在HDFS上，HBase天生的支持MR的操作，我们可以通过MR直接处理HBase表中的数据，并且MR可以将处理后的结果直接存储到HBase表中。<ul>
<li>参考地址：<a href="http://hbase.apache.org/book.html#mapreduce" target="_blank" rel="noopener">http://hbase.apache.org/book.html#mapreduce</a></li>
</ul>
</li>
</ul>
<h3 id="8-1-实战一"><a href="#8-1-实战一" class="headerlink" title="8.1 实战一"></a>8.1 实战一</h3><ul>
<li>需求：==读取HBase当中myuser这张表的数据，将数据写入到另外一张myuser2表里面去==</li>
</ul>
<ul>
<li><p>第一步：创建myuser2这张hbase表</p>
<p><strong>注意：</strong>列族的名字要与myuser表的列族名字相同</p>
</li>
</ul>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase(main):010:0&gt;</span> create <span class="string">'myuser2'</span>,<span class="string">'f1'</span></span><br></pre></td></tr></table></figure>
<ul>
<li>第二步：创建maven工程并导入jar包</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0-mr1-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.testng<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>testng<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>6.14.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">                   <span class="comment">&lt;!--    &lt;verbal&gt;true&lt;/verbal&gt;--&gt;</span></span><br><span class="line">               <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                               <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                   <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                   <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                       <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                       <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                       <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*/RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                   <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                               <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                           <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                       <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                   <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>第三步：开发MR程序实现功能</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.Cell;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.CellUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.TableName;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Result;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Scan;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseMR</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseMapper</span> <span class="keyword">extends</span> <span class="title">TableMapper</span>&lt;<span class="title">Text</span>,<span class="title">Put</span>&gt;</span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(ImmutableBytesWritable key, Result value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">             <span class="comment">//获取rowkey的字节数组</span></span><br><span class="line">            <span class="keyword">byte</span>[] bytes = key.get();</span><br><span class="line">            String rowkey = Bytes.toString(bytes);</span><br><span class="line">            <span class="comment">//构建一个put对象</span></span><br><span class="line">            Put put = <span class="keyword">new</span> Put(bytes);</span><br><span class="line">            <span class="comment">//获取一行中所有的cell对象</span></span><br><span class="line">            Cell[] cells = value.rawCells();</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">                  <span class="comment">// f1列族</span></span><br><span class="line">                <span class="keyword">if</span>(<span class="string">"f1"</span>.equals(Bytes.toString(CellUtil.cloneFamily(cell))))&#123;</span><br><span class="line">                    <span class="comment">// name列名</span></span><br><span class="line">                     <span class="keyword">if</span>(<span class="string">"name"</span>.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123;</span><br><span class="line">                          put.add(cell);</span><br><span class="line">                     &#125;</span><br><span class="line">                     <span class="comment">// age列名</span></span><br><span class="line">                    <span class="keyword">if</span>(<span class="string">"age"</span>.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123;</span><br><span class="line">                        put.add(cell);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(!put.isEmpty())&#123;</span><br><span class="line">              context.write(<span class="keyword">new</span> Text(rowkey),put);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">     <span class="keyword">public</span>  <span class="keyword">static</span>  <span class="class"><span class="keyword">class</span> <span class="title">HbaseReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">Text</span>,<span class="title">Put</span>,<span class="title">ImmutableBytesWritable</span>&gt;</span>&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Put&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">             <span class="keyword">for</span> (Put put : values) &#123;</span><br><span class="line">                 context.write(<span class="keyword">null</span>,put);</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(HBaseMR.class);</span><br><span class="line">        <span class="comment">//使用TableMapReduceUtil 工具类来初始化我们的mapper</span></span><br><span class="line">        TableMapReduceUtil.initTableMapperJob(TableName.valueOf(args[<span class="number">0</span>]),scan,HBaseMapper.class,Text.class,Put.class,job);</span><br><span class="line">        <span class="comment">//使用TableMapReduceUtil 工具类来初始化我们的reducer</span></span><br><span class="line">        TableMapReduceUtil.initTableReducerJob(args[<span class="number">1</span>],HbaseReducer.class,job);</span><br><span class="line">        <span class="comment">//设置reduce task个数</span></span><br><span class="line">         job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>打成jar包提交到集群中运行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hbase_java_api-1.0-SNAPSHOT.jar com.kaikeba.HBaseMR t1 t2</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="8-2-实战二"><a href="#8-2-实战二" class="headerlink" title="8.2 实战二"></a>8.2 实战二</h3><ul>
<li><p>需求 读取hdfs上面的数据，写入到hbase表里面去</p>
<p>node03执行以下命令准备数据文件，并将数据文件上传到HDFS上面去</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -<span class="selector-tag">p</span> /hbase/input</span><br><span class="line">cd /kfly/install</span><br><span class="line">vim </span><br><span class="line">user.txt</span><br><span class="line"></span><br><span class="line"><span class="number">0007</span>	zhangsan	<span class="number">18</span></span><br><span class="line"><span class="number">0008</span>	lisi	<span class="number">25</span></span><br><span class="line"><span class="number">0009</span>	wangwu	<span class="number">20</span></span><br><span class="line"></span><br><span class="line">将文件上传到hdfs的路径下面去</span><br><span class="line">hdfs dfs -put kflyb/install/user<span class="selector-class">.txt</span>   /hbase/input/</span><br></pre></td></tr></table></figure>
</li>
<li><p>代码开发</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hdfs2Hbase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            context.write(value,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HBASEReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">Text</span>,<span class="title">NullWritable</span>,<span class="title">ImmutableBytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] split = key.toString().split(<span class="string">" "</span>);</span><br><span class="line">            Put put = <span class="keyword">new</span> Put(Bytes.toBytes(split[<span class="number">0</span>]));</span><br><span class="line">            put.addColumn(<span class="string">"f1"</span>.getBytes(),<span class="string">"name"</span>.getBytes(),split[<span class="number">1</span>].getBytes());</span><br><span class="line">            put.addColumn(<span class="string">"f1"</span>.getBytes(),<span class="string">"age"</span>.getBytes(), split[<span class="number">2</span>].getBytes());</span><br><span class="line">            context.write(<span class="keyword">new</span> ImmutableBytesWritable(Bytes.toBytes(split[<span class="number">0</span>])),put);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(Hdfs2Hbase.class);</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        <span class="comment">//输入文件路径</span></span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        job.setMapperClass(HdfsMapper.class);</span><br><span class="line">        <span class="comment">//map端的输出的key value 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定输出到hbase的表名</span></span><br><span class="line">        TableMapReduceUtil.initTableReducerJob(args[<span class="number">1</span>],HBASEReducer.class,job);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置reduce个数</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建hbase表 t3</p>
</li>
</ul>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create <span class="string">'t3'</span>,<span class="string">'f1'</span></span><br></pre></td></tr></table></figure>
<ul>
<li>打成jar包提交到集群中运行</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hbase_java_api-1.0-SNAPSHOT.jar com.kaikeba.Hdfs2Hbase /data/user.txt t3</span><br></pre></td></tr></table></figure>
<h3 id="8-3-实战三"><a href="#8-3-实战三" class="headerlink" title="8.3 实战三"></a>8.3 实战三</h3><ul>
<li><p>需求</p>
<ul>
<li>==通过bulkload的方式批量加载数据到HBase表中==</li>
<li>==将我们hdfs上面的这个路径/hbase/input/user.txt的数据文件，转换成HFile格式，然后load到myuser2这张表里面去==</li>
</ul>
</li>
<li><p>知识点描述</p>
<ul>
<li>加载数据到HBase当中去的方式多种多样，我们可以使用HBase的javaAPI或者使用sqoop将我们的数据写入或者导入到HBase当中去，但是这些方式不是慢就是在导入的过程的占用Region资源导致效率低下</li>
<li>我们也可以通过MR的程序，将我们的数据直接转换成HBase的最终存储格式HFile，然后直接load数据到HBase当中去即可</li>
</ul>
</li>
<li><p>HBase数据正常写流程回顾</p>
<p><img src="assets/hbase-write.png" alt="hbase-write"></p>
</li>
<li><p>bulkload方式的处理示意图</p>
</li>
</ul>
<p><img src="assets/bulkload.png" alt=""></p>
<ul>
<li><p>好处</p>
<ul>
<li>导入过程不占用Region资源</li>
<li>能快速导入海量的数据</li>
<li>节省内存</li>
</ul>
</li>
<li><p>==1、开发生成HFile文件的代码==</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.TableName;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Connection;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseLoad</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">LoadMapper</span>  <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">ImmutableBytesWritable</span>,<span class="title">Put</span>&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Mapper.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] split = value.toString().split(<span class="string">" "</span>);</span><br><span class="line">            Put put = <span class="keyword">new</span> Put(Bytes.toBytes(split[<span class="number">0</span>]));</span><br><span class="line">            put.addColumn(<span class="string">"f1"</span>.getBytes(),<span class="string">"name"</span>.getBytes(),split[<span class="number">1</span>].getBytes());</span><br><span class="line">            put.addColumn(<span class="string">"f1"</span>.getBytes(),<span class="string">"age"</span>.getBytes(), split[<span class="number">2</span>].getBytes());</span><br><span class="line">            context.write(<span class="keyword">new</span> ImmutableBytesWritable(Bytes.toBytes(split[<span class="number">0</span>])),put);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">final</span> String INPUT_PATH=  <span class="string">"hdfs://node01:8020/hbase/input"</span>;</span><br><span class="line">            <span class="keyword">final</span> String OUTPUT_PATH= <span class="string">"hdfs://node01:8020/hbase/output_file"</span>;</span><br><span class="line">            Configuration conf = HBaseConfiguration.create();</span><br><span class="line"></span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(<span class="string">"t4"</span>));</span><br><span class="line">            Job job= Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">            job.setJarByClass(HBaseLoad.class);</span><br><span class="line">            job.setMapperClass(LoadMapper.class);</span><br><span class="line">            job.setMapOutputKeyClass(ImmutableBytesWritable.class);</span><br><span class="line">            job.setMapOutputValueClass(Put.class);</span><br><span class="line">        </span><br><span class="line">            <span class="comment">//指定输出的类型HFileOutputFormat2</span></span><br><span class="line">            job.setOutputFormatClass(HFileOutputFormat2.class);</span><br><span class="line"></span><br><span class="line">         HFileOutputFormat2.configureIncrementalLoad(job,table,connection.getRegionLocator(TableName.valueOf(<span class="string">"t4"</span>)));</span><br><span class="line">            FileInputFormat.addInputPath(job,<span class="keyword">new</span> Path(INPUT_PATH));</span><br><span class="line">            FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(OUTPUT_PATH));</span><br><span class="line">            System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>==2、打成jar包提交到集群中运行==</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hbase_java_api-1.0-SNAPSHOT.jar com.kaikeba.HBaseLoad</span><br></pre></td></tr></table></figure>
<ul>
<li>==3、观察HDFS上输出的结果==</li>
</ul>
<p><img src="assets/f1.png" alt="f1"></p>
<p><img src="assets/HFile文件.png" alt="HFile文件"></p>
<ul>
<li><p>==4、加载HFile文件到hbase表中==</p>
<ul>
<li>方式一：代码加载</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kaikeba;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.TableName;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Admin;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Connection;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoadData</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"node01,node02,node03"</span>);</span><br><span class="line">    <span class="comment">//获取数据库连接</span></span><br><span class="line">    Connection connection =  ConnectionFactory.createConnection(configuration);</span><br><span class="line">    <span class="comment">//获取表的管理器对象</span></span><br><span class="line">    Admin admin = connection.getAdmin();</span><br><span class="line">    <span class="comment">//获取table对象</span></span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">"t4"</span>);</span><br><span class="line">    Table table = connection.getTable(tableName);</span><br><span class="line">    <span class="comment">//构建LoadIncrementalHFiles加载HFile文件</span></span><br><span class="line">    LoadIncrementalHFiles load = <span class="keyword">new</span> LoadIncrementalHFiles(configuration);</span><br><span class="line">    load.doBulkLoad(<span class="keyword">new</span> Path(<span class="string">"hdfs://node01:8020/hbase/output_file"</span>), admin,table,connection.getRegionLocator(tableName));</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>方式二：命令加载</li>
</ul>
<p>先将hbase的jar包添加到hadoop的classpath路径下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">先将hbase的jar包添加到hadoop的classpath路径下</span><br><span class="line">export HBASE_HOME=/kfly/install/hbase-1.2.0-cdh5.14.2/</span><br><span class="line">export HADOOP_HOME=/kfly/install/hadoop-2.6.0/</span><br><span class="line">export HADOOP_CLASSPATH=`$&#123;HBASE_HOME&#125;/bin/hbase mapredcp`</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>运行命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn jar /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar   completebulkload /hbase/output_hfile myuser2</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>​    </p>
<h2 id="8-HBase集成Hive"><a href="#8-HBase集成Hive" class="headerlink" title="8. HBase集成Hive"></a>8. HBase集成Hive</h2><ul>
<li>Hive提供了与HBase的集成，使得能够在HBase表上使用hive sql 语句进行查询、插入操作以及进行Join和Union等复杂查询，同时也可以将hive表中的数据映射到Hbase中</li>
</ul>
<h3 id="8-1-HBase与Hive的对比（"><a href="#8-1-HBase与Hive的对比（" class="headerlink" title="8.1 HBase与Hive的对比（"></a>8.1 HBase与Hive的对比（</h3><h4 id="8-1-1-Hive"><a href="#8-1-1-Hive" class="headerlink" title="8.1.1 Hive"></a>8.1.1 Hive</h4><ul>
<li><p>数据仓库</p>
<p>Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。</p>
</li>
<li><p>用于数据分析、清洗                </p>
<p>Hive适用于离线的数据分析和清洗，延迟较高</p>
</li>
<li><p>基于HDFS、MapReduce</p>
<p>Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。（不要钻不需要执行MapReduce代码的情况的牛角尖）</p>
</li>
</ul>
<h4 id="8-1-2-HBase"><a href="#8-1-2-HBase" class="headerlink" title="8.1.2 HBase"></a>8.1.2 HBase</h4><ul>
<li><p>数据库</p>
<p>是一种面向列存储的非关系型数据库。</p>
</li>
<li><p>用于存储结构化和非结构话的数据</p>
<p>适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</p>
</li>
<li><p>基于HDFS</p>
<p>数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。</p>
</li>
<li><p>延迟较低，接入在线业务使用</p>
<p>面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。</p>
</li>
</ul>
<h5 id="8-1-3-总结：Hive与HBase"><a href="#8-1-3-总结：Hive与HBase" class="headerlink" title="8.1.3 总结：Hive与HBase"></a>8.1.3 总结：Hive与HBase</h5><ul>
<li>Hive和Hbase是两种基于Hadoop的不同技术，Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到HBase，或者从HBase写回Hive。</li>
</ul>
<h3 id="9-2-整合配置"><a href="#9-2-整合配置" class="headerlink" title="9.2 整合配置"></a>9.2 整合配置</h3><h4 id="9-2-1-拷贝jar包"><a href="#9-2-1-拷贝jar包" class="headerlink" title="9.2.1 拷贝jar包"></a>9.2.1 拷贝jar包</h4><ul>
<li><p>将我们HBase的五个jar包拷贝到hive的lib目录下</p>
</li>
<li><p>hbase的jar包都在/kfly/install/hbase-1.2.0-cdh5.14.2/lib</p>
</li>
<li><p>我们需要拷贝五个jar包名字如下</p>
</li>
</ul>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">hbase-client-1</span><span class="selector-class">.2</span><span class="selector-class">.0-cdh5</span><span class="selector-class">.14</span><span class="selector-class">.2</span><span class="selector-class">.jar</span>                  </span><br><span class="line"><span class="selector-tag">hbase-hadoop2-compat-1</span><span class="selector-class">.2</span><span class="selector-class">.0-cdh5</span><span class="selector-class">.14</span><span class="selector-class">.2</span><span class="selector-class">.jar</span> </span><br><span class="line"><span class="selector-tag">hbase-hadoop-compat-1</span><span class="selector-class">.2</span><span class="selector-class">.0-cdh5</span><span class="selector-class">.14</span><span class="selector-class">.2</span><span class="selector-class">.jar</span>  </span><br><span class="line"><span class="selector-tag">hbase-it-1</span><span class="selector-class">.2</span><span class="selector-class">.0-cdh5</span><span class="selector-class">.14</span><span class="selector-class">.2</span><span class="selector-class">.jar</span>    </span><br><span class="line"><span class="selector-tag">hbase-server-1</span><span class="selector-class">.2</span><span class="selector-class">.0-cdh5</span><span class="selector-class">.14</span><span class="selector-class">.2</span><span class="selector-class">.jar</span></span><br></pre></td></tr></table></figure>
<ul>
<li>我们直接在node03执行以下命令，通过创建软连接的方式来进行jar包的依赖</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar              /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar   </span><br><span class="line"></span><br><span class="line">ln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-hadoop2-compat-1.2.0-cdh5.14.2.jar      /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-hadoop2-compat-1.2.0-cdh5.14.2.jar             </span><br><span class="line">ln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-hadoop-compat-1.2.0-cdh5.14.2.jar       /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-hadoop-compat-1.2.0-cdh5.14.2.jar            </span><br><span class="line">ln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-it-1.2.0-cdh5.14.2.jar     /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-it-1.2.0-cdh5.14.2.jar    </span><br><span class="line"></span><br><span class="line">ln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar          /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar</span><br></pre></td></tr></table></figure>
<h4 id="9-2-2-修改hive的配置文件"><a href="#9-2-2-修改hive的配置文件" class="headerlink" title="9.2.2 修改hive的配置文件"></a>9.2.2 修改hive的配置文件</h4><ul>
<li>编辑<strong>node03</strong>服务器上面的hive的配置文件hive-site.xml</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /kfly/install/hive-1.1.0-cdh5.14.2/conf</span><br><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure>
<ul>
<li>添加以下两个属性的配置</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>node01,node02,node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>node01,node02,node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="9-2-3-修改hive-env-sh配置文件"><a href="#9-2-3-修改hive-env-sh配置文件" class="headerlink" title="9.2.3 修改hive-env.sh配置文件"></a>9.2.3 修改hive-env.sh配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /kfly/install/hive-1.1.0-cdh5.14.2/conf</span><br><span class="line">vim hive-env.sh</span><br></pre></td></tr></table></figure>
<ul>
<li>添加以下配置</li>
</ul>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">HADOOP_HOME</span>=/export/servers/hadoop-2.6.0</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">HBASE_HOME</span>=/export/servers/hbase-1.2.0-cdh5.14.2</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">HIVE_CONF_DIR</span>=/export/servers/hive-1.1.0-cdh5.14.2/conf</span><br></pre></td></tr></table></figure>
<h3 id="9-3-需求一：将hive表当中分析的结果保存到hbase表当中去"><a href="#9-3-需求一：将hive表当中分析的结果保存到hbase表当中去" class="headerlink" title="9.3 需求一：将hive表当中分析的结果保存到hbase表当中去"></a>9.3 需求一：将hive表当中分析的结果保存到hbase表当中去</h3><h4 id="9-3-1-hive当中建表"><a href="#9-3-1-hive当中建表" class="headerlink" title="9.3.1 hive当中建表"></a>9.3.1 hive当中建表</h4><ul>
<li>node03执行以下命令，进入hive客户端，并创建hive表</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /kfly/install/hive-1.1.0-cdh5.14.2/</span><br><span class="line">bin/hive</span><br></pre></td></tr></table></figure>
<ul>
<li>创建hive数据库与hive对应的数据库表</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create database course;</span><br><span class="line">use course;</span><br><span class="line"></span><br><span class="line">create external table if not exists course.score(id int,cname string,score int) row format delimited fields terminated by &apos;\t&apos; stored as textfile ;</span><br></pre></td></tr></table></figure>
<h4 id="9-3-2-准备数据内容如下并加载到hive表"><a href="#9-3-2-准备数据内容如下并加载到hive表" class="headerlink" title="9.3.2 准备数据内容如下并加载到hive表"></a>9.3.2 准备数据内容如下并加载到hive表</h4><ul>
<li>node03执行以下命令，创建数据文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /kfly/install/hivedatas</span><br><span class="line">vi hive-hbase.txt</span><br></pre></td></tr></table></figure>
<ul>
<li>文件内容如下</li>
</ul>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>	zhangsan	<span class="number">80</span></span><br><span class="line"><span class="number">2</span>	lisi	<span class="number">60</span></span><br><span class="line"><span class="number">3</span>	wangwu	<span class="number">30</span></span><br><span class="line"><span class="number">4</span>	zhaoliu	<span class="number">70</span></span><br></pre></td></tr></table></figure>
<ul>
<li>进入hive客户端进行加载数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (course)&gt; load data local inpath &apos;/kfly/doc/hive-hbase.txt&apos; into table score;</span><br><span class="line">hive (course)&gt; select * from score;</span><br></pre></td></tr></table></figure>
<h4 id="9-3-3-创建hive管理表与HBase进行映射"><a href="#9-3-3-创建hive管理表与HBase进行映射" class="headerlink" title="9.3.3 创建hive管理表与HBase进行映射"></a>9.3.3 创建hive管理表与HBase进行映射</h4><ul>
<li><p>我们可以创建一个hive的管理表与hbase当中的表进行映射，hive管理表当中的数据，都会存储到hbase上面去</p>
</li>
<li><p>hive当中创建内部表</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> course.hbase_score(<span class="keyword">id</span> <span class="built_in">int</span>,cname <span class="keyword">string</span>,score <span class="built_in">int</span>) <span class="keyword">stored</span> <span class="keyword">by</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span>  <span class="keyword">with</span> serdeproperties(<span class="string">"hbase.columns.mapping"</span> = <span class="string">"cf:name,cf:score"</span>) tblproperties(<span class="string">"hbase.table.name"</span> = <span class="string">"hbase_score"</span>);</span><br></pre></td></tr></table></figure>
<ul>
<li>通过insert  overwrite select  插入数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table course.hbase_score select id,cname,score from course.score;</span><br></pre></td></tr></table></figure>
<h4 id="9-3-4-hbase当中查看表hbase-score"><a href="#9-3-4-hbase当中查看表hbase-score" class="headerlink" title="9.3.4 hbase当中查看表hbase_score"></a>9.3.4 hbase当中查看表hbase_score</h4><ul>
<li>进入hbase的客户端查看表hbase_score，并查看当中的数据</li>
</ul>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase(main):023:0&gt;</span> list</span><br><span class="line"></span><br><span class="line">TABLE                                                                                 hbase_score                                                                           myuser                                                                                 myuser2                                                                               student                                                                               user                                                                                   <span class="number">5</span> row(s) <span class="keyword">in</span> <span class="number">0</span>.<span class="number">0210</span> seconds</span><br><span class="line">=&gt; [<span class="string">"hbase_score"</span>, <span class="string">"myuser"</span>, <span class="string">"myuser2"</span>, <span class="string">"student"</span>, <span class="string">"user"</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">hbase(main):024:0&gt;</span> scan <span class="string">'hbase_score'</span></span><br><span class="line"></span><br><span class="line">ROW                      COLUMN+CELL                                                   </span><br><span class="line"> <span class="number">1</span>                       column=<span class="symbol">cf:</span>name, timestamp=<span class="number">1550628395266</span>, value=zhangsan       </span><br><span class="line"> <span class="number">1</span>                       column=<span class="symbol">cf:</span>score, timestamp=<span class="number">1550628395266</span>, value=<span class="number">80</span>           </span><br><span class="line"> <span class="number">2</span>                       column=<span class="symbol">cf:</span>name, timestamp=<span class="number">1550628395266</span>, value=lisi           </span><br><span class="line"> <span class="number">2</span>                       column=<span class="symbol">cf:</span>score, timestamp=<span class="number">1550628395266</span>, value=<span class="number">60</span>           </span><br><span class="line"> <span class="number">3</span>                       column=<span class="symbol">cf:</span>name, timestamp=<span class="number">1550628395266</span>, value=wangwu         </span><br><span class="line"> <span class="number">3</span>                       column=<span class="symbol">cf:</span>score, timestamp=<span class="number">1550628395266</span>, value=<span class="number">30</span>           </span><br><span class="line"> <span class="number">4</span>                       column=<span class="symbol">cf:</span>name, timestamp=<span class="number">1550628395266</span>, value=zhaoliu       </span><br><span class="line"> <span class="number">4</span>                       column=<span class="symbol">cf:</span>score, timestamp=<span class="number">1550628395266</span>, value=<span class="number">70</span>           </span><br><span class="line"><span class="number">4</span> row(s) <span class="keyword">in</span> <span class="number">0</span>.<span class="number">0360</span> seconds</span><br></pre></td></tr></table></figure>
<h3 id="9-4-需求二：创建hive外部表，映射HBase当中已有的表模型（5分钟）"><a href="#9-4-需求二：创建hive外部表，映射HBase当中已有的表模型（5分钟）" class="headerlink" title="9.4 需求二：创建hive外部表，映射HBase当中已有的表模型（5分钟）"></a>9.4 需求二：创建hive外部表，映射HBase当中已有的表模型（5分钟）</h3><h4 id="9-4-1-HBase当中创建表并手动插入加载一些数据"><a href="#9-4-1-HBase当中创建表并手动插入加载一些数据" class="headerlink" title="9.4.1 HBase当中创建表并手动插入加载一些数据"></a>9.4.1 HBase当中创建表并手动插入加载一些数据</h4><ul>
<li>进入HBase的shell客户端，</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hbase shell</span><br></pre></td></tr></table></figure>
<ul>
<li>手动创建一张表，并插入加载一些数据进去</li>
</ul>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一张表</span></span><br><span class="line">create <span class="string">'hbase_hive_score'</span>,&#123; NAME =&gt;<span class="string">'cf'</span>&#125;</span><br><span class="line"><span class="comment"># 通过put插入数据到hbase表</span></span><br><span class="line">put <span class="string">'hbase_hive_score'</span>,<span class="string">'1'</span>,<span class="string">'cf:name'</span>,<span class="string">'zhangsan'</span></span><br><span class="line">put <span class="string">'hbase_hive_score'</span>,<span class="string">'1'</span>,<span class="string">'cf:score'</span>, <span class="string">'95'</span></span><br><span class="line">put <span class="string">'hbase_hive_score'</span>,<span class="string">'2'</span>,<span class="string">'cf:name'</span>,<span class="string">'lisi'</span></span><br><span class="line">put <span class="string">'hbase_hive_score'</span>,<span class="string">'2'</span>,<span class="string">'cf:score'</span>, <span class="string">'96'</span></span><br><span class="line">put <span class="string">'hbase_hive_score'</span>,<span class="string">'3'</span>,<span class="string">'cf:name'</span>,<span class="string">'wangwu'</span></span><br><span class="line">put <span class="string">'hbase_hive_score'</span>,<span class="string">'3'</span>,<span class="string">'cf:score'</span>, <span class="string">'97'</span></span><br></pre></td></tr></table></figure>
<h4 id="9-4-2-建立hive的外部表，映射HBase当中的表以及字段"><a href="#9-4-2-建立hive的外部表，映射HBase当中的表以及字段" class="headerlink" title="9.4.2 建立hive的外部表，映射HBase当中的表以及字段"></a>9.4.2 建立hive的外部表，映射HBase当中的表以及字段</h4><ul>
<li><p>在hive当中建立外部表</p>
</li>
<li><p>进入hive客户端，然后执行以下命令进行创建hive外部表，就可以实现映射HBase当中的表数据</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE external TABLE course.hbase2hive(id int, name string, score int) STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,cf:name,cf:score&quot;) TBLPROPERTIES(&quot;hbase.table.name&quot; =&quot;hbase_hive_score&quot;);</span><br></pre></td></tr></table></figure>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
