<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="天行健、君子以自强不息；地势坤，君子以厚德载物。">
    <meta name="keyword"  content="兰草">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        Flume日志采集框架 - kfly的博客 | kfly&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_1598291_q3el2wqimj.css" type="text/css">
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>kfly</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont iconhome"></i>
                    <span>主页</span>
                </a>
            </li>
 	   <li >
                <a href="/spec/">
                    <i class="iconfont iconzhuanti"></i>
                    <span>专题</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>简历</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Flume日志采集框架"><span class="toc-text">Flume日志采集框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Flume是什么"><span class="toc-text">1. Flume是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Flume的架构"><span class="toc-text">2. Flume的架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-核心概念"><span class="toc-text">2.1 核心概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-Agent结构"><span class="toc-text">2.2 Agent结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-Source"><span class="toc-text">2.3      Source</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4、Channel"><span class="toc-text">2.4、Channel</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4、Sink"><span class="toc-text">2.4、Sink</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Flume采集系统结构图"><span class="toc-text">3. Flume采集系统结构图</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-简单结构"><span class="toc-text">3.1 简单结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-复杂结构"><span class="toc-text">3.2 复杂结构</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Flume安装部署"><span class="toc-text">4. Flume安装部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Flume实战"><span class="toc-text">5. Flume实战</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-采集文件到控制台"><span class="toc-text">5.1 采集文件到控制台</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-采集目录到HDFS"><span class="toc-text">5.3 采集目录到HDFS</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Name-the-components-on-this-agent"><span class="toc-text">Name the components on this agent</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#配置source"><span class="toc-text">配置source</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#是否将文件的绝对路径添加到header"><span class="toc-text">是否将文件的绝对路径添加到header</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-高可用配置案例"><span class="toc-text">6. 高可用配置案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-failover故障转移"><span class="toc-text">6.1 failover故障转移</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-load-balance负载均衡"><span class="toc-text">6.2 load balance负载均衡</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-flume企业案例"><span class="toc-text">7. flume企业案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-1-flume案例之静态拦截器使用"><span class="toc-text">7.1 flume案例之静态拦截器使用</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Name-the-components-on-this-agent-1"><span class="toc-text">Name the components on this agent</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Describe-configure-the-source"><span class="toc-text">Describe/configure the source</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#static拦截器的功能就是往采集到的数据的header中插入自己定义的key-value对"><span class="toc-text">static拦截器的功能就是往采集到的数据的header中插入自己定义的key-value对</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#自己进行设置-我们这里的key和value相当于键值对-k-type-v-access"><span class="toc-text">自己进行设置,我们这里的key和value相当于键值对,k=type v=access</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#自己进行设置"><span class="toc-text">自己进行设置</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#自己进行设置-1"><span class="toc-text">自己进行设置</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Use-a-channel-which-buffers-events-in-memory"><span class="toc-text">Use a channel which buffers events in memory</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Describe-the-sink"><span class="toc-text">Describe the sink</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Bind-the-source-and-sink-to-the-channel"><span class="toc-text">Bind the source and sink to the channel</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-flume案例之自定义拦截器"><span class="toc-text">7.2 flume案例之自定义拦截器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-flume自定义Source"><span class="toc-text">8. flume自定义Source</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-场景描述"><span class="toc-text">8.1 场景描述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-2-自定义MysqlSource步骤"><span class="toc-text">8.2 自定义MysqlSource步骤</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Name-the-components-on-this-agent-2"><span class="toc-text">Name the components on this agent</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Describe-configure-the-source-1"><span class="toc-text">Describe/configure the source</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#老师的是node01-同学们改成自己的节点-一定要注意"><span class="toc-text">老师的是node01,同学们改成自己的节点 一定要注意</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Describe-the-channel"><span class="toc-text">Describe the channel</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Describe-the-sink-1"><span class="toc-text">Describe the sink</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-flume自定义Sink"><span class="toc-text">9. flume自定义Sink</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#9-1-场景描述"><span class="toc-text">9.1 场景描述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-2-自定义MysqlSink步骤"><span class="toc-text">9.2 自定义MysqlSink步骤</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-Flume实际使用注意事项"><span class="toc-text">10. Flume实际使用注意事项</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1、注意启动脚本命名的书写"><span class="toc-text">1、注意启动脚本命名的书写</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2、channel参数"><span class="toc-text">2、channel参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3、日志采集到HDFS配置说明1（sink端）"><span class="toc-text">3、日志采集到HDFS配置说明1（sink端）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4、日志采集到HDFS配置说明2（sink端）"><span class="toc-text">4、日志采集到HDFS配置说明2（sink端）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5、实现数据的断点续传"><span class="toc-text">5、实现数据的断点续传</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6、flume的header参数配置讲解"><span class="toc-text">6、flume的header参数配置讲解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-tail-tail-f-tail-F区别"><span class="toc-text">7 tail / tail -f /tail -F区别</span></a></li></ol></li></ol></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        Flume日志采集框架
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-11-21 14:46:37</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#flume" title="flume">flume</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h1 id="Flume日志采集框架"><a href="#Flume日志采集框架" class="headerlink" title="Flume日志采集框架"></a>Flume日志采集框架</h1><h3 id="1-Flume是什么"><a href="#1-Flume是什么" class="headerlink" title="1. Flume是什么"></a>1. Flume是什么</h3><p><img src="assets/image-20191126223347760.png" alt="image-20191126223347760"></p>
<pre><code>    在一个完整的离线大数据处理系统中，除了hdfs+mapreduce+hive组成分析系统的核心之外，还需要数据采集、结果数据导出、任务调度等不可或缺的辅助系统，而这些辅助工具在hadoop生态体系中都有便捷的开源框架。
</code></pre><ul>
<li>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的==海量日志采集、聚合和传输的系统==</li>
<li>Flume支持在日志系统中定制各类数据发送方，用于收集数据；</li>
<li>Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</li>
</ul>
<p>重构后的版本统称为 Flume NG（next generation）；改动的另一原因是将 Flume 纳入 apache 旗下，cloudera Flume 改名为 Apache Flume。</p>
<p>　　备注：Flume参考资料</p>
<p>　　　　官方网站： <a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a><br>　　　　用户文档： <a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html</a><br>　　　　开发文档： <a href="http://flume.apache.org/FlumeDeveloperGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeDeveloperGuide.html</a></p>
<h3 id="2-Flume的架构"><a href="#2-Flume的架构" class="headerlink" title="2. Flume的架构"></a>2. Flume的架构</h3><ul>
<li>Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。</li>
<li>Flume分布式系统中==最核心的角色是agent==，flume采集系统就是由一个个agent所连接起来形成。</li>
</ul>
<h4 id="2-1-核心概念"><a href="#2-1-核心概念" class="headerlink" title="2.1 核心概念"></a>2.1 核心概念</h4><ul>
<li><p>Client：</p>
<ul>
<li>Client生产数据，运行在一个独立的线程。</li>
</ul>
</li>
<li><p>Event：</p>
<ul>
<li>一个数据单元，消息头和消息体组成。（Events可以是日志记录、 avro 对象等。）</li>
</ul>
</li>
<li><p>Flow：</p>
<ul>
<li>Event从源点到达目的点的迁移的抽象。</li>
</ul>
</li>
<li><p>Agent：</p>
<ul>
<li>一个独立的Flume进程，包含组件Source、 Channel、 Sink。（Agent使用JVM 运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。）</li>
</ul>
</li>
<li><p>Source：</p>
<ul>
<li>数据收集组件。（source从Client收集数据，传递给Channel）</li>
</ul>
</li>
<li><p>Channel：</p>
<ul>
<li>中转Event的一个临时存储，保存由Source组件传递过来的Event。（Channel连接 sources 和 sinks ，这个有点像一个队列。）</li>
</ul>
</li>
<li><p>Sink： </p>
<ul>
<li>从Channel中读取并移除Event， 将Event传递到FlowPipeline中的下一个Agent（如果有的话）（Sink从Channel收集数据，运行在一个独立线程。） </li>
</ul>
</li>
</ul>
<h4 id="2-2-Agent结构"><a href="#2-2-Agent结构" class="headerlink" title="2.2 Agent结构"></a>2.2 Agent结构</h4><p>　Flume 运行的核心是 Agent。Flume以agent为最小的独立运行单位。一个agent就是一个JVM。它是一个完整的数据收集工具，含有三个核心组件，分别是</p>
<p>　　 source、 channel、 sink。通过这些组件， Event 可以从一个地方流向另一个地方，如下图所示。</p>
<p>　　<img src="https://images2017.cnblogs.com/blog/999804/201711/999804-20171108130603872-780242084.png" alt="img"></p>
<h4 id="2-3-Source"><a href="#2-3-Source" class="headerlink" title="2.3      Source"></a>2.3      Source</h4><ul>
<li><p>　    Source是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入Channel中。 Flume提供了很多内置的Source， 支持 Avro， log4j， syslog 和 http post(body为json格式)。可以让应用程序同已有的Source直接打交道，如AvroSource，SyslogTcpSource。 如果内置的Source无法满足需要， Flume还支持自定义Source。<br>　　<img src="https://images2017.cnblogs.com/blog/999804/201711/999804-20171108130818481-670496307.png" alt="img"></p>
</li>
<li><p>　　source类型：</p>
</li>
</ul>
<p><img src="https://images2017.cnblogs.com/blog/999804/201711/999804-20171108130931325-512757774.png" alt="img"></p>
<h4 id="2-4、Channel"><a href="#2-4、Channel" class="headerlink" title="2.4、Channel"></a>2.4、Channel</h4><ul>
<li><p>　　Channel是连接Source和Sink的组件，大家可以将它看做一个数据的缓冲区（数据队列），它可以将事件暂存到内存中也可以持久化到本地磁盘上， 直到Sink处理完该事件。介绍两个较为常用的Channel， MemoryChannel和FileChannel。</p>
</li>
<li><p>　　Channel类型：</p>
</li>
</ul>
<p>　　<img src="https://images2017.cnblogs.com/blog/999804/201711/999804-20171108131248653-2068131817.png" alt="img"></p>
<h4 id="2-4、Sink"><a href="#2-4、Sink" class="headerlink" title="2.4、Sink"></a>2.4、Sink</h4><ul>
<li>　　Sink从Channel中取出事件，然后将数据发到别处，可以向文件系统、数据库、 hadoop存数据， 也可以是其他agent的Source。在日志数据较少时，可以将数据存储在文件系统中，并且设定一定的时间间隔保存数据。</li>
</ul>
<p><img src="https://images2017.cnblogs.com/blog/999804/201711/999804-20171108131438466-1752568401.png" alt="img"></p>
<ul>
<li>　　Sink类型：</li>
</ul>
<p>　　<img src="https://images2017.cnblogs.com/blog/999804/201711/999804-20171108131516809-1930573599.png" alt="img"></p>
<h3 id="3-Flume采集系统结构图"><a href="#3-Flume采集系统结构图" class="headerlink" title="3. Flume采集系统结构图"></a>3. Flume采集系统结构图</h3><h4 id="3-1-简单结构"><a href="#3-1-简单结构" class="headerlink" title="3.1 简单结构"></a>3.1 简单结构</h4><ul>
<li>单个agent采集数据</li>
</ul>
<p><img src="assets/flume.png" alt=""></p>
<h4 id="3-2-复杂结构"><a href="#3-2-复杂结构" class="headerlink" title="3.2 复杂结构"></a>3.2 复杂结构</h4><ul>
<li>2个agent串联</li>
</ul>
<p><img src="assets/UserGuide_image03.png" alt="UserGuide_image03"></p>
<ul>
<li>多个agent串联</li>
</ul>
<p><img src="assets/UserGuide_image02.png" alt=""></p>
<ul>
<li>多个channel</li>
</ul>
<p><img src="assets/UserGuide_image04.png" alt=""></p>
<h3 id="4-Flume安装部署"><a href="#4-Flume安装部署" class="headerlink" title="4. Flume安装部署"></a>4. Flume安装部署</h3><p>==Flume安装很简单，解压好基本上就可以使用==</p>
<ul>
<li><p>1、下载安装包</p>
<ul>
<li><a href="http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.2.tar.gz</a></li>
<li>flume-ng-1.6.0-cdh5.14.2.tar.gz</li>
</ul>
</li>
<li><p>2、规划安装目录</p>
<ul>
<li>/kkb/install</li>
</ul>
</li>
<li><p>3、上传安装包到服务器</p>
</li>
<li><p>4、解压安装包到指定的规划目录</p>
<ul>
<li>tar -zxvf flume-ng-1.6.0-cdh5.14.2.tar.gz -C /kkb/install</li>
</ul>
</li>
<li><p>5、重命名解压目录</p>
<ul>
<li>mv apache-flume-1.6.0-cdh5.14.2-bin  flume-1.6.0-cdh5.14.2</li>
</ul>
</li>
<li><p>6、修改配置</p>
<ul>
<li><p>进入到flume安装目录下的conf文件夹中</p>
<ul>
<li><p>先重命名文件</p>
<ul>
<li>mv flume-env.sh.template flume-env.sh</li>
</ul>
</li>
<li><p>修改文件，添加java环境变量</p>
<ul>
<li>vim flume-env.sh</li>
</ul>
<pre><code class="shell">export JAVA_HOME=/kkb/install/jdk1.8.0_141
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="5-Flume实战"><a href="#5-Flume实战" class="headerlink" title="5. Flume实战"></a>5. Flume实战</h3><h4 id="5-1-采集文件到控制台"><a href="#5-1-采集文件到控制台" class="headerlink" title="5.1 采集文件到控制台"></a>5.1 采集文件到控制台</h4><ul>
<li><p>1、需求描述</p>
<pre><code>监控一个文件如果有新增的内容就把数据采集之后打印控制台，通常用于测试/调试目的
</code></pre></li>
<li><p>2、==flume配置文件开发==</p>
<ul>
<li>在flume的安装目录下创建一个文件夹myconf， 后期存放flume开发的配置文件<ul>
<li>mkdir /kfly/install/flume-1.6.0-cdh5.14.2/myconf</li>
</ul>
</li>
</ul>
<ul>
<li>vim tail-memory-logger.conf</li>
</ul>
<pre><code class="properties"># Name the components on this agent
#定义一个agent，分别指定source、channel、sink别名
a1.sources = r1
a1.sinks = k1
a1.channels = c1

#配置source
#指定source的类型为exec，通过Unix命令来传输结果数据
a1.sources.r1.type = exec
#监控一个文件，有新的数据产生就不断采集走
a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log
#指定source的数据流入的channel中
a1.sources.r1.channels = c1

#配置channel
#指定channel的类型为memory
a1.channels.c1.type = memory
#指定channel的最多可以存放数据的容量
a1.channels.c1.capacity = 1000
#指定在一个事务中source写数据到channel或者sink从channel取数据最大条数
a1.channels.c1.transactionCapacity = 100

#配置sink
a1.sinks.k1.channel = c1
#类型是日志格式，结果会打印在控制台
a1.sinks.k1.type = logger
</code></pre>
</li>
<li><p><strong>3、启动agent</strong></p>
<ul>
<li>进入到node01上的/kkb/install/flume-1.6.0-cdh5.14.2目录下执行</li>
</ul>
<p><code>`</code>shell<br>bin/flume-ng agent -n a1 -c myconf -f myconf/tail-memory-logger.conf -Dflume.root.logger=info,console</p>
</li>
</ul>
<p>  其中：<br>  -n表示指定该agent名称<br>  -c表示配置文件所在的目录<br>  -f表示配置文件的路径名称<br>  -D表示指定key=value键值对—这里指定的是启动的日志输出级别</p>
<pre><code>


#### 5.2 采集文件到HDFS

- 1、需求描述

</code></pre><p>  监控一个文件如果有新增的内容就把数据采集到HDFS上</p>
<pre><code>
- 2、结构示意图

![file-Flume-HDFS](assets/file-Flume-HDFS.png)

- ==3、flume配置文件开发==

  - vim file2Hdfs.conf

  ```properties
  # Name the components on this agent
  a1.sources = r1
  a1.sinks = k1
  a1.channels = c1

  #配置source
  a1.sources.r1.type = exec
  a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log
  a1.sources.r1.channels = c1

  #配置channel
  a1.channels.c1.type = file
  #设置检查点目录--该目录是记录下event在数据目录下的位置
  a1.channels.c1.checkpointDir=/kfly/data/flume_checkpoint
  #数据存储所在的目录
  a1.channels.c1.dataDirs=/kfly/data/flume_data


  #配置sink
  a1.sinks.k1.channel = c1
  #指定sink类型为hdfs
  a1.sinks.k1.type = hdfs
  #指定数据收集到hdfs目录
  a1.sinks.k1.hdfs.path = hdfs://node01:8020/tailFile/%Y-%m-%d/%H%M
  #指定生成文件名的前缀
  a1.sinks.k1.hdfs.filePrefix = events-

  #是否启用时间上的”舍弃”   --&gt;控制目录 
  a1.sinks.k1.hdfs.round = true
  #时间上进行“舍弃”的值
  # 如 12:10 -- 12:19 =&gt; 12:10
  # 如 12:20 -- 12:29 =&gt; 12:20
  a1.sinks.k1.hdfs.roundValue = 10
  #时间上进行“舍弃”的单位
  a1.sinks.k1.hdfs.roundUnit = minute

  # 控制文件个数
  #60s或者50字节或者10条数据，谁先满足，就开始滚动生成新文件
  a1.sinks.k1.hdfs.rollInterval = 60
  a1.sinks.k1.hdfs.rollSize = 50
  a1.sinks.k1.hdfs.rollCount = 10

  #每个批次写入的数据量
  a1.sinks.k1.hdfs.batchSize = 100

  #开始本地时间戳--开启后就可以使用%Y-%m-%d去解析时间
  a1.sinks.k1.hdfs.useLocalTimeStamp = true

  #生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本
  a1.sinks.k1.hdfs.fileType = DataStream
</code></pre><ul>
<li><p><strong>4、启动agent</strong></p>
<ul>
<li><p>进入到node01上的/kkb/install/flume-1.6.0-cdh5.14.2目录下执行</p>
<pre><code class="shell">bin/flume-ng agent -n a1 -c myconf -f myconf/file2Hdfs.conf -Dflume.root.logger=info,console
</code></pre>
</li>
</ul>
</li>
</ul>
<h4 id="5-3-采集目录到HDFS"><a href="#5-3-采集目录到HDFS" class="headerlink" title="5.3 采集目录到HDFS"></a>5.3 采集目录到HDFS</h4><ul>
<li><p>1、需求描述</p>
<pre><code>一个目录中不断有新的文件产生，需要把目录中的文件不断地进行数据收集保存到HDFS上

</code></pre></li>
<li><p>2、结构示意图</p>
<p><img src="/Users/dingchuangshi/Documents/hexo-kfly-blog/source/_posts/flume/assets/Dir-Flume-HDFS.png" alt="Flume-HDFS"></p>
</li>
<li><p>==3、flume配置文件开发==</p>
<ul>
<li>在myconf目录中创建配置文件添加内容<ul>
<li>vim  dir2Hdfs.conf</li>
</ul>
</li>
</ul>
<p>~~~properties</p>
<h1 id="Name-the-components-on-this-agent"><a href="#Name-the-components-on-this-agent" class="headerlink" title="Name the components on this agent"></a>Name the components on this agent</h1><p>a1.sources = r1<br>a1.sinks = k1<br>a1.channels = c1</p>
<h1 id="配置source"><a href="#配置source" class="headerlink" title="配置source"></a>配置source</h1><p>##注意：不能往监控目中重复丢同名文件<br>a1.sources.r1.type = spooldir<br>a1.sources.r1.spoolDir = /kfly/install/flumeData/files</p>
<h1 id="是否将文件的绝对路径添加到header"><a href="#是否将文件的绝对路径添加到header" class="headerlink" title="是否将文件的绝对路径添加到header"></a>是否将文件的绝对路径添加到header</h1><p>a1.sources.r1.fileHeader = true<br>a1.sources.r1.channels = c1</p>
</li>
</ul>
<p>  #配置channel<br>  a1.channels.c1.type = memory<br>  a1.channels.c1.capacity = 1000<br>  a1.channels.c1.transactionCapacity = 100</p>
<p>  #配置sink<br>  a1.sinks.k1.type = hdfs<br>  a1.sinks.k1.channel = c1<br>  a1.sinks.k1.hdfs.path = hdfs://node01:8020/spooldir/%Y-%m-%d/%H%M<br>  a1.sinks.k1.hdfs.filePrefix = events-<br>  a1.sinks.k1.hdfs.round = true<br>  a1.sinks.k1.hdfs.roundValue = 10<br>  a1.sinks.k1.hdfs.roundUnit = minute<br>  a1.sinks.k1.hdfs.rollInterval = 60<br>  a1.sinks.k1.hdfs.rollSize = 50<br>  a1.sinks.k1.hdfs.rollCount = 10<br>  a1.sinks.k1.hdfs.batchSize = 100<br>  a1.sinks.k1.hdfs.useLocalTimeStamp = true</p>
<p>  #生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本<br>  a1.sinks.k1.hdfs.fileType = DataStream</p>
<pre><code>

* **4、启动agent**


  * 进入到node01上的/kkb/install/flume-1.6.0-cdh5.14.2目录下执行

  ```shell
  bin/flume-ng agent -n a1 -c myconf -f myconf/dir2Hdfs.conf -Dflume.root.logger=info,console

  ```




#### 5.4 两个agent级联

* 1、需求描述

</code></pre><pre><code>第一个agent负责监控某个目录中新增的文件进行数据收集，通过网络发送到第二个agent当中去，第二个agent负责接收第一个agent发送的数据，并将数据保存到hdfs上面去。
</code></pre><pre><code>
* 2、结构示意图

![](assets/2个agent级联.png)

* 3、在node01和node02上分别都安装flume

* 4、创建node01上的flume配置文件

  * vim dir2avro.conf

  ~~~properties
  # Name the components on this agent
  a1.sources = r1
  a1.sinks = k1
  a1.channels = c1

  # 配置source
  ##注意：不能往监控目中重复丢同名文件
  a1.sources.r1.type = spooldir
  a1.sources.r1.spoolDir = /kfly/install/flumeData/files
  a1.sources.r1.fileHeader = true
  a1.sources.r1.channels = c1


  #配置channel
  a1.channels.c1.type = memory
  a1.channels.c1.capacity = 1000
  a1.channels.c1.transactionCapacity = 100

  #配置sink
  a1.sinks.k1.channel = c1
  #AvroSink是用来通过网络来传输数据的,可以将event发送到RPC服务器(比如AvroSource)
  a1.sinks.k1.type = avro

  #node02 注意修改为自己的hostname
  a1.sinks.k1.hostname = node02
  a1.sinks.k1.port = 5211

</code></pre><ul>
<li><p>5、创建node02上的flume配置文件</p>
<ul>
<li>vim avro2Hdfs.conf</li>
</ul>
<pre><code class="properties"># Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

#配置source
#通过AvroSource接受AvroSink的网络数据
a1.sources.r1.type = avro
a1.sources.r1.channels = c1
#AvroSource服务的ip地址
a1.sources.r1.bind = node02
#AvroSource服务的端口
a1.sources.r1.port = 5211

#配置channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

#配置sink
a1.sinks.k1.channel = c1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://node01:8020/avro-hdfs/%Y-%m-%d/%H-%M
a1.sinks.k1.hdfs.filePrefix = events-
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
a1.sinks.k1.hdfs.rollInterval = 60
a1.sinks.k1.hdfs.rollSize = 50
a1.sinks.k1.hdfs.rollCount = 10
a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本
a1.sinks.k1.hdfs.fileType = DataStream

</code></pre>
</li>
<li><p><strong>6、启动agent</strong></p>
<ul>
<li><p>先启动node02上的flume。然后在启动node01上的flume</p>
<ul>
<li>在node02上的flume安装目录下执行</li>
</ul>
<pre><code class="shell">bin/flume-ng agent -n a1 -c myconf -f myconf/avro2Hdfs.conf -Dflume.root.logger=info,console

</code></pre>
<ul>
<li>在node01上的flume安装目录下执行</li>
</ul>
<pre><code class="shell">bin/flume-ng agent -n a1 -c myconf -f myconf/dir2avro.conf -Dflume.root.logger=info,console

</code></pre>
<ul>
<li>最后在node01上的/kfly/install/flumeData/files目录下创建一些数据文件，最后去HDFS上查看数据。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="6-高可用配置案例"><a href="#6-高可用配置案例" class="headerlink" title="6. 高可用配置案例"></a>6. 高可用配置案例</h3><h4 id="6-1-failover故障转移"><a href="#6-1-failover故障转移" class="headerlink" title="6.1 failover故障转移"></a>6.1 failover故障转移</h4><p><img src="assets/flume-failover.png" alt="flume-failover"></p>
<ul>
<li>1、节点分配</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:center">服务器主机名</th>
<th style="text-align:center">ip地址</th>
<th style="text-align:center">角色</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Agent1</td>
<td style="text-align:center">node01</td>
<td style="text-align:center">192.168.200.200</td>
<td style="text-align:center">WebServer</td>
</tr>
<tr>
<td style="text-align:center">Collector1</td>
<td style="text-align:center">node02</td>
<td style="text-align:center">192.168.200.210</td>
<td style="text-align:center">AgentMstr1</td>
</tr>
<tr>
<td style="text-align:center">Collector2</td>
<td style="text-align:center">node03</td>
<td style="text-align:center">192.168.200.220</td>
<td style="text-align:center">AgentMstr2</td>
</tr>
</tbody>
</table>
<pre><code>Agent1数据分别流入到Collector1和Collector2，Flume NG本身提供了Failover机制，可以自动切换和恢复。

</code></pre><ul>
<li><p>2、开发配置文件</p>
<ul>
<li><p>node01、node02、node03分别都要安装flume</p>
</li>
<li><p>==创建node01上的flume配置文件==</p>
<ul>
<li>vim flume-client-failover.conf</li>
</ul>
<p>~~~properties<br>#agent name<br>a1.channels = c1<br>a1.sources = r1<br>#定义了2个sink<br>a1.sinks = k1 k2</p>
<p>#set gruop<br>#设置一个sink组，一个sink组下可以包含很多个sink<br>a1.sinkgroups = g1</p>
<p>#set sink group<br>#指定g1这个sink组下有k1  k2 这2个sink<br>a1.sinkgroups.g1.sinks = k1 k2</p>
<p>#set source<br>a1.sources.r1.channels = c1<br>a1.sources.r1.type = exec<br>a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log</p>
<p>#set channel<br>a1.channels.c1.type = memory<br>a1.channels.c1.capacity = 1000<br>a1.channels.c1.transactionCapacity = 100</p>
</li>
</ul>
</li>
</ul>
<pre><code># set sink1    指定sink1的数据会传输给node02
a1.sinks.k1.channel = c1
a1.sinks.k1.type = avro
a1.sinks.k1.hostname = node02
a1.sinks.k1.port = 52020

# set sink2    指定sink2的数据会传输给node03
a1.sinks.k2.channel = c1
a1.sinks.k2.type = avro
a1.sinks.k2.hostname = node03
a1.sinks.k2.port = 52020

#set failover
#指定sink组高可用的策略---failover故障转移
a1.sinkgroups.g1.processor.type = failover
#指定k1这个sink的优先级
a1.sinkgroups.g1.processor.priority.k1 = 10
#指定k2这个sink的优先级
a1.sinkgroups.g1.processor.priority.k2 = 5
#指定故障转移的最大时间，如果超时会出现异常
a1.sinkgroups.g1.processor.maxpenalty = 10000

~~~

~~~properties
说明：
#这里首先要申明一个sinkgroups,然后再设置2个sink ,k1与k2,其中2个优先级是10和5。
#而processor的maxpenalty被设置为10秒，默认是30秒.表示故障转移的最大时间

~~~
</code></pre><ul>
<li><p>==创建node02和node03上的flume配置文件==</p>
</li>
<li><p>node02和node03上配置信息相同</p>
<ul>
<li>vim flume-server-failover.conf</li>
</ul>
<pre><code class="properties">#set Agent name
a1.sources = r1
a1.channels = c1
a1.sinks = k1

#set channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# set source
a1.sources.r1.type = avro
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 52020
a1.sources.r1.channels = c1

#配置拦截器
#指定2个拦截器  i1  i2 
a1.sources.r1.interceptors = i1 i2
#i1的类型为时间戳拦截器  可以解析%Y-%m-%d 时间
a1.sources.r1.interceptors.i1.type = timestamp
#i2的类型为主机拦截器，可以获取当前event中携带的主机名
a1.sources.r1.interceptors.i2.type = host
#指定主机名变量
a1.sources.r1.interceptors.i2.hostHeader=hostname

#set sink to hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.type=hdfs
a1.sinks.k1.hdfs.path=hdfs://node01:8020/failover/logs/%{hostname}
a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
a1.sinks.k1.hdfs.rollInterval = 60
a1.sinks.k1.hdfs.rollSize = 50
a1.sinks.k1.hdfs.rollCount = 10
a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.fileType = DataStream

</code></pre>
</li>
</ul>
<ul>
<li><p>3、启动flume配置</p>
<ul>
<li>先分别在node02和node03上启动flume<ul>
<li>分别进入到flume的安装目录下执行命令</li>
</ul>
</li>
</ul>
<pre><code class="shell">bin/flume-ng agent -n a1 -c myconf -f myconf/flume-server-failover.conf -Dflume.root.logger=info,console

</code></pre>
</li>
</ul>
<ul>
<li><p>然后在node01上启动flume</p>
<ul>
<li>进入到flume的安装目录下执行命令</li>
</ul>
<pre><code class="shell">bin/flume-ng agent -n a1 -c myconf -f myconf/flume-client-failover.conf -Dflume.root.logger=info,console

</code></pre>
</li>
</ul>
<ul>
<li><p>最后在hdfs目录上观察数据</p>
<pre><code class="shell">hdfs://node01:8020/failover/logs

</code></pre>
</li>
</ul>
<h4 id="6-2-load-balance负载均衡"><a href="#6-2-load-balance负载均衡" class="headerlink" title="6.2 load balance负载均衡"></a>6.2 load balance负载均衡</h4><ul>
<li><p>实现多个flume采集数据的时候避免单个flume的负载比较高，实现多个flume采集器负载均衡。</p>
</li>
<li><p>1、节点分配</p>
<ul>
<li>与failover故障转移的节点分配</li>
</ul>
</li>
<li><p>2、开发配置文件</p>
<ul>
<li><p>==创建node01上的flume配置文件==</p>
<ul>
<li>vim  flume-client-loadbalance.conf</li>
</ul>
<p>~~~properties<br>#agent name<br>a1.channels = c1<br>a1.sources = r1<br>a1.sinks = k1 k2</p>
<p>#set gruop<br>a1.sinkgroups = g1</p>
<p>#set sink group<br>a1.sinkgroups.g1.sinks = k1 k2</p>
<p>#set source<br>a1.sources.r1.channels = c1<br>a1.sources.r1.type = exec<br>a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log</p>
<p>#set channel<br>a1.channels.c1.type = memory<br>a1.channels.c1.capacity = 1000<br>a1.channels.c1.transactionCapacity = 100</p>
</li>
</ul>
</li>
</ul>
<pre><code># set sink1
a1.sinks.k1.channel = c1
a1.sinks.k1.type = avro
a1.sinks.k1.hostname = node02
a1.sinks.k1.port = 52020

# set sink2
a1.sinks.k2.channel = c1
a1.sinks.k2.type = avro
a1.sinks.k2.hostname = node03
a1.sinks.k2.port = 52020

#set load-balance
#指定sink组高可用的策略---load_balance负载均衡
a1.sinkgroups.g1.processor.type =load_balance
# 默认是round_ robin，还可以选择random
a1.sinkgroups.g1.processor.selector = round_robin
#如果backoff被开启，则sink processor会屏蔽故障的sink
a1.sinkgroups.g1.processor.backoff = true

~~~
</code></pre><ul>
<li><p>==创建node02和node03上的flume配置文件==</p>
<ul>
<li>vim  flume-server-loadbalance.conf</li>
</ul>
</li>
</ul>
<pre><code class="properties">  #set Agent name
  a1.sources = r1
  a1.channels = c1
  a1.sinks = k1

  #set channel
  a1.channels.c1.type = memory
  a1.channels.c1.capacity = 1000
  a1.channels.c1.transactionCapacity = 100

  # set source
  a1.sources.r1.type = avro
  a1.sources.r1.bind = 0.0.0.0
  a1.sources.r1.port = 52020
  a1.sources.r1.channels = c1

  #配置拦截器
  a1.sources.r1.interceptors = i1 i2
  a1.sources.r1.interceptors.i1.type = timestamp
  a1.sources.r1.interceptors.i2.type = host
  a1.sources.r1.interceptors.i2.hostHeader=hostname
  #hostname不使用ip显示，直接就是该服务器对应的主机名
  a1.sources.r1.interceptors.i2.useIP=false

  #set sink to hdfs
  a1.sinks.k1.channel = c1
  a1.sinks.k1.type=hdfs
  a1.sinks.k1.hdfs.path=hdfs://node01:8020/loadbalance/logs/%{hostname}
  a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d
  a1.sinks.k1.hdfs.round = true
  a1.sinks.k1.hdfs.roundValue = 10
  a1.sinks.k1.hdfs.roundUnit = minute
  a1.sinks.k1.hdfs.rollInterval = 60
  a1.sinks.k1.hdfs.rollSize = 50
  a1.sinks.k1.hdfs.rollCount = 10
  a1.sinks.k1.hdfs.batchSize = 100
  a1.sinks.k1.hdfs.fileType = DataStream

</code></pre>
<ul>
<li><p>3、启动flume配置</p>
<ul>
<li>先分别在node02和node03上启动flume<ul>
<li>分别进入到flume的安装目录下执行命令</li>
</ul>
</li>
</ul>
<pre><code class="shell">bin/flume-ng agent -n a1 -c myconf -f myconf/flume-server-loadbalance.conf -Dflume.root.logger=info,console

</code></pre>
</li>
</ul>
<ul>
<li><p>然后在node01上启动flume</p>
<ul>
<li>分别进入到flume的安装目录下执行命令</li>
</ul>
<pre><code class="shell">bin/flume-ng agent -n a1 -c myconf -f myconf/flume-client-loadbalance.conf -Dflume.root.logger=info,console

</code></pre>
</li>
</ul>
<ul>
<li><p>最后在hdfs上目录观察数据</p>
<pre><code class="shell">hdfs://node01:8020/loadbalance/logs

</code></pre>
</li>
</ul>
<h3 id="7-flume企业案例"><a href="#7-flume企业案例" class="headerlink" title="7. flume企业案例"></a>7. flume企业案例</h3><h4 id="7-1-flume案例之静态拦截器使用"><a href="#7-1-flume案例之静态拦截器使用" class="headerlink" title="7.1 flume案例之静态拦截器使用"></a>7.1 flume案例之静态拦截器使用</h4><ul>
<li>1、案例场景</li>
</ul>
<pre><code>A、B两台日志服务机器实时生产日志主要类型为access.log、nginx.log、web.log 
现在需要把A、B 机器中的access.log、nginx.log、web.log 采集汇总到C机器上然后统一收集到hdfs中。
但是在hdfs中要求的目录为：
/source/logs/access/20180101/**
/source/logs/nginx/20180101/**
/source/logs/web/20180101/**

</code></pre><ul>
<li>2、场景分析</li>
</ul>
<p><img src="assets/flume采集不同的日志数据.png" alt="flume采集不同的日志数据"></p>
<ul>
<li>3、数据流程处理分析</li>
</ul>
<p><img src="assets/flume采集不同的日志数据流程分析.png" alt=""></p>
<ul>
<li><p>4、开发配置文件</p>
<ul>
<li><p>==在node01与node02服务器开发flume的配置文件==</p>
<ul>
<li>vim exec_source_avro_sink.conf</li>
</ul>
<p>~~~properties</p>
<h1 id="Name-the-components-on-this-agent-1"><a href="#Name-the-components-on-this-agent-1" class="headerlink" title="Name the components on this agent"></a>Name the components on this agent</h1><p>#定义三个source<br>a1.sources = r1 r2 r3<br>a1.sinks = k1<br>a1.channels = c1</p>
<h1 id="Describe-configure-the-source"><a href="#Describe-configure-the-source" class="headerlink" title="Describe/configure the source"></a>Describe/configure the source</h1><p>a1.sources.r1.type = exec<br>a1.sources.r1.command = tail -F /kfly/install/flumeData/access.log<br>#指定source r1 使用拦截器i1<br>a1.sources.r1.interceptors = i1<br>#拦截器类型static静态<br>a1.sources.r1.interceptors.i1.type = static</p>
<h2 id="static拦截器的功能就是往采集到的数据的header中插入自己定义的key-value对"><a href="#static拦截器的功能就是往采集到的数据的header中插入自己定义的key-value对" class="headerlink" title="static拦截器的功能就是往采集到的数据的header中插入自己定义的key-value对"></a>static拦截器的功能就是往采集到的数据的header中插入自己定义的key-value对</h2><h1 id="自己进行设置-我们这里的key和value相当于键值对-k-type-v-access"><a href="#自己进行设置-我们这里的key和value相当于键值对-k-type-v-access" class="headerlink" title="自己进行设置,我们这里的key和value相当于键值对,k=type v=access"></a>自己进行设置,我们这里的key和value相当于键值对,k=type v=access</h1><p>a1.sources.r1.interceptors.i1.key = type<br>a1.sources.r1.interceptors.i1.value = access</p>
<p>a1.sources.r2.type = exec<br>a1.sources.r2.command = tail -F /kfly/install/flumeData/nginx.log<br>#指定source r2 使用拦截器i2<br>a1.sources.r2.interceptors = i2<br>#拦截器类型static静态<br>a1.sources.r2.interceptors.i2.type = static</p>
<h1 id="自己进行设置"><a href="#自己进行设置" class="headerlink" title="自己进行设置"></a>自己进行设置</h1><p>a1.sources.r2.interceptors.i2.key = type<br>a1.sources.r2.interceptors.i2.value = nginx</p>
<p>a1.sources.r3.type = exec<br>a1.sources.r3.command = tail -F /kfly/install/flumeData/web.log<br>#指定source r3 使用拦截器i3<br>a1.sources.r3.interceptors = i3<br>#拦截器类型static静态<br>a1.sources.r3.interceptors.i3.type = static</p>
<h1 id="自己进行设置-1"><a href="#自己进行设置-1" class="headerlink" title="自己进行设置"></a>自己进行设置</h1><p>a1.sources.r3.interceptors.i3.key = type<br>a1.sources.r3.interceptors.i3.value = web</p>
<h1 id="Use-a-channel-which-buffers-events-in-memory"><a href="#Use-a-channel-which-buffers-events-in-memory" class="headerlink" title="Use a channel which buffers events in memory"></a>Use a channel which buffers events in memory</h1><p>a1.channels.c1.type = memory<br>a1.channels.c1.capacity = 20000<br>a1.channels.c1.transactionCapacity = 10000</p>
<h1 id="Describe-the-sink"><a href="#Describe-the-sink" class="headerlink" title="Describe the sink"></a>Describe the sink</h1><p>a1.sinks.k1.type = avro<br>a1.sinks.k1.hostname = node03<br>a1.sinks.k1.port = 41414<br>a1.sinks.k1.channel = c1</p>
<h1 id="Bind-the-source-and-sink-to-the-channel"><a href="#Bind-the-source-and-sink-to-the-channel" class="headerlink" title="Bind the source and sink to the channel"></a>Bind the source and sink to the channel</h1><p>a1.sources.r1.channels = c1<br>a1.sources.r2.channels = c1<br>a1.sources.r3.channels = c1</p>
</li>
</ul>
</li>
</ul>
<pre><code>~~~
</code></pre><ul>
<li><p>==在node03服务器上开发flume配置文件==</p>
<ul>
<li>vim avro_source_hdfs_sink.conf</li>
</ul>
<pre><code class="properties">a1.sources = r1
a1.sinks = k1
a1.channels = c1
#定义source
a1.sources.r1.type = avro
a1.sources.r1.bind = node03
a1.sources.r1.port =41414

#定义channels
a1.channels.c1.type = memory
a1.channels.c1.capacity = 20000
a1.channels.c1.transactionCapacity = 1000

#定义sink
a1.sinks.k1.type = hdfs
# 此处的%{type} 这里是取我们在node01和node02定义的type的值,也就是value
a1.sinks.k1.hdfs.path=hdfs://node01:8020/source/logs/%{type}/%Y%m%d
a1.sinks.k1.hdfs.filePrefix =events-
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
#时间类型
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#生成的文件不按条数生成
a1.sinks.k1.hdfs.rollCount = 0
#生成的文件按时间生成
a1.sinks.k1.hdfs.rollInterval = 30
#生成的文件按大小生成
a1.sinks.k1.hdfs.rollSize  = 10485760
#批量写入hdfs的个数
a1.sinks.k1.hdfs.batchSize = 1000
#flume操作hdfs的线程数（包括新建，写入等）
a1.sinks.k1.hdfs.threadsPoolSize=10
#操作hdfs超时时间
a1.sinks.k1.hdfs.callTimeout=30000

#组装source、channel、sink
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

</code></pre>
</li>
</ul>
<ul>
<li><p>5、启动flume配置</p>
<ul>
<li>先在node03上启动flume</li>
</ul>
<pre><code class="shell">bin/flume-ng agent -n a1 -c myconf -f myconf/avro_source_hdfs_sink.conf -Dflume.root.logger=info,console

</code></pre>
</li>
</ul>
<h4 id="7-2-flume案例之自定义拦截器"><a href="#7-2-flume案例之自定义拦截器" class="headerlink" title="7.2 flume案例之自定义拦截器"></a>7.2 flume案例之自定义拦截器</h4><ul>
<li>1、案例场景</li>
</ul>
<pre><code>在数据采集之后，通过flume的拦截器，实现不需要的数据过滤掉，并将指定的第一个字段进行加密，加密之后再往hdfs上面保存

</code></pre><ul>
<li>2、数据文件 user.txt</li>
</ul>
<pre><code>13901007610,male,30,sing,beijing
18600000035,male,40,dance,shanghai
13366666659,male,20,Swimming,wuhan
13801179888,female,18,dance,tianjin
18511111114,male,35,sing,beijing
13718428888,female,40,Foodie,shanghai
13901057088,male,50,Basketball,taiwan
13671057777,male,60,Bodybuilding,xianggang

</code></pre><p><img src="assets/flume-custom-interceptor.png" alt=""></p>
<ul>
<li>3、创建maven工程添加依赖</li>
</ul>
<pre><code class="xml">&lt;dependency&gt;
        &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;
        &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt;
        &lt;version&gt;1.6.0-cdh5.14.2&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
<ul>
<li>4、代码开发</li>
</ul>
<pre><code class="java">package bigdata.flume;

import com.google.common.base.Charsets;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;
import java.math.BigInteger;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.util.ArrayList;
import java.util.List;

/**
 * @author dingchuangshi
 */
public class CustomInterceptor implements Interceptor {

    /**
     * encrypted_field_index.
     * 指定需要加密的字段下标
     */
    private final String encrypted_field_index;


    /**
     * The out_index.
     * 指定不需要对应列的下标
     */
    private final String out_index;


    /**
     * 提供构建方法，后期可以接受配置文件中的参数
     * @param encrypted_field_index
     * @param out_index
     */
    public CustomInterceptor(String encrypted_field_index, String out_index) {
        this.encrypted_field_index = encrypted_field_index;
        this.out_index = out_index;
    }

    /**
     * 定义拦截器规则
     * @param event
     * @return
     */
    @Override
    public Event intercept(Event event) {
        if(event == null){
            return null;
        }
       try{
           String line = new String(event.getBody(), Charsets.UTF_8);
           String newLine = &quot;&quot;;
           String[] splits = line.split(&quot;,&quot;);
           for (int i = 0; i &lt; splits.length; i++) {
               // 加密索引
               int encryptedField = Integer.parseInt(encrypted_field_index);
               // 忽略索引
               int outIndex = Integer.parseInt(out_index);

               if(i == encryptedField){
                   // 加密
                   newLine += md5(splits[encryptedField]) + &quot;,&quot;;
               }else if(i != outIndex){
                   // 忽略取消数据
                   newLine += splits[i] + &quot;,&quot;;
               }
           }
           // 去掉最后一个&#39;，&#39;符号
           newLine = newLine.substring(0,newLine.length() - 1);
           event.setBody(newLine.getBytes(Charsets.UTF_8));
       }catch (Exception e){
           e.printStackTrace();
       }
        return event;
    }

    @Override
    public List&lt;Event&gt; intercept(List&lt;Event&gt; events) {
        List&lt;Event&gt; out = new ArrayList&lt;Event&gt;();
        for (Event event : events) {
            Event outEvent = intercept(event);
            if (outEvent != null) {
                out.add(outEvent);
            }
        }
        return out;
    }

    @Override
    public void initialize() {

    }

    @Override
    public void close() {

    }

    /**
     * md5加密
     * @return
     */
    public String md5(String plainText){
        byte[] secretBytes = null;
        try {
            MessageDigest instance = MessageDigest.getInstance(&quot;md5&quot;);
            instance.update(plainText.getBytes());
            secretBytes = instance.digest();
        } catch (NoSuchAlgorithmException e) {
            System.out.println(&quot;没有md5这个算法&quot;);
            e.printStackTrace();
        }
        String md5Code = new BigInteger(1,secretBytes).toString(16);
        for (int i = 0; i &lt; 32 - md5Code.length(); i++) {
            md5Code =&quot;0&quot; + md5Code;
        }
        return md5Code;
    }


    public static class MyBuilder  implements CustomInterceptor.Builder {
        /**
         * encrypted_field_index.
         * 指定需要加密的字段下标
         */
        private String encrypted_field_index;

        /**
         * The out_index.
         * 指定不需要对应列的下标
         */
        private String out_index;

        @Override
        public CustomInterceptor build() {
            return new CustomInterceptor(encrypted_field_index, out_index);
        }


        @Override
        public void configure(Context context) {
            this.encrypted_field_index = context.getString(&quot;encrypted_field_index&quot;, &quot;&quot;);
            this.out_index = context.getString(&quot;out_index&quot;, &quot;&quot;);
        }
    }
}

</code></pre>
<ul>
<li>5、打成jar包后放到flume安装目录下的lib中</li>
</ul>
<ul>
<li>6、创建配置文件 flume-interceptor-hdfs.conf</li>
</ul>
<pre><code class="properties"># Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

#配置source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /kfly/install/flumeData/user.txt
a1.sources.r1.channels = c1
a1.sources.r1.interceptors =i1
a1.sources.r1.interceptors.i1.type =bigdata.flume.CustomInterceptor$MyBuilder
a1.sources.r1.interceptors.i1.encrypted_field_index=0
a1.sources.r1.interceptors.i1.out_index=3

#配置channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100



#配置sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = hdfs://node01:8020/interceptor/files/%Y-%m-%d/%H%M
a1.sinks.k1.hdfs.filePrefix = events-
# 时间舍弃
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
# 限定生成文件时机
a1.sinks.k1.hdfs.rollInterval = 5
a1.sinks.k1.hdfs.rollSize = 50
a1.sinks.k1.hdfs.rollCount = 10
a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本
a1.sinks.k1.hdfs.fileType = DataStream

</code></pre>
<ul>
<li>7、进入到flume安装目录下启动flume</li>
</ul>
<pre><code class="shell">bin/flume-ng agent -n a1 -c myconf -f myconf/flume-interceptor-hdfs.conf -Dflume.root.logger=info,console

</code></pre>
<h3 id="8-flume自定义Source"><a href="#8-flume自定义Source" class="headerlink" title="8. flume自定义Source"></a>8. flume自定义Source</h3><h4 id="8-1-场景描述"><a href="#8-1-场景描述" class="headerlink" title="8.1 场景描述"></a>8.1 场景描述</h4><pre><code>    官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。如：实时监控MySQL，从MySQL中获取数据传输到HDFS或者其他存储框架，所以此时需要我们自己实现MySQLSource。

官方也提供了自定义source的接口：
官网说明：https://flume.apache.org/FlumeDeveloperGuide.html#source

</code></pre><h4 id="8-2-自定义MysqlSource步骤"><a href="#8-2-自定义MysqlSource步骤" class="headerlink" title="8.2 自定义MysqlSource步骤"></a>8.2 自定义MysqlSource步骤</h4><ul>
<li>1、根据官方说明自定义mysqlsource需要继承AbstractSource类并实现Configurable和PollableSource接口。</li>
</ul>
<ul>
<li>2、实现对应的方法<ul>
<li>configure(Context context)<ul>
<li>初始化context</li>
</ul>
</li>
<li>process()<ul>
<li>从mysql表中获取数据，然后把数据封装成event对象写入到channel，该方法被一直调用</li>
</ul>
</li>
<li>stop()<ul>
<li>关闭相关资源</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>3、开发流程</p>
<ul>
<li>3.1 创建mysql数据库以及mysql数据库表</li>
</ul>
<p>~~~sql<br>–创建一个数据库<br>CREATE DATABASE IF NOT EXISTS mysqlsource DEFAULT CHARACTER SET utf8 ;</p>
<p>–创建一个表，用户保存拉取目标表位置的信息<br>CREATE TABLE mysqlsource.flume_meta (<br>  source_tab varchar(255) NOT NULL,<br>  currentIndex varchar(255) NOT NULL,<br>  PRIMARY KEY (source_tab)<br>) ENGINE=InnoDB DEFAULT CHARSET=utf8;</p>
<p>–插入数据<br>insert  into mysqlsource.flume_meta(source_tab,currentIndex) values (‘student’,’4’);</p>
</li>
</ul>
<p>  –创建要拉取数据的表<br>  CREATE TABLE mysqlsource.student(<br>    id int(11) NOT NULL AUTO_INCREMENT,<br>    name varchar(255) NOT NULL,<br>    PRIMARY KEY (id)<br>  ) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;</p>
<p>  –向student表中添加测试数据<br>  insert  into mysqlsource.student(id,name) values (1,’zhangsan’),(2,’lisi’),(3,’wangwu’),(4,’zhaoliu’);</p>
<pre><code>
  * 3.2 代码开发实现

    *  构建maven工程，添加依赖

    ~~~xml
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;mysql&lt;/groupId&gt;
                &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
                &lt;version&gt;5.1.38&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;
                &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;
                &lt;version&gt;3.6&lt;/version&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;


</code></pre><pre><code>* 在resources资源文件夹下添加jdbc.properties

  * ==jdbc.properties==

  ~~~~properties
  dbDriver=com.mysql.jdbc.Driver
  dbUrl=jdbc:mysql://node03:3306/mysqlsource?useUnicode=true&amp;characterEncoding=utf-8
  dbUser=root
  dbPassword=123456

  ~~~~

* ==核心代码==

  ~~~java
  /**
   * 自定义source
   * @author dingchuangshi
   */
  public class CustomSource extends AbstractSource implements Configurable, PollableSource {

      private static Logger logger = LoggerFactory.getLogger(CustomSource.class);
          // 自定义的查询类
      private QueryMysql sqlSourceHelp;

      @Override
      public Status process() throws EventDeliveryException {
          try {
              //查询数据表
              List&lt;List&lt;Object&gt;&gt; result = sqlSourceHelp.executeQuery();
              //存放event的集合
              List&lt;Event&gt; events = new ArrayList&lt;&gt;();
              //存放event头集合
              Map&lt;String, String&gt; header = new HashMap&lt;&gt;();
              //如果有返回数据，则将数据封装为event
              if (!result.isEmpty()) {
                  List&lt;String&gt; allRows = sqlSourceHelp.getAllRows(result);
                  Event event = null;
                  for (String row : allRows) {
                      event = new SimpleEvent();
                      event.setBody(row.getBytes());
                      event.setHeaders(header);
                      events.add(event);
                  }
                  //将event写入channel
                  this.getChannelProcessor().processEventBatch(events);
                  //更新数据表中的offset信息
                  sqlSourceHelp.updateOffset2DB(result.size());
              }
              //等待时长
              Thread.sleep(sqlSourceHelp.getRunQueryDelay());
              return Status.READY;
          } catch (InterruptedException e) {
              logger.error(&quot;Error procesing row&quot;, e);
              return Status.BACKOFF;
          }
      }
  }

  ~~~
</code></pre><ul>
<li><p>4、测试</p>
<ul>
<li><p>4.1 ==程序打成jar包，上传jar包到flume的lib目录下==</p>
</li>
<li><p>4.2 ==配置文件准备==</p>
<ul>
<li>vim mysqlsource.conf</li>
</ul>
<p>~~~properties</p>
<h1 id="Name-the-components-on-this-agent-2"><a href="#Name-the-components-on-this-agent-2" class="headerlink" title="Name the components on this agent"></a>Name the components on this agent</h1><p>a1.sources = r1<br>a1.sinks = k1<br>a1.channels = c1</p>
<h1 id="Describe-configure-the-source-1"><a href="#Describe-configure-the-source-1" class="headerlink" title="Describe/configure the source"></a>Describe/configure the source</h1><p>a1.sources.r1.type = bigdata.flume.source.CustomSource</p>
<h1 id="老师的是node01-同学们改成自己的节点-一定要注意"><a href="#老师的是node01-同学们改成自己的节点-一定要注意" class="headerlink" title="老师的是node01,同学们改成自己的节点 一定要注意"></a>老师的是node01,同学们改成自己的节点 一定要注意</h1><p>a1.sources.r1.connection.url = jdbc:mysql://node03:3306/mysqlsource<br>a1.sources.r1.connection.user = root<br>a1.sources.r1.connection.password = 123456<br>a1.sources.r1.table = student<br>a1.sources.r1.columns.to.select = *<br>a1.sources.r1.start.from=0<br>a1.sources.r1.run.query.delay=3000</p>
<h1 id="Describe-the-channel"><a href="#Describe-the-channel" class="headerlink" title="Describe the channel"></a>Describe the channel</h1><p>a1.channels.c1.type = memory<br>a1.channels.c1.capacity = 1000<br>a1.channels.c1.transactionCapacity = 100</p>
<h1 id="Describe-the-sink-1"><a href="#Describe-the-sink-1" class="headerlink" title="Describe the sink"></a>Describe the sink</h1><p>a1.sinks.k1.type = logger</p>
</li>
</ul>
</li>
</ul>
<pre><code># Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

~~~
</code></pre><ul>
<li><p>4.3 ==启动flume配置==</p>
<pre><code class="shell">bin/flume-ng agent -n a1 -c myconf -f myconf/mysqlsource.conf -Dflume.root.logger=info,console

</code></pre>
</li>
</ul>
<ul>
<li>4.4 最后向表添加数据，观察控制台信息</li>
</ul>
<h3 id="9-flume自定义Sink"><a href="#9-flume自定义Sink" class="headerlink" title="9. flume自定义Sink"></a>9. flume自定义Sink</h3><h4 id="9-1-场景描述"><a href="#9-1-场景描述" class="headerlink" title="9.1 场景描述"></a>9.1 场景描述</h4><pre><code>    官方提供的sink类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些sink。如：需要把接受到的数据按照规则进行过滤之后写入到某张mysql表中，所以此时需要我们自己实现MySQLSink。

官方也提供了自定义sink的接口：
官网说明：https://flume.apache.org/FlumeDeveloperGuide.html#sink
</code></pre><h4 id="9-2-自定义MysqlSink步骤"><a href="#9-2-自定义MysqlSink步骤" class="headerlink" title="9.2 自定义MysqlSink步骤"></a>9.2 自定义MysqlSink步骤</h4><ul>
<li><p>1、根据官方说明自定义MysqlSink需要继承AbstractSink类并实现Configurable</p>
</li>
<li><p>2、实现对应的方法</p>
<ul>
<li><p>configure(Context context)</p>
<ul>
<li>初始化context</li>
</ul>
</li>
<li><p>start()</p>
<ul>
<li>启动准备操作</li>
</ul>
</li>
<li><p>process()</p>
<ul>
<li>从channel获取数据，然后解析之后，保存在mysql表中</li>
</ul>
</li>
<li><p>stop()</p>
<ul>
<li>关闭相关资源</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>3、开发流程</p>
<ul>
<li>3.1 ==创建mysql数据库以及mysql数据库表==</li>
</ul>
<pre><code class="SQL">--创建一个数据库
CREATE DATABASE IF NOT EXISTS mysqlsource DEFAULT CHARACTER SET utf8 ;

--创建一个表，用户保存拉取目标表位置的信息
CREATE TABLE mysqlsource.flume2mysql (
  id int(11) NOT NULL AUTO_INCREMENT,
  create_time varchar(64) NOT NULL,
  content varchar(255) NOT NULL,
  PRIMARY KEY (id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
</code></pre>
</li>
</ul>
<ul>
<li><p>3.2  代码开发实现</p>
<ul>
<li>==定义MysqlSink类==</li>
</ul>
</li>
</ul>
<pre><code class="java">package com.kaikeba.sink;

import org.apache.flume.conf.Configurable;
import org.apache.flume.*;
import org.apache.flume.sink.AbstractSink;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.text.SimpleDateFormat;
import java.util.Date;

/**
 * 自定义MysqlSink
 */
public class MysqlSink extends AbstractSink implements Configurable {
    private String mysqlurl = &quot;&quot;;
    private String username = &quot;&quot;;
    private String password = &quot;&quot;;
    private String tableName = &quot;&quot;;

    Connection con = null;

    @Override
    public Status process(){
        Status status = null;
        // Start transaction
        Channel ch = getChannel();
        Transaction txn = ch.getTransaction();
        txn.begin();
        try
        {
            Event event = ch.take();

            if (event != null)
            {
                    //获取body中的数据
                    String body = new String(event.getBody(), &quot;UTF-8&quot;);

                    //如果日志中有以下关键字的不需要保存，过滤掉
                if(body.contains(&quot;delete&quot;) || body.contains(&quot;drop&quot;) || body.contains(&quot;alert&quot;)){
                    status = Status.BACKOFF;
                }else {

                    //存入Mysql
                    SimpleDateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);
                    String createtime = df.format(new Date());

                    PreparedStatement stmt = con.prepareStatement(&quot;insert into &quot; + tableName + &quot; (createtime, content) values (?, ?)&quot;);
                    stmt.setString(1, createtime);
                    stmt.setString(2, body);
                    stmt.execute();
                    stmt.close();
                    status = Status.READY;
                }
           }else {
                    status = Status.BACKOFF;
                }

            txn.commit();
        } catch (Throwable t){
            txn.rollback();
            t.getCause().printStackTrace();
            status = Status.BACKOFF;
        } finally{
            txn.close();
        }

        return status;
    }
    /**
     * 获取配置文件中指定的参数
     * @param context
     */
    @Override
    public void configure(Context context) {
        mysqlurl = context.getString(&quot;mysqlurl&quot;);
        username = context.getString(&quot;username&quot;);
        password = context.getString(&quot;password&quot;);
        tableName = context.getString(&quot;tablename&quot;);
    }    

    @Override
    public synchronized void start() {
        try{
              //初始化数据库连接
            con = DriverManager.getConnection(mysqlurl, username, password);
            super.start();
            System.out.println(&quot;finish start&quot;);
        }catch (Exception ex){
            ex.printStackTrace();
        }
    }

    @Override
    public synchronized void stop(){
        try{
            con.close();
        }catch(SQLException e) {
            e.printStackTrace();
        }
        super.stop();
    }

}

</code></pre>
<ul>
<li><p>4、测试</p>
<ul>
<li><p>4.1 ==程序打成jar包，上传jar包到flume的lib目录下==</p>
</li>
<li><p>4.2 ==配置文件准备==</p>
<ul>
<li>vim mysqlsink.conf</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="properties">    a1.sources = r1
    a1.sinks = k1
    a1.channels = c1

    #配置source
    a1.sources.r1.type = exec
    a1.sources.r1.command = tail -F /kfly/install/flumeData/data.log
    a1.sources.r1.channels = c1

    #配置channel
    a1.channels.c1.type = memory
    a1.channels.c1.capacity = 1000
    a1.channels.c1.transactionCapacity = 100

    #配置sink
    a1.sinks.k1.channel = c1
    a1.sinks.k1.type = bigdata.flume.sink.CustomSink
    a1.sinks.k1.mysqlurl=jdbc:mysql://node03:3306/mysqlsource?useSSL=false
    a1.sinks.k1.username=root
    a1.sinks.k1.password=123456
    a1.sinks.k1.tablename=flume2mysql
</code></pre>
<ul>
<li><p>4.3 ==启动flume配置==</p>
<pre><code class="shell">bin/flume-ng agent -n a1 -c myconf -f myconf/mysqlsink.conf -Dflume.root.logger=info,console
</code></pre>
</li>
<li><p>4.4 最后向文件中添加数据，观察mysql表中的数据</p>
</li>
</ul>
<h3 id="10-Flume实际使用注意事项"><a href="#10-Flume实际使用注意事项" class="headerlink" title="10. Flume实际使用注意事项"></a>10. Flume实际使用注意事项</h3><h4 id="1、注意启动脚本命名的书写"><a href="#1、注意启动脚本命名的书写" class="headerlink" title="1、注意启动脚本命名的书写"></a>1、注意启动脚本命名的书写</h4><pre><code>agent 的名称别写错了，后台执行加上 nohup ... &amp;
</code></pre><h4 id="2、channel参数"><a href="#2、channel参数" class="headerlink" title="2、channel参数"></a>2、channel参数</h4><pre><code>capacity：默认该通道中最大的可以存储的event数量
trasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量
注意：capacity &gt; trasactionCapacity

</code></pre><h4 id="3、日志采集到HDFS配置说明1（sink端）"><a href="#3、日志采集到HDFS配置说明1（sink端）" class="headerlink" title="3、日志采集到HDFS配置说明1（sink端）"></a>3、日志采集到HDFS配置说明1（sink端）</h4><pre><code class="shell">#定义sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path=hdfs://node01:8020/source/logs/%{type}/%Y%m%d
a1.sinks.k1.hdfs.filePrefix =events
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
#时间类型
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#生成的文件不按条数生成
a1.sinks.k1.hdfs.rollCount = 0
#生成的文件按时间生成
a1.sinks.k1.hdfs.rollInterval = 0
#生成的文件按大小生成
a1.sinks.k1.hdfs.rollSize  = 10485760
#批量写入hdfs的个数
a1.sinks.k1.hdfs.batchSize = 10000
#flume操作hdfs的线程数（包括新建，写入等）
a1.sinks.k1.hdfs.threadsPoolSize=10
#操作hdfs超时时间
a1.sinks.k1.hdfs.callTimeout=30000


</code></pre>
<h4 id="4、日志采集到HDFS配置说明2（sink端）"><a href="#4、日志采集到HDFS配置说明2（sink端）" class="headerlink" title="4、日志采集到HDFS配置说明2（sink端）"></a>4、日志采集到HDFS配置说明2（sink端）</h4><table>
<thead>
<tr>
<th>hdfs.round</th>
<th>false</th>
<th>Should the timestamp be rounded down (if true, affects all time based escape sequences except %t)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>hdfs.roundValue</strong></td>
<td>1</td>
<td>Rounded down to the highest multiple of this (in the unit configured usinghdfs.roundUnit), less than current time.</td>
</tr>
<tr>
<td><strong>hdfs.roundUnit</strong></td>
<td>second</td>
<td>The unit of the round down value - second, minute or hour.</td>
</tr>
</tbody>
</table>
<p>Ø round： 默认值：false 是否启用时间上的”舍弃”，这里的”舍弃”，类似于”四舍五入”</p>
<p>Ø roundValue：默认值：1  时间上进行“舍弃”的值；</p>
<p>Ø roundUnit： 默认值：seconds时间上进行”舍弃”的单位，包含：second,minute,hour</p>
<pre><code class="properties"># 案例一：
a1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d/%H:%M/%S
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
# 当时间为2015-10-16 17:38:59时候，hdfs.path依然会被解析为：
# /flume/events/2015-10-16/17:30/00
# /flume/events/2015-10-16/17:40/00
# /flume/events/2015-10-16/17:50/00
# 因为设置的是舍弃10分钟内的时间，因此，该目录每10分钟新生成一个。

# 案例二：
a1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d/%H:%M/%S
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = second
# 现象：10秒为时间梯度生成对应的目录，目录下面包括很多小文件！！！
# 格式如下：
# /flume/events/2016-07-28/18:45/10
# /flume/events/2016-07-28/18:45/20
# /flume/events/2016-07-28/18:45/30
# /flume/events/2016-07-28/18:45/40
# /flume/events/2016-07-28/18:45/50
# /flume/events/2016-07-28/18:46/10
# /flume/events/2016-07-28/18:46/20
# /flume/events/2016-07-28/18:46/30
# /flume/events/2016-07-28/18:46/40
# /flume/events/2016-07-28/18:46/50

</code></pre>
<h4 id="5、实现数据的断点续传"><a href="#5、实现数据的断点续传" class="headerlink" title="5、实现数据的断点续传"></a>5、实现数据的断点续传</h4><ul>
<li>当一个flume挂掉之后重启的时候还是可以接着上一次的数据继续收集<ul>
<li>flume在1.7版本之前使用的监控一个文件（source exec）、监控一个目录（source spooldir）都无法直接实现</li>
</ul>
</li>
<li>flume在1.7版本之后已经集成了该功能<ul>
<li>其本质就是记录下每一次消费的位置，把消费信息的位置保存到文件中，后续程序挂掉了再重启的时候，可以接着上一次消费的数据位置继续拉取。</li>
</ul>
</li>
<li>配置文件<ul>
<li>==vim taildir.conf==<ul>
<li>source 类型—-&gt;taildir</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="properties">a1.sources = s1
a1.channels = ch1
a1.sinks = hdfs-sink1

#channel
a1.channels.ch1.type = memory
a1.channels.ch1.capacity=10000
a1.channels.ch1.transactionCapacity=500

#source
a1.sources.s1.channels = ch1
#监控一个目录下的多个文件新增的内容
a1.sources.s1.type = taildir
#通过 json 格式存下每个文件消费的偏移量，避免从头消费
a1.sources.s1.positionFile = /k/install/flumeData/index/taildir_position.json
a1.sources.s1.filegroups = f1 f2 f3 
a1.sources.s1.filegroups.f1 = /home/hadoop/taillogs/access.log
a1.sources.s1.filegroups.f2 = /home/hadoop/taillogs/nginx.log
a1.sources.s1.filegroups.f3 = /home/hadoop/taillogs/web.log
a1.sources.s1.headers.f1.headerKey = access
a1.sources.s1.headers.f2.headerKey = nginx
a1.sources.s1.headers.f3.headerKey = web
a1.sources.s1.fileHeader  = true

##sink
a1.sinks.hdfs-sink1.channel = ch1
a1.sinks.hdfs-sink1.type = hdfs
a1.sinks.hdfs-sink1.hdfs.path =hdfs://node01:8020/demo/data/%{headerKey}
a1.sinks.hdfs-sink1.hdfs.filePrefix = event_data
a1.sinks.hdfs-sink1.hdfs.fileSuffix = .log
a1.sinks.hdfs-sink1.hdfs.rollSize = 1048576
a1.sinks.hdfs-sink1.hdfs.rollInterval =20
a1.sinks.hdfs-sink1.hdfs.rollCount = 10
a1.sinks.hdfs-sink1.hdfs.batchSize = 1500
a1.sinks.hdfs-sink1.hdfs.round = true
a1.sinks.hdfs-sink1.hdfs.roundUnit = minute
a1.sinks.hdfs-sink1.hdfs.threadsPoolSize = 25
a1.sinks.hdfs-sink1.hdfs.fileType =DataStream
a1.sinks.hdfs-sink1.hdfs.writeFormat = Text
a1.sinks.hdfs-sink1.hdfs.callTimeout = 60000

</code></pre>
<pre><code class="json"># 运行后生成的 taildir_position.json文件信息如下：
[
{&quot;inode&quot;:102626782,&quot;pos&quot;:123,&quot;file&quot;:&quot;/home/hadoop/taillogs/access.log&quot;},{&quot;inode&quot;:102626785,&quot;pos&quot;:123,&quot;file&quot;:&quot;/home/hadoop/taillogs/web.log&quot;},{&quot;inode&quot;:102626786,&quot;pos&quot;:123,&quot;file&quot;:&quot;/home/hadoop/taillogs/nginx.log&quot;}
]

#这里inode就是标记文件的，文件名称改变，这个iNode不会变，pos记录偏移量，file就是绝对路径

</code></pre>
<h4 id="6、flume的header参数配置讲解"><a href="#6、flume的header参数配置讲解" class="headerlink" title="6、flume的header参数配置讲解"></a>6、flume的header参数配置讲解</h4><ul>
<li>==vim test-header.conf==    </li>
</ul>
<pre><code class="properties">#配置信息test-header.conf
a1.channels=c1
a1.sources=r1
a1.sinks=k1

#source
a1.sources.r1.channels=c1
a1.sources.r1.type= spooldir
a1.sources.r1.spoolDir= /home/hadoop/test
a1.sources.r1.batchSize= 100
a1.sources.r1.inputCharset= UTF-8
#是否添加一个key存储目录下文件的绝对路径
a1.sources.r1.fileHeader= true
#指定存储目录下文件的绝对路径的key
a1.sources.r1.fileHeaderKey= mm
#是否添加一个key存储目录下的文件名称
a1.sources.r1.basenameHeader= true
#指定存储目录下文件的名称的key
a1.sources.r1.basenameHeaderKey= nn

#channel
a1.channels.c1.type= memory
a1.channels.c1.capacity=10000
a1.channels.c1.transactionCapacity=500


#sink
a1.sinks.k1.type=logger
a1.sinks.k1.channel=c1

</code></pre>
<ul>
<li>准备数据文件，添加内容</li>
</ul>
<pre><code>/home/hadoop/test/abc.txt
/home/hadoop/test/def.txt
</code></pre><ul>
<li>启动flume配置</li>
</ul>
<pre><code>bin/flume-ng agent -n a1 -c myconf -f myconf/test-header.conf -Dflume.root.logger=info,console
</code></pre><ul>
<li>查看控制台</li>
</ul>
<pre><code>Event: { headers:{mm=/home/hadoop/test/abc.txt, nn=abc.txt} body: 68 65 6C 6C 6F 20 73 70 61 72 6B                hello spark }
19/08/30 19:23:15 INFO sink.LoggerSink: Event: { headers:{mm=/home/hadoop/test/abc.txt, nn=abc.txt} body: 68 65 6C 6C 6F 20 68 61 64 6F 6F 70             hello hadoop }
</code></pre><h4 id="7-tail-tail-f-tail-F区别"><a href="#7-tail-tail-f-tail-F区别" class="headerlink" title="7 tail / tail -f /tail -F区别"></a>7 tail / tail -f /tail -F区别</h4><pre><code>tail -f

等同于--follow=descriptor，根据文件描述符进行追踪，当文件改名或被删除，追踪停止

tail -F

等同于--follow=name --retry，根据文件名进行追踪，并保持重试，即该文件被删除或改名后，如果再次创建相同的文件名，会继续追踪

tailf

等同于tail -f -n 10（貌似tail -f或-F默认也是打印最后10行，然后追踪文件），与tail -f不同的是，如果文件不增长，它不会去访问磁盘文件，所以tailf特别适合那些便携机上跟踪日志文件，因为它减少了磁盘访问，
</code></pre>
        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="https://blog.sev7e0.site/">大数据施工现场</a></span>
        <span>/</span>
        
        <span><a href="https://wangchujiang.com/linux-command/">linux命令行工具</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="/js/gitment.js"></script>
<script>
    var gitment = new Gitment({
        id: 'Flume日志采集框架',
        owner: 'orchid-ding',
        repo: 'kfly-blog-comment',
        oauth: {
            client_id: '0770cdab79393197b6f5',
            client_secret: '376fb6c7bcd5047718b356712f596b89e490360c',
        },
    })
    gitment.render('comment-container')
</script>




</html>
