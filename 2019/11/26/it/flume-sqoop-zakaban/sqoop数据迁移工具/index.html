<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="天行健、君子以自强不息；地势坤，君子以厚德载物。">
    <meta name="keyword"  content="兰草">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        sqoop数据迁移工具 - kfly的博客 | kfly&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_1598291_q3el2wqimj.css" type="text/css">
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>kfly</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont iconhome"></i>
                    <span>主页</span>
                </a>
            </li>
 	   <li >
                <a href="/spec/">
                    <i class="iconfont iconzhuanti"></i>
                    <span>专题</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>简历</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Sqoop是什么"><span class="toc-text">1. Sqoop是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Sqoop的工作机制"><span class="toc-text">2. Sqoop的工作机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Sqoop基本架构"><span class="toc-text">3. Sqoop基本架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Sqoop安装部署"><span class="toc-text">4. Sqoop安装部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Sqooq数据的导入"><span class="toc-text">5. Sqooq数据的导入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-列举出所有的数据库"><span class="toc-text">5.1 列举出所有的数据库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-导入数据库表数据到HDFS"><span class="toc-text">5.2 导入数据库表数据到HDFS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-导入数据库表数据到HDFS指定目录"><span class="toc-text">5.3 导入数据库表数据到HDFS指定目录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-导入数据库表数据到HDFS指定目录并且指定数据字段的分隔符"><span class="toc-text">5.4 导入数据库表数据到HDFS指定目录并且指定数据字段的分隔符</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-导入关系表到Hive中"><span class="toc-text">5.5 导入关系表到Hive中</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-6-导入数据库表数据到hive中-并自动创建hive表"><span class="toc-text">5.6 导入数据库表数据到hive中(并自动创建hive表)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-7-导入表数据子集"><span class="toc-text">5.7 导入表数据子集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-8-sql语句查找导入hdfs"><span class="toc-text">5.8 sql语句查找导入hdfs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-10-mysql表的数据导入到hbase中"><span class="toc-text">5.10 mysql表的数据导入到hbase中</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Sqoop数据的导出"><span class="toc-text">6. Sqoop数据的导出</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-hdfs文件导出到mysql表中"><span class="toc-text">6.1 hdfs文件导出到mysql表中</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Sqoop-job"><span class="toc-text">7. Sqoop job</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-1-创建job"><span class="toc-text">7.1 创建job</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-2-验证-job"><span class="toc-text">7.2 验证 job</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-4-执行job"><span class="toc-text">7.4 执行job</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-5-删除job"><span class="toc-text">7.5 删除job</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        sqoop数据迁移工具
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-11-26 22:45:52</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#sqoop" title="sqoop">sqoop</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h3 id="1-Sqoop是什么"><a href="#1-Sqoop是什么" class="headerlink" title="1. Sqoop是什么"></a>1. Sqoop是什么</h3><ul>
<li>Sqoop是apache旗下的一款 ”==Hadoop和关系数据库之间传输数据==”的工具<ul>
<li>==导入数据== import<ul>
<li>将MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统</li>
</ul>
</li>
<li>==导出数据== export<ul>
<li>从Hadoop的文件系统中导出数据到关系数据库</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="assets/sqoop.png" alt="sqoop"></p>
<h3 id="2-Sqoop的工作机制"><a href="#2-Sqoop的工作机制" class="headerlink" title="2. Sqoop的工作机制"></a>2. Sqoop的工作机制</h3><ul>
<li>将导入和导出的命令翻译成mapreduce程序实现<ul>
<li>在翻译出的mapreduce中主要是对inputformat和outputformat进行定制</li>
</ul>
</li>
</ul>
<h3 id="3-Sqoop基本架构"><a href="#3-Sqoop基本架构" class="headerlink" title="3. Sqoop基本架构"></a>3. Sqoop基本架构</h3><ul>
<li><p>sqoop在发展中的过程中演进出来了两种不同的架构.<a href="https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop#comment-1561314193000" target="_blank" rel="noopener">架构演变史</a></p>
</li>
<li><p>==sqoop1的架构图==</p>
</li>
</ul>
<p><img src="assets/sqoop1.jpg" alt="sqoop1"></p>
<pre><code>版本号为1.4.x0
</code></pre><ul>
<li>==sqoop2的架构图==</li>
</ul>
<p><img src="assets/sqoop2.jpg" alt="sqoop2"></p>
<pre><code>版本号为1.99x为sqoop2 
在架构上：sqoop2引入了sqoop server，对connector实现了集中的管理 
访问方式：REST API、 JAVA API、 WEB UI以及CLI控制台方式进行访问 
</code></pre><p><img src="assets/sqoop1 VS sqoop2.png" alt="sqoop1 VS sqoop2"></p>
<h3 id="4-Sqoop安装部署"><a href="#4-Sqoop安装部署" class="headerlink" title="4. Sqoop安装部署"></a>4. Sqoop安装部署</h3><p>​        <a href="https://kfly.top/2019/11/26/hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">点击查看</a></p>
<h3 id="5-Sqooq数据的导入"><a href="#5-Sqooq数据的导入" class="headerlink" title="5. Sqooq数据的导入"></a>5. Sqooq数据的导入</h3><ul>
<li>导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据） </li>
</ul>
<h4 id="5-1-列举出所有的数据库"><a href="#5-1-列举出所有的数据库" class="headerlink" title="5.1 列举出所有的数据库"></a>5.1 列举出所有的数据库</h4><ul>
<li>命令行查看帮助文档</li>
</ul>
<pre><code class="shell">sqoop list-databases --help
</code></pre>
<ul>
<li>列出node03上mysql数据库中所有的数据库名称</li>
</ul>
<pre><code class="shell">sqoop list-databases --connect jdbc:mysql://node03:3306/ --username root --password 123456
</code></pre>
<ul>
<li>查看某一个数据库下面的所有数据表</li>
</ul>
<pre><code class="shell">sqoop list-tables --connect jdbc:mysql://node03:3306/hive --username root --password 123456
</code></pre>
<h4 id="5-2-导入数据库表数据到HDFS"><a href="#5-2-导入数据库表数据到HDFS" class="headerlink" title="5.2 导入数据库表数据到HDFS"></a>5.2 导入数据库表数据到HDFS</h4><ul>
<li><p>在MySQL数据库服务器中创建一个数据库userdb, 然后在创建一张表 emp，添加点测试数据到表中</p>
</li>
<li><p>从MySQL数据库服务器中的userdb数据库下的emp表导入HDFS上</p>
</li>
</ul>
<pre><code class="shell">sqoop import \
--connect jdbc:mysql://node02:3306/userdb \
--username root   \
--password 123456 \
--table emp \
--m 1



#参数解释
--connect   指定mysql链接地址
--username  连接mysql的用户名
--password  连接mysql的密码
--table     指定要导入的mysql表名称
--m:        表示这个MR程序需要多少个MapTask去运行，默认为4
默认路径是/user/hadoop下
</code></pre>
<ul>
<li>提交之后，会运行一个MR程序，最后查看HDFS上的目录看是否有数据生成</li>
</ul>
<p><img src="assets/table2hdfs.png" alt="table2hdfs"></p>
<h4 id="5-3-导入数据库表数据到HDFS指定目录"><a href="#5-3-导入数据库表数据到HDFS指定目录" class="headerlink" title="5.3 导入数据库表数据到HDFS指定目录"></a>5.3 导入数据库表数据到HDFS指定目录</h4><ul>
<li>在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。</li>
<li>使用参数 ==–target-dir==来指定导出目的地，</li>
<li>使用参数==–delete-target-dir==来判断导出目录是否存在，如果存在就删掉</li>
</ul>
<pre><code class="shell">sqoop import  --connect jdbc:mysql://node03:3306/userdb --username root --password 123456  --table emp  --target-dir /sqoop/emp  --delete-target-dir --m 1
</code></pre>
<ul>
<li>提交查看HDFS上的目录看是否有数据生成</li>
</ul>
<p><img src="assets/table2hdfsDir.png" alt="table2hdfsDir"></p>
<h4 id="5-4-导入数据库表数据到HDFS指定目录并且指定数据字段的分隔符"><a href="#5-4-导入数据库表数据到HDFS指定目录并且指定数据字段的分隔符" class="headerlink" title="5.4 导入数据库表数据到HDFS指定目录并且指定数据字段的分隔符"></a>5.4 导入数据库表数据到HDFS指定目录并且指定数据字段的分隔符</h4><ul>
<li>这里使用参数 <ul>
<li>==–fields-terminated-by 分隔符==</li>
</ul>
</li>
</ul>
<pre><code class="shell">sqoop import  \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456 \
--delete-target-dir \
--table emp  \
--target-dir /sqoop/emp1 \
--fields-terminated-by &#39;#&#39; \
--m 1
</code></pre>
<h4 id="5-5-导入关系表到Hive中"><a href="#5-5-导入关系表到Hive中" class="headerlink" title="5.5 导入关系表到Hive中"></a>5.5 导入关系表到Hive中</h4><ul>
<li>(1) 将我们mysql表当中的数据直接导入到hive表中的话，需要将hive的一个叫做==hive-exec-1.1.0-cdh5.14.2.jar==包拷贝到sqoop的lib目录下</li>
</ul>
<pre><code class="shell">cp /kkb/install/hive-1.1.0-cdh5.14.2/lib/hive-exec-1.1.0-cdh5.14.2.jar /kkb/install/sqoop-1.4.6-cdh5.14.2/lib/
</code></pre>
<ul>
<li><p>(2) 准备hive数据库与表</p>
<ul>
<li>在hive中创建一个数据库和表</li>
</ul>
<pre><code class="sql">create database sqooptohive;

create external table sqooptohive.emp_hive(id int,name string,deg string,salary double ,dept string) row format delimited fields terminated by &#39;\001&#39;;
</code></pre>
</li>
<li><p>(3) 把mysql表数据导入到hive表中</p>
</li>
</ul>
<pre><code class="shell">sqoop import \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456 \
--table emp \
--fields-terminated-by &#39;\001&#39; \
--hive-import \
--hive-table sqooptohive.emp_hive \
--hive-overwrite \
--delete-target-dir \
--m 1

##参数解释
--hive-table      指定要导入到hive表名
--hive-import     导入数据到hive表中
--hive-overwrite  覆盖hive表中已存有的数据

</code></pre>
<p>分为两步</p>
<ul>
<li>(4) 执行完成了查看hive中表的数据<ul>
<li><strong>select * from sqooptohive.emp_hive;</strong></li>
</ul>
</li>
</ul>
<p><img src="assets/emp_hive.png" alt="emp_hive"></p>
<h4 id="5-6-导入数据库表数据到hive中-并自动创建hive表"><a href="#5-6-导入数据库表数据到hive中-并自动创建hive表" class="headerlink" title="5.6 导入数据库表数据到hive中(并自动创建hive表)"></a>5.6 导入数据库表数据到hive中(并自动创建hive表)</h4><ul>
<li>可以通过命令来将我们的mysql的表直接导入到hive表当中去，==不需要事先创建hive表==</li>
</ul>
<pre><code class="shell">sqoop import \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456 \
--table emp \
--hive-database sqooptohive \
--hive-table emp1 \
--hive-import \
--m 1 


drop table emp1
</code></pre>
<ul>
<li>执行完成了查看hive中表的数据<ul>
<li><strong>select * from sqooptohive.emp1;</strong></li>
</ul>
</li>
</ul>
<p><img src="assets/em1.png" alt="em1"></p>
<h4 id="5-7-导入表数据子集"><a href="#5-7-导入表数据子集" class="headerlink" title="5.7 导入表数据子集"></a>5.7 导入表数据子集</h4><ul>
<li>我们可以导入表的使用Sqoop导入工具，”where”子句的一个子集。它执行在各自的数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。</li>
<li>按照条件进行查找，通过==<strong>–where</strong>==参数来查找表emp当中==<strong>dept</strong>==字段的值为 <strong>==TP==</strong> 的所有数据导入到hdfs上面去</li>
</ul>
<pre><code class="shell">sqoop import \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456 \
--table emp \
--target-dir /sqoop/emp_where \
--delete-target-dir \
--where &quot;dept = &#39;TP&#39;&quot; \
--m 1 
</code></pre>
<ul>
<li><p>提交查看HDFS上的目录看是否有数据生成</p>
<p><img src="assets/emp_where.png" alt=""></p>
</li>
</ul>
<h4 id="5-8-sql语句查找导入hdfs"><a href="#5-8-sql语句查找导入hdfs" class="headerlink" title="5.8 sql语句查找导入hdfs"></a>5.8 sql语句查找导入hdfs</h4><ul>
<li>我们还可以通过 -–query参数来指定我们的sql语句，通过sql语句来过滤我们的数据进行导入</li>
</ul>
<pre><code class="shell">sqoop import \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456 \
--target-dir /sqoop/emp_sql \
--delete-target-dir \
--query &#39;select * from emp where salary &gt;30000 and $CONDITIONS&#39; \
--m 1
</code></pre>
<ul>
<li>提交查看HDFS上的目录看是否有数据生成</li>
</ul>
<p><img src="assets/emp_conditions.png" alt="emp_conditions"></p>
<ul>
<li><p>==补充：==</p>
<p>~~~shell<br>$CONTITONS是linux系统的变量，如果你想通过并行的方式导入结果，每个map task需要执行sql查询后脚语句的副本，结果会根据sqoop推测的边界条件分区。query必须包含$CONDITIONS。这样每个sqoop程序都会被替换为一个独立的条件。同时你必须指定 –split-by ‘字段’，后期是按照字段进行数据划分，最后可以达到多个MapTask并行运行。</p>
</li>
</ul>
<p>  sqoop import \<br>  –connect jdbc:mysql://node03:3306/userdb \<br>  –username root \<br>  –password 123456 \<br>  –target-dir /sqoop/emp_sql_2 \<br>  –delete-target-dir \<br>  –query ‘select * from emp where salary &gt;30000 and $CONDITIONS’ \<br>  –split-by ‘id’ \<br>  –m 2</p>
<p>  sqoop import \<br>  –connect jdbc:mysql://node03:3306/userdb \<br>  –username root \<br>  –password 123456 \<br>  –target-dir /sqoop/emp_sql_2 \<br>  –delete-target-dir \<br>  –query ‘select * from emp where id &gt;1 and $CONDITIONS’ \<br>  –split-by ‘salary’ \<br>  –m 2</p>
<p>  sqoop import \<br>  –connect jdbc:mysql://node03:3306/userdb \<br>  –username root \<br>  –password 123456 \<br>  –target-dir /sqoop/emp_sql_2 \<br>  –delete-target-dir \<br>  –query ‘select * from emp where id &gt;1 and $CONDITIONS’ \<br>  –split-by ‘id’ \<br>  –m 7</p>
<p>  –split-by ‘字段’： 后期按照字段进行数据划分实现并行运行多个MapTask。</p>
<pre><code>

#### 5.9 增量导入

* 在实际工作当中，数据的导入很多时候都是==只需要导入增量数据即可==，并不需要将表中的数据全部导入到hive或者hdfs当中去，肯定会出现重复的数据的状况，所以我们一般都是选用一些字段进行增量的导入，为了支持增量的导入，sqoop也给我们考虑到了这种情况并且支持增量的导入数据

* 增量导入是仅导入新添加的表中的行的技术。

* 它需要添加 ==‘incremental’, ‘check-column’, 和 ‘last-value’==选项来执行增量导入。

</code></pre><p>  –incremental <mode><br>  –check-column <column name=""><br>  –last value <last check="" column="" value=""></last></column></mode></p>
<pre><code>
* ==第一种增量导入实现==

  * ==基于递增列的增量数据导入（Append方式）==
  * 导入emp表当中id大于1202的所有数据
    * 注意：==这里不能加上 --delete-target-dir  参数，添加就报错==

  ~~~shell
  sqoop import \
  --connect jdbc:mysql://node03:3306/userdb \
  --username root \
  --password 123456 \
  --table emp \
  --incremental append \
  --check-column id \
  --last-value 1202  \
  --target-dir /sqoop/increment \
  --m 1


  ##参数解释
  --incremental   这里使用基于递增列的增量数据导入
  --check-column  递增列字段
  --last-value    指定上一次导入中检查列指定字段最大值
  --target-dir    数据导入的目录
</code></pre><ul>
<li>提交查看HDFS上的目录看是否有数据生成</li>
</ul>
<p><img src="assets/sqoop_increment1.png" alt="sqoop_increment1"></p>
<ul>
<li><p>==第二种增量导入实现==</p>
<ul>
<li><p>==基于时间列的增量数据导入（LastModified方式）==</p>
<ul>
<li>此方式要求原有表中有time字段，它能指定一个时间戳<ul>
<li>user表结构和数据</li>
</ul>
</li>
</ul>
<p><img src="assets/table_user.png" alt="table_user"></p>
</li>
</ul>
<pre><code class="shell">sqoop import \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456  \
--table user \
--incremental lastmodified  \
--check-column createTime  \
--last-value &#39;2019-10-01 10:30:00&#39;  \
--target-dir /sqoop/increment2 \
--m 1

##参数解释
--incremental   这里使用基于时间列的增量导入
--check-column  时间字段
--last-value    指定上一次导入中检查列指定字段最大值
--target-dir    数据导入的目录
                如果该目录存在(可能已经有数据)
                再使用的时候需要添加 --merge-key or --append
        --merge-key 指定合并key（对于有修改的）
        --append    直接追加修改的数据
</code></pre>
<ul>
<li>提交查看HDFS上的目录看是否有数据生成</li>
</ul>
</li>
</ul>
<p><img src="assets/sqoop_increment2.png" alt="sqoop_increment1"></p>
<h4 id="5-10-mysql表的数据导入到hbase中"><a href="#5-10-mysql表的数据导入到hbase中" class="headerlink" title="5.10 mysql表的数据导入到hbase中"></a>5.10 mysql表的数据导入到hbase中</h4><ul>
<li>实现把一张mysql表数据导入到hbase中</li>
</ul>
<pre><code class="shell">sqoop import \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456  \
--table emp \
--hbase-table  mysqluser \
--column-family  f1 \
--hbase-create-table \
--hbase-row-key id  \
--m 1 


#参数说明
--hbase-table              指定hbase表名
--column-family         指定表的列族
--hbase-create-table     表不存在就创建
--hbase-row-key         指定hbase表的id
--m                      指定使用的MapTask个数
list
scan &#39;mysqluser&#39;
disable &#39;mysqluser&#39;
drop &#39;mysqluser&#39;


# mysql导入hbase 不同的列族
sqoop import \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456  \
--columns id,salary,dept \
--table emp \
--hbase-table  mysqluser2 \
--column-family  f1 \
--hbase-create-table \
--hbase-row-key id  \
--m 1 


sqoop import \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456  \
--columns id,name,deg \
--table emp \
--hbase-table  mysqluser2 \
--column-family  f2 \
--hbase-create-table \
--hbase-row-key id  \
--m 1 


</code></pre>
<h3 id="6-Sqoop数据的导出"><a href="#6-Sqoop数据的导出" class="headerlink" title="6. Sqoop数据的导出"></a>6. Sqoop数据的导出</h3><ul>
<li>将数据从HDFS把文件导出到RDBMS数据库<ul>
<li>导出前，目标表必须存在于目标数据库中。<ul>
<li>默认操作是从将文件中的数据使用INSERT语句插入到表中</li>
<li>更新模式下，是生成UPDATE语句更新表数据</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="6-1-hdfs文件导出到mysql表中"><a href="#6-1-hdfs文件导出到mysql表中" class="headerlink" title="6.1 hdfs文件导出到mysql表中"></a>6.1 hdfs文件导出到mysql表中</h4><ul>
<li>1、数据是在HDFS当中的如下目录/user/hive/warehouse/hive_source，数据内容如下</li>
</ul>
<pre><code>1 zhangsan 20 hubei
2 lisi 30 hunan
3 wangwu 40 beijing
4 xiaoming 50 shanghai
</code></pre><ul>
<li>2、创建一张mysql表<ul>
<li>注意mysql中的这个表一定要先创建！ 不然报错！</li>
</ul>
</li>
</ul>
<pre><code class="sql">CREATE TABLE  userdb.fromhdfs (
   id INT DEFAULT NULL,
   name VARCHAR(100) DEFAULT NULL,
   age int DEFAULT NULL,
   address VARCHAR(100) DEFAULT NULL
) ENGINE=INNODB DEFAULT CHARSET=utf8;
</code></pre>
<ul>
<li>3、执行导出命令</li>
</ul>
<pre><code class="shell">sqoop export \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456 \
--table fromhdfs \
--export-dir /user/hive/warehouse/hive_source \
--input-fields-terminated-by &quot; &quot; 


##参数解释
--table                       指定导出的mysql表名
--export-dir                   指定hdfs数据文件目录
--input-fields-terminated-by  指定文件数据字段的分隔符
</code></pre>
<ul>
<li>4、验证mysql表数据</li>
</ul>
<p><img src="assets/fromhdfs.png" alt="fromhdfs"></p>
<h3 id="7-Sqoop-job"><a href="#7-Sqoop-job" class="headerlink" title="7. Sqoop job"></a>7. Sqoop job</h3><ul>
<li><p>将事先定义好的数据导入导出任务按照指定流程运行</p>
</li>
<li><p>语法</p>
</li>
</ul>
<pre><code>sqoop job (generic-args) (job-args)
   [-- [subtool-name] (subtool-args)]
</code></pre><h4 id="7-1-创建job"><a href="#7-1-创建job" class="headerlink" title="7.1 创建job"></a>7.1 创建job</h4><ul>
<li>==–create==<ul>
<li>创建一个名为myjob,实现从mysql表数据导入到hdfs上的作业<ul>
<li>注意<ul>
<li>在创建job时，==命令”– import” 中间有个空格==</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="shell">sqoop job --help

##创建一个sqoop作业
sqoop job \
--create myjob1 \
-- import \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456 \
--table emp \
--target-dir /sqoop/myjob \
--delete-target-dir \
--m 1

##创建一个sqoop增量导入的作业
sqoop  job  \
--create incrementJob1 \
-- import \
--connect jdbc:mysql://node03:3306/userdb \
--username root \
--password 123456  \
--table user \
--target-dir /sqoop/incrementJob \
--incremental append  \
--check-column createTime  \
--last-value &#39;2019-11-19 16:40:21&#39;  \
--m 1


# incremental.last.value
</code></pre>
<h4 id="7-2-验证-job"><a href="#7-2-验证-job" class="headerlink" title="7.2 验证 job"></a>7.2 验证 job</h4><ul>
<li><p>==–list==</p>
</li>
<li><p>验证作业是否创建成功</p>
<ul>
<li>执行如下命令</li>
</ul>
<p>~~~shell<br>sqoop job –list</p>
</li>
</ul>
<p>  最后显示：<br>  Available jobs:<br>    myjob</p>
<pre><code>


#### 7.3 查看job

* ==--show==
* 查看作业的详细信息
  * 执行如下命令

~~~shell
sqoop job --show myjob1
</code></pre><h4 id="7-4-执行job"><a href="#7-4-执行job" class="headerlink" title="7.4 执行job"></a>7.4 执行job</h4><ul>
<li><p>==–exec==</p>
<ul>
<li>用于执行保存的作业</li>
</ul>
<pre><code class="shell">sqoop job --exec myjob1
</code></pre>
<ul>
<li>解决sqoop需要输入密码的问题<ul>
<li>修改配置文件<ul>
<li>vi /kkb/install/sqoop-1.4.6-cdh5.14.2/conf/sqoop-site.xml</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="xml">&lt;property&gt;
    &lt;name&gt;sqoop.metastore.client.record.password&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;If true, allow saved passwords in the metastore.
    &lt;/description&gt;
&lt;/property&gt;
</code></pre>
</li>
</ul>
<h4 id="7-5-删除job"><a href="#7-5-删除job" class="headerlink" title="7.5 删除job"></a>7.5 删除job</h4><ul>
<li>==–delete==<ul>
<li>用于删除保存作业</li>
</ul>
</li>
</ul>
<pre><code class="shell">sqoop job --delete myjob
</code></pre>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="https://blog.sev7e0.site/">大数据施工现场</a></span>
        <span>/</span>
        
        <span><a href="https://wangchujiang.com/linux-command/">linux命令行工具</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="/js/gitment.js"></script>
<script>
    var gitment = new Gitment({
        id: 'sqoop数据迁移工具',
        owner: 'orchid-ding',
        repo: 'kfly-blog-comment',
        oauth: {
            client_id: '0770cdab79393197b6f5',
            client_secret: '376fb6c7bcd5047718b356712f596b89e490360c',
        },
    })
    gitment.render('comment-container')
</script>




</html>
