<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="天行健、君子以自强不息；地势坤，君子以厚德载物。">
    <meta name="keyword"  content="兰草">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        SparkCore:SQL知识梳理 - kfly的博客 | kfly&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_1598291_q3el2wqimj.css" type="text/css">
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>kfly</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont iconhome"></i>
                    <span>主页</span>
                </a>
            </li>
 	   <li >
                <a href="/spec/">
                    <i class="iconfont iconzhuanti"></i>
                    <span>专题</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>简历</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-SQL"><span class="toc-text">Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#的四大特性"><span class="toc-text">的四大特性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame概述"><span class="toc-text">DataFrame概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DataFrame和RDD的优缺点"><span class="toc-text">DataFrame和RDD的优缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataSet概述"><span class="toc-text">DataSet概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#示例代码"><span class="toc-text">示例代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#自定义函数"><span class="toc-text">自定义函数</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 君子谦谦，温和有礼，有才而不骄，得志而不傲，居于谷而不卑。 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        SparkCore:SQL知识梳理
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-08-25 10:26:52</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#spark sql" title="spark sql">spark sql</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><ul>
<li>Spark SQL is Apache Spark’s module for working with structured data.</li>
<li>SparkSQL是apache Spark用来处理结构化数据的一个模块</li>
</ul>
<h3 id="的四大特性"><a href="#的四大特性" class="headerlink" title="的四大特性"></a>的四大特性</h3><ul>
<li><p>==1、易整合(Integrated)==</p>
<ul>
<li><pre><code class="sql">results = spark.sql(&quot;SELECT * FROM people&quot;)
</code></pre>
</li>
</ul>
</li>
<li><p>==2、统一的数据源访问(Uniform Data Access)==</p>
<ul>
<li><pre><code class="scala">spark.read.json(&quot;s3n://...&quot;)
spark.read.text(&quot;s3n://...&quot;)
spark.read.parquet(&quot;s3n://...&quot;)
</code></pre>
</li>
</ul>
</li>
<li><p>==3、兼容hive(Hive Integration)==</p>
</li>
<li><p>==4、支持标准的数据库连接(Standard Connectivity)==</p>
<ul>
<li><pre><code class="scala">spark.read.jdbc(---)
</code></pre>
</li>
</ul>
</li>
</ul>
<h3 id="DataFrame概述"><a href="#DataFrame概述" class="headerlink" title="DataFrame概述"></a>DataFrame概述</h3><ul>
<li>在Spark中，DataFrame是一种==以RDD为基础的分布式数据集==，类似于==传统数据库的二维表格==</li>
<li>DataFrame带有==Schema元信息==，即DataFrame所表示的二维表数据集的每一列都带有名称和类型，但底层做了更多的优化</li>
<li>DataFrame可以从很多数据源构建<ul>
<li>比如：已经存在的RDD、结构化文件、外部数据库、Hive表。</li>
</ul>
</li>
<li>RDD可以把它内部元素看成是一个java对象</li>
<li>DataFrame可以把内部是一个Row对象，它表示一行一行的数据</li>
</ul>
<h4 id="DataFrame和RDD的优缺点"><a href="#DataFrame和RDD的优缺点" class="headerlink" title="DataFrame和RDD的优缺点"></a>DataFrame和RDD的优缺点</h4><ul>
<li><p>==1、RDD==</p>
<ul>
<li><p>==优点==</p>
<ul>
<li>1、编译时类型安全<ul>
<li>开发会进行类型检查，在编译的时候及时发现错误</li>
</ul>
</li>
<li>2、具有面向对象编程的风格</li>
</ul>
</li>
<li><p>==缺点==</p>
<ul>
<li><p>1、构建大量的java对象占用了大量heap堆空间，导致频繁的GC</p>
<pre><code>由于数据集RDD它的数据量比较大，后期都需要存储在heap堆中，这里有heap堆中的内存空间有限，出现频繁的垃圾回收（GC），程序在进行垃圾回收的过程中，所有的任务都是暂停。影响程序执行的效率
</code></pre></li>
<li><p>2、数据的序列化和反序列性能开销很大</p>
<pre><code>  在分布式程序中，对象(对象的内容和结构)是先进行序列化，发送到其他服务器，进行大量的网络传输，然后接受到这些序列化的数据之后，再进行反序列化来恢复该对象
</code></pre></li>
</ul>
</li>
</ul>
</li>
<li><p>==2、DataFrame==</p>
<ul>
<li>==DataFrame引入了schema元信息和off-heap(堆外)==</li>
<li>==优点==<ul>
<li>1、DataFrame引入off-heap，大量的对象构建直接使用操作系统层面上的内存，不在使用heap堆中的内存，这样一来heap堆中的内存空间就比较充足，不会导致频繁GC，程序的运行效率比较高，它是解决了RDD构建大量的java对象占用了大量heap堆空间，导致频繁的GC这个缺点。</li>
<li>2、DataFrame引入了schema元信息—就是数据结构的描述信息，后期spark程序中的大量对象在进行网络传输的时候，只需要把数据的内容本身进行序列化就可以，数据结构信息可以省略掉。这样一来数据网络传输的数据量是有所减少，数据的序列化和反序列性能开销就不是很大了。它是解决了RDD数据的序列化和反序列性能开销很大这个缺点</li>
<li>==缺点==<ul>
<li>DataFrame引入了schema元信息和off-heap(堆外)它是分别解决了RDD的缺点，同时它也丢失了RDD的优点<ul>
<li>1、编译时类型不安全<ul>
<li>编译时不会进行类型的检查，这里也就意味着前期是无法在编译的时候发现错误，只有在运行的时候才会发现</li>
</ul>
</li>
<li>2、不在具有面向对象编程的风格</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="SCALA">// 1. 读取文本文件
val personDF=spark.read.text(&quot;/person.txt&quot;)
val peopleDF=spark.read.json(&quot;/people.json&quot;)
val usersDF=spark.read.parquet(&quot;/users.parquet&quot;)

// 2. 加载数据
val rdd1=sc.textFile(&quot;/person.txt&quot;).map(x=&gt;x.split(&quot; &quot;))
    //定义一个样例类
    case class Person(id:String,name:String,age:Int)
    //把rdd与样例类进行关联
    val personRDD=rdd1.map(x=&gt;Person(x(0),x(1),x(2).toInt))
    //把rdd转换成DataFrame
    val personDF=personRDD.toDF
// 3.语法风格
// 3.1 DSL
        personDF.select(&quot;name&quot;).show
        personDF.select($&quot;name&quot;).show
        personDF.select(col(&quot;name&quot;).show
        //实现age+1
         personDF.select($&quot;name&quot;,$&quot;age&quot;,$&quot;age&quot;+1)).show   
        //实现age大于30过滤
         personDF.filter($&quot;age&quot; &gt; 30).show
         //按照age分组统计次数
         personDF.groupBy(&quot;age&quot;).count.show 
        //按照age分组统计次数降序
         personDF.groupBy(&quot;age&quot;).count().sort($&quot;count&quot;.desc)show  
// 3.2 sql
        //DataFrame注册成表
        personDF.createTempView(&quot;person&quot;)

        //使用SparkSession调用sql方法统计查询
        spark.sql(&quot;select * from person&quot;).show
        spark.sql(&quot;select name from person&quot;).show
        spark.sql(&quot;select name,age from person&quot;).show
</code></pre>
<h3 id="DataSet概述"><a href="#DataSet概述" class="headerlink" title="DataSet概述"></a>DataSet概述</h3><ul>
<li>DataSet是分布式的数据集合，Dataset提供了强类型支持，也是在RDD的每行数据加了类型约束。</li>
<li>DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型和可以用强大lambda函数）以及使用了Spark SQL优化的执行引擎。</li>
</ul>
<pre><code class="properties">1. 假设RDD中的两行数据长这样
        1,张三,23
        2,李四,35
2.那么DataFrame中的数据长这样
                ID:String    Name:String    Age:int
                1                张三                23
                2                李四                35
3.Dataset中的数据长这样 
            value:String
        1,张三,23
        2,李四,35
  或者
  value:People(age:bigint,id:bigint,name:string)
        People(id=1,name=&quot;张三&quot;,age=23)
        People(id=2,name=&quot;李四&quot;,age=23)
</code></pre>
<pre><code class="properties">DataSet包含了DataFrame的功能，Spark2.0中两者统一，DataFrame表示为DataSet[Row]，即DataSet的子集。
（1）DataSet可以在编译时检查类型
（2）并且是面向对象的编程接口
</code></pre>
<p>DataFrame DataSet转换 构建dataset</p>
<pre><code class="scala">// 把一个DataFrame转换成DataSet
val dataSet=dataFrame.as[强类型]
//  2、把一个DataSet转换成DataFrame
val dataFrame=dataSet.toDF

// 补充说明,可以从dataFrame和dataSet获取得到rdd
val rdd1=dataFrame.rdd
val rdd2=dataSet.rdd

// 1、 通过sparkSession调用createDataset方法
  val ds=spark.createDataset(1 to 10) //scala集合
  val ds=spark.createDataset(sc.textFile(&quot;/person.txt&quot;))  //rdd

// 2、使用scala集合和rdd调用toDS方法
  sc.textFile(&quot;/person.txt&quot;).toDS
  List(1,2,3,4,5).toDS

// 3、把一个DataFrame转换成DataSet
  val dataSet=dataFrame.as[强类型]

// 4、通过一个DataSet转换生成一个新的DataSet
   List(1,2,3,4,5).toDS.map(x=&gt;x*10)

// 5、将rdd与Row对象进行关联
    val rowRDD: RDD[Row] = data.map(x=&gt;Row(x(0),x(1),x(2).toInt))
    //指定dataFrame的schema信息   
    //这里指定的字段个数和类型必须要跟Row对象保持一致
    val schema=StructType(
        StructField(&quot;id&quot;,StringType)::
        StructField(&quot;name&quot;,StringType)::
        StructField(&quot;age&quot;,IntegerType)::Nil
    )
    val dataFrame: DataFrame = spark.createDataFrame(rowRDD,schema)
</code></pre>
<h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><pre><code class="scala">// EG
 // 1、构建SparkSession对象,开启hive支持
    val spark: SparkSession = SparkSession.builder()
      .appName(&quot;HiveSupport&quot;)
      .master(&quot;local[2]&quot;)
      .enableHiveSupport() //开启对hive的支持
      .getOrCreate()

// 2. 读取mysql数据
        val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
        val url=&quot;jdbc:mysql://node03:3306/spark&quot;
        val tableName=&quot;user&quot;
        val properties = new Properties()
      properties.setProperty(&quot;user&quot;,&quot;root&quot;)
      properties.setProperty(&quot;password&quot;,&quot;123456&quot;)
   val mysqlDF: DataFrame = spark.read.jdbc(url,tableName,properties)

// 3. 保存数据到mysql表中
     //mode:指定数据的插入模式
        //overwrite: 表示覆盖，如果表不存在，事先帮我们创建
        //append   :表示追加， 如果表不存在，事先帮我们创建
        //ignore   :表示忽略，如果表事先存在，就不进行任何操作
        //error    :如果表事先存在就报错（默认选项）
    result.write.mode(&quot;append&quot;).jdbc(url,&quot;kaikeba&quot;,properties)
</code></pre>
<h3 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h3><pre><code class="scala">//小写转大写
sparkSession.udf.register(&quot;low2Up&quot;,new UDF1[String,String]() {
  override def call(t1: String): String = {
    t1.toUpperCase
  }
},StringType)
//大写转小写
sparkSession.udf.register(&quot;up2low&quot;,(x:String)=&gt;x.toLowerCase)
// 把数据文件中的单词统一转换成大小写
sparkSession.sql(&quot;select  value from t_udf&quot;).show()
sparkSession.sql(&quot;select  low2Up(value) from t_udf&quot;).show()
sparkSession.sql(&quot;select  up2low(value) from t_udf&quot;).show()
</code></pre>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span>/</span>
        
        <span><a href="https://blog.sev7e0.site/">大数据施工现场</a></span>
        <span>/</span>
        
        <span><a href="https://wangchujiang.com/linux-command/">linux命令行工具</a></span>
        <span>/</span>
        
        <span><a href="https://github.com/orchid-ding">github</a></span>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="/js/gitment.js"></script>
<script>
    var gitment = new Gitment({
        id: 'SparkCore:SQL知识梳理',
        owner: 'orchid-ding',
        repo: 'kfly-blog-comment',
        oauth: {
            client_id: '0770cdab79393197b6f5',
            client_secret: '376fb6c7bcd5047718b356712f596b89e490360c',
        },
    })
    gitment.render('comment-container')
</script>




</html>
