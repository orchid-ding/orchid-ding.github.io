(window.webpackJsonp=window.webpackJsonp||[]).push([[99],{584:function(a,s,t){"use strict";t.r(s);var r=t(19),e=Object(r.a)({},(function(){var a=this,s=a.$createElement,t=a._self._c||s;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h2",{attrs:{id:"spark任务调度"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark任务调度"}},[a._v("#")]),a._v(" Spark任务调度")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://gitee.com/kflys/uPic/raw/master/uPic/1612261065400-f46d6505-c719-41c1-a571-9c0c9f31cabb.png",alt:"img"}})]),a._v(" "),t("div",{staticClass:"language-markdown line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-markdown"}},[t("code",[t("span",{pre:!0,attrs:{class:"token list punctuation"}},[a._v("-")]),a._v(" Driver端运行客户端的main方法，构建SparkContext对象，在SparkContext对象内部依次构建DAGScheduler和TaskScheduler\n"),t("span",{pre:!0,attrs:{class:"token list punctuation"}},[a._v("-")]),a._v(" 按照rdd的一系列操作顺序，来生成DAG有向无环图\n"),t("span",{pre:!0,attrs:{class:"token list punctuation"}},[a._v("-")]),a._v(" DAGScheduler拿到DAG有向无环图之后，按照宽依赖进行stage的划分。每一个stage内部有很多可以并行运行的task，最后封装在一个一个的taskSet集合中，然后把taskSet发送给TaskScheduler\n"),t("span",{pre:!0,attrs:{class:"token list punctuation"}},[a._v("-")]),a._v(" 所有task运行完成，整个任务也就结束了\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br")])]),t("h2",{attrs:{id:"spark的运行架构"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark的运行架构"}},[a._v("#")]),a._v(" spark的运行架构")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2021/png/434900/1612261065396-09e39e85-4e45-497f-846c-d7fa66b250eb.png",alt:"img"}})]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("(1) Driver端向资源管理器Master发送注册和申请计算资源的请求\n\n(2) Master通知对应的worker节点启动executor进程(计算资源)\n\n(3) executor进程向Driver端发送注册并且申请task请求\n\n(4) Driver端运行客户端的main方法，构建SparkContext对象，在SparkContext对象内部依次构建DAGScheduler和TaskScheduler\n\n(5) 按照客户端代码洪rdd的一系列操作顺序，生成DAG有向无环图\n\n(6) DAGScheduler拿到DAG有向无环图之后，按照宽依赖进行stage的划分。每一个stage内部有很多可以并行运行的task，最后封装在一个一个的taskSet集合中，然后把taskSet发送给TaskScheduler\n\n(7) TaskScheduler得到taskSet集合之后，依次遍历取出每一个task提交到worker节点上的executor进程中运行\n\n(8) 所有task运行完成，Driver端向Master发送注销请求，Master通知Worker关闭executor进程，Worker上的计算资源得到释放，最后整个任务也就结束了。\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br"),t("span",{staticClass:"line-number"},[a._v("9")]),t("br"),t("span",{staticClass:"line-number"},[a._v("10")]),t("br"),t("span",{staticClass:"line-number"},[a._v("11")]),t("br"),t("span",{staticClass:"line-number"},[a._v("12")]),t("br"),t("span",{staticClass:"line-number"},[a._v("13")]),t("br"),t("span",{staticClass:"line-number"},[a._v("14")]),t("br"),t("span",{staticClass:"line-number"},[a._v("15")]),t("br")])]),t("ul",[t("li",[a._v("基于wordcount程序剖析spark任务的提交、划分、调度流程")])]),a._v(" "),t("p",[t("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2021/png/434900/1612261065410-1eab9be5-3917-4647-b467-35e730734098.png",alt:"img"}})]),a._v(" "),t("h2",{attrs:{id:"自定义分区"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#自定义分区"}},[a._v("#")]),a._v(" 自定义分区")]),a._v(" "),t("ul",[t("li",[a._v("在对RDD数据进行分区时，默认使用的是HashPartitioner")]),a._v(" "),t("li",[a._v("该函数对key进行哈希，然后对分区总数取模，取模结果相同的就会被分到同一个partition中")])]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("HashPartitioner分区逻辑：\n\tkey.hashcode % 分区总数 = 分区号\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br")])]),t("ul",[t("li",[a._v("如果嫌HashPartitioner功能单一，可以自定义partitioner")]),a._v(" "),t("li",[a._v("实现自定义partitioner大致分为3个步骤\n"),t("ul",[t("li",[a._v("1、继承org.apache.spark.Partitioner")]),a._v(" "),t("li",[a._v("2、重写numPartitions方法")]),a._v(" "),t("li",[a._v("3、重写getPartition方法")])])])]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("//1、对应上面的rdd数据进行自定义分区\n val result: RDD[(String, Int)] = wordLengthRDD.partitionBy(new MyPartitioner(3))\n\n//2、自定义分区\nclass MyPartitioner(num:Int) extends Partitioner{\n  //指定rdd的总的分区数\n  override def numPartitions: Int = {\n    num\n  }\n  //消息按照key的某种规则进入到指定的分区号中\n  override def getPartition(key: Any): Int ={\n    //这里的key就是单词\n    val length: Int = key.toString.length\n    length match {\n      case 4 =>0\n      case 5 =>1\n    }\n  }\n}\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br"),t("span",{staticClass:"line-number"},[a._v("9")]),t("br"),t("span",{staticClass:"line-number"},[a._v("10")]),t("br"),t("span",{staticClass:"line-number"},[a._v("11")]),t("br"),t("span",{staticClass:"line-number"},[a._v("12")]),t("br"),t("span",{staticClass:"line-number"},[a._v("13")]),t("br"),t("span",{staticClass:"line-number"},[a._v("14")]),t("br"),t("span",{staticClass:"line-number"},[a._v("15")]),t("br"),t("span",{staticClass:"line-number"},[a._v("16")]),t("br"),t("span",{staticClass:"line-number"},[a._v("17")]),t("br"),t("span",{staticClass:"line-number"},[a._v("18")]),t("br"),t("span",{staticClass:"line-number"},[a._v("19")]),t("br")])]),t("h2",{attrs:{id:"共享变量-broadcast-variable"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#共享变量-broadcast-variable"}},[a._v("#")]),a._v(" 共享变量(broadcast variable)")]),a._v(" "),t("ul",[t("li",[a._v("Spark中分布式执行的代码需要传递到各个Executor的Task上运行。对于一些只读、固定的数据(比如从DB中读出的数据),每次都需要Driver广播到各个Task上，这样效率低下。")]),a._v(" "),t("li",[a._v("广播变量允许将变量只广播给各个Executor。该Executor上的各个Task再从所在节点的BlockManager获取变量，而不是从Driver获取变量，以减少通信的成本，减少内存的占用，从而提升了效率。")])]),a._v(" "),t("p",[t("img",{attrs:{src:"https://gitee.com/kflys/uPic/raw/master/uPic/1612261065412-e9f9f63a-c1e9-4d99-9680-d981feb34f27.png",alt:"img"}})]),a._v(" "),t("h3",{attrs:{id:"广播变量使用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#广播变量使用"}},[a._v("#")]),a._v(" 广播变量使用")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("(1) 通过对一个类型T的对象调用 SparkContext.broadcast创建出一个Broadcast[T]对象。\n    任何可序列化的类型都可以这么实现\n(2) 通过 value 属性访问该对象的值\n(3) 变量只会被发到各个节点一次，应作为只读值处理（修改这个值不会影响到别的节点）\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br")])]),t("ul",[t("li",[a._v("使用广播变量代码示例")])]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('val word="spark"\nval rddData = rdd.collect\n//通过调用sparkContext对象的broadcast方法把数据广播出去\nval broadCast = sc.broadcast(word)\nval broadRddData = sc.broadcast(rddData)\n\n//在executor中通过调用广播变量的value属性获取广播变量的值,分布式环境下广播变量通过网络传输需要序列化\nval rdd2=rdd1.flatMap(_.split(" ")).filter(x=>x.equals(broadCast.value))\n')])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br")])]),t("h3",{attrs:{id:"广播变量使用注意事项"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#广播变量使用注意事项"}},[a._v("#")]),a._v(" 广播变量使用注意事项")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("1、不能将一个RDD使用广播变量广播出去\n\n2、广播变量只能在Driver端定义，不能在Executor端定义\n\n3、在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值\n\n4、如果executor端用到了Driver的变量，如果不使用广播变量在Executor有多少task就有多少Driver端的变量副本\n\n5、如果Executor端用到了Driver的变量，如果使用广播变量在每个Executor中只有一份Driver端的变量副本\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br"),t("span",{staticClass:"line-number"},[a._v("9")]),t("br")])]),t("h3",{attrs:{id:"累加器-accumulator"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#累加器-accumulator"}},[a._v("#")]),a._v(" 累加器(accumulator)")]),a._v(" "),t("ul",[t("li",[a._v("累加器（accumulator）是Spark中提供的一种分布式的变量机制，其原理类似于mapreduce，即分布式的改变，然后聚合这些改变")]),a._v(" "),t("li",[a._v("==累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数。可以使用累加器来进行全局的计数")])]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("val accumulator = sc.accumulator(0); \n    val result = linesRDD.map(s => {\n      accumulator.add(1) //有一条数据就增加1\n    })\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br")])]),t("h2",{attrs:{id:"序列化问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#序列化问题"}},[a._v("#")]),a._v(" 序列化问题")]),a._v(" "),t("ul",[t("li",[a._v("spark是分布式执行引擎，其核心抽象是弹性分布式数据集RDD，其代表了分布在不同节点的数据。Spark的计算是在executor上分布式执行的，故用户开发的关于RDD的map，flatMap，reduceByKey等transformation 操作（闭包）有如下执行过程：\n"),t("ul",[t("li",[a._v("（1）代码中对象在driver本地序列化")]),a._v(" "),t("li",[a._v("（2）对象序列化后传输到远程executor节点")]),a._v(" "),t("li",[a._v("（3）远程executor节点反序列化对象")]),a._v(" "),t("li",[a._v("（4）最终远程节点执行")])])]),a._v(" "),t("li",[a._v("故对象在执行中需要序列化通过网络传输，则必须经过序列化过程。")])]),a._v(" "),t("h3",{attrs:{id:"spark的任务序列化异常"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark的任务序列化异常"}},[a._v("#")]),a._v(" spark的任务序列化异常")]),a._v(" "),t("ul",[t("li",[a._v("在编写spark程序中，由于在map，foreachPartition等算子内部使用了外部定义的变量和函数，从而引发Task未序列化问题。")]),a._v(" "),t("li",[a._v("然而spark算子在计算过程中使用外部变量在许多情形下确实在所难免，比如在filter算子根据外部指定的条件进行过滤，map根据相应的配置进行变换。")]),a._v(" "),t("li",[a._v("经常会出现“org.apache.spark.SparkException: Task not serializable”这个错误\n"),t("ul",[t("li",[a._v("其原因就在于这些算子使用了外部的变量，但是这个变量不能序列化。")]),a._v(" "),t("li",[a._v("当前类使用了“extends Serializable”声明支持序列化，但是由于某些字段不支持序列化，仍然会导致整个类序列化时出现问题，最终导致出现Task未序列化问题。")])])])]),a._v(" "),t("h3",{attrs:{id:"解决序列化的办法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#解决序列化的办法"}},[a._v("#")]),a._v(" 解决序列化的办法")]),a._v(" "),t("ul",[t("li",[a._v("(1) 如果函数中使用了该类对象，该类要实现序列化\n"),t("ul",[t("li",[a._v("类  extends  Serializable")])])]),a._v(" "),t("li",[a._v("(2) 如果函数中使用了该类对象的成员变量，该类除了要实现序列化之外，所有的成员变量必须要实现序列化")]),a._v(" "),t("li",[a._v("(3) 对于不能序列化的成员变量使用==“@transient”==标注，告诉编译器不需要序列化")]),a._v(" "),t("li",[a._v("(4) 也可将依赖的变量独立放到一个小的class中，让这个class支持序列化，这样做可以减少网络传输量，提高效率。")]),a._v(" "),t("li",[a._v("(5) 可以把对象的创建直接在该函数中构建这样避免需要序列化")])]),a._v(" "),t("h2",{attrs:{id:"application、job、stage、task之间的关系"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#application、job、stage、task之间的关系"}},[a._v("#")]),a._v(" application、job、stage、task之间的关系")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://gitee.com/kflys/uPic/raw/master/uPic/1612261065419-0d4c5a1e-6c18-4890-aa4e-7e8e1887c907.png",alt:"img"}})]),a._v(" "),t("ul",[t("li",[a._v("一个application就是一个应用程序，包含了客户端所有的代码和计算资源")]),a._v(" "),t("li",[a._v("一个action操作对应一个DAG有向无环图，即一个action操作就是一个job")]),a._v(" "),t("li",[a._v("一个job中包含了大量的宽依赖，按照宽依赖进行stage划分，一个job产生了很多个stage")]),a._v(" "),t("li",[a._v("一个stage中有很多分区，一个分区就是一个task，即一个stage中有很多个task")]),a._v(" "),t("li",[a._v("总结\n"),t("ul",[t("li",[a._v("一个application包含了很多个job")]),a._v(" "),t("li",[a._v("一个job包含了很多个stage")]),a._v(" "),t("li",[a._v("一个stage包含了很多个task")])])])]),a._v(" "),t("h1",{attrs:{id:"spark内存计算框架"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark内存计算框架"}},[a._v("#")]),a._v(" Spark内存计算框架")]),a._v(" "),t("h2",{attrs:{id:"spark运行模式"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark运行模式"}},[a._v("#")]),a._v(" Spark运行模式")]),a._v(" "),t("div",{staticClass:"language-shell line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[a._v("spark-submit --class org.apache.spark.examples.SparkPi "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n--master "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("yarn")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# cluster / client")]),a._v("\n--deploy-mode cluster "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n--driver-memory 1g "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n--executor-memory 1g "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n--executor-cores "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n/kfly/install/spark-2.3.3-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.3.jar "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("\\")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("10")]),a._v("\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br"),t("span",{staticClass:"line-number"},[a._v("9")]),t("br")])]),t("h3",{attrs:{id:"yarn-cluster模式"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#yarn-cluster模式"}},[a._v("#")]),a._v(" yarn-cluster模式")]),a._v(" "),t("ul",[t("li",[a._v("spark程序的Driver程序在YARN中运行，运行结果不能在客户端显示，并且客户端可以在启动应用程序后消失。")]),a._v(" "),t("li",[a._v("最好运行那些将结果最终保存在外部存储介质（如HDFS、Redis、Mysql），客户端的终端显示的仅是作为YARN的job的简单运行状况。")])]),a._v(" "),t("p",[t("img",{attrs:{src:"https://gitee.com/kflys/uPic/raw/master/uPic/1612261065415-bc95bfb7-b61c-45fa-bef9-35f29b4553ce.png",alt:"img"}})]),a._v(" "),t("h3",{attrs:{id:"yarn-client模式"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#yarn-client模式"}},[a._v("#")]),a._v(" yarn-client模式")]),a._v(" "),t("ul",[t("li",[a._v("spark程序的Driver运行在Client上，应用程序运行结果会在客户端显示，所有适合运行结果有输出的应用程序（如spark-shell）")])]),a._v(" "),t("p",[t("img",{attrs:{src:"https://gitee.com/kflys/uPic/raw/master/uPic/1612261065419-b5fef086-0984-456b-9054-642a29f96394.png",alt:"img"}})]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("最大的区别就是Driver端的位置不一样。\n\nyarn-cluster: Driver端运行在yarn集群中，与ApplicationMaster进程在一起。\nyarn-client:  Driver端运行在提交任务的客户端,与ApplicationMaster进程没关系,经常用于进行测试\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br")])]),t("h2",{attrs:{id:"collect-算子操作剖析"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#collect-算子操作剖析"}},[a._v("#")]),a._v(" collect 算子操作剖析")]),a._v(" "),t("h3",{attrs:{id:"collect算子操作的作用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#collect算子操作的作用"}},[a._v("#")]),a._v(" collect算子操作的作用")]),a._v(" "),t("ul",[t("li",[a._v("1、它是一个action操作，会触发任务的运行")]),a._v(" "),t("li",[a._v("2、它会把RDD的数据进行收集之后，以数组的形式返回给Driver端")])]),a._v(" "),t("blockquote",[t("p",[a._v("1、默认Driver端的内存大小为1G，由参数 spark.driver.memory 设置\n2、如果某个rdd的数据量超过了Driver端默认的1G内存，对rdd调用collect操作，这里会出现Driver端的内存溢出，所有这个collect操作存在一定的风险，实际开发代码一般不会使用。\n3、 实际企业中一般都会把该参数调大，比如5G/10G等")])]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('// 可以在代码中修改该参数，如下\nnew SparkConf().set("spark.driver.memory","5G")\n')])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("比如说rdd的数据量达到了10G\n\nrdd.collect这个操作非常危险，很有可能出现driver端的内存不足\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br")])]),t("h2",{attrs:{id:"spark任务中资源参数剖析"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark任务中资源参数剖析"}},[a._v("#")]),a._v(" spark任务中资源参数剖析")]),a._v(" "),t("h3",{attrs:{id:"executor-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#executor-memory"}},[a._v("#")]),a._v(" --executor-memory")]),a._v(" "),t("ul",[t("li",[a._v("表示每一个executor进程需要的内存大小，它决定了后期操作数据的速度")])]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("比如说一个rdd的数据量大小为5g,这里给定的executor-memory为2g, 在这种情况下，内存是存储不下，它会把一部分数据保存在内存中，还有一部分数据保存在磁盘，后续需要用到该rdd的结果数据，可以从内存和磁盘中获取得到，这里就涉及到一定的磁盘io操作。\n\n,这里给定的executor-memory为10g，这里数据就可以完全在内存中存储下，后续需要用到该rdd的数据，就可以直接从内存中获取，这样一来，避免了大量的磁盘io操作。性能得到提升。\n\n\n在实际的工作，这里 --executor-memory 需要设置的大一点。\n比如说10G/20G/30G等\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br")])]),t("h3",{attrs:{id:"total-executor-cores"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#total-executor-cores"}},[a._v("#")]),a._v(" --total-executor-cores")]),a._v(" "),t("h4",{attrs:{id:"表示任务运行需要总的cpu核数-它决定了任务并行运行的粒度"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#表示任务运行需要总的cpu核数-它决定了任务并行运行的粒度"}},[a._v("#")]),a._v(" 表示任务运行需要总的cpu核数，它决定了任务并行运行的粒度")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("比如说要处理100个task，注意一个cpu在同一时间只能处理一个task线程。\n\n如果给定的总的cpu核数是5个，这里就需要100/5=20个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行20分钟。\n\n如果给定的总的cpu核数是20个，这里就需要100/20=5个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行5分钟。\n\n如果如果给定的总的cpu核数是100个，这里就需要100/100=1个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行1分钟。\n\n\n在实际的生产环境中，--total-executor-cores 这个参数一般也会设置的大一点，\n比如说 30个/50个/100个\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br"),t("span",{staticClass:"line-number"},[a._v("9")]),t("br"),t("span",{staticClass:"line-number"},[a._v("10")]),t("br"),t("span",{staticClass:"line-number"},[a._v("11")]),t("br")])]),t("h4",{attrs:{id:"总结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[a._v("#")]),a._v(" 总结")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("\t后期对于spark程序的优化，可以从这2个参数入手，无论你把哪一个参数调大，对程序运行的效率来说都会达到一定程度的提升\n    加大计算资源它是最直接、最有效果的优化手段。\n    在计算资源有限的情况下，可以考虑其他方面，比如说代码层面，JVM层面等\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br")])]),t("h2",{attrs:{id:"spark任务的调度模式"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark任务的调度模式"}},[a._v("#")]),a._v(" spark任务的调度模式")]),a._v(" "),t("h3",{attrs:{id:"spark中的调度模式主要有两种-fifo-和-fair"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark中的调度模式主要有两种-fifo-和-fair"}},[a._v("#")]),a._v(" Spark中的调度模式主要有两种：FIFO 和 FAIR")]),a._v(" "),t("h4",{attrs:{id:"fifo-先进先出"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#fifo-先进先出"}},[a._v("#")]),a._v(" FIFO（先进先出）")]),a._v(" "),t("p",[a._v("默认情况下Spark的调度模式是FIFO，谁先提交谁先执行，后面的任务需要等待前面的任务执行。")]),a._v(" "),t("h4",{attrs:{id:"fair-公平调度"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#fair-公平调度"}},[a._v("#")]),a._v(" FAIR（公平调度）")]),a._v(" "),t("p",[a._v("支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。避免大任务运行时间长，占用了大量的资源，后面小任务无法提交运行。")]),a._v(" "),t("h2",{attrs:{id:"spark任务的分配资源策略"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark任务的分配资源策略"}},[a._v("#")]),a._v(" spark任务的分配资源策略")]),a._v(" "),t("p",[a._v("给application分配资源选择worker（executor），现在有两种策略")]),a._v(" "),t("h3",{attrs:{id:"尽量的打散"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#尽量的打散"}},[a._v("#")]),a._v(" 尽量的打散")]),a._v(" "),t("p",[a._v("即一个Application尽可能多的分配到不同的节点。这个可以通过设置spark.deploy.spreadOut来实现。默认值为true，即尽量的打散（默认）。可以充分的发挥数据的本地性，提升执行效率")]),a._v(" "),t("h3",{attrs:{id:"尽量的集中"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#尽量的集中"}},[a._v("#")]),a._v(" 尽量的集中")]),a._v(" "),t("p",[a._v("即一个Application尽量分配到尽可能少的节点。")]),a._v(" "),t("div",{staticClass:"language-sh line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-sh"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 假如集群有两个节点，worker1,worker2。各 cores 4 memory 128G，需要分配 4cores。 32g")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 1. 尽量的集中(尽可能分配更少的节点，worker1 4 32g)")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 2. 尽量打散（尽可能多的分配，worker1 worker2按照顺序依次分配，不够再次循环）")]),a._v("\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br")])])])}),[],!1,null,null,null);s.default=e.exports}}]);