(window.webpackJsonp=window.webpackJsonp||[]).push([[103],{587:function(a,s,e){"use strict";e.r(s);var t=e(19),n=Object(t.a)({},(function(){var a=this,s=a.$createElement,e=a._self._c||s;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("ul",[e("li",[a._v("Spark SQL is Apache Spark's module for working with structured data.")]),a._v(" "),e("li",[a._v("SparkSQL是apache Spark用来处理结构化数据的一个模块")])]),a._v(" "),e("h2",{attrs:{id:"spark-sql四大特性"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#spark-sql四大特性"}},[a._v("#")]),a._v(" Spark SQL四大特性")]),a._v(" "),e("h3",{attrs:{id:"易整合-integrated"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#易整合-integrated"}},[a._v("#")]),a._v(" 易整合(Integrated)")]),a._v(" "),e("div",{staticClass:"language-sql line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-sql"}},[e("code",[a._v("  results "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" spark"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("sql")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[a._v('"SELECT * FROM people"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("h3",{attrs:{id:"统一的数据源访问-uniform-data-access"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#统一的数据源访问-uniform-data-access"}},[a._v("#")]),a._v(" 统一的数据源访问(Uniform Data Access)")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('spark.read.json("s3n://...")\nspark.read.text("s3n://...")\nspark.read.parquet("s3n://...")\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br")])]),e("h3",{attrs:{id:"兼容hive-hive-integration"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#兼容hive-hive-integration"}},[a._v("#")]),a._v(" 兼容hive(Hive Integration)")]),a._v(" "),e("h3",{attrs:{id:"支持标准的数据库连接-standard-connectivity"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#支持标准的数据库连接-standard-connectivity"}},[a._v("#")]),a._v(" 支持标准的数据库连接(Standard Connectivity)")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("  spark.read.jdbc(---)\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("h2",{attrs:{id:"dataframe概述"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#dataframe概述"}},[a._v("#")]),a._v(" DataFrame概述")]),a._v(" "),e("ul",[e("li",[a._v("在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库的二维表格")]),a._v(" "),e("li",[a._v("DataFrame带有Schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型，但底层做了更多的优化")]),a._v(" "),e("li",[a._v("DataFrame可以从很多数据源构建\n"),e("ul",[e("li",[a._v("比如：已经存在的RDD、结构化文件、外部数据库、Hive表。")])])]),a._v(" "),e("li",[a._v("RDD可以把它内部元素看成是一个java对象")]),a._v(" "),e("li",[a._v("DataFrame可以把内部是一个Row对象，它表示一行一行的数据")])]),a._v(" "),e("h3",{attrs:{id:"dataframe和rdd的优缺点"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#dataframe和rdd的优缺点"}},[a._v("#")]),a._v(" DataFrame和RDD的优缺点")]),a._v(" "),e("h4",{attrs:{id:"rdd"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#rdd"}},[a._v("#")]),a._v(" RDD")]),a._v(" "),e("ul",[e("li",[a._v("优点")])]),a._v(" "),e("blockquote",[e("p",[a._v("1、编译时类型安全，开发会进行类型检查，在编译的时候及时发现错误\n2、具有面向对象编程的风格")])]),a._v(" "),e("ul",[e("li",[a._v("缺点")])]),a._v(" "),e("blockquote",[e("p",[a._v("1、构建大量的java对象占用了大量heap堆空间，导致频繁的GC。")]),a._v(" "),e("ul",[e("li",[a._v("由于数据集RDD它的数据量比较大，后期都需要存储在heap堆中，这里有heap堆中的内存空间有限，出现频繁的垃圾回收（GC），程序在进行垃圾回收的过程中，所有的任务都是暂停。影响程序执行的效率。")])])]),a._v(" "),e("p",[a._v("2、数据的序列化和反序列性能开销很大")]),a._v(" "),e("blockquote",[e("ul",[e("li",[a._v("在分布式程序中，对象(对象的内容和结构)是先进行序列化，发送到其他服务器，进行大量的网络传输，然后接受到这些序列化的数据之后，再进行反序列化来恢复该对象")])])]),a._v(" "),e("h4",{attrs:{id:"dataframe"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#dataframe"}},[a._v("#")]),a._v(" DataFrame")]),a._v(" "),e("ul",[e("li",[a._v("DataFrame引入了schema元信息和off-heap(堆外)")]),a._v(" "),e("li",[a._v("优点")])]),a._v(" "),e("blockquote",[e("p",[a._v("1、DataFrame引入off-heap")]),a._v(" "),e("ul",[e("li",[a._v("大量的对象构建直接使用操作系统层面上的内存，不在使用heap堆中的内存，这样一来heap堆中的内存空间就比较充足，不会导致频繁GC，程序的运行效率比较高，它是解决了RDD构建大量的java对象占用了大量heap堆空间，导致频繁的GC这个缺点。")])])]),a._v(" "),e("p",[a._v("2、DataFrame引入了schema元信息")]),a._v(" "),e("blockquote",[e("ul",[e("li",[a._v("就是数据结构的描述信息，后期spark程序中的大量对象在进行网络传输的时候，只需要把数据的内容本身进行序列化就可以，数据结构信息可以省略掉。这样一来数据网络传输的数据量是有所减少，数据的序列化和反序列性能开销就不是很大了。它是解决了RDD数据的序列化和反序列性能开销很大这个缺点")])])]),a._v(" "),e("ul",[e("li",[a._v("缺点\n"),e("ul",[e("li",[a._v("DataFrame引入了schema元信息和off-heap(堆外)它是分别解决了RDD的缺点，同时它也丢失了RDD的优点")])])])]),a._v(" "),e("blockquote",[e("p",[a._v("1、编译时类型不安全")]),a._v(" "),e("ul",[e("li",[a._v("编译时不会进行类型的检查，这里也就意味着前期是无法在编译的时候发现错误，只有在运行的时候才会发现")])])]),a._v(" "),e("p",[a._v("2、不在具有面向对象编程的风格")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('// 1. 读取文本文件\nval personDF=spark.read.text("/person.txt")\nval peopleDF=spark.read.json("/people.json")\nval usersDF=spark.read.parquet("/users.parquet")\n\n// 2. 加载数据\nval rdd1=sc.textFile("/person.txt").map(x=>x.split(" "))\n    //定义一个样例类\n    case class Person(id:String,name:String,age:Int)\n    //把rdd与样例类进行关联\n    val personRDD=rdd1.map(x=>Person(x(0),x(1),x(2).toInt))\n    //把rdd转换成DataFrame\n    val personDF=personRDD.toDF\n// 3.语法风格\n// 3.1 DSL\n        personDF.select("name").show\n        personDF.select($"name").show\n        personDF.select(col("name").show\n        //实现age+1\n         personDF.select($"name",$"age",$"age"+1)).show   \n        //实现age大于30过滤\n         personDF.filter($"age" > 30).show\n         //按照age分组统计次数\n         personDF.groupBy("age").count.show \n        //按照age分组统计次数降序\n         personDF.groupBy("age").count().sort($"count".desc)show  \n// 3.2 sql\n        //DataFrame注册成表\n        personDF.createTempView("person")\n\n        //使用SparkSession调用sql方法统计查询\n        spark.sql("select * from person").show\n        spark.sql("select name from person").show\n        spark.sql("select name,age from person").show\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br"),e("span",{staticClass:"line-number"},[a._v("22")]),e("br"),e("span",{staticClass:"line-number"},[a._v("23")]),e("br"),e("span",{staticClass:"line-number"},[a._v("24")]),e("br"),e("span",{staticClass:"line-number"},[a._v("25")]),e("br"),e("span",{staticClass:"line-number"},[a._v("26")]),e("br"),e("span",{staticClass:"line-number"},[a._v("27")]),e("br"),e("span",{staticClass:"line-number"},[a._v("28")]),e("br"),e("span",{staticClass:"line-number"},[a._v("29")]),e("br"),e("span",{staticClass:"line-number"},[a._v("30")]),e("br"),e("span",{staticClass:"line-number"},[a._v("31")]),e("br"),e("span",{staticClass:"line-number"},[a._v("32")]),e("br"),e("span",{staticClass:"line-number"},[a._v("33")]),e("br"),e("span",{staticClass:"line-number"},[a._v("34")]),e("br")])]),e("h2",{attrs:{id:"dataset概述"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#dataset概述"}},[a._v("#")]),a._v(" DataSet概述")]),a._v(" "),e("ul",[e("li",[a._v("DataSet是分布式的数据集合，Dataset提供了强类型支持，也是在RDD的每行数据加了类型约束。")]),a._v(" "),e("li",[a._v("DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型和可以用强大lambda函数）以及使用了Spark SQL优化的执行引擎。")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('1. 假设RDD中的两行数据长这样\n        1,张三,23\n        2,李四,35\n2.那么DataFrame中的数据长这样\n\t\t\t\tID:String\tName:String\tAge:int\n        \t\t1\t\t\t\t张三\t\t\t\t23\n        \t\t2\t\t\t\t李四\t\t\t\t35\n3.Dataset中的数据长这样 \n\t\t\tvalue:String\n        1,张三,23\n        2,李四,35\n  或者\n  value:People(age:bigint,id:bigint,name:string)\n        People(id=1,name="张三",age=23)\n        People(id=2,name="李四",age=23)\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br")])]),e("div",{staticClass:"language-properties line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[a._v("DataSet包含了DataFrame的功能，Spark2.0中两者统一，DataFrame表示为DataSet[Row]，即DataSet的子集。\n（1）DataSet可以在编译时检查类型\n（2）并且是面向对象的编程接口\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br")])]),e("h3",{attrs:{id:"dataframe-dataset转换-构建dataset"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#dataframe-dataset转换-构建dataset"}},[a._v("#")]),a._v(" DataFrame DataSet转换 构建dataset")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('// 把一个DataFrame转换成DataSet\nval dataSet=dataFrame.as[强类型]\n//  2、把一个DataSet转换成DataFrame\nval dataFrame=dataSet.toDF\n\n// 补充说明,可以从dataFrame和dataSet获取得到rdd\nval rdd1=dataFrame.rdd\nval rdd2=dataSet.rdd\n\n// 1、 通过sparkSession调用createDataset方法\n  val ds=spark.createDataset(1 to 10) //scala集合\n  val ds=spark.createDataset(sc.textFile("/person.txt"))  //rdd\n\n// 2、使用scala集合和rdd调用toDS方法\n  sc.textFile("/person.txt").toDS\n  List(1,2,3,4,5).toDS\n\n// 3、把一个DataFrame转换成DataSet\n  val dataSet=dataFrame.as[强类型]\n\n// 4、通过一个DataSet转换生成一个新的DataSet\n   List(1,2,3,4,5).toDS.map(x=>x*10)\n\n// 5、将rdd与Row对象进行关联\n    val rowRDD: RDD[Row] = data.map(x=>Row(x(0),x(1),x(2).toInt))\n    //指定dataFrame的schema信息   \n    //这里指定的字段个数和类型必须要跟Row对象保持一致\n    val schema=StructType(\n        StructField("id",StringType)::\n        StructField("name",StringType)::\n        StructField("age",IntegerType)::Nil\n    )\n    val dataFrame: DataFrame = spark.createDataFrame(rowRDD,schema)\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br"),e("span",{staticClass:"line-number"},[a._v("22")]),e("br"),e("span",{staticClass:"line-number"},[a._v("23")]),e("br"),e("span",{staticClass:"line-number"},[a._v("24")]),e("br"),e("span",{staticClass:"line-number"},[a._v("25")]),e("br"),e("span",{staticClass:"line-number"},[a._v("26")]),e("br"),e("span",{staticClass:"line-number"},[a._v("27")]),e("br"),e("span",{staticClass:"line-number"},[a._v("28")]),e("br"),e("span",{staticClass:"line-number"},[a._v("29")]),e("br"),e("span",{staticClass:"line-number"},[a._v("30")]),e("br"),e("span",{staticClass:"line-number"},[a._v("31")]),e("br"),e("span",{staticClass:"line-number"},[a._v("32")]),e("br"),e("span",{staticClass:"line-number"},[a._v("33")]),e("br")])]),e("h2",{attrs:{id:"示例代码"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#示例代码"}},[a._v("#")]),a._v(" 示例代码")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('// EG\n // 1、构建SparkSession对象,开启hive支持\n    val spark: SparkSession = SparkSession.builder()\n      .appName("HiveSupport")\n      .master("local[2]")\n      .enableHiveSupport() //开启对hive的支持\n      .getOrCreate()\n\n// 2. 读取mysql数据\n\t\tval spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()\n        val url="jdbc:mysql://node03:3306/spark"\n        val tableName="user"\n        val properties = new Properties()\n      properties.setProperty("user","root")\n      properties.setProperty("password","123456")\n   val mysqlDF: DataFrame = spark.read.jdbc(url,tableName,properties)\n\n// 3. 保存数据到mysql表中\n     //mode:指定数据的插入模式\n        //overwrite: 表示覆盖，如果表不存在，事先帮我们创建\n        //append   :表示追加， 如果表不存在，事先帮我们创建\n        //ignore   :表示忽略，如果表事先存在，就不进行任何操作\n        //error    :如果表事先存在就报错（默认选项）\n    result.write.mode("append").jdbc(url,"kaikeba",properties)\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br"),e("span",{staticClass:"line-number"},[a._v("22")]),e("br"),e("span",{staticClass:"line-number"},[a._v("23")]),e("br"),e("span",{staticClass:"line-number"},[a._v("24")]),e("br")])]),e("h2",{attrs:{id:"自定义函数"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#自定义函数"}},[a._v("#")]),a._v(" 自定义函数")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('//小写转大写\nsparkSession.udf.register("low2Up",new UDF1[String,String]() {\n  override def call(t1: String): String = {\n    t1.toUpperCase\n  }\n},StringType)\n//大写转小写\nsparkSession.udf.register("up2low",(x:String)=>x.toLowerCase)\n// 把数据文件中的单词统一转换成大小写\nsparkSession.sql("select  value from t_udf").show()\nsparkSession.sql("select  low2Up(value) from t_udf").show()\nsparkSession.sql("select  up2low(value) from t_udf").show()\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br")])])])}),[],!1,null,null,null);s.default=n.exports}}]);