(window.webpackJsonp=window.webpackJsonp||[]).push([[74],{559:function(s,a,e){"use strict";e.r(a);var t=e(19),n=Object(t.a)({},(function(){var s=this,a=s.$createElement,e=s._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[e("h2",{attrs:{id:"大数据概论"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#大数据概论"}},[s._v("#")]),s._v(" 大数据概论")]),s._v(" "),e("blockquote",[e("p",[s._v("概念： 大数据（big data）是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产")])]),s._v(" "),e("table",[e("thead",[e("tr",[e("th",[s._v("数据单位")]),s._v(" "),e("th",[s._v("B")]),s._v(" "),e("th",[s._v("KB")]),s._v(" "),e("th",[s._v("MB")]),s._v(" "),e("th",[s._v("GB")]),s._v(" "),e("th",[s._v("PE")]),s._v(" "),e("th",[s._v("PB")]),s._v(" "),e("th",[s._v("EB")]),s._v(" "),e("th",[s._v("ZB")]),s._v(" "),e("th",[s._v("YB")])])]),s._v(" "),e("tbody",[e("tr",[e("td",[s._v("基数")]),s._v(" "),e("td"),s._v(" "),e("td",[s._v("2")]),s._v(" "),e("td",[s._v("2")]),s._v(" "),e("td",[s._v("2")]),s._v(" "),e("td",[s._v("2")]),s._v(" "),e("td",[s._v("2")]),s._v(" "),e("td",[s._v("2")]),s._v(" "),e("td",[s._v("10")]),s._v(" "),e("td",[s._v("10")])]),s._v(" "),e("tr",[e("td",[s._v("次方")]),s._v(" "),e("td",[s._v("0")]),s._v(" "),e("td",[s._v("10")]),s._v(" "),e("td",[s._v("20")]),s._v(" "),e("td",[s._v("30")]),s._v(" "),e("td",[s._v("40")]),s._v(" "),e("td",[s._v("50")]),s._v(" "),e("td",[s._v("60")]),s._v(" "),e("td",[s._v("21")]),s._v(" "),e("td",[s._v("24")])])])]),s._v(" "),e("h3",{attrs:{id:"大数据特性"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#大数据特性"}},[s._v("#")]),s._v(" 大数据特性")]),s._v(" "),e("ol",[e("li",[s._v("数据量大（Volume）")]),s._v(" "),e("li",[s._v("类型繁多（Variety）")]),s._v(" "),e("li",[s._v("价值密度低（Value）")]),s._v(" "),e("li",[s._v("速度快时效高（Velocity）")])]),s._v(" "),e("h3",{attrs:{id:"大数据的挑战"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#大数据的挑战"}},[s._v("#")]),s._v(" 大数据的挑战")]),s._v(" "),e("ol",[e("li",[s._v("存储： 每天几TB、GB的数据增量，并且还在持续的增长中。")]),s._v(" "),e("li",[s._v("分析： 如何从巨大的数据中挖掘出隐藏的商业价值。")]),s._v(" "),e("li",[s._v("管理： 如何快速构建并且保证系统的安全、简单可用。")])]),s._v(" "),e("h2",{attrs:{id:"分布式文件系统"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#分布式文件系统"}},[s._v("#")]),s._v(" 分布式文件系统")]),s._v(" "),e("h3",{attrs:{id:"hadoop简介"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hadoop简介"}},[s._v("#")]),s._v(" Hadoop简介")]),s._v(" "),e("ol",[e("li",[s._v("Hadoop架构")])]),s._v(" "),e("p",[e("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E5%9F%BA%E7%A1%80/img/Image201906191834.png#height=298&id=avvmS&originHeight=426&originWidth=564&originalType=binary&ratio=1&status=done&style=none&width=395",alt:""}})]),s._v(" "),e("ol",{attrs:{start:"2"}},[e("li",[s._v("Hadoop历史"),e("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E5%9F%BA%E7%A1%80/img/Image201906202055.png#height=598&id=FxxxW&originHeight=598&originWidth=1862&originalType=binary&ratio=1&status=done&style=none&width=1862",alt:""}})])]),s._v(" "),e("h3",{attrs:{id:"hdfs"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hdfs"}},[s._v("#")]),s._v(" HDFS")]),s._v(" "),e("ul",[e("li",[s._v("HDFS是Hadoop中的一个存储子模块")]),s._v(" "),e("li",[s._v("HDFS (全称Hadoop Distributed File System)，即hadoop的分布式文件系统")]),s._v(" "),e("li",[s._v("File System"),e("strong",[s._v("文件系统")]),s._v("：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读出、修改、转储、删除文件等")]),s._v(" "),e("li",[s._v("当数据集大小超出一台计算机的存储能力时，就有必要将它拆分成若干部分，然后分散到不同的计算机中存储。管理网络中跨多台计算机存储的文件系统称之为"),e("strong",[s._v("分布式文件系统")]),s._v("（distributed filesystem）")])]),s._v(" "),e("h4",{attrs:{id:"hdfs特点"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hdfs特点"}},[s._v("#")]),s._v(" HDFS特点")]),s._v(" "),e("p",[e("strong",[s._v("2.1.1 优点：")])]),s._v(" "),e("ul",[e("li",[s._v("适合存储大文件，能用来存储管理PB级的数据；不适合存储小文件")]),s._v(" "),e("li",[s._v("处理非结构化数据")]),s._v(" "),e("li",[s._v("流式的访问数据，一次写入、多次读写")]),s._v(" "),e("li",[s._v("运行于廉价的商用机器集群上，成本低")]),s._v(" "),e("li",[s._v("高容错：故障时能继续运行且不让用户察觉到明显的中断")]),s._v(" "),e("li",[s._v("可扩展： 数据量小可以存储在单台服务器。 数据量大可以分散存储在不同的服务器中，假如单个文件100T可以分为多个block块分散在不同的服务器。")])]),s._v(" "),e("p",[e("strong",[s._v("2.1.2 局限性")])]),s._v(" "),e("ul",[e("li",[s._v("不适合处理低延迟数据访问\n"),e("ul",[e("li",[s._v("HDFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的")]),s._v(" "),e("li",[s._v("对于低延时的访问需求，HBase是更好的选择")])])]),s._v(" "),e("li",[s._v("无法高效存储大量的小文件\n"),e("ul",[e("li",[s._v("小文件会给Hadoop的扩展性和性能带来严重问题")]),s._v(" "),e("li",[s._v("利用SequenceFile、MapFile等方式归档小文件")])])]),s._v(" "),e("li",[s._v("不支持多用户写入及任意修改文件\n"),e("ul",[e("li",[s._v("文件有一个写入者，只能执行追加操作")]),s._v(" "),e("li",[s._v("不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改")])])])]),s._v(" "),e("h4",{attrs:{id:"hdfs常用命令"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hdfs常用命令"}},[s._v("#")]),s._v(" HDFS常用命令")]),s._v(" "),e("div",{staticClass:"language-shell line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# HDFS两种命令风格，两种命令效果等同")]),s._v("\nhadoop fs / hdfs dfs\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 如何查看hdfs或hadoop子命令的**帮助信息**，如ls子命令")]),s._v("\nhadoop fs -help "),e("span",{pre:!0,attrs:{class:"token function"}},[s._v("ls")]),s._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 查看hdfs文件系统中已经存在的文件。对比linux命令ls")]),s._v("\nhdfs dfs -ls /\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 在hdfs文件系统中创建文件")]),s._v("\nhdfs dfs -touchz /edits.txt\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#将本地磁盘当前目录的edit1.xml内容追加到HDFS根目录 的edits.txt文件")]),s._v("\nhadoop fs -appendToFile edit1.xml /edits.txt \n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 查看HDFS文件内容")]),s._v("\nhdfs dfs -cat /edits.txt\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#用法：hdfs dfs -put /本地路径 /hdfs路径")]),s._v("\nhdfs dfs -put hadoop-2.7.3.tar.gz /\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#根put作用一样")]),s._v("\nhdfs dfs -copyFromLocal hadoop-2.7.3.tar.gz /  \n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#根put作用一样，只不过，源文件被拷贝成功后，会被删除")]),s._v("\nhdfs dfs -moveFromLocal hadoop-2.7.3.tar.gz / \n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 在hdfs文件系统中下载文件")]),s._v("\nhdfs dfs -get /hdfs路径 /本地路径\nhdfs dfs -copyToLocal /hdfs路径 /本地路径  "),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#根get作用一样")]),s._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 在hdfs文件系统中**创建目录**")]),s._v("\nhdfs dfs -mkdir /shell\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 在hdfs文件系统中**删除**文件")]),s._v("\nhdfs dfs -rm /edits.txt\nhdfs dfs -rm -r /shell\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 递归删除目录")]),s._v("\nhdfs dfs -rmr /shell\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 在hdfs文件系统中**修改文件名称**（也可以用来**移动**文件到目录）")]),s._v("\nhdfs dfs -mv /xcall.sh /call.sh\nhdfs dfs -mv /call.sh /shell\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 在hdfs中拷贝文件到目录")]),s._v("\nhdfs dfs -cp /xrsync.sh /shell\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 列出本地文件的内容（默认是hdfs文件系统）")]),s._v("\nhdfs dfs -ls file:///home/bruce/\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# linux find命令")]),s._v("\n"),e("span",{pre:!0,attrs:{class:"token function"}},[s._v("find")]),s._v(" "),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(".")]),s._v(" -name "),e("span",{pre:!0,attrs:{class:"token string"}},[s._v("'edit*'")]),s._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# HDFS find命令")]),s._v("\nhadoop fs -find / -name part-r-00000 "),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 在HDFS根目录中，查找part-r-00000文件")]),s._v("\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br"),e("span",{staticClass:"line-number"},[s._v("28")]),e("br"),e("span",{staticClass:"line-number"},[s._v("29")]),e("br"),e("span",{staticClass:"line-number"},[s._v("30")]),e("br"),e("span",{staticClass:"line-number"},[s._v("31")]),e("br"),e("span",{staticClass:"line-number"},[s._v("32")]),e("br"),e("span",{staticClass:"line-number"},[s._v("33")]),e("br"),e("span",{staticClass:"line-number"},[s._v("34")]),e("br"),e("span",{staticClass:"line-number"},[s._v("35")]),e("br"),e("span",{staticClass:"line-number"},[s._v("36")]),e("br"),e("span",{staticClass:"line-number"},[s._v("37")]),e("br"),e("span",{staticClass:"line-number"},[s._v("38")]),e("br"),e("span",{staticClass:"line-number"},[s._v("39")]),e("br"),e("span",{staticClass:"line-number"},[s._v("40")]),e("br"),e("span",{staticClass:"line-number"},[s._v("41")]),e("br"),e("span",{staticClass:"line-number"},[s._v("42")]),e("br"),e("span",{staticClass:"line-number"},[s._v("43")]),e("br"),e("span",{staticClass:"line-number"},[s._v("44")]),e("br"),e("span",{staticClass:"line-number"},[s._v("45")]),e("br"),e("span",{staticClass:"line-number"},[s._v("46")]),e("br"),e("span",{staticClass:"line-number"},[s._v("47")]),e("br"),e("span",{staticClass:"line-number"},[s._v("48")]),e("br"),e("span",{staticClass:"line-number"},[s._v("49")]),e("br"),e("span",{staticClass:"line-number"},[s._v("50")]),e("br"),e("span",{staticClass:"line-number"},[s._v("51")]),e("br"),e("span",{staticClass:"line-number"},[s._v("52")]),e("br"),e("span",{staticClass:"line-number"},[s._v("53")]),e("br"),e("span",{staticClass:"line-number"},[s._v("54")]),e("br"),e("span",{staticClass:"line-number"},[s._v("55")]),e("br")])]),e("h3",{attrs:{id:"hdfs与getconf结合使用"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hdfs与getconf结合使用"}},[s._v("#")]),s._v(" hdfs与getconf结合使用")]),s._v(" "),e("div",{staticClass:"language-shell line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 获取NameNode的节点名称（可能有多个）")]),s._v("\nhdfs getconf -namenodes\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 获取hdfs最小块信息")]),s._v("\nhdfs getconf -confKey dfs.namenode.fs-limits.min-block-size\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 查找hdfs的NameNode的RPC地址")]),s._v("\nhdfs getconf -nnRpcAddresses\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br")])]),e("h5",{attrs:{id:"hdfs与dfsadmin结合使用"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hdfs与dfsadmin结合使用"}},[s._v("#")]),s._v(" hdfs与dfsadmin结合使用")]),s._v(" "),e("div",{staticClass:"language-shell line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 帮助信息")]),s._v("\nhdfs dfsadmin -help safemode\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 查看当前的模式")]),s._v("\nhdfs dfsadmin -safemode get\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 进入安全模式")]),s._v("\nhdfs dfsadmin -safemode enter\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br")])]),e("h5",{attrs:{id:"hdfs与fsck结合使用"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hdfs与fsck结合使用"}},[s._v("#")]),s._v(" hdfs与fsck结合使用")]),s._v(" "),e("div",{staticClass:"language-shell line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# fsck指令**显示HDFS块信息**")]),s._v("\nhdfs "),e("span",{pre:!0,attrs:{class:"token function"}},[s._v("fsck")]),s._v(" /02-041-0029.mp4 -files -blocks -locations "),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 查看文件02-041-0029.mp4的块信息")]),s._v("\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br")])]),e("h5",{attrs:{id:"其他命令"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#其他命令"}},[s._v("#")]),s._v(" 其他命令")]),s._v(" "),e("div",{staticClass:"language-shell line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#m检查压缩库本地安装情况")]),s._v("\nhadoop checknative\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 格式化名称节点（**慎用**，一般只在初次搭建集群，使用一次；格式化成功后，不要再使用）")]),s._v("\nhadoop namenode -format\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 执行自定义jar包")]),s._v("\nhadoop jar kfly-example-1.0-SNAPSHOT.jar org.kfly.WordCount /world.txt /out\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br")])]),e("h4",{attrs:{id:"hdfs编程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hdfs编程"}},[s._v("#")]),s._v(" HDFS编程")]),s._v(" "),e("ul",[e("li",[s._v("1.向hdfs中,上传一个文本文件")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v('   // 获取文件输入流\n  InputStream  inputStreamSourceFile = new BufferedInputStream(new FileInputStream(source));\n  // HDFS 读写配置文件\n  Configuration configuration = new Configuration();\n  // 通过url 返回文件系统实例\n  FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);\n  //调用Filesystem的create方法返回的是FSDataOutputStream对象\n  //该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n  // 获取文件系用的输出流\n  OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n  // 将文件输入流，写入输入流\n  IOUtils.copyBytes(inputStreamSourceFile,outputStreamTarget,4069,true);\n  System.out.println("上传成功");\n')])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br")])]),e("ul",[e("li",[s._v("2.读取hdfs上的文件")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("// HDFS 读写文件配置\nConfiguration configuration = new Configuration();\n// HDFS文件系统\nFileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n// 文件输入流，用于读取文件\nInputStream inputStream = fileSystem.open(new Path(source));\nBufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream));\nresult = readBufferReader(bufferedReader).toString();\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br")])]),e("ul",[e("li",[s._v("3.列出某一个文件夹下的所有文件")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("// HDFS文件系统\nFileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n// 传参 -> recursive：true 继续深入遍历\nRemoteIterator<LocatedFileStatus> iterator = fileSystem.listFiles(new Path(source),true);\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br")])]),e("ul",[e("li",[s._v("4.列出多级目录名称和目录下的文件名称()")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v('\t\t/**\n     * 递归目录和文件\n     * @param stringBuffer  文件目录名称集合\n     * @param fileSystem  hdfs 文件系统\n     * @param source path 路径\n     * @throws IOException\n     */\nprivate static void list(FileSystem fileSystem, Path source) {\n  FileStatus[] iterator = fileSystem.listStatus(source);\n  for (FileStatus status:iterator) {\n    stringBuffer.append(status.getPath() + "\\n");\n    if(status.isDirectory()){\n      list(stringBuffer,fileSystem,status.getPath());\n    }\n  }\n}\n')])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br")])]),e("h4",{attrs:{id:"hdfs架构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hdfs架构"}},[s._v("#")]),s._v(" HDFS架构")]),s._v(" "),e("p",[e("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E5%9F%BA%E7%A1%80/img/1558073557041.png#height=314&id=MvVIG&originHeight=314&originWidth=749&originalType=binary&ratio=1&status=done&style=none&width=749",alt:""}})]),s._v(" "),e("ul",[e("li",[s._v("大多数分布式框架都是主从架构")]),s._v(" "),e("li",[s._v("HDFS也是主从架构Master|Slave或称为管理节点|工作节点")])]),s._v(" "),e("h5",{attrs:{id:"namenode"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#namenode"}},[s._v("#")]),s._v(" NameNode")]),s._v(" "),e("p",[e("strong",[s._v("文件系统")])]),s._v(" "),e("ul",[e("li",[s._v("file system文件系统：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读取、修改、转储、删除文件等")]),s._v(" "),e("li",[s._v("读文件 =>>找到文件 =>> 在哪 + 叫啥？")]),s._v(" "),e("li",[s._v("元数据\n"),e("ul",[e("li",[s._v("关于文件或目录的描述信息，如文件所在路径、文件名称、文件类型等等，这些信息称为文件的元数据metadata")])])]),s._v(" "),e("li",[s._v("命名空间\n"),e("ul",[e("li",[s._v("文件系统中，为了便于管理存储介质上的，给每个目录、目录中的文件、子目录都起了名字，这样形成的层级结构，称之为命名空间")]),s._v(" "),e("li",[s._v("同一个目录中，不能有同名的文件或目录")]),s._v(" "),e("li",[s._v("这样通过目录+文件名称的方式能够唯一的定位一个文件")])])])]),s._v(" "),e("p",[s._v("HDFS-NameNode**")]),s._v(" "),e("ul",[e("li",[s._v("HDFS本质上也是文件系统filesystem，所以它也有元数据metadata；")]),s._v(" "),e("li",[s._v("元数据metadata保存在NameNode"),e("strong",[s._v("内存")]),s._v("中")]),s._v(" "),e("li",[s._v("NameNode作用\n"),e("ul",[e("li",[s._v("HDFS的主节点，负责管理文件系统的命名空间，将HDFS的元数据存储在NameNode节点的内存中")]),s._v(" "),e("li",[s._v("负责响应客户端对文件的读写请求")])])]),s._v(" "),e("li",[s._v("HDFS元数据\n"),e("ul",[e("li",[s._v("文件目录树、所有的文件（目录）名称、文件属性（生成时间、副本、权限）、每个文件的块列表、每个block块所在的datanode列表")]),s._v(" "),e("li",[s._v("每个文件、目录、block占用大概"),e("strong",[s._v("150Byte字节的元数据")]),s._v("；所以HDFS适合存储大文件，不适合存储小文件")]),s._v(" "),e("li",[s._v("HDFS元数据信息以两种形式保存：①编辑日志"),e("strong",[s._v("edits log")]),s._v("②命名空间镜像文件"),e("strong",[s._v("fsimage")]),s._v(" "),e("ul",[e("li",[s._v("edits log：HDFS编辑日志文件 ，保存客户端对HDFS的所有更改记录，如增、删、重命名文件（目录），这些操作会修改HDFS目录树；NameNode会在编辑日志edit日志中记录下来；")]),s._v(" "),e("li",[s._v("fsimage：HDFS元数据镜像文件 ，即将namenode内存中的数据落入磁盘生成的文件；保存了文件系统目录树信息以及文件、块、datanode的映射关系，如下图")])])])])])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("# 说明：\n# ①为hdfs-site.xml中属性dfs.namenode.edits.dir的值决定；用于namenode保存edits.log文件\n# ②为hdfs-site.xml中属性dfs.namenode.name.dir的值决定；用于namenode保存fsimage文件\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br")])]),e("h5",{attrs:{id:"datanode"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#datanode"}},[s._v("#")]),s._v(" DataNode")]),s._v(" "),e("ul",[e("li",[s._v("DataNode数据节点的作用\n"),e("ul",[e("li",[s._v("存储block以及block元数据到datanode本地磁盘；此处的元数据包括数据块的长度、块数据的校验和、时间戳")])])])]),s._v(" "),e("h5",{attrs:{id:"seconddarynamenode"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#seconddarynamenode"}},[s._v("#")]),s._v(" SeconddaryNameNode")]),s._v(" "),e("ul",[e("li",[s._v("为什么引入SecondaryNameNode\n"),e("ul",[e("li",[s._v("为什么元数据存储在NameNode在内存中？")]),s._v(" "),e("li",[s._v("这样做有什么问题？如何解决？")]),s._v(" "),e("li",[s._v("HDFS编辑日志文件 editlog：在NameNode节点中的编辑日志editlog中，记录下来客户端对HDFS的所有更改的记录，如增、删、重命名文件（目录）；")]),s._v(" "),e("li",[s._v("作用：一旦系统出故障，可以从editlog进行恢复；")]),s._v(" "),e("li",[s._v("但editlog日志大小会随着时间变在越来越大，导致系统重启根据日志恢复的时候会越来越长；")]),s._v(" "),e("li",[s._v("为了避免这种情况，引入"),e("strong",[s._v("检查点机制checkpoint")]),s._v("，命名空间镜像fsimage就是HDFS元数据的持久性检查点，即将内存中的元数据落磁盘生成的文件；")]),s._v(" "),e("li",[s._v("此时，namenode如果重启，可以将磁盘中的fsimage文件读入内容，将元数据恢复到某一个检查点，然后再执行检查点之后记录的编辑日志，最后完全恢复元数据。")]),s._v(" "),e("li",[s._v("但是依然，随着时间的推移，editlog记录的日志会变多，那么当namenode重启，恢复元数据过程中，会花越来越长的时间执行editlog中的每一个日志；而在namenode元数据恢复期间，HDFS不可用。")]),s._v(" "),e("li",[s._v("为了解决此问题，引入secondarynamenode辅助namenode，用来合并fsimage及editlog")])])])]),s._v(" "),e("p",[e("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E5%9F%BA%E7%A1%80/img/Image201906211525.png#height=720&id=Lwbtv&originHeight=720&originWidth=961&originalType=binary&ratio=1&status=done&style=none&width=961",alt:""}})]),s._v(" "),e("ul",[e("li",[s._v("SecondaryNameNode定期做checkpoint检查点操作\n"),e("ul",[e("li",[s._v("创建检查点checkpoint的两大条件：\n"),e("ul",[e("li",[s._v("SecondaryNameNode每隔1小时创建一个检查点")]),s._v(" "),e("li",[s._v("另外，Secondary NameNode每1分钟检查一次，从上一检查点开始，edits日志文件中是否已包括100万个事务，如果是，也会创建检查点")])])]),s._v(" "),e("li",[s._v("Secondary NameNode首先请求原NameNode进行edits的滚动，这样新的编辑操作就能够进入新的文件中")]),s._v(" "),e("li",[s._v("Secondary NameNode通过HTTP GET方式读取原NameNode中的fsimage及edits")]),s._v(" "),e("li",[s._v("Secondary NameNode读取fsimage到内存中，然后执行edits中的每个操作，并创建一个新的统一的fsimage文件")]),s._v(" "),e("li",[s._v("Secondary NameNode通过HTTP PUT方式将新的fsimage发送到原NameNode")]),s._v(" "),e("li",[s._v("原NameNode用新的fsimage替换旧的fsimage，同时系统会更新fsimage文件到记录检查点的时间。")]),s._v(" "),e("li",[s._v("这个过程结束后，NameNode就有了最新的fsimage文件和更小的edits文件")])])]),s._v(" "),e("li",[s._v("SecondaryNameNode一般部署在另外一台节点上\n"),e("ul",[e("li",[s._v("因为它需要占用大量的CPU时间")]),s._v(" "),e("li",[s._v("并需要与namenode一样多的内存，来执行合并操作")])])]),s._v(" "),e("li",[s._v("如何查看edits日志文件")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("hdfs oev -i edits_0000000000000000256-0000000000000000363 -o /home/hadoop/edit1.xml\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("ul",[e("li",[s._v("如何查看fsimage文件")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("hdfs oiv -p XML -i fsimage_0000000000000092691 -o fsimage.xml\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("h4",{attrs:{id:"checkpoint相关属性"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#checkpoint相关属性"}},[s._v("#")]),s._v(" checkpoint相关属性")]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("# 3600秒(即1小时) 每隔1小时创建一个检查点\n# The number of seconds between two periodic checkpoints\ndfs.namenode.checkpoint.period = 3600\n\n# edits日志文件中是否已包括100万个事务，如果是，也会创建检查点\ndfs.namenode.checkpoint.txns = 1000000\n\n# SecondaryNameNode每1分钟检查一次\ndfs.namenode.checkpoint.check.period = 60\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br")])]),e("h4",{attrs:{id:"心跳机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#心跳机制"}},[s._v("#")]),s._v(" 心跳机制")]),s._v(" "),e("p",[e("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2021/png/434900/1611128156504-8132229b-d84e-41aa-acc1-56d3169975f6.png#height=348&id=LTki3&originHeight=348&originWidth=729&originalType=binary&ratio=1&size=0&status=done&style=none&width=729",alt:""}}),s._v(" "),e("strong",[e("strong",[s._v("工作原理：")])])]),s._v(" "),e("ol",[e("li",[s._v("NameNode启动的时候，会开一个ipc server在那里")]),s._v(" "),e("li",[s._v("DataNode启动后向NameNode注册，每隔"),e("strong",[s._v("3秒钟")]),s._v("向NameNode发送一个“"),e("strong",[s._v("心跳heartbeat")]),s._v("”")]),s._v(" "),e("li",[s._v("心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一DataNode，或删除某个数据块")]),s._v(" "),e("li",[s._v("如果超过"),e("strong",[s._v("10分钟")]),s._v("NameNode没有收到某个DataNode 的心跳，则认为该DataNode节点不可用")]),s._v(" "),e("li",[s._v("DataNode周期性（"),e("strong",[s._v("6小时")]),s._v("）的向NameNode上报当前DataNode上的块状态报告BlockReport；块状态报告包含了一个该 Datanode上所有数据块的列表")])]),s._v(" "),e("p",[e("strong",[e("strong",[s._v("心跳的作用：")])])]),s._v(" "),e("ol",[e("li",[s._v("通过周期心跳，NameNode可以向DataNode返回指令")]),s._v(" "),e("li",[s._v("可以判断DataNode是否在线")]),s._v(" "),e("li",[s._v("通过BlockReport，NameNode能够知道各DataNode的存储情况，如磁盘利用率、块列表；跟"),e("strong",[s._v("负载均衡")]),s._v("有关")]),s._v(" "),e("li",[e("strong",[s._v("hadoop集群刚开始启动时，99.9%的block没有达到最小副本数(dfs.namenode.replication.min默认值为1)，集群处于安全模式，涉及BlockReport；")])])]),s._v(" "),e("p",[e("strong",[s._v("心跳相关配置")])]),s._v(" "),e("ul",[e("li",[e("p",[e("a",{attrs:{href:"https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml",target:"_blank",rel:"noopener noreferrer"}},[s._v("hdfs-default.xml"),e("OutboundLink")],1)])]),s._v(" "),e("li",[e("p",[s._v("心跳间隔")])])]),s._v(" "),e("table",[e("thead",[e("tr",[e("th",[s._v("属性")]),s._v(" "),e("th",[s._v("值")]),s._v(" "),e("th",[s._v("解释")])])]),s._v(" "),e("tbody",[e("tr",[e("td",[s._v("dfs.heartbeat.interval")]),s._v(" "),e("td",[s._v("3")]),s._v(" "),e("td",[s._v("Determines datanode heartbeat interval in seconds.")])])])]),s._v(" "),e("ul",[e("li",[e("strong",[s._v("block report")])])]),s._v(" "),e("table",[e("thead",[e("tr",[e("th",[s._v("More Actions属性")]),s._v(" "),e("th",[s._v("值")]),s._v(" "),e("th",[s._v("解释")])])]),s._v(" "),e("tbody",[e("tr",[e("td",[s._v("dfs.blockreport.intervalMsec")]),s._v(" "),e("td",[s._v("21600000 (6小时)")]),s._v(" "),e("td",[s._v("Determines block reporting interval in milliseconds.")])])])]),s._v(" "),e("ul",[e("li",[s._v("查看hdfs-default.xml默认配置文件\n"),e("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2021/png/434900/1611128200504-4e192b1b-93c3-4e6c-b2c4-89e3a0c040cd.png#height=680&id=ckQpj&margin=%5Bobject%20Object%5D&name=&originHeight=680&originWidth=1228&originalType=binary&ratio=1&size=0&status=done&style=none&width=1228",alt:""}})])]),s._v(" "),e("h4",{attrs:{id:"负载均衡"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#负载均衡"}},[s._v("#")]),s._v(" 负载均衡")]),s._v(" "),e("ul",[e("li",[s._v("什么原因会有可能造成不均衡？")]),s._v(" "),e("li",[s._v("机器与机器之间磁盘利用率不平衡是HDFS集群非常容易出现的情况")]),s._v(" "),e("li",[s._v("尤其是在DataNode节点出现故障或在现有的集群上增添新的DataNode的时候")]),s._v(" "),e("li",[s._v("为什么需要均衡？")]),s._v(" "),e("li",[s._v("提升集群存储资源利用率")]),s._v(" "),e("li",[s._v("从存储与计算两方面提高集群性能")]),s._v(" "),e("li",[s._v("如何手动负载均衡？")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("$HADOOP_HOME/sbin/start-balancer.sh -t 5%\t# 磁盘利用率最高的节点若比最少的节点，大于5%，触发均衡\n\n\n\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br")])])])}),[],!1,null,null,null);a.default=n.exports}}]);