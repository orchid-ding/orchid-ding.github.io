(window.webpackJsonp=window.webpackJsonp||[]).push([[80],{564:function(t,e,s){"use strict";s.r(e);var a=s(19),r=Object(a.a)({},(function(){var t=this,e=t.$createElement,s=t._self._c||e;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"文件存储格式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#文件存储格式"}},[t._v("#")]),t._v(" 文件存储格式")]),t._v(" "),s("p",[t._v("Hive支持的存储数的格式主要有；TEXTFILE（行式存储） 、SEQUENCEFILE(行式存储)、ORC（列式存储）、PARQUET（列式存储）。")]),t._v(" "),s("h3",{attrs:{id:"列式存储和行式存储"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#列式存储和行式存储"}},[t._v("#")]),t._v(" 列式存储和行式存储")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/31/uPic/hive/clip_image002.jpg#height=226&id=NJ5nv&originHeight=226&originWidth=612&originalType=binary&ratio=1&status=done&style=none&width=612",alt:""}})]),t._v(" "),s("p",[t._v("上图左边为逻辑表，右边第一个为行式存储，第二个为列式存储。")]),t._v(" "),s("p",[s("strong",[t._v("行存储的特点：")]),t._v(" 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。select  *")]),t._v(" "),s("p",[s("strong",[t._v("列存储的特点：")]),t._v(" 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。  select   某些字段效率更高")]),t._v(" "),s("p",[t._v("TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；")]),t._v(" "),s("p",[t._v("ORC和PARQUET是基于列式存储的。")]),t._v(" "),s("ul",[s("li",[t._v("TEXTFILE格式\n"),s("ul",[s("li",[t._v("默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。")])])]),t._v(" "),s("li",[t._v("ORC格式\n"),s("ul",[s("li",[t._v("Orc (Optimized Row Columnar)是hive 0.11版里引入的新的存储格式。")]),t._v(" "),s("li",[t._v("可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB->250MB，这样能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data,Row Data,Stripe Footer：")])])])]),t._v(" "),s("p",[s("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/31/uPic/hive/clip_image003.png#height=552&id=utVAj&originHeight=552&originWidth=406&originalType=binary&ratio=1&status=done&style=none&width=406",alt:""}})]),t._v(" "),s("div",{staticClass:"language- line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("一个orc文件可以分为若干个Stripe\n一个stripe可以分为三个部分\nindexData：某些列的索引数据\nrowData :真正的数据存储\nStripFooter：stripe的元数据信息\n   \t1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引只是记录某行的各字段在Row Data中的offset。\n​    2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。\n​    3）Stripe Footer：存的是各个stripe的元数据信息\n每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br")])]),s("ul",[s("li",[t._v("PARQUET格式\n"),s("ul",[s("li",[t._v("Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。")]),t._v(" "),s("li",[t._v("Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。")]),t._v(" "),s("li",[t._v("通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。")])])])]),t._v(" "),s("p",[s("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/31/uPic/hive/clip_image005.jpg#height=456&id=EYRbZ&originHeight=456&originWidth=609&originalType=binary&ratio=1&status=done&style=none&width=609",alt:""}})]),t._v(" "),s("ul",[s("li",[t._v("上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。")])]),t._v(" "),s("h3",{attrs:{id:"文件存储格式对比实验"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#文件存储格式对比实验"}},[t._v("#")]),t._v(" 文件存储格式对比实验")]),t._v(" "),s("p",[t._v("从存储文件的压缩比和查询速度两个角度对比。")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("压缩格式")]),t._v(" "),s("th",[t._v("压缩后文件大小")]),t._v(" "),s("th",[t._v("查询速度（s）")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("stored as textfile")]),t._v(" "),s("td",[t._v("18.1 M")]),t._v(" "),s("td",[t._v("21.54")])]),t._v(" "),s("tr",[s("td",[t._v("stored as orc")]),t._v(" "),s("td",[t._v("2.8  M")]),t._v(" "),s("td",[t._v("20.867")])]),t._v(" "),s("tr",[s("td",[t._v("stored as parquet")]),t._v(" "),s("td",[t._v("13.1 M")]),t._v(" "),s("td",[t._v("22.922")])])])]),t._v(" "),s("h2",{attrs:{id:"存储和压缩结合"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#存储和压缩结合"}},[t._v("#")]),t._v(" 存储和压缩结合")]),t._v(" "),s("p",[t._v("官网："),s("a",{attrs:{href:"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("ORC存储方式的压缩：")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("Key")]),t._v(" "),s("th",[t._v("Default")]),t._v(" "),s("th",[t._v("Notes")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("orc.compress")]),t._v(" "),s("td",[t._v("ZLIB")]),t._v(" "),s("td",[t._v("high level   compression (one of NONE, ZLIB, SNAPPY)")])]),t._v(" "),s("tr",[s("td",[t._v("orc.compress.size")]),t._v(" "),s("td",[t._v("262,144")]),t._v(" "),s("td",[t._v("number of bytes in   each compression chunk")])]),t._v(" "),s("tr",[s("td",[t._v("orc.stripe.size")]),t._v(" "),s("td",[t._v("67,108,864")]),t._v(" "),s("td",[t._v("number of bytes in   each stripe")])]),t._v(" "),s("tr",[s("td",[t._v("orc.row.index.stride")]),t._v(" "),s("td",[t._v("10,000")]),t._v(" "),s("td",[t._v("number of rows   between index entries (must be >= 1000)")])]),t._v(" "),s("tr",[s("td",[t._v("orc.create.index")]),t._v(" "),s("td",[t._v("true")]),t._v(" "),s("td",[t._v("whether to create row   indexes")])]),t._v(" "),s("tr",[s("td",[t._v("orc.bloom.filter.columns")]),t._v(" "),s("td",[t._v('""')]),t._v(" "),s("td",[t._v("comma separated list of column names for which bloom filter   should be created")])]),t._v(" "),s("tr",[s("td",[t._v("orc.bloom.filter.fpp")]),t._v(" "),s("td",[t._v("0.05")]),t._v(" "),s("td",[t._v("false positive probability for bloom filter (must >0.0 and   <1.0)")])])])]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("文件格式")]),t._v(" "),s("th",[t._v("压缩格式")]),t._v(" "),s("th",[t._v("最终文件")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("orc")]),t._v(" "),s("td",[t._v("无")]),t._v(" "),s("td",[t._v("7.7 M")])]),t._v(" "),s("tr",[s("td",[t._v("orc")]),t._v(" "),s("td",[t._v("snappy")]),t._v(" "),s("td",[t._v("3.8 M")])])])]),t._v(" "),s("h2",{attrs:{id:"serde"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#serde"}},[t._v("#")]),t._v(" SerDe")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("Serde是 Serializer/Deserializer的简写。hive使用Serde进行行对象的序列与反序列化。最后实现把文件内容映射到 hive 表中的字段数据类型。\n\n\n为了更好的阐述使用 SerDe 的场景，我们需要了解一下 Hive 是如何读数据的(类似于 HDFS 中数据的读写操作)：\n")])])]),s("div",{staticClass:"language-sql line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[t._v("HDFS files –"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" InputFileFormat –"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("key")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("value")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" –"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" Deserializer –"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("Row")]),t._v(" object\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("Row")]),t._v(" object –"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" Serializer –"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("key")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("value")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" –"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" OutputFileFormat –"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" HDFS files\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br")])]),s("h3",{attrs:{id:"serde-类型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#serde-类型"}},[t._v("#")]),t._v(" SerDe 类型")]),t._v(" "),s("ul",[s("li",[t._v("Hive 中内置org.apache.hadoop.hive.serde2 库，内部封装了很多不同的SerDe类型。")]),t._v(" "),s("li",[t._v("你可以创建表时使用用户"),s("strong",[t._v("自定义的Serde或者native Serde")]),t._v("， "),s("strong",[t._v("如果 ROW FORMAT没有指定或者指定了 ROW FORMAT DELIMITED就会使用native Serde")]),t._v("。")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://cwiki.apache.org/confluence/display/Hive/SerDe",target:"_blank",rel:"noopener noreferrer"}},[t._v("Hive SerDes"),s("OutboundLink")],1),t._v(":\n"),s("ul",[s("li",[t._v("Avro (Hive 0.9.1 and later)")]),t._v(" "),s("li",[t._v("ORC (Hive 0.11 and later)")]),t._v(" "),s("li",[t._v("RegEx")]),t._v(" "),s("li",[t._v("Thrift")]),t._v(" "),s("li",[t._v("Parquet (Hive 0.13 and later)")]),t._v(" "),s("li",[t._v("CSV (Hive 0.14 and later)")]),t._v(" "),s("li",[t._v("MultiDelimitSerDe")])])])]),t._v(" "),s("h4",{attrs:{id:"多字符分割场景"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#多字符分割场景"}},[t._v("#")]),t._v(" 多字符分割场景")]),t._v(" "),s("div",{staticClass:"language-sql line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("##xiaoming")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("##xiaowang")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("##xiaozhang")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- MultiDelimitSerDe")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" kfly_mul "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("id String"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name string"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("row")]),t._v(" format serde "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe'")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WITH")]),t._v(" SERDEPROPERTIES "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"field.delim"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"##"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- RegexSerDe 解决多字符分割场景")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" t2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("id "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name string"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("row")]),t._v(" format serde "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'org.apache.hadoop.hive.serde2.RegexSerDe'")]),t._v(" \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WITH")]),t._v(" SERDEPROPERTIES "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"input.regex"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"^(.*)\\\\#\\\\#(.*)$"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br")])])])}),[],!1,null,null,null);e.default=r.exports}}]);