(window.webpackJsonp=window.webpackJsonp||[]).push([[124],{610:function(t,s,a){"use strict";a.r(s);var n=a(19),r=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"_4-1-线性回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-线性回归"}},[t._v("#")]),t._v(" 4.1 线性回归")]),t._v(" "),a("h2",{attrs:{id:"学习目标"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#学习目标"}},[t._v("#")]),t._v(" 学习目标")]),t._v(" "),a("ul",[a("li",[t._v("目标\n"),a("ul",[a("li",[t._v("记忆线性回归的原理过程")]),t._v(" "),a("li",[t._v("应用LinearRegression或SGDRegressor实现回归预测")]),t._v(" "),a("li",[t._v("记忆回归算法的评估标准及其公式")])])])]),t._v(" "),a("ul",[a("li",[t._v("应用\n"),a("ul",[a("li",[t._v("波士顿房价预测")])])]),t._v(" "),a("li",[t._v("内容预览\n"),a("ul",[a("li",[t._v("4.1.1 线性回归的原理")]),t._v(" "),a("li",[t._v("4.1.2 线性回归的损失和优化原理（理解记忆）")]),t._v(" "),a("li",[t._v("4.1.3 线性回归API")]),t._v(" "),a("li",[t._v("4.1.4 波士顿房价预测")])])])]),t._v(" "),a("h4",{attrs:{id:"回忆一下回归问题的判定是什么"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#回忆一下回归问题的判定是什么"}},[t._v("#")]),t._v(" 回忆一下回归问题的判定是什么？")]),t._v(" "),a("h2",{attrs:{id:"_4-1-1-线性回归的原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-1-线性回归的原理"}},[t._v("#")]),t._v(" 4.1.1 线性回归的原理")]),t._v(" "),a("h3",{attrs:{id:"_1-线性回归应用场景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-线性回归应用场景"}},[t._v("#")]),t._v(" 1 线性回归应用场景")]),t._v(" "),a("ul",[a("li",[t._v("房价预测")]),t._v(" "),a("li",[t._v("销售额度预测")]),t._v(" "),a("li",[t._v("金融：贷款额度预测、利用线性回归以及系数分析因子")])]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E9%94%80%E5%94%AE%E9%A2%9D%E5%BA%A6.png",alt:"销售额度"}})]),t._v(" "),a("h3",{attrs:{id:"_2-什么是线性回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-什么是线性回归"}},[t._v("#")]),t._v(" 2 什么是线性回归")]),t._v(" "),a("h4",{attrs:{id:"_1-定义与公式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-定义与公式"}},[t._v("#")]),t._v(" 1）定义与公式")]),t._v(" "),a("p",[t._v("线性回归(Linear regression)是利用"),a("strong",[t._v("回归方程(函数)"),a("strong",[t._v("对一个或")]),t._v("多个自变量(特征值)和因变量(目标值)之间")]),t._v("关系进行建模的一种分析方式。")]),t._v(" "),a("ul",[a("li",[t._v("特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归")])]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%85%AC%E5%BC%8F.png",alt:"线性回归公式"}})]),t._v(" "),a("p",[t._v("那么怎么理解呢？我们来看几个例子")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("期末成绩：0.7×考试成绩+0.3×平时成绩")])]),t._v(" "),a("li",[a("strong",[t._v("房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率")])])]),t._v(" "),a("p",[t._v("上面两个例子，"),a("strong",[t._v("我们看到特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型")]),t._v("。")]),t._v(" "),a("ul",[a("li",[t._v("模型")])]),t._v(" "),a("h4",{attrs:{id:"_2-线性回归的特征与目标的关系分析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-线性回归的特征与目标的关系分析"}},[t._v("#")]),t._v(" 2） 线性回归的特征与目标的关系分析")]),t._v(" "),a("p",[t._v("线性回归当中线性模型有两种，一种是线性关系，另一种是非线性关系。"),a("strong",[t._v("在这里我们只能画一个平面更好去理解，所以都用单个特征或两个特征举例子。")])]),t._v(" "),a("ul",[a("li",[t._v("线性关系 "),a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%E5%9B%BE.png",alt:"线性关系图"}})])]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB.png",alt:"多变量线性关系"}})]),t._v(" "),a("blockquote",[a("p",[t._v("注释：单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面的关系")]),t._v(" "),a("p",[t._v("更高维度的我们不用自己去想，记住这种关系即可")])]),t._v(" "),a("ul",[a("li",[t._v("非线性关系")])]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB.png",alt:"非线性关系"}})]),t._v(" "),a("blockquote",[a("p",[t._v("注释：为什么会这样的关系呢？原因是什么？")]),t._v(" "),a("p",[t._v("如果是非线性关系，那么回归方程可以理解为：w1x1+w2x2^2+w3x3^2")])]),t._v(" "),a("h2",{attrs:{id:"_4-1-2-线性回归的损失和优化原理-理解记忆"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-2-线性回归的损失和优化原理-理解记忆"}},[t._v("#")]),t._v(" 4.1.2 线性回归的损失和优化原理（理解记忆）")]),t._v(" "),a("p",[a("strong",[t._v("假设刚才的房子例子，真实的数据之间存在这样的关系")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("真实关系：真实房子价格 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.02")]),t._v("×中心区域的距离 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.04")]),t._v("×城市一氧化氮浓度 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.12")]),t._v("×自住房平均房价"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.254")]),t._v("×城镇犯罪率\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("那么现在呢，我们随意指定一个关系（猜测）")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("随机指定关系：预测房子价格 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.25")]),t._v("×中心区域的距离 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.14")]),t._v("×城市一氧化氮浓度 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.42")]),t._v("×自住房平均房价 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.34")]),t._v("×城镇犯罪率\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("请问这样的话，会发生什么？真实结果与我们预测的结果之间是不是存在一定的误差呢？类似这样样子")]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E8%AF%AF%E5%B7%AE.png",alt:"误差"}})]),t._v(" "),a("p",[t._v("既然存在这个误差，那我们就将这个误差给衡量出来")]),t._v(" "),a("h3",{attrs:{id:"_1-损失函数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-损失函数"}},[t._v("#")]),t._v(" 1 损失函数")]),t._v(" "),a("p",[t._v("总损失定义为：")]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png",alt:"线性回归损失函数"}})]),t._v(" "),a("ul",[a("li",[t._v("y_i为第i个训练样本的真实值")]),t._v(" "),a("li",[t._v("h(x_i)为第i个训练样本特征值组合预测函数")]),t._v(" "),a("li",[t._v("又称最小二乘法")])]),t._v(" "),a("p",[a("strong",[t._v("如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！！")])]),t._v(" "),a("h3",{attrs:{id:"_2-优化算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-优化算法"}},[t._v("#")]),t._v(" 2 优化算法")]),t._v(" "),a("p",[a("strong",[t._v("如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）")])]),t._v(" "),a("p",[t._v("线性回归经常使用的两种优化算法")]),t._v(" "),a("ul",[a("li",[t._v("正规方程")])]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B.png",alt:"正规方程"}})]),t._v(" "),a("blockquote",[a("p",[t._v("理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果")]),t._v(" "),a("p",[t._v("缺点：当特征过多过复杂时，求解速度太慢并且得不到结果")])]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E6%8D%9F%E5%A4%B1%E8%A1%8C%E6%95%B0%E6%B1%82%E8%A7%A31.png",alt:"损失行数求解1"}})]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("梯度下降(Gradient Descent)")])])]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F.png",alt:"梯度下降公式"}})]),t._v(" "),a("blockquote",[a("p",[t._v("理解：α为学习速率，需要手动指定（超参数），α旁边的整体表示方向")]),t._v(" "),a("p",[t._v("沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后更新W值")]),t._v(" "),a("p",[t._v("使用：面对训练数据规模十分庞大的任务 ，能够找到较好的结果")])]),t._v(" "),a("p",[t._v("我们通过两个图更好理解梯度下降的过程")]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E5%8D%95%E5%8F%98%E9%87%8F%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png",alt:"单变量的梯度下降"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png",alt:"多变量的梯度下降"}})]),t._v(" "),a("p",[a("strong",[t._v('所以有了梯度下降这样一个优化算法，回归就有了"自动学习"的能力')])]),t._v(" "),a("h3",{attrs:{id:"_3-优化动态图演示"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-优化动态图演示"}},[t._v("#")]),t._v(" 3 优化动态图演示")]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BC%98%E5%8C%96%E5%8A%A8%E6%80%81%E5%9B%BE.gif",alt:"线性回归优化动态图"}})]),t._v(" "),a("h2",{attrs:{id:"_4-1-3-线性回归api"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-3-线性回归api"}},[t._v("#")]),t._v(" 4.1.3 线性回归API")]),t._v(" "),a("ul",[a("li",[t._v("sklearn.linear_model.LinearRegression(fit_intercept=True)\n"),a("ul",[a("li",[t._v("通过正规方程优化")]),t._v(" "),a("li",[t._v("fit_intercept：是否计算偏置")]),t._v(" "),a("li",[t._v("LinearRegression.coef_：回归系数")]),t._v(" "),a("li",[t._v("LinearRegression.intercept_：偏置")])])]),t._v(" "),a("li",[t._v("sklearn.linear_model.SGDRegressor(loss=\"squared_loss\", fit_intercept=True, learning_rate ='invscaling', eta0=0.01)\n"),a("ul",[a("li",[t._v("SGDRegressor类实现了随机梯度下降学习，它支持不同的"),a("strong",[t._v("loss函数和正则化惩罚项")]),t._v("来拟合线性回归模型。")]),t._v(" "),a("li",[t._v("loss:损失类型\n"),a("ul",[a("li",[a("strong",[t._v("loss=”squared_loss”: 普通最小二乘法")])])])]),t._v(" "),a("li",[t._v("fit_intercept：是否计算偏置")]),t._v(" "),a("li",[t._v("learning_rate : string, optional\n"),a("ul",[a("li",[t._v("学习率填充")]),t._v(" "),a("li",[a("strong",[t._v("'constant': eta = eta0")])]),t._v(" "),a("li",[a("strong",[t._v("'optimal': eta = 1.0 / (alpha * (t + t0)) [default]")])]),t._v(" "),a("li",[t._v("'invscaling': eta = eta0 / pow(t, power_t)\n"),a("ul",[a("li",[a("strong",[t._v("power_t=0.25:存在父类当中")])])])]),t._v(" "),a("li",[a("strong",[t._v("对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。")])])])]),t._v(" "),a("li",[t._v("SGDRegressor.coef_：回归系数")]),t._v(" "),a("li",[t._v("SGDRegressor.intercept_：偏置")])])])]),t._v(" "),a("blockquote",[a("p",[t._v("sklearn提供给我们两种实现的API， 可以根据选择使用")])]),t._v(" "),a("h2",{attrs:{id:"_4-1-4-波士顿房价预测"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-4-波士顿房价预测"}},[t._v("#")]),t._v(" 4.1.4 波士顿房价预测")]),t._v(" "),a("ul",[a("li",[t._v("数据介绍")])]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E6%88%BF%E4%BB%B7%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D.png",alt:"房价数据集介绍"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E5%B1%9E%E6%80%A7.png",alt:"属性"}})]),t._v(" "),a("blockquote",[a("p",[t._v("给定的这些特征，是专家们得出的影响房价的结果属性。我们此阶段不需要自己去探究特征是否有用，只需要使用这些特征。到后面量化很多特征需要我们自己去寻找")])]),t._v(" "),a("h3",{attrs:{id:"_1-分析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-分析"}},[t._v("#")]),t._v(" 1 分析")]),t._v(" "),a("p",[t._v("回归当中的数据大小不一致，是否会导致结果影响较大。所以需要做标准化处理。")]),t._v(" "),a("ul",[a("li",[t._v("数据分割与标准化处理")]),t._v(" "),a("li",[t._v("回归预测")]),t._v(" "),a("li",[t._v("线性回归的算法效果评估")])]),t._v(" "),a("h3",{attrs:{id:"_2-回归性能评估"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-回归性能评估"}},[t._v("#")]),t._v(" 2 回归性能评估")]),t._v(" "),a("p",[t._v("均方误差(Mean Squared Error)MSE)评价机制：")]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E8%AF%84%E4%BC%B0.png",alt:"线性回归评估"}})]),t._v(" "),a("blockquote",[a("p",[t._v("注：y^i为预测值，¯y为真实值")])]),t._v(" "),a("ul",[a("li",[t._v("sklearn.metrics.mean_squared_error(y_true, y_pred)\n"),a("ul",[a("li",[t._v("均方误差回归损失")]),t._v(" "),a("li",[t._v("y_true:真实值")]),t._v(" "),a("li",[t._v("y_pred:预测值")]),t._v(" "),a("li",[t._v("return:浮点数结果")])])])]),t._v(" "),a("h3",{attrs:{id:"_3-代码"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-代码"}},[t._v("#")]),t._v(" 3 代码")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("linear1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    用正规方程直接求出模型参数的方法进行对波士顿房价预测的线性回归案例\n    :return: None\n    """')]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1、获取数据集")]),t._v("\n    boston "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" load_boston"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"boston:\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" boston"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DESCR"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2、划分数据集")]),t._v("\n    x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_test "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_test_split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("boston"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" boston"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("target"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" random_state"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3、特征工程：标准化")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1）实例化一个转换器类")]),t._v("\n    transfer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" StandardScaler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2）调用fit_transform")]),t._v("\n    x_train "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" transfer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    x_test "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" transfer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4、线性回归的预估器流程")]),t._v("\n    estimator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LinearRegression"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    y_predict "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"正规方程求出模型参数的方法预测的房屋价格为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 5、得出模型")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"正规方程求出的回归系数为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("coef_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"正规方程求出的偏置为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("intercept_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6、模型评估——均方误差")]),t._v("\n    error "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mean_squared_error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"正规方程的均方误差为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("linear2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    用梯度下降优化模型参数的方法进行对波士顿房价预测的线性回归案例\n    :return: None\n    """')]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1、获取数据集")]),t._v("\n    boston "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" load_boston"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# print("boston:\\n", boston)')]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2、划分数据集")]),t._v("\n    x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_test "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_test_split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("boston"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" boston"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("target"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" random_state"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3、特征工程：标准化")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1）实例化一个转换器类")]),t._v("\n    transfer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" StandardScaler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2）调用fit_transform")]),t._v("\n    x_train "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" transfer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    x_test "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" transfer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4、线性回归的预估器流程")]),t._v("\n    estimator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SGDRegressor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    y_predict "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"梯度下降求出模型参数的方法预测的房屋价格为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 5、得出模型")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"梯度下降求出的回归系数为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("coef_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"梯度下降求出的偏置为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("intercept_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6、模型评估——均方误差")]),t._v("\n    error "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mean_squared_error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"梯度下降的均方误差为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br"),a("span",{staticClass:"line-number"},[t._v("29")]),a("br"),a("span",{staticClass:"line-number"},[t._v("30")]),a("br"),a("span",{staticClass:"line-number"},[t._v("31")]),a("br"),a("span",{staticClass:"line-number"},[t._v("32")]),a("br"),a("span",{staticClass:"line-number"},[t._v("33")]),a("br"),a("span",{staticClass:"line-number"},[t._v("34")]),a("br"),a("span",{staticClass:"line-number"},[t._v("35")]),a("br"),a("span",{staticClass:"line-number"},[t._v("36")]),a("br"),a("span",{staticClass:"line-number"},[t._v("37")]),a("br"),a("span",{staticClass:"line-number"},[t._v("38")]),a("br"),a("span",{staticClass:"line-number"},[t._v("39")]),a("br"),a("span",{staticClass:"line-number"},[t._v("40")]),a("br"),a("span",{staticClass:"line-number"},[t._v("41")]),a("br"),a("span",{staticClass:"line-number"},[t._v("42")]),a("br"),a("span",{staticClass:"line-number"},[t._v("43")]),a("br"),a("span",{staticClass:"line-number"},[t._v("44")]),a("br"),a("span",{staticClass:"line-number"},[t._v("45")]),a("br"),a("span",{staticClass:"line-number"},[t._v("46")]),a("br"),a("span",{staticClass:"line-number"},[t._v("47")]),a("br"),a("span",{staticClass:"line-number"},[t._v("48")]),a("br"),a("span",{staticClass:"line-number"},[t._v("49")]),a("br"),a("span",{staticClass:"line-number"},[t._v("50")]),a("br"),a("span",{staticClass:"line-number"},[t._v("51")]),a("br"),a("span",{staticClass:"line-number"},[t._v("52")]),a("br"),a("span",{staticClass:"line-number"},[t._v("53")]),a("br"),a("span",{staticClass:"line-number"},[t._v("54")]),a("br"),a("span",{staticClass:"line-number"},[t._v("55")]),a("br"),a("span",{staticClass:"line-number"},[t._v("56")]),a("br"),a("span",{staticClass:"line-number"},[t._v("57")]),a("br"),a("span",{staticClass:"line-number"},[t._v("58")]),a("br")])]),a("p",[t._v("我们也可以尝试去修改学习率")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("estimator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SGDRegressor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learning_rate"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'constant'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" eta0"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.001")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("此时我们可以通过调参数，找到学习率效果更好的值。")]),t._v(" "),a("h3",{attrs:{id:"_4-正规方程和梯度下降对比"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-正规方程和梯度下降对比"}},[t._v("#")]),t._v(" 4 正规方程和梯度下降对比")]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%AF%B9%E6%AF%94.png",alt:"正规方程和梯度下降对比"}})]),t._v(" "),a("ul",[a("li",[t._v("文字对比")])]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[t._v("梯度下降")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("正规方程")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("需要选择学习率")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("不需要")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("需要迭代求解")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("一次运算得出")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("特征数量较大可以使用")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("需要计算方程，时间复杂度高O(n3)")])])])]),t._v(" "),a("ul",[a("li",[t._v("选择：\n"),a("ul",[a("li",[t._v("小规模数据：\n"),a("ul",[a("li",[a("strong",[t._v("LinearRegression(不能解决拟合问题)")])]),t._v(" "),a("li",[t._v("岭回归")])])]),t._v(" "),a("li",[t._v("大规模数据：SGDRegressor")])])])]),t._v(" "),a("blockquote",[a("p",[t._v("4.1.5 拓展-关于优化方法GD、SGD、SAG")]),t._v(" "),a("p",[t._v("1 GD")]),t._v(" "),a("p",[a("strong",[t._v("梯度下降(Gradient Descent)，原始的梯度下降法需要计算所有样本的值才能够得出梯度，计算量大，所以后面才有会一系列的改进。")])]),t._v(" "),a("p",[t._v("2 SGD")]),t._v(" "),a("p",[a("strong",[t._v("随机梯度下降(Stochastic gradient descent)是一个优化方法。它在一次迭代时只考虑一个训练样本。")])]),t._v(" "),a("ul",[a("li",[t._v("SGD的优点是：\n"),a("ul",[a("li",[t._v("高效")]),t._v(" "),a("li",[t._v("容易实现")])])]),t._v(" "),a("li",[t._v("SGD的缺点是：\n"),a("ul",[a("li",[t._v("SGD需要许多超参数：比如正则项参数、迭代数。")]),t._v(" "),a("li",[t._v("SGD对于特征标准化是敏感的。")])])])]),t._v(" "),a("p",[t._v("3 SAG")]),t._v(" "),a("p",[t._v("随机平均梯度法(Stochasitc Average Gradient)，由于收敛的速度太慢，有人提出SAG等基于梯度下降的算法")]),t._v(" "),a("p",[t._v("Scikit-learn：岭回归、逻辑回归等当中都会有SAG优化")])]),t._v(" "),a("h2",{attrs:{id:"_4-1-6-总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-6-总结"}},[t._v("#")]),t._v(" 4.1.6 总结")]),t._v(" "),a("ul",[a("li",[t._v("线性回归的损失函数-均方误差")]),t._v(" "),a("li",[t._v("线性回归的优化方法\n"),a("ul",[a("li",[t._v("正规方程")]),t._v(" "),a("li",[t._v("梯度下降")])])]),t._v(" "),a("li",[t._v("线性回归的性能衡量方法-均方误差")]),t._v(" "),a("li",[t._v("sklearn的SGDRegressor API 参数")])])])}),[],!1,null,null,null);s.default=r.exports}}]);