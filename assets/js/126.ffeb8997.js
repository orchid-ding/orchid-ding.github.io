(window.webpackJsonp=window.webpackJsonp||[]).push([[126],{611:function(s,t,a){"use strict";a.r(t);var n=a(19),e=Object(n.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"_4-3-线性回归的改进-岭回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-线性回归的改进-岭回归"}},[s._v("#")]),s._v(" 4.3 线性回归的改进-岭回归")]),s._v(" "),a("h2",{attrs:{id:"学习目标"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#学习目标"}},[s._v("#")]),s._v(" 学习目标")]),s._v(" "),a("ul",[a("li",[s._v("目标\n"),a("ul",[a("li",[s._v("说明岭回归的原理即与线性回归的不同之处")]),s._v(" "),a("li",[s._v("说明正则化对于权重参数的影响")]),s._v(" "),a("li",[s._v("说明L1和L2正则化的区别")])])]),s._v(" "),a("li",[s._v("应用\n"),a("ul",[a("li",[s._v("波士顿房价预测")])])])]),s._v(" "),a("ul",[a("li",[s._v("内容预览\n"),a("ul",[a("li",[s._v("4.3.1 带有L2正则化的线性回归-岭回归\n"),a("ul",[a("li",[s._v("1 API")]),s._v(" "),a("li",[s._v("2 观察正则化程度的变化，对结果的影响？")]),s._v(" "),a("li",[s._v("3 波士顿房价预测")])])])])])]),s._v(" "),a("h2",{attrs:{id:"_4-3-1-带有l2正则化的线性回归-岭回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-1-带有l2正则化的线性回归-岭回归"}},[s._v("#")]),s._v(" 4.3.1 带有L2正则化的线性回归-岭回归")]),s._v(" "),a("p",[s._v("岭回归，其实也是一种线性回归。只不过在算法建立回归方程时候，加上正则化的限制，从而达到解决过拟合的效果")]),s._v(" "),a("h3",{attrs:{id:"_1-api"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-api"}},[s._v("#")]),s._v(" 1 API")]),s._v(" "),a("ul",[a("li",[s._v('sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)\n'),a("ul",[a("li",[s._v("具有l2正则化的线性回归")]),s._v(" "),a("li",[s._v("alpha:正则化力度，也叫 λ\n"),a("ul",[a("li",[a("strong",[s._v("λ取值：0~1 1~10")])])])]),s._v(" "),a("li",[s._v("solver:会根据数据自动选择优化方法\n"),a("ul",[a("li",[a("strong",[s._v("sag:如果数据集、特征都比较大，选择该随机梯度下降优化")])])])]),s._v(" "),a("li",[s._v("normalize:数据是否进行标准化\n"),a("ul",[a("li",[s._v("normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据")])])]),s._v(" "),a("li",[s._v("Ridge.coef_:回归权重")]),s._v(" "),a("li",[s._v("Ridge.intercept_:回归偏置")])])])]),s._v(" "),a("blockquote",[a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("All last four solvers support both dense "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("and")]),s._v(" sparse data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v(" However"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\nonly "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'sag'")]),s._v(" supports sparse "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("input")]),s._v(" when `fit_intercept` "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("is")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])])]),s._v(" "),a("h4",{attrs:{id:"ridge方法相当于sgdregressor-penalty-l2-loss-squared-loss-只不过sgdregressor实现了一个普通的随机梯度下降学习-推荐使用ridge-实现了sag"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ridge方法相当于sgdregressor-penalty-l2-loss-squared-loss-只不过sgdregressor实现了一个普通的随机梯度下降学习-推荐使用ridge-实现了sag"}},[s._v("#")]),s._v(" Ridge方法相当于SGDRegressor(penalty='l2', loss=\"squared_loss\"),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)")]),s._v(" "),a("ul",[a("li",[s._v("sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)\n"),a("ul",[a("li",[s._v("具有l2正则化的线性回归，可以进行交叉验证")]),s._v(" "),a("li",[s._v("coef_:回归系数")])])])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("_BaseRidgeCV")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("LinearModel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" alphas"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                 fit_intercept"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" normalize"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" scoring"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                 cv"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" gcv_mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                 store_cv_values"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("h3",{attrs:{id:"_2-观察正则化程度的变化-对结果的影响"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-观察正则化程度的变化-对结果的影响"}},[s._v("#")]),s._v(" 2 观察正则化程度的变化，对结果的影响？")]),s._v(" "),a("p",[a("img",{attrs:{src:"http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E6%AD%A3%E5%88%99%E5%8C%96%E5%8A%9B%E5%BA%A6.png",alt:"正则化力度"}})]),s._v(" "),a("ul",[a("li",[s._v("正则化力度越大，权重系数会越小")]),s._v(" "),a("li",[s._v("正则化力度越小，权重系数会越大")])]),s._v(" "),a("h3",{attrs:{id:"_3-波士顿房价预测"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-波士顿房价预测"}},[s._v("#")]),s._v(" 3 波士顿房价预测")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("linear3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[s._v('"""\n    用岭回归的方法进行对波士顿房价预测的案例\n    :return: None\n    """')]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 1、获取数据集")]),s._v("\n    boston "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" load_boston"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v('# print("boston:\\n", boston)')]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2、划分数据集")]),s._v("\n    x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_test "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" train_test_split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("boston"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" boston"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("target"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" random_state"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("6")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 3、特征工程：标准化")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 1）实例化一个转换器类")]),s._v("\n    transfer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" StandardScaler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 2）调用fit_transform")]),s._v("\n    x_train "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" transfer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit_transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    x_test "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" transfer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("transform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 4、线性回归的预估器流程")]),s._v("\n    estimator "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" Ridge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    y_predict "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"岭回归求出模型参数的方法预测的房屋价格为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 5、得出模型")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"岭回归求出的回归系数为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("coef_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"岭回归求出的偏置为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" estimator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("intercept_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 6、模型评估——均方误差")]),s._v("\n    error "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" mean_squared_error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"岭回归的均方误差为：\\n"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" error"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br")])])])}),[],!1,null,null,null);t.default=e.exports}}]);