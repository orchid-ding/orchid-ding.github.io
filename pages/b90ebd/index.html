<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>线性回归 | clivia‘s blog</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="icon" href="/img/favicon.ico">
    <script data-ad-client="ca-pub-7828333725993554" async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <meta name="description" content="Java后端、大数据等技术博客，专注于各种技术总结。Java、高并发、hadoop、spark、hbase、hive、zookeeper、mysql、mongodb、redis">
    <meta name="keywords" content="前端博客,个人技术博客,前端,前端开发,前端框架,web前端,前端面试题,技术文档,学习,面试,JavaScript,js,ES6,TypeScript,vue,python,css3,html5,Node,git,github,markdown">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    <link rel="preload" href="/assets/css/0.styles.027adcb0.css" as="style"><link rel="preload" href="/assets/js/app.06f86fe0.js" as="script"><link rel="preload" href="/assets/js/2.ed69dcfc.js" as="script"><link rel="preload" href="/assets/js/3.b6cd915d.js" as="script"><link rel="preload" href="/assets/js/124.de810dba.js" as="script"><link rel="prefetch" href="/assets/js/10.2b0a2d8d.js"><link rel="prefetch" href="/assets/js/100.16d5c2bc.js"><link rel="prefetch" href="/assets/js/101.c4ff0a78.js"><link rel="prefetch" href="/assets/js/102.4e574824.js"><link rel="prefetch" href="/assets/js/103.ac1661b8.js"><link rel="prefetch" href="/assets/js/104.dfdf274c.js"><link rel="prefetch" href="/assets/js/105.f792ba33.js"><link rel="prefetch" href="/assets/js/106.4347b2a7.js"><link rel="prefetch" href="/assets/js/107.7248a258.js"><link rel="prefetch" href="/assets/js/108.a3c43671.js"><link rel="prefetch" href="/assets/js/109.e8c087b3.js"><link rel="prefetch" href="/assets/js/11.0c3d61f1.js"><link rel="prefetch" href="/assets/js/110.2b89ad56.js"><link rel="prefetch" href="/assets/js/111.d35e07d0.js"><link rel="prefetch" href="/assets/js/112.2402331a.js"><link rel="prefetch" href="/assets/js/113.82a39665.js"><link rel="prefetch" href="/assets/js/114.4c8d3fcb.js"><link rel="prefetch" href="/assets/js/115.f038010e.js"><link rel="prefetch" href="/assets/js/116.c7865d0c.js"><link rel="prefetch" href="/assets/js/117.2729b824.js"><link rel="prefetch" href="/assets/js/118.b8c9bb1f.js"><link rel="prefetch" href="/assets/js/119.a9eea003.js"><link rel="prefetch" href="/assets/js/12.e4b75a11.js"><link rel="prefetch" href="/assets/js/120.06f1eb1b.js"><link rel="prefetch" href="/assets/js/121.354a7fd4.js"><link rel="prefetch" href="/assets/js/122.af00dd2b.js"><link rel="prefetch" href="/assets/js/123.976e22ad.js"><link rel="prefetch" href="/assets/js/125.2c17fa05.js"><link rel="prefetch" href="/assets/js/126.ffeb8997.js"><link rel="prefetch" href="/assets/js/127.6b2f64dd.js"><link rel="prefetch" href="/assets/js/128.f9b62119.js"><link rel="prefetch" href="/assets/js/129.ce5c5c0e.js"><link rel="prefetch" href="/assets/js/13.2c54c92b.js"><link rel="prefetch" href="/assets/js/130.7db38d34.js"><link rel="prefetch" href="/assets/js/131.5abdd66a.js"><link rel="prefetch" href="/assets/js/132.5dd5eece.js"><link rel="prefetch" href="/assets/js/133.651d4b9b.js"><link rel="prefetch" href="/assets/js/134.68173250.js"><link rel="prefetch" href="/assets/js/135.ca520569.js"><link rel="prefetch" href="/assets/js/136.1515cd12.js"><link rel="prefetch" href="/assets/js/137.d783808b.js"><link rel="prefetch" href="/assets/js/138.05efd534.js"><link rel="prefetch" href="/assets/js/139.a65dce22.js"><link rel="prefetch" href="/assets/js/14.5cb33b3a.js"><link rel="prefetch" href="/assets/js/140.44dc5c4e.js"><link rel="prefetch" href="/assets/js/141.0632e3aa.js"><link rel="prefetch" href="/assets/js/142.66ac9883.js"><link rel="prefetch" href="/assets/js/143.8a75f55a.js"><link rel="prefetch" href="/assets/js/144.349bba43.js"><link rel="prefetch" href="/assets/js/15.06dff62d.js"><link rel="prefetch" href="/assets/js/16.ad74e0a9.js"><link rel="prefetch" href="/assets/js/17.e0d7a93c.js"><link rel="prefetch" href="/assets/js/18.0c217b19.js"><link rel="prefetch" href="/assets/js/19.33d29011.js"><link rel="prefetch" href="/assets/js/20.5c47e53a.js"><link rel="prefetch" href="/assets/js/21.4623dcf3.js"><link rel="prefetch" href="/assets/js/22.1d2819bf.js"><link rel="prefetch" href="/assets/js/23.da3264cc.js"><link rel="prefetch" href="/assets/js/24.a3976766.js"><link rel="prefetch" href="/assets/js/25.52b1b746.js"><link rel="prefetch" href="/assets/js/26.527a58d4.js"><link rel="prefetch" href="/assets/js/27.1a611ca9.js"><link rel="prefetch" href="/assets/js/28.19848920.js"><link rel="prefetch" href="/assets/js/29.0399b7cc.js"><link rel="prefetch" href="/assets/js/30.f4967fc4.js"><link rel="prefetch" href="/assets/js/31.ec15c096.js"><link rel="prefetch" href="/assets/js/32.ea29ed83.js"><link rel="prefetch" href="/assets/js/33.2506e8df.js"><link rel="prefetch" href="/assets/js/34.b317cb6a.js"><link rel="prefetch" href="/assets/js/35.e98b1f8e.js"><link rel="prefetch" href="/assets/js/36.5a3c937d.js"><link rel="prefetch" href="/assets/js/37.d8e66f15.js"><link rel="prefetch" href="/assets/js/38.583bd78a.js"><link rel="prefetch" href="/assets/js/39.02884927.js"><link rel="prefetch" href="/assets/js/4.4f8d037f.js"><link rel="prefetch" href="/assets/js/40.11d31d5e.js"><link rel="prefetch" href="/assets/js/41.e3b7229f.js"><link rel="prefetch" href="/assets/js/42.ed31fd5c.js"><link rel="prefetch" href="/assets/js/43.71601b99.js"><link rel="prefetch" href="/assets/js/44.7235eb98.js"><link rel="prefetch" href="/assets/js/45.8adef6f7.js"><link rel="prefetch" href="/assets/js/46.44d68224.js"><link rel="prefetch" href="/assets/js/47.ec6b4de8.js"><link rel="prefetch" href="/assets/js/48.ee56118d.js"><link rel="prefetch" href="/assets/js/49.0f6101ee.js"><link rel="prefetch" href="/assets/js/5.0e4db68d.js"><link rel="prefetch" href="/assets/js/50.2d01b40c.js"><link rel="prefetch" href="/assets/js/51.ddf132cb.js"><link rel="prefetch" href="/assets/js/52.fbc9619b.js"><link rel="prefetch" href="/assets/js/53.089b304e.js"><link rel="prefetch" href="/assets/js/54.34c0d377.js"><link rel="prefetch" href="/assets/js/55.41e71cff.js"><link rel="prefetch" href="/assets/js/56.ed6b0ac8.js"><link rel="prefetch" href="/assets/js/57.538bf884.js"><link rel="prefetch" href="/assets/js/58.d0051104.js"><link rel="prefetch" href="/assets/js/59.00792d53.js"><link rel="prefetch" href="/assets/js/6.6a15c5c7.js"><link rel="prefetch" href="/assets/js/60.44529245.js"><link rel="prefetch" href="/assets/js/61.6a6e2fe6.js"><link rel="prefetch" href="/assets/js/62.b02509d6.js"><link rel="prefetch" href="/assets/js/63.c865f599.js"><link rel="prefetch" href="/assets/js/64.7aafe0f4.js"><link rel="prefetch" href="/assets/js/65.029254a7.js"><link rel="prefetch" href="/assets/js/66.1d8c00be.js"><link rel="prefetch" href="/assets/js/67.191949ec.js"><link rel="prefetch" href="/assets/js/68.fc11d430.js"><link rel="prefetch" href="/assets/js/69.5e5d346e.js"><link rel="prefetch" href="/assets/js/7.4353a828.js"><link rel="prefetch" href="/assets/js/70.61cd3f71.js"><link rel="prefetch" href="/assets/js/71.1a87ddd4.js"><link rel="prefetch" href="/assets/js/72.f561b5cf.js"><link rel="prefetch" href="/assets/js/73.d1c67ae7.js"><link rel="prefetch" href="/assets/js/74.3725d6d7.js"><link rel="prefetch" href="/assets/js/75.310c7556.js"><link rel="prefetch" href="/assets/js/76.b8a53386.js"><link rel="prefetch" href="/assets/js/77.e04be33a.js"><link rel="prefetch" href="/assets/js/78.17c06f28.js"><link rel="prefetch" href="/assets/js/79.3ff2fb40.js"><link rel="prefetch" href="/assets/js/8.248194f5.js"><link rel="prefetch" href="/assets/js/80.96d575dc.js"><link rel="prefetch" href="/assets/js/81.770ca2a4.js"><link rel="prefetch" href="/assets/js/82.8900493c.js"><link rel="prefetch" href="/assets/js/83.4716d242.js"><link rel="prefetch" href="/assets/js/84.5ae9874f.js"><link rel="prefetch" href="/assets/js/85.924e7d3d.js"><link rel="prefetch" href="/assets/js/86.5073d16d.js"><link rel="prefetch" href="/assets/js/87.1148acd0.js"><link rel="prefetch" href="/assets/js/88.629d8beb.js"><link rel="prefetch" href="/assets/js/89.36b4cdb0.js"><link rel="prefetch" href="/assets/js/9.85151a25.js"><link rel="prefetch" href="/assets/js/90.fac275aa.js"><link rel="prefetch" href="/assets/js/91.93484ec9.js"><link rel="prefetch" href="/assets/js/92.01e42c63.js"><link rel="prefetch" href="/assets/js/93.50a55dd7.js"><link rel="prefetch" href="/assets/js/94.6a3f96c8.js"><link rel="prefetch" href="/assets/js/95.7dd7befb.js"><link rel="prefetch" href="/assets/js/96.38d049b2.js"><link rel="prefetch" href="/assets/js/97.a129eb48.js"><link rel="prefetch" href="/assets/js/98.f848c937.js"><link rel="prefetch" href="/assets/js/99.2dc3271b.js">
    <link rel="stylesheet" href="/assets/css/0.styles.027adcb0.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/C-logo.png" alt="clivia‘s blog" class="logo"> <span class="site-name can-hide">clivia‘s blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/bigdata/" class="nav-link">大数据</a></div><div class="nav-item"><a href="/technology/" class="nav-link">技术</a></div><div class="nav-item"><a href="/project/" class="nav-link">项目</a></div><div class="nav-item"><a href="/more/" class="nav-link">更多</a></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div><div class="nav-item"><a href="/pages/beb6c0bd8a66cea6/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://gitee.com/kflys/clivia-blog" target="_blank" rel="noopener noreferrer" class="repo-link">
    Gitee
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="/img/logo.png"> <div class="blogger-info"><h3>clivia’s blog</h3> <span>专注于后端开发，致力于简洁知识。</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/bigdata/" class="nav-link">大数据</a></div><div class="nav-item"><a href="/technology/" class="nav-link">技术</a></div><div class="nav-item"><a href="/project/" class="nav-link">项目</a></div><div class="nav-item"><a href="/more/" class="nav-link">更多</a></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div><div class="nav-item"><a href="/pages/beb6c0bd8a66cea6/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://gitee.com/kflys/clivia-blog" target="_blank" rel="noopener noreferrer" class="repo-link">
    Gitee
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>人工智能介绍</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>数据集</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>分类算法</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>回归算法</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/b90ebd/" aria-current="page" class="active sidebar-link">线性回归</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/pages/b90ebd/#学习目标" class="sidebar-link">学习目标</a></li><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_4-1-1-线性回归的原理" class="sidebar-link">4.1.1 线性回归的原理</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_1-线性回归应用场景" class="sidebar-link">1 线性回归应用场景</a></li><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_2-什么是线性回归" class="sidebar-link">2 什么是线性回归</a></li></ul></li><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_4-1-2-线性回归的损失和优化原理-理解记忆" class="sidebar-link">4.1.2 线性回归的损失和优化原理（理解记忆）</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_1-损失函数" class="sidebar-link">1 损失函数</a></li><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_2-优化算法" class="sidebar-link">2 优化算法</a></li><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_3-优化动态图演示" class="sidebar-link">3 优化动态图演示</a></li></ul></li><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_4-1-3-线性回归api" class="sidebar-link">4.1.3 线性回归API</a></li><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_4-1-4-波士顿房价预测" class="sidebar-link">4.1.4 波士顿房价预测</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_1-分析" class="sidebar-link">1 分析</a></li><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_2-回归性能评估" class="sidebar-link">2 回归性能评估</a></li><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_3-代码" class="sidebar-link">3 代码</a></li><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_4-正规方程和梯度下降对比" class="sidebar-link">4 正规方程和梯度下降对比</a></li></ul></li><li class="sidebar-sub-header"><a href="/pages/b90ebd/#_4-1-6-总结" class="sidebar-link">4.1.6 总结</a></li></ul></li><li><a href="/pages/2c7b3c/" class="sidebar-link">欠拟合与过拟合</a></li><li><a href="/pages/172221/" class="sidebar-link">岭回归</a></li><li><a href="/pages/261db4/" class="sidebar-link">逻辑回归</a></li><li><a href="/pages/e78220/" class="sidebar-link">模拟保存预加载</a></li><li><a href="/pages/20a714/" class="sidebar-link">无监督学习</a></li></ul></section></li></ul> <div class="sidebar-slot sidebar-slot-bottom"><!-- 正方形 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="3508773082"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div></aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-70a2d273><div class="articleInfo" data-v-70a2d273><ul class="breadcrumbs" data-v-70a2d273><li data-v-70a2d273><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-70a2d273></a></li> <li data-v-70a2d273><a href="/categories/?category=%E3%80%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0" title="分类" data-v-70a2d273>《人工智能与机器学习》笔记</a></li> <li data-v-70a2d273><a href="/categories/?category=%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95" title="分类" data-v-70a2d273>回归算法</a></li> <!----></ul> <div class="info" data-v-70a2d273><div title="作者" class="author iconfont icon-touxiang" data-v-70a2d273><a href="https://gitee.com/kflys" target="_blank" title="作者" class="beLink" data-v-70a2d273>kflys</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-70a2d273><a href="javascript:;" data-v-70a2d273>2022-01-26</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">
          线性回归
        </h1> <div class="page-slot page-slot-top"><!-- 固定100% * 90px可显示，max-height:90px未见显示-->
     <ins class="adsbygoogle"
          style="display:inline-block;width:100%;max-height:90px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6625304284"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="theme-vdoing-content content__default"><h1 id="_4-1-线性回归"><a href="#_4-1-线性回归" class="header-anchor">#</a> 4.1 线性回归</h1> <h2 id="学习目标"><a href="#学习目标" class="header-anchor">#</a> 学习目标</h2> <ul><li>目标
<ul><li>记忆线性回归的原理过程</li> <li>应用LinearRegression或SGDRegressor实现回归预测</li> <li>记忆回归算法的评估标准及其公式</li></ul></li></ul> <ul><li>应用
<ul><li>波士顿房价预测</li></ul></li> <li>内容预览
<ul><li>4.1.1 线性回归的原理</li> <li>4.1.2 线性回归的损失和优化原理（理解记忆）</li> <li>4.1.3 线性回归API</li> <li>4.1.4 波士顿房价预测</li></ul></li></ul> <h4 id="回忆一下回归问题的判定是什么"><a href="#回忆一下回归问题的判定是什么" class="header-anchor">#</a> 回忆一下回归问题的判定是什么？</h4> <h2 id="_4-1-1-线性回归的原理"><a href="#_4-1-1-线性回归的原理" class="header-anchor">#</a> 4.1.1 线性回归的原理</h2> <h3 id="_1-线性回归应用场景"><a href="#_1-线性回归应用场景" class="header-anchor">#</a> 1 线性回归应用场景</h3> <ul><li>房价预测</li> <li>销售额度预测</li> <li>金融：贷款额度预测、利用线性回归以及系数分析因子</li></ul> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E9%94%80%E5%94%AE%E9%A2%9D%E5%BA%A6.png" alt="销售额度"></p> <h3 id="_2-什么是线性回归"><a href="#_2-什么是线性回归" class="header-anchor">#</a> 2 什么是线性回归</h3> <h4 id="_1-定义与公式"><a href="#_1-定义与公式" class="header-anchor">#</a> 1）定义与公式</h4> <p>线性回归(Linear regression)是利用<strong>回归方程(函数)<strong>对一个或</strong>多个自变量(特征值)和因变量(目标值)之间</strong>关系进行建模的一种分析方式。</p> <ul><li>特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归</li></ul> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%85%AC%E5%BC%8F.png" alt="线性回归公式"></p> <p>那么怎么理解呢？我们来看几个例子</p> <ul><li><strong>期末成绩：0.7×考试成绩+0.3×平时成绩</strong></li> <li><strong>房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率</strong></li></ul> <p>上面两个例子，<strong>我们看到特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型</strong>。</p> <ul><li>模型</li></ul> <h4 id="_2-线性回归的特征与目标的关系分析"><a href="#_2-线性回归的特征与目标的关系分析" class="header-anchor">#</a> 2） 线性回归的特征与目标的关系分析</h4> <p>线性回归当中线性模型有两种，一种是线性关系，另一种是非线性关系。<strong>在这里我们只能画一个平面更好去理解，所以都用单个特征或两个特征举例子。</strong></p> <ul><li>线性关系 <img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%E5%9B%BE.png" alt="线性关系图"></li></ul> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB.png" alt="多变量线性关系"></p> <blockquote><p>注释：单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面的关系</p> <p>更高维度的我们不用自己去想，记住这种关系即可</p></blockquote> <ul><li>非线性关系</li></ul> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB.png" alt="非线性关系"></p> <blockquote><p>注释：为什么会这样的关系呢？原因是什么？</p> <p>如果是非线性关系，那么回归方程可以理解为：w1x1+w2x2^2+w3x3^2</p></blockquote> <h2 id="_4-1-2-线性回归的损失和优化原理-理解记忆"><a href="#_4-1-2-线性回归的损失和优化原理-理解记忆" class="header-anchor">#</a> 4.1.2 线性回归的损失和优化原理（理解记忆）</h2> <p><strong>假设刚才的房子例子，真实的数据之间存在这样的关系</strong></p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>真实关系：真实房子价格 <span class="token operator">=</span> <span class="token number">0.02</span>×中心区域的距离 <span class="token operator">+</span> <span class="token number">0.04</span>×城市一氧化氮浓度 <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.12</span>×自住房平均房价<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">0.254</span>×城镇犯罪率
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>那么现在呢，我们随意指定一个关系（猜测）</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>随机指定关系：预测房子价格 <span class="token operator">=</span> <span class="token number">0.25</span>×中心区域的距离 <span class="token operator">+</span> <span class="token number">0.14</span>×城市一氧化氮浓度 <span class="token operator">+</span> <span class="token number">0.42</span>×自住房平均房价 <span class="token operator">+</span> <span class="token number">0.34</span>×城镇犯罪率
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>请问这样的话，会发生什么？真实结果与我们预测的结果之间是不是存在一定的误差呢？类似这样样子</p> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E8%AF%AF%E5%B7%AE.png" alt="误差"></p> <p>既然存在这个误差，那我们就将这个误差给衡量出来</p> <h3 id="_1-损失函数"><a href="#_1-损失函数" class="header-anchor">#</a> 1 损失函数</h3> <p>总损失定义为：</p> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" alt="线性回归损失函数"></p> <ul><li>y_i为第i个训练样本的真实值</li> <li>h(x_i)为第i个训练样本特征值组合预测函数</li> <li>又称最小二乘法</li></ul> <p><strong>如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！！</strong></p> <h3 id="_2-优化算法"><a href="#_2-优化算法" class="header-anchor">#</a> 2 优化算法</h3> <p><strong>如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）</strong></p> <p>线性回归经常使用的两种优化算法</p> <ul><li>正规方程</li></ul> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B.png" alt="正规方程"></p> <blockquote><p>理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果</p> <p>缺点：当特征过多过复杂时，求解速度太慢并且得不到结果</p></blockquote> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E6%8D%9F%E5%A4%B1%E8%A1%8C%E6%95%B0%E6%B1%82%E8%A7%A31.png" alt="损失行数求解1"></p> <ul><li><strong>梯度下降(Gradient Descent)</strong></li></ul> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F.png" alt="梯度下降公式"></p> <blockquote><p>理解：α为学习速率，需要手动指定（超参数），α旁边的整体表示方向</p> <p>沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后更新W值</p> <p>使用：面对训练数据规模十分庞大的任务 ，能够找到较好的结果</p></blockquote> <p>我们通过两个图更好理解梯度下降的过程</p> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E5%8D%95%E5%8F%98%E9%87%8F%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png" alt="单变量的梯度下降"></p> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png" alt="多变量的梯度下降"></p> <p><strong>所以有了梯度下降这样一个优化算法，回归就有了&quot;自动学习&quot;的能力</strong></p> <h3 id="_3-优化动态图演示"><a href="#_3-优化动态图演示" class="header-anchor">#</a> 3 优化动态图演示</h3> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BC%98%E5%8C%96%E5%8A%A8%E6%80%81%E5%9B%BE.gif" alt="线性回归优化动态图"></p> <h2 id="_4-1-3-线性回归api"><a href="#_4-1-3-线性回归api" class="header-anchor">#</a> 4.1.3 线性回归API</h2> <ul><li>sklearn.linear_model.LinearRegression(fit_intercept=True)
<ul><li>通过正规方程优化</li> <li>fit_intercept：是否计算偏置</li> <li>LinearRegression.coef_：回归系数</li> <li>LinearRegression.intercept_：偏置</li></ul></li> <li>sklearn.linear_model.SGDRegressor(loss=&quot;squared_loss&quot;, fit_intercept=True, learning_rate ='invscaling', eta0=0.01)
<ul><li>SGDRegressor类实现了随机梯度下降学习，它支持不同的<strong>loss函数和正则化惩罚项</strong>来拟合线性回归模型。</li> <li>loss:损失类型
<ul><li><strong>loss=”squared_loss”: 普通最小二乘法</strong></li></ul></li> <li>fit_intercept：是否计算偏置</li> <li>learning_rate : string, optional
<ul><li>学习率填充</li> <li><strong>'constant': eta = eta0</strong></li> <li><strong>'optimal': eta = 1.0 / (alpha * (t + t0)) [default]</strong></li> <li>'invscaling': eta = eta0 / pow(t, power_t)
<ul><li><strong>power_t=0.25:存在父类当中</strong></li></ul></li> <li><strong>对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。</strong></li></ul></li> <li>SGDRegressor.coef_：回归系数</li> <li>SGDRegressor.intercept_：偏置</li></ul></li></ul> <blockquote><p>sklearn提供给我们两种实现的API， 可以根据选择使用</p></blockquote> <h2 id="_4-1-4-波士顿房价预测"><a href="#_4-1-4-波士顿房价预测" class="header-anchor">#</a> 4.1.4 波士顿房价预测</h2> <ul><li>数据介绍</li></ul> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E6%88%BF%E4%BB%B7%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D.png" alt="房价数据集介绍"></p> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E5%B1%9E%E6%80%A7.png" alt="属性"></p> <blockquote><p>给定的这些特征，是专家们得出的影响房价的结果属性。我们此阶段不需要自己去探究特征是否有用，只需要使用这些特征。到后面量化很多特征需要我们自己去寻找</p></blockquote> <h3 id="_1-分析"><a href="#_1-分析" class="header-anchor">#</a> 1 分析</h3> <p>回归当中的数据大小不一致，是否会导致结果影响较大。所以需要做标准化处理。</p> <ul><li>数据分割与标准化处理</li> <li>回归预测</li> <li>线性回归的算法效果评估</li></ul> <h3 id="_2-回归性能评估"><a href="#_2-回归性能评估" class="header-anchor">#</a> 2 回归性能评估</h3> <p>均方误差(Mean Squared Error)MSE)评价机制：</p> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E8%AF%84%E4%BC%B0.png" alt="线性回归评估"></p> <blockquote><p>注：y^i为预测值，¯y为真实值</p></blockquote> <ul><li>sklearn.metrics.mean_squared_error(y_true, y_pred)
<ul><li>均方误差回归损失</li> <li>y_true:真实值</li> <li>y_pred:预测值</li> <li>return:浮点数结果</li></ul></li></ul> <h3 id="_3-代码"><a href="#_3-代码" class="header-anchor">#</a> 3 代码</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">linear1</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    用正规方程直接求出模型参数的方法进行对波士顿房价预测的线性回归案例
    :return: None
    &quot;&quot;&quot;</span>
    <span class="token comment"># 1、获取数据集</span>
    boston <span class="token operator">=</span> load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;boston:\n&quot;</span><span class="token punctuation">,</span> boston<span class="token punctuation">.</span>DESCR<span class="token punctuation">)</span>
    <span class="token comment"># 2、划分数据集</span>
    x_train<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>data<span class="token punctuation">,</span> boston<span class="token punctuation">.</span>target<span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">)</span>
    <span class="token comment"># 3、特征工程：标准化</span>
    <span class="token comment"># 1）实例化一个转换器类</span>
    transfer <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 2）调用fit_transform</span>
    x_train <span class="token operator">=</span> transfer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
    x_test <span class="token operator">=</span> transfer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
    <span class="token comment"># 4、线性回归的预估器流程</span>
    estimator <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>
    estimator<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
    y_predict <span class="token operator">=</span> estimator<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;正规方程求出模型参数的方法预测的房屋价格为：\n&quot;</span><span class="token punctuation">,</span> y_predict<span class="token punctuation">)</span>
    <span class="token comment"># 5、得出模型</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;正规方程求出的回归系数为：\n&quot;</span><span class="token punctuation">,</span> estimator<span class="token punctuation">.</span>coef_<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;正规方程求出的偏置为：\n&quot;</span><span class="token punctuation">,</span> estimator<span class="token punctuation">.</span>intercept_<span class="token punctuation">)</span>
    <span class="token comment"># 6、模型评估——均方误差</span>
    error <span class="token operator">=</span> mean_squared_error<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_predict<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;正规方程的均方误差为：\n&quot;</span><span class="token punctuation">,</span> error<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token boolean">None</span>


<span class="token keyword">def</span> <span class="token function">linear2</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    用梯度下降优化模型参数的方法进行对波士顿房价预测的线性回归案例
    :return: None
    &quot;&quot;&quot;</span>
    <span class="token comment"># 1、获取数据集</span>
    boston <span class="token operator">=</span> load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># print(&quot;boston:\n&quot;, boston)</span>
    <span class="token comment"># 2、划分数据集</span>
    x_train<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>data<span class="token punctuation">,</span> boston<span class="token punctuation">.</span>target<span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">)</span>
    <span class="token comment"># 3、特征工程：标准化</span>
    <span class="token comment"># 1）实例化一个转换器类</span>
    transfer <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 2）调用fit_transform</span>
    x_train <span class="token operator">=</span> transfer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
    x_test <span class="token operator">=</span> transfer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
    <span class="token comment"># 4、线性回归的预估器流程</span>
    estimator <span class="token operator">=</span> SGDRegressor<span class="token punctuation">(</span><span class="token punctuation">)</span>
    estimator<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
    y_predict <span class="token operator">=</span> estimator<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;梯度下降求出模型参数的方法预测的房屋价格为：\n&quot;</span><span class="token punctuation">,</span> y_predict<span class="token punctuation">)</span>
    <span class="token comment"># 5、得出模型</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;梯度下降求出的回归系数为：\n&quot;</span><span class="token punctuation">,</span> estimator<span class="token punctuation">.</span>coef_<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;梯度下降求出的偏置为：\n&quot;</span><span class="token punctuation">,</span> estimator<span class="token punctuation">.</span>intercept_<span class="token punctuation">)</span>
    <span class="token comment"># 6、模型评估——均方误差</span>
    error <span class="token operator">=</span> mean_squared_error<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_predict<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;梯度下降的均方误差为：\n&quot;</span><span class="token punctuation">,</span> error<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token boolean">None</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br></div></div><p>我们也可以尝试去修改学习率</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>estimator <span class="token operator">=</span> SGDRegressor<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span><span class="token string">'constant'</span><span class="token punctuation">,</span> eta0<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>此时我们可以通过调参数，找到学习率效果更好的值。</p> <h3 id="_4-正规方程和梯度下降对比"><a href="#_4-正规方程和梯度下降对比" class="header-anchor">#</a> 4 正规方程和梯度下降对比</h3> <p><img src="http://kflys.gitee.io/upic/2020/03/22/uPic/mlib/images/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%AF%B9%E6%AF%94.png" alt="正规方程和梯度下降对比"></p> <ul><li>文字对比</li></ul> <table><thead><tr><th style="text-align:center;">梯度下降</th> <th style="text-align:center;">正规方程</th></tr></thead> <tbody><tr><td style="text-align:center;">需要选择学习率</td> <td style="text-align:center;">不需要</td></tr> <tr><td style="text-align:center;">需要迭代求解</td> <td style="text-align:center;">一次运算得出</td></tr> <tr><td style="text-align:center;">特征数量较大可以使用</td> <td style="text-align:center;">需要计算方程，时间复杂度高O(n3)</td></tr></tbody></table> <ul><li>选择：
<ul><li>小规模数据：
<ul><li><strong>LinearRegression(不能解决拟合问题)</strong></li> <li>岭回归</li></ul></li> <li>大规模数据：SGDRegressor</li></ul></li></ul> <blockquote><p>4.1.5 拓展-关于优化方法GD、SGD、SAG</p> <p>1 GD</p> <p><strong>梯度下降(Gradient Descent)，原始的梯度下降法需要计算所有样本的值才能够得出梯度，计算量大，所以后面才有会一系列的改进。</strong></p> <p>2 SGD</p> <p><strong>随机梯度下降(Stochastic gradient descent)是一个优化方法。它在一次迭代时只考虑一个训练样本。</strong></p> <ul><li>SGD的优点是：
<ul><li>高效</li> <li>容易实现</li></ul></li> <li>SGD的缺点是：
<ul><li>SGD需要许多超参数：比如正则项参数、迭代数。</li> <li>SGD对于特征标准化是敏感的。</li></ul></li></ul> <p>3 SAG</p> <p>随机平均梯度法(Stochasitc Average Gradient)，由于收敛的速度太慢，有人提出SAG等基于梯度下降的算法</p> <p>Scikit-learn：岭回归、逻辑回归等当中都会有SAG优化</p></blockquote> <h2 id="_4-1-6-总结"><a href="#_4-1-6-总结" class="header-anchor">#</a> 4.1.6 总结</h2> <ul><li>线性回归的损失函数-均方误差</li> <li>线性回归的优化方法
<ul><li>正规方程</li> <li>梯度下降</li></ul></li> <li>线性回归的性能衡量方法-均方误差</li> <li>sklearn的SGDRegressor API 参数</li></ul></div></div> <div class="page-slot page-slot-bottom"><!-- 横向自适应 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6620245489"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="page-edit"><div class="edit-link"><a href="https://gitee.com/kflys/clivia-blog/edit/master/docs/《人工智能与机器学习》笔记/40.回归算法/01.线性回归.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">1/26/2022, 2:50:30 PM</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/7505ba/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">随机森林</div></a> <a href="/pages/2c7b3c/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">欠拟合与过拟合</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/7505ba/" class="prev">随机森林</a></span> <span class="next"><a href="/pages/2c7b3c/">欠拟合与过拟合</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/pages/8e0b33/"><div>阿里官方Redis开发规范</div></a> <span>10-08</span></dt></dl><dl><dd>02</dd> <dt><a href="/pages/20a714/"><div>无监督学习</div></a> <span>01-26</span></dt></dl><dl><dd>03</dd> <dt><a href="/pages/e78220/"><div>模拟保存预加载</div></a> <span>01-26</span></dt></dl> <dl><dd></dd> <dt><a href="/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="mailto:clivia.pro@gmail.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/kflys" title="Gitee" target="_blank" class="iconfont icon-gitee"></a><a href="https://y.qq.com/n/ryqq/songDetail/003lgoEG1SWHmF" title="music" target="_blank" class="iconfont icon-erji"></a></div> <div><span> Theme by <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> | Powered by <a href="https://webify.cloudbase.net/" target="_blank">CloudBase Webify</a></span></div> <span>Clivia‘s <a href="https://gitee.com/kflys/clivia-blog" target="_blank">blog</a> </span>
    | Copyright © 2019-2022
    <span><a href="http://beian.miit.gov.cn/" target="_blank">皖ICP备2021014093号</a> </span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <div class="custom-html-window custom-html-window-rb" style="display:;"><div class="custom-wrapper"><i class="close-but">×</i> <div><!-- 固定160*160px -->
      <ins class="adsbygoogle"
          style="display:inline-block;max-width:160px;max-height:160px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="8377369658"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script>
      </div></div></div></div><div class="global-ui"><div id="live2d-widget" class="live2d-widget-container" style="position:fixed;left:65px;bottom:0px;width:135px;height:300px;z-index:99999;opacity:0.8;pointer-events:none;"><!----></div></div></div>
    <script src="/assets/js/app.06f86fe0.js" defer></script><script src="/assets/js/2.ed69dcfc.js" defer></script><script src="/assets/js/3.b6cd915d.js" defer></script><script src="/assets/js/124.de810dba.js" defer></script>
  </body>
</html>