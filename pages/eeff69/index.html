<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Hadoop之HDFS详解 | clivia‘s blog</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="icon" href="/img/favicon.ico">
    <script data-ad-client="ca-pub-7828333725993554" async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <meta name="description" content="Java后端、大数据等技术博客，专注于各种技术总结。Java、高并发、hadoop、spark、hbase、hive、zookeeper、mysql、mongodb、redis">
    <meta name="keywords" content="前端博客,个人技术博客,前端,前端开发,前端框架,web前端,前端面试题,技术文档,学习,面试,JavaScript,js,ES6,TypeScript,vue,python,css3,html5,Node,git,github,markdown">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    <link rel="preload" href="/assets/css/0.styles.027adcb0.css" as="style"><link rel="preload" href="/assets/js/app.06f86fe0.js" as="script"><link rel="preload" href="/assets/js/2.ed69dcfc.js" as="script"><link rel="preload" href="/assets/js/3.b6cd915d.js" as="script"><link rel="preload" href="/assets/js/75.310c7556.js" as="script"><link rel="prefetch" href="/assets/js/10.2b0a2d8d.js"><link rel="prefetch" href="/assets/js/100.16d5c2bc.js"><link rel="prefetch" href="/assets/js/101.c4ff0a78.js"><link rel="prefetch" href="/assets/js/102.4e574824.js"><link rel="prefetch" href="/assets/js/103.ac1661b8.js"><link rel="prefetch" href="/assets/js/104.dfdf274c.js"><link rel="prefetch" href="/assets/js/105.f792ba33.js"><link rel="prefetch" href="/assets/js/106.4347b2a7.js"><link rel="prefetch" href="/assets/js/107.7248a258.js"><link rel="prefetch" href="/assets/js/108.a3c43671.js"><link rel="prefetch" href="/assets/js/109.e8c087b3.js"><link rel="prefetch" href="/assets/js/11.0c3d61f1.js"><link rel="prefetch" href="/assets/js/110.2b89ad56.js"><link rel="prefetch" href="/assets/js/111.d35e07d0.js"><link rel="prefetch" href="/assets/js/112.2402331a.js"><link rel="prefetch" href="/assets/js/113.82a39665.js"><link rel="prefetch" href="/assets/js/114.4c8d3fcb.js"><link rel="prefetch" href="/assets/js/115.f038010e.js"><link rel="prefetch" href="/assets/js/116.c7865d0c.js"><link rel="prefetch" href="/assets/js/117.2729b824.js"><link rel="prefetch" href="/assets/js/118.b8c9bb1f.js"><link rel="prefetch" href="/assets/js/119.a9eea003.js"><link rel="prefetch" href="/assets/js/12.e4b75a11.js"><link rel="prefetch" href="/assets/js/120.06f1eb1b.js"><link rel="prefetch" href="/assets/js/121.354a7fd4.js"><link rel="prefetch" href="/assets/js/122.af00dd2b.js"><link rel="prefetch" href="/assets/js/123.976e22ad.js"><link rel="prefetch" href="/assets/js/124.de810dba.js"><link rel="prefetch" href="/assets/js/125.2c17fa05.js"><link rel="prefetch" href="/assets/js/126.ffeb8997.js"><link rel="prefetch" href="/assets/js/127.6b2f64dd.js"><link rel="prefetch" href="/assets/js/128.f9b62119.js"><link rel="prefetch" href="/assets/js/129.ce5c5c0e.js"><link rel="prefetch" href="/assets/js/13.2c54c92b.js"><link rel="prefetch" href="/assets/js/130.7db38d34.js"><link rel="prefetch" href="/assets/js/131.5abdd66a.js"><link rel="prefetch" href="/assets/js/132.5dd5eece.js"><link rel="prefetch" href="/assets/js/133.651d4b9b.js"><link rel="prefetch" href="/assets/js/134.68173250.js"><link rel="prefetch" href="/assets/js/135.ca520569.js"><link rel="prefetch" href="/assets/js/136.1515cd12.js"><link rel="prefetch" href="/assets/js/137.d783808b.js"><link rel="prefetch" href="/assets/js/138.05efd534.js"><link rel="prefetch" href="/assets/js/139.a65dce22.js"><link rel="prefetch" href="/assets/js/14.5cb33b3a.js"><link rel="prefetch" href="/assets/js/140.44dc5c4e.js"><link rel="prefetch" href="/assets/js/141.0632e3aa.js"><link rel="prefetch" href="/assets/js/142.66ac9883.js"><link rel="prefetch" href="/assets/js/143.8a75f55a.js"><link rel="prefetch" href="/assets/js/144.349bba43.js"><link rel="prefetch" href="/assets/js/15.06dff62d.js"><link rel="prefetch" href="/assets/js/16.ad74e0a9.js"><link rel="prefetch" href="/assets/js/17.e0d7a93c.js"><link rel="prefetch" href="/assets/js/18.0c217b19.js"><link rel="prefetch" href="/assets/js/19.33d29011.js"><link rel="prefetch" href="/assets/js/20.5c47e53a.js"><link rel="prefetch" href="/assets/js/21.4623dcf3.js"><link rel="prefetch" href="/assets/js/22.1d2819bf.js"><link rel="prefetch" href="/assets/js/23.da3264cc.js"><link rel="prefetch" href="/assets/js/24.a3976766.js"><link rel="prefetch" href="/assets/js/25.52b1b746.js"><link rel="prefetch" href="/assets/js/26.527a58d4.js"><link rel="prefetch" href="/assets/js/27.1a611ca9.js"><link rel="prefetch" href="/assets/js/28.19848920.js"><link rel="prefetch" href="/assets/js/29.0399b7cc.js"><link rel="prefetch" href="/assets/js/30.f4967fc4.js"><link rel="prefetch" href="/assets/js/31.ec15c096.js"><link rel="prefetch" href="/assets/js/32.ea29ed83.js"><link rel="prefetch" href="/assets/js/33.2506e8df.js"><link rel="prefetch" href="/assets/js/34.b317cb6a.js"><link rel="prefetch" href="/assets/js/35.e98b1f8e.js"><link rel="prefetch" href="/assets/js/36.5a3c937d.js"><link rel="prefetch" href="/assets/js/37.d8e66f15.js"><link rel="prefetch" href="/assets/js/38.583bd78a.js"><link rel="prefetch" href="/assets/js/39.02884927.js"><link rel="prefetch" href="/assets/js/4.4f8d037f.js"><link rel="prefetch" href="/assets/js/40.11d31d5e.js"><link rel="prefetch" href="/assets/js/41.e3b7229f.js"><link rel="prefetch" href="/assets/js/42.ed31fd5c.js"><link rel="prefetch" href="/assets/js/43.71601b99.js"><link rel="prefetch" href="/assets/js/44.7235eb98.js"><link rel="prefetch" href="/assets/js/45.8adef6f7.js"><link rel="prefetch" href="/assets/js/46.44d68224.js"><link rel="prefetch" href="/assets/js/47.ec6b4de8.js"><link rel="prefetch" href="/assets/js/48.ee56118d.js"><link rel="prefetch" href="/assets/js/49.0f6101ee.js"><link rel="prefetch" href="/assets/js/5.0e4db68d.js"><link rel="prefetch" href="/assets/js/50.2d01b40c.js"><link rel="prefetch" href="/assets/js/51.ddf132cb.js"><link rel="prefetch" href="/assets/js/52.fbc9619b.js"><link rel="prefetch" href="/assets/js/53.089b304e.js"><link rel="prefetch" href="/assets/js/54.34c0d377.js"><link rel="prefetch" href="/assets/js/55.41e71cff.js"><link rel="prefetch" href="/assets/js/56.ed6b0ac8.js"><link rel="prefetch" href="/assets/js/57.538bf884.js"><link rel="prefetch" href="/assets/js/58.d0051104.js"><link rel="prefetch" href="/assets/js/59.00792d53.js"><link rel="prefetch" href="/assets/js/6.6a15c5c7.js"><link rel="prefetch" href="/assets/js/60.44529245.js"><link rel="prefetch" href="/assets/js/61.6a6e2fe6.js"><link rel="prefetch" href="/assets/js/62.b02509d6.js"><link rel="prefetch" href="/assets/js/63.c865f599.js"><link rel="prefetch" href="/assets/js/64.7aafe0f4.js"><link rel="prefetch" href="/assets/js/65.029254a7.js"><link rel="prefetch" href="/assets/js/66.1d8c00be.js"><link rel="prefetch" href="/assets/js/67.191949ec.js"><link rel="prefetch" href="/assets/js/68.fc11d430.js"><link rel="prefetch" href="/assets/js/69.5e5d346e.js"><link rel="prefetch" href="/assets/js/7.4353a828.js"><link rel="prefetch" href="/assets/js/70.61cd3f71.js"><link rel="prefetch" href="/assets/js/71.1a87ddd4.js"><link rel="prefetch" href="/assets/js/72.f561b5cf.js"><link rel="prefetch" href="/assets/js/73.d1c67ae7.js"><link rel="prefetch" href="/assets/js/74.3725d6d7.js"><link rel="prefetch" href="/assets/js/76.b8a53386.js"><link rel="prefetch" href="/assets/js/77.e04be33a.js"><link rel="prefetch" href="/assets/js/78.17c06f28.js"><link rel="prefetch" href="/assets/js/79.3ff2fb40.js"><link rel="prefetch" href="/assets/js/8.248194f5.js"><link rel="prefetch" href="/assets/js/80.96d575dc.js"><link rel="prefetch" href="/assets/js/81.770ca2a4.js"><link rel="prefetch" href="/assets/js/82.8900493c.js"><link rel="prefetch" href="/assets/js/83.4716d242.js"><link rel="prefetch" href="/assets/js/84.5ae9874f.js"><link rel="prefetch" href="/assets/js/85.924e7d3d.js"><link rel="prefetch" href="/assets/js/86.5073d16d.js"><link rel="prefetch" href="/assets/js/87.1148acd0.js"><link rel="prefetch" href="/assets/js/88.629d8beb.js"><link rel="prefetch" href="/assets/js/89.36b4cdb0.js"><link rel="prefetch" href="/assets/js/9.85151a25.js"><link rel="prefetch" href="/assets/js/90.fac275aa.js"><link rel="prefetch" href="/assets/js/91.93484ec9.js"><link rel="prefetch" href="/assets/js/92.01e42c63.js"><link rel="prefetch" href="/assets/js/93.50a55dd7.js"><link rel="prefetch" href="/assets/js/94.6a3f96c8.js"><link rel="prefetch" href="/assets/js/95.7dd7befb.js"><link rel="prefetch" href="/assets/js/96.38d049b2.js"><link rel="prefetch" href="/assets/js/97.a129eb48.js"><link rel="prefetch" href="/assets/js/98.f848c937.js"><link rel="prefetch" href="/assets/js/99.2dc3271b.js">
    <link rel="stylesheet" href="/assets/css/0.styles.027adcb0.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/C-logo.png" alt="clivia‘s blog" class="logo"> <span class="site-name can-hide">clivia‘s blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/bigdata/" class="nav-link">大数据</a></div><div class="nav-item"><a href="/technology/" class="nav-link">技术</a></div><div class="nav-item"><a href="/project/" class="nav-link">项目</a></div><div class="nav-item"><a href="/more/" class="nav-link">更多</a></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div><div class="nav-item"><a href="/pages/beb6c0bd8a66cea6/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://gitee.com/kflys/clivia-blog" target="_blank" rel="noopener noreferrer" class="repo-link">
    Gitee
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="/img/logo.png"> <div class="blogger-info"><h3>clivia’s blog</h3> <span>专注于后端开发，致力于简洁知识。</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/bigdata/" class="nav-link">大数据</a></div><div class="nav-item"><a href="/technology/" class="nav-link">技术</a></div><div class="nav-item"><a href="/project/" class="nav-link">项目</a></div><div class="nav-item"><a href="/more/" class="nav-link">更多</a></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div><div class="nav-item"><a href="/pages/beb6c0bd8a66cea6/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://gitee.com/kflys/clivia-blog" target="_blank" rel="noopener noreferrer" class="repo-link">
    Gitee
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Hadoop基础理论</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/479df0/" class="sidebar-link">Hadoop简介</a></li><li><a href="/pages/eeff69/" aria-current="page" class="active sidebar-link">Hadoop之HDFS详解</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/pages/eeff69/#hdfs读写流程" class="sidebar-link">HDFS读写流程</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/pages/eeff69/#hadoop-ha高可用" class="sidebar-link">Hadoop HA高可用</a></li><li class="sidebar-sub-header"><a href="/pages/eeff69/#hadoop联邦" class="sidebar-link">Hadoop联邦</a></li><li class="sidebar-sub-header"><a href="/pages/eeff69/#文件压缩" class="sidebar-link">文件压缩</a></li><li class="sidebar-sub-header"><a href="/pages/eeff69/#小文件治理" class="sidebar-link">小文件治理</a></li><li class="sidebar-sub-header"><a href="/pages/eeff69/#文件快照" class="sidebar-link">文件快照</a></li></ul></li><li class="sidebar-sub-header"><a href="/pages/eeff69/#计算机知识" class="sidebar-link">计算机知识</a></li></ul></li><li><a href="/pages/1c6825/" class="sidebar-link">Hadoop之MapReduce详解</a></li><li><a href="/pages/975721/" class="sidebar-link">Hadoop架构原理Yarn</a></li></ul></section></li></ul> <div class="sidebar-slot sidebar-slot-bottom"><!-- 正方形 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="3508773082"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div></aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-70a2d273><div class="articleInfo" data-v-70a2d273><ul class="breadcrumbs" data-v-70a2d273><li data-v-70a2d273><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-70a2d273></a></li> <li data-v-70a2d273><a href="/categories/?category=%E3%80%8AHadoop%E3%80%8B%E7%AC%94%E8%AE%B0" title="分类" data-v-70a2d273>《Hadoop》笔记</a></li> <li data-v-70a2d273><a href="/categories/?category=Hadoop%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA" title="分类" data-v-70a2d273>Hadoop基础理论</a></li> <!----></ul> <div class="info" data-v-70a2d273><div title="作者" class="author iconfont icon-touxiang" data-v-70a2d273><a href="https://gitee.com/kflys" target="_blank" title="作者" class="beLink" data-v-70a2d273>kflys</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-70a2d273><a href="javascript:;" data-v-70a2d273>2021-09-29</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">
          Hadoop之HDFS详解
        </h1> <div class="page-slot page-slot-top"><!-- 固定100% * 90px可显示，max-height:90px未见显示-->
     <ins class="adsbygoogle"
          style="display:inline-block;width:100%;max-height:90px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6625304284"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="theme-vdoing-content content__default"><h1 id="hdfs分布式文件系统"><a href="#hdfs分布式文件系统" class="header-anchor">#</a> HDFS分布式文件系统</h1> <h2 id="hdfs读写流程"><a href="#hdfs读写流程" class="header-anchor">#</a> HDFS读写流程</h2> <h4 id="数据写流程"><a href="#数据写流程" class="header-anchor">#</a> 数据写流程</h4> <p><img src="http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/HDFS%E5%86%99%E5%85%A5%E6%96%87%E4%BB%B6%E6%B5%81%E7%A8%8B.png#height=1054&amp;id=IjL9Z&amp;originHeight=1054&amp;originWidth=1112&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=1112" alt=""></p> <p><strong>详细流程</strong></p> <ul><li>创建文件：
<ul><li>HDFS客户端向HDFS写数据，先调用DistributedFileSystem.create()方法，在HDFS创建新的空文件</li> <li>RPC（ClientProtocol.create()）远程过程调用NameNode（NameNodeRpcServer）的create()，首先在HDFS目录树指定路径添加新文件</li> <li>然后将创建新文件的操作记录在editslog中</li> <li>NameNode.create方法执行完后，DistributedFileSystem.create()返回FSDataOutputStream，它本质是封装了一个DFSOutputStream对象</li></ul></li> <li>建立数据流管道：
<ul><li>客户端调用DFSOutputStream.write()写数据</li> <li>DFSOutputStream调用ClientProtocol.addBlock()，首先向NameNode申请一个空的数据块</li> <li>addBlock()返回LocatedBlock对象，对象包含当前数据块的所有datanode的位置信息</li> <li>根据位置信息，建立数据流管道</li></ul></li> <li>向数据流管道pipeline中写当前块的数据：
<ul><li>客户端向流管道中写数据，先将数据写入一个检验块chunk中，大小512Byte，写满后，计算chunk的检验和checksum值（4Byte）</li> <li>然后将chunk数据本身加上checksum，形成一个带checksum值的chunk（516Byte）</li> <li>保存到一个更大一些的结构<strong>packet数据包</strong>中，packet为64kB大小</li></ul></li> <li>packet写满后，先被写入一个<strong>dataQueue</strong>队列中
<ul><li>packet被从队列中取出，向pipeline中写入，先写入datanode1，再从datanoe1传到datanode2，再从datanode2传到datanode3中</li></ul></li> <li>一个packet数据取完后，后被放入到<strong>ackQueue</strong>中等待pipeline关于该packet的ack的反馈
<ul><li>每个packet都会有ack确认包，逆pipeline（dn3 -&gt; dn2 -&gt; dn1）传回输出流</li></ul></li> <li>若packet的ack是SUCCESS成功的，则从ackQueue中，将packet删除；否则，将packet从ackQueue中取出，重新放入dataQueue，重新发送
<ul><li>如果当前块写完后，文件还有其它块要写，那么再调用addBlock方法（<strong>流程同上</strong>）</li></ul></li> <li>文件最后一个block块数据写完后，会再发送一个空的packet，表示当前block写完了，然后关闭pipeline
<ul><li>所有块写完，close()关闭流</li></ul></li> <li>ClientProtocol.complete()通知namenode当前文件所有块写完了</li></ul> <p><strong>容错</strong></p> <ul><li>在写的过程中，pipeline中的datanode出现故障（如网络不通），输出流如何恢复
<ul><li>输出流中ackQueue缓存的所有packet会被重新加入dataQueue</li> <li>输出流调用ClientProtocol.updateBlockForPipeline()，为block申请一个新的时间戳，namenode会记录新时间戳</li> <li>确保故障datanode即使恢复，但由于其上的block时间戳与namenode记录的新的时间戳不一致，故障datanode上的block进而被删除</li> <li>故障的datanode从pipeline中删除</li> <li>输出流调用ClientProtocol.getAdditionalDatanode()通知namenode分配新的datanode到数据流pipeline中，并使用新的时间戳建立pipeline</li> <li>新添加到pipeline中的datanode，目前还没有存储这个新的block，HDFS客户端通过DataTransferProtocol通知pipeline中的一个datanode复制这个block到新的datanode中</li> <li>pipeline重建后，输出流调用ClientProtocol.updatePipeline()，更新namenode中的元数据</li> <li>故障恢复完毕，完成后续的写入流程</li></ul></li></ul> <h4 id="数据读流程"><a href="#数据读流程" class="header-anchor">#</a> 数据读流程</h4> <p><img src="http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/HDFS%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B.png#height=938&amp;id=kBFrQ&amp;originHeight=938&amp;originWidth=1003&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=1003" alt=""></p> <ul><li>1、client端读取HDFS文件，client调用文件系统对象DistributedFileSystem的open方法</li> <li>2、返回FSDataInputStream对象（对DFSInputStream的包装）</li> <li>3、构造DFSInputStream对象时，调用namenode的getBlockLocations方法，获得file的开始若干block（如blk1, blk2, blk3, blk4）的存储datanode（以下简称dn）列表；针对每个block的dn列表，会根据网络拓扑做排序，离client近的排在前；</li> <li>4、调用DFSInputStream的read方法，先读取blk1的数据，与client最近的datanode建立连接，读取数据</li> <li>5、读取完后，关闭与dn建立的流</li> <li>6、读取下一个block，如blk2的数据（重复步骤4、5、6）</li> <li>7、这一批block读取完后，再读取下一批block的数据（重复3、4、5、6、7）</li> <li>8、完成文件数据读取后，调用FSDataInputStream的close方法</li></ul> <p><strong>容错</strong></p> <ul><li>情况一：读取block过程中，client与datanode通信中断
<ul><li>client与存储此block的第二个datandoe建立连接，读取数据</li> <li>记录此有问题的datanode，不会再从它上读取数据</li></ul></li> <li>情况二：client读取block，发现block数据有问题
<ul><li>client读取block数据时，同时会读取到block的校验和，若client针对读取过来的block数据，计算检验和，其值与读取过来的校验和不一样，说明block数据损坏</li> <li>client从存储此block副本的其它datanode上读取block数据（也会计算校验和）</li> <li>同时，client会告知namenode此情况；</li></ul></li></ul> <h3 id="hadoop-ha高可用"><a href="#hadoop-ha高可用" class="header-anchor">#</a> Hadoop HA高可用</h3> <h4 id="hdfs高可用原理"><a href="#hdfs高可用原理" class="header-anchor">#</a> HDFS高可用原理</h4> <p><img src="http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201905211519.png#height=525&amp;id=thf7s&amp;originHeight=525&amp;originWidth=991&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=991" alt=""></p> <ul><li>对于HDFS ，NN存储元数据在内存中，并负责管理文件系统的命名空间和客户端对HDFS的读写请求。但是，如果只存在一个NN，一旦发生“单点故障”，会使整个系统失效。</li> <li>虽然有个SNN，但是它并不是NN的热备份</li> <li>因为SNN无法提供“热备份”功能，在NN故障时，无法立即切换到SNN对外提供服务，即HDFS处于停服状态。</li> <li>HDFS2.x采用了HA（High Availability高可用）架构。
<ul><li>在HA集群中，可设置两个NN，一个处于“活跃（Active）”状态，另一个处于“待命（Standby）”状态。</li> <li>由zookeeper确保一主一备（讲zookeeper时具体展开）</li> <li>处于Active状态的NN负责响应所有客户端的请求，处于Standby状态的NN作为热备份节点，保证与active的NN的元数据同步</li> <li>Active节点发生故障时，zookeeper集群会发现此情况，通知Standby节点立即切换到活跃状态对外提供服务</li> <li>确保集群一直处于可用状态</li></ul></li> <li>如何热备份元数据：
<ul><li>Standby NN是Active NN的“热备份”，因此Active NN的状态信息必须实时同步到StandbyNN。</li> <li>可借助一个共享存储系统来实现状态同步，如NFS(NetworkFile System)、QJM(Quorum Journal Manager)或者Zookeeper。</li> <li>Active NN将更新数据写入到共享存储系统，Standby NN一直监听该系统，一旦发现有新的数据写入，就立即从公共存储系统中读取这些数据并加载到Standby NN自己内存中，从而保证元数据与Active NN状态一致。</li></ul></li> <li>块报告：
<ul><li>NN保存了数据块到实际存储位置的映射信息，为了实现故障时的快速切换，必须保证StandbyNN中也包含最新的块映射信息</li> <li>因此需要给所有DN配置Active和Standby两个NN的地址，把块的位置和心跳信息同时发送到两个NN上。</li></ul></li></ul> <h3 id="hadoop联邦"><a href="#hadoop联邦" class="header-anchor">#</a> Hadoop联邦</h3> <h4 id="为什么需要联邦"><a href="#为什么需要联邦" class="header-anchor">#</a> 为什么需要联邦</h4> <ul><li>虽然HDFS HA解决了“单点故障”问题，但HDFS在扩展性、整体性能和隔离性方面仍有问题
<ul><li>系统扩展性方面，元数据存储在NN内存中，受限于内存上限（每个文件、目录、block占用约150字节）</li> <li>整体性能方面，吞吐量受单个NN的影响</li> <li>隔离性方面，一个程序可能会影响其他程序的运行，如果一个程序消耗过多资源会导致其他程序无法顺利运行</li> <li>HDFS HA本质上还是单名称节点</li></ul></li></ul> <h4 id="联邦"><a href="#联邦" class="header-anchor">#</a> 联邦</h4> <p><img src="http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201909041239.png#height=531&amp;id=m0sh1&amp;originHeight=531&amp;originWidth=878&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=878" alt=""></p> <ul><li>HDFS联邦可以解决以上三个问题
<ul><li>HDFS联邦中，设计了多个命名空间；每个命名空间有一个NN或一主一备两个NN，使得HDFS的命名服务能够水平扩展</li> <li>这些NN分别进行各自命名空间namespace和块的管理，相互独立，不需要彼此协调</li> <li>每个DN要向集群中所有的NN注册，并周期性的向所有NN发送心跳信息和块信息，报告自己的状态</li> <li>HDFS联邦每个相互独立的NN对应一个独立的命名空间</li> <li>每一个命名空间管理属于自己的一组块，这些属于同一命名空间的块对应一个“块池”的概念。</li> <li>每个DN会为所有块池提供块的存储，块池中的各个块实际上是存储在不同DN中的</li></ul></li></ul> <p><a href="https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="noopener noreferrer">联邦-官网<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h3 id="文件压缩"><a href="#文件压缩" class="header-anchor">#</a> 文件压缩</h3> <h4 id="压缩算法"><a href="#压缩算法" class="header-anchor">#</a> 压缩算法</h4> <ul><li><p>文件压缩好处：</p> <ul><li>减少数据所占用的磁盘空间</li> <li>加快数据在磁盘、网络上的IO</li></ul></li> <li><p>常用压缩格式
| 压缩格式 | UNIX工具 | 算      法 | 文件扩展名 | 可分割 |
| --- | --- | --- | --- | --- |
| DEFLATE | 无 | DEFLATE | .deflate | No |
| gzip | gzip | DEFLATE | .gz | No |
| zip | zip | DEFLATE | .zip | YES |
| bzip | bzip2 | bzip2 | .bz2 | YES |
| LZO | lzop | LZO | .lzo | No |
| Snappy | 无 | Snappy | .snappy | No |</p></li> <li><p>Hadoop的压缩实现类；均实现CompressionCodec接口
| 压缩格式 | 对应的编码/解码器 |
| --- | --- |
| DEFLATE | org.apache.hadoop.io.compress.DefaultCodec |
| gzip | org.apache.hadoop.io.compress.GzipCodec |
| bzip2 | org.apache.hadoop.io.compress.BZip2Codec |
| LZO | com.hadoop.compression.lzo.LzopCodec |
| Snappy | org.apache.hadoop.io.compress.SnappyCodec |</p></li> <li><p>查看集群是否支持本地压缩（所有节点都要确认）</p></li></ul> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token punctuation">[</span>hadoop@node01 ~<span class="token punctuation">]</span>$ hadoop checknative
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>编程：案例</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>// 压缩类型
BZip2Codec codec = new BZip2Codec();
codec.setConf(configuration);
//调用Filesystem的create方法返回的是FSDataOutputStream对象
//该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加
// 获取文件系用的输出流
OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));
// 对输出流进行压缩
CompressionOutputStream compressionOut = codec.createOutputStream(outputStreamTarget);
// 将文件输入流，写入输入流
IOUtils.copyBytes(inputStreamSourceFile,compressionOut,4069,true);
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><ul><li><a href="https://blog.csdn.net/qq_38262266/article/details/79171524" target="_blank" rel="noopener noreferrer">HDFS文件压缩<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h3 id="小文件治理"><a href="#小文件治理" class="header-anchor">#</a> 小文件治理</h3> <ul><li>NameNode存储着文件系统的元数据，每个文件、目录、块大概有150字节的元数据；</li> <li>因此文件数量的限制也由NN内存大小决定，如果小文件过多则会造成NN的压力过大</li> <li>且HDFS能存储的数据总量也会变小</li></ul> <h4 id="har文件方案"><a href="#har文件方案" class="header-anchor">#</a> HAR文件方案</h4> <ul><li>本质启动mr程序，所以需要启动yarn(手动压缩文件)</li></ul> <p><img src="http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/1558004541101.png#height=200&amp;id=R40yD&amp;originHeight=200&amp;originWidth=522&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=522" alt=""></p> <p>用法：</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>archive -archiveName <span class="token operator">&lt;</span>NAME<span class="token operator">&gt;</span>.har -p <span class="token operator">&lt;</span>parent path<span class="token operator">&gt;</span> <span class="token punctuation">[</span>-r <span class="token operator">&lt;</span>replication factor<span class="token operator">&gt;</span><span class="token punctuation">]</span><span class="token operator">&lt;</span>src<span class="token operator">&gt;</span>* <span class="token operator">&lt;</span>dest<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token comment"># 创建archive文件；/testhar有两个子目录th1、th2；两个子目录中有若干文件</span>
hadoop archive -archiveName test.har -p /testhar -r <span class="token number">3</span> th1 th2 /outhar <span class="token comment"># 原文件还存在，需手动删除</span>

<span class="token comment"># 查看archive文件</span>
hdfs dfs -ls -R har:///outhar/test.har

<span class="token comment"># 解压archive文件</span>
<span class="token comment"># 方式一</span>
hdfs dfs -cp har:///outhar/test.har/th1 hdfs:/unarchivef <span class="token comment"># 顺序</span>
hadoop fs -ls /unarchivef	
<span class="token comment"># 方式二</span>
hadoop distcp har:///outhar/test.har/th1 hdfs:/unarchivef2 <span class="token comment"># 并行，启动MR</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><h4 id="sequence-files方案"><a href="#sequence-files方案" class="header-anchor">#</a> Sequence Files方案</h4> <ul><li>SequenceFile文件，主要由一条条record记录组成；每个record是键值对形式的</li> <li>SequenceFile文件可以作为小文件的存储容器；
<ul><li>每条record保存一个小文件的内容</li> <li>小文件名作为当前record的键；</li> <li>小文件的内容作为当前record的值；</li> <li>如10000个100KB的小文件，可以编写程序将这些文件放到一个SequenceFile文件。</li></ul></li> <li>一个SequenceFile是<strong>可分割</strong>的，所以MapReduce可将文件切分成块，每一块独立操作。</li> <li>具体结构（如下图）：
<ul><li>一个SequenceFile首先有一个4字节的header（文件版本号）</li> <li>接着是若干record记录</li> <li>记录间会随机的插入一些同步点sync marker，用于方便定位到记录边界</li></ul></li> <li>不像HAR，SequenceFile<strong>支持压缩</strong>。记录的结构取决于是否启动压缩
<ul><li>支持两类压缩：
<ul><li>不压缩NONE，如下图</li> <li>压缩RECORD，如下图</li> <li>压缩BLOCK，①一次性压缩多条记录；②每一个新块Block开始处都需要插入同步点；如下图</li></ul></li> <li>在大多数情况下，以block（注意：指的是SequenceFile中的block）为单位进行压缩是最好的选择</li> <li>因为一个block包含多条记录，利用record间的相似性进行压缩，压缩效率更高</li> <li>把已有的数据转存为SequenceFile比较慢。比起先写小文件，再将小文件写入SequenceFile，一个更好的选择是直接将数据写入一个SequenceFile文件，省去小文件作为中间媒介.</li></ul></li></ul> <p><img src="http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201907101934.png#height=463&amp;id=Jz6bk&amp;originHeight=463&amp;originWidth=855&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=855" alt=""></p> <p><img src="http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201907101935.png#height=308&amp;id=onoRV&amp;originHeight=308&amp;originWidth=704&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=704" alt=""></p> <ul><li>向SequenceFile写入数据</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>//1. 创建向SequenceFile文件写入数据时的一些选项
//2. 要写入的SequenceFile的路径
SequenceFile.Writer.Option pathOption = SequenceFile.Writer.file(path);
//3. record的key类型选项
SequenceFile.Writer.Option keyOption = SequenceFile.Writer.keyClass(IntWritable.class);
//4. record的value类型选项
SequenceFile.Writer.Option valueOption = SequenceFile.Writer.valueClass(Text.class);
// SequenceFile压缩方式：NONE | RECORD | BLOCK三选一
// 方案一：RECORD、不指定压缩算法
SequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD);
SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);


// 方案二：BLOCK、不指定压缩算法
SequenceFile.Writer.Option compressOption = SequenceFile.Writer.compression(SequenceFile.CompressionType.BLOCK);
SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);



// 方案三：使用BLOCK、压缩算法BZip2Codec；压缩耗时间 再加压缩算法
BZip2Codec codec = new BZip2Codec();
codec.setConf(conf);
SequenceFile.Writer.Option compressAlgorithm = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD, codec);
// 创建写数据的Writer实例
SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressAlgorithm);


// 填充小文件数据
for (int i = 0; i &lt; 100000; i++) {
  //分别设置key、value值
  key.set(100 - i);
  value.set(DATA[i % DATA.length]);
  writer.append(key, value);
}

// 关闭流
IOUtils.closeStream(writer);
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><ul><li>命令查看SequenceFile内容</li></ul> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code> hadoop fs -text /writeSequenceFile
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>读取SequenceFile文件</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>//1. 读取SequenceFile的Reader的路径选项
SequenceFile.Reader.Option pathOption = SequenceFile.Reader.file(path);
//2. 实例化Reader对象
reader = new SequenceFile.Reader(conf, pathOption);
//3. 根据反射，求出key类型
Writable key = (Writable)
  ReflectionUtils.newInstance(reader.getKeyClass(), conf);
//根据反射，求出value类型
Writable value = (Writable)
  ReflectionUtils.newInstance(reader.getValueClass(), conf);

long position = reader.getPosition();

while (reader.next(key, value)) {
  String syncSeen = reader.syncSeen() ? &quot;*&quot; : &quot;&quot;;
  System.out.printf(&quot;[%s%s]\t%s\t%s\n&quot;, position, syncSeen, key, value);
  position = reader.getPosition(); // beginning of next record
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><h3 id="文件快照"><a href="#文件快照" class="header-anchor">#</a> 文件快照</h3> <h4 id="什么是快照"><a href="#什么是快照" class="header-anchor">#</a> 什么是快照</h4> <ul><li>快照比较常见的应用场景是数据备份，以防一些用户错误或灾难恢复</li> <li>快照snapshots是HDFS文件系统的，只读的、某时间点的拷贝</li> <li>可以针对<strong>某个目录</strong>，或者<strong>整个文件系统</strong>做快照</li> <li>创建快照时，block块并不会被拷贝。快照文件中只是记录了block列表和文件大小，<strong>不会做任何数据拷贝</strong></li></ul> <h4 id="快照操作"><a href="#快照操作" class="header-anchor">#</a> 快照操作</h4> <ul><li>允许快照
允许一个快照目录被创建。如果这个操作成功完成，这个目录就变成snapshottable
用法：hdfs dfsadmin -allowSnapshot</li></ul> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>hdfs dfsadmin -allowSnapshot /wordcount
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>禁用快照
用法：hdfs dfsadmin -disallowSnapshot</li></ul> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>hdfs dfsadmin -disallowSnapshot /wordcount
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>创建快照
用法：hdfs dfs -createSnapshot  []</li></ul> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token comment">#注意：先将/wordcount目录变成允许快照的</span>
hdfs dfs -createSnapshot /wordcount wcSnapshot
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><ul><li>查看快照</li></ul> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>hdfs dfs -ls /wordcount/.snapshot
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>重命名快照
这个操作需要拥有snapshottabl目录所有者权限
用法：hdfs dfs -renameSnapshot</li></ul> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>hdfs dfs -renameSnapshot /wordcount wcSnapshot newWCSnapshot
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>用快照恢复误删除数据
HFDS的/wordcount目录，文件列表如下
<img src="http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201909041356.png#height=189&amp;id=zQZ4c&amp;originHeight=189&amp;originWidth=1275&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=1275" alt="">
误删除/wordcount/edit.xml文件</li></ul> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>hadoop fs -rm /wordcount/edit.xml
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p><img src="http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201909041400.png#height=190&amp;id=UXWii&amp;originHeight=190&amp;originWidth=1631&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=1631" alt=""></p> <p>恢复数据</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>  hadoop fs -cp /wordcount/.snapshot/newWCSnapshot/edit.xml /wordcount
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>删除快照
这个操作需要拥有snapshottabl目录所有者权限
用法：hdfs dfs -deleteSnapshot</li></ul> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>hdfs dfs -deleteSnapshot /wordcount newWCSnapshot
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h2 id="计算机知识"><a href="#计算机知识" class="header-anchor">#</a> 计算机知识</h2> <ol><li>HDFS存储地位</li> <li><strong>block块为什么设置的比较大</strong></li></ol> <ul><li><a href="https://www.cnblogs.com/jswang/p/9071847.html" target="_blank" rel="noopener noreferrer">磁盘基础知识<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <ul><li>盘片platter、磁头head、磁道track、扇区sector、柱面cylinder</li> <li>为了最小化寻址开销；从磁盘传输数据的时间明显大于定位这个块开始位置所需的时间</li></ul></li> <li>问：块的大小是不是设置的越大越好呢？
1、 不是，寻址的时间大概是 100ms，设计一般设置为寻址时间占用十分之一，也就是一秒。 硬盘的传输速录大概是100m/s 一秒大概为100M，最接近100的大小为128M。</li></ul> <p><img src="http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201906211143.png#height=704&amp;id=cp2i5&amp;originHeight=704&amp;originWidth=1545&amp;originalType=binary&amp;ratio=1&amp;status=done&amp;style=none&amp;width=1545" alt=""></p></div></div> <div class="page-slot page-slot-bottom"><!-- 横向自适应 -->
      <ins class="adsbygoogle"
          style="display:block"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="6620245489"
          data-ad-format="auto"
          data-full-width-responsive="true"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script></div> <div class="page-edit"><div class="edit-link"><a href="https://gitee.com/kflys/clivia-blog/edit/master/docs/《Hadoop》笔记/10.Hadoop基础理论/02.Hadoop之HDFS详解.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <div class="tags"><a href="/tags/?tag=hadoop" title="标签">#hadoop</a><a href="/tags/?tag=hdfs" title="标签">#hdfs</a></div> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">10/11/2021, 5:53:41 PM</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/479df0/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Hadoop简介</div></a> <a href="/pages/1c6825/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Hadoop之MapReduce详解</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/479df0/" class="prev">Hadoop简介</a></span> <span class="next"><a href="/pages/1c6825/">Hadoop之MapReduce详解</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/pages/8e0b33/"><div>阿里官方Redis开发规范</div></a> <span>10-08</span></dt></dl><dl><dd>02</dd> <dt><a href="/pages/20a714/"><div>无监督学习</div></a> <span>01-26</span></dt></dl><dl><dd>03</dd> <dt><a href="/pages/e78220/"><div>模拟保存预加载</div></a> <span>01-26</span></dt></dl> <dl><dd></dd> <dt><a href="/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="mailto:clivia.pro@gmail.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/kflys" title="Gitee" target="_blank" class="iconfont icon-gitee"></a><a href="https://y.qq.com/n/ryqq/songDetail/003lgoEG1SWHmF" title="music" target="_blank" class="iconfont icon-erji"></a></div> <div><span> Theme by <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> | Powered by <a href="https://webify.cloudbase.net/" target="_blank">CloudBase Webify</a></span></div> <span>Clivia‘s <a href="https://gitee.com/kflys/clivia-blog" target="_blank">blog</a> </span>
    | Copyright © 2019-2022
    <span><a href="http://beian.miit.gov.cn/" target="_blank">皖ICP备2021014093号</a> </span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <div class="custom-html-window custom-html-window-rb" style="display:;"><div class="custom-wrapper"><i class="close-but">×</i> <div><!-- 固定160*160px -->
      <ins class="adsbygoogle"
          style="display:inline-block;max-width:160px;max-height:160px"
          data-ad-client="ca-pub-7828333725993554"
          data-ad-slot="8377369658"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script>
      </div></div></div></div><div class="global-ui"><div id="live2d-widget" class="live2d-widget-container" style="position:fixed;left:65px;bottom:0px;width:135px;height:300px;z-index:99999;opacity:0.8;pointer-events:none;"><!----></div></div></div>
    <script src="/assets/js/app.06f86fe0.js" defer></script><script src="/assets/js/2.ed69dcfc.js" defer></script><script src="/assets/js/3.b6cd915d.js" defer></script><script src="/assets/js/75.310c7556.js" defer></script>
  </body>
</html>