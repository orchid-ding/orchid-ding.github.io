[{"url":"/2019/10/10/Java大数据/Java大数据基础概论/","content":"## 大数据概论\n\n> 概念： 大数据（big data）是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产\n\n| 数据单位 | B    | KB   | MB   | GB   | PE   | PB   | EB   | ZB   | YB   |\n| -------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| 基数     |      | 2    | 2    | 2    | 2    | 2    | 2    | 10   | 10   |\n| 次方     | 0    | 10   | 20   | 30   | 40   | 50   | 60   | 21   | 24   |\n\n###一、大数据特性\n\n1. 数据量大（Volume） \n2. 类型繁多（Variety） \n3. 价值密度低（Value） \n4. 速度快时效高（Velocity）\n\n###二、大数据的挑战\n\n1. 存储： 每天几TB、GB的数据增量，并且还在持续的增长中。\n2. 分析： 如何从巨大的数据中挖掘出隐藏的商业价值。\n3. 管理： 如何快速构建并且保证系统的安全、简单可用。\n\n### 三、传统的大数据项目流程\n\n```flow\nst=>start: 开始\ndataCollect=>operation: 数据收集 ： Flume、Kafaka、Scribe\ndataStore=>operation: 数据存储 ： HDFS、HBase、Cassadra\ndataCaculate=>operation: 数据计算 : Mapreduce、Strom、Impala、Spark、Spark Streaming...\n数据计算三大类：\n1、离线处理平台： Spark、Spark Core\n2、交互式处理平台： Spark SQL、Hive 、Impala\n3、流处理平台 ： Strom、Spring Stoeaming 、 Flink\ndataAnalyse=>operation: 分析与挖掘 ： Mahour、R语言、Hive、Pig\ndataEtl=>operation: ETL ： sqoop、DataX\ndataView=>operation: 可视化 ： Echarts.js 、 E3.js、 数据报表系统\ndataActual=>operation: 项目实战\ne=>end: 结束\n\nst->dataCollect->dataStore->dataCaculate->dataAnalyse->dataEtl->dataView->dataActual->e\n\n```\n\n## 分布式文件系统\n\n###一、Hadoop简介\n\n1. Hadoop架构\n\n   ![Image201906191834](/Users/dingchuangshi/Documents/Java大数据课件/第八章HDFS课件/20191009-HDFS-第一次/assets/Image201906191834.png)\n\n2. Hadoop历史\n\n   ![Image201906202055](/Users/dingchuangshi/Documents/Java大数据课件/第八章HDFS课件/20191009-HDFS-第一次/assets/Image201906202055.png)\n\n### 二、HDFS是\n\n- HDFS是Hadoop中的一个存储子模块\n- HDFS (全称Hadoop Distributed File System)，即hadoop的分布式文件系统\n- File System**文件系统**：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读出、修改、转储、删除文件等\n- 当数据集大小超出一台计算机的存储能力时，就有必要将它拆分成若干部分，然后分散到不同的计算机中存储。管理网络中跨多台计算机存储的文件系统称之为**分布式文件系统**（distributed filesystem）\n\n#### 2.1 HDFS特点\n\n**2.1.1 优点：**\n\n- 适合存储大文件，能用来存储管理PB级的数据；不适合存储小文件\n- 处理非结构化数据\n- 流式的访问数据，一次写入、多次读写\n- 运行于廉价的商用机器集群上，成本低\n- 高容错：故障时能继续运行且不让用户察觉到明显的中断\n- 可扩展\n\n![](/Users/dingchuangshi/Documents/Java大数据课件/第八章HDFS课件/20191009-HDFS-第一次/assets/Image201907081216.png)\n\n**2.1.2 局限性**\n\n- 不适合处理低延迟数据访问\n  - DFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的\n  - 对于低延时的访问需求，HBase是更好的选择\n- 无法高效存储大量的小文件\n  - 小文件会给Hadoop的扩展性和性能带来严重问题\n  - 利用SequenceFile、MapFile等方式归档小文件\n- 不支持多用户写入及任意修改文件\n  - 文件有一个写入者，只能执行追加操作\n  - 不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改\n\n#### 2.2 HDFS常用命令\n\n> HDFS两种命令风格，两种命令效果等同\n>\n> hadoop fs / hdfs dfs\n\n![image-20191010155353956](/Users/dingchuangshi/Library/Application Support/typora-user-images/image-20191010155353956.png)\n\n\n\n1. 如何查看hdfs或hadoop子命令的**帮助信息**，如ls子命令\n\n   ```shell\n   hdfs dfs -help ls\n   hadoop fs -help ls\t#两个命令等价\n   ```\n\n2. **查看**hdfs文件系统中已经存在的文件。对比linux命令ls\n\n   ```shell\n   hdfs dfs -ls /\n   hadoop fs -ls /\n   ```\n\n3. 在hdfs文件系统中创建文件\n\n   ```shell\n   hdfs dfs -touchz /edits.txt\n   ```\n\n4. 向HDFS文件中追加内容\n\n    ```shell\n    hadoop fs -appendToFile edit1.xml /edits.txt #将本地磁盘当前目录的edit1.xml内容追加到HDFS根目录 的edits.txt文件\n    ```\n\n5. 查看HDFS文件内容\n\n    ```shell\n    hdfs dfs -cat /edits.txt\n    ```\n\n6. **从本地路径上传文件至HDFS**\n\n    ````` shell\n    #用法：hdfs dfs -put /本地路径 /hdfs路径\n    hdfs dfs -put hadoop-2.7.3.tar.gz /\n    hdfs dfs -copyFromLocal hadoop-2.7.3.tar.gz /  #根put作用一样\n    hdfs dfs -moveFromLocal hadoop-2.7.3.tar.gz /  #根put作用一样，只不过，源文件被拷贝成功后，会被删除\n    `````\n\n7. **在hdfs文件系统中下载文件**\n\n     ```shell\n     hdfs dfs -get /hdfs路径 /本地路径\n     hdfs dfs -copyToLocal /hdfs路径 /本地路径  #根get作用一样\n     ```\n\n8. 在hdfs文件系统中**创建目录**\n\n     ```shell\n     hdfs dfs -mkdir /shell\n     ```\n\n9. 在hdfs文件系统中**删除**文件\n\n     ```shell\n     hdfs dfs -rm /edits.txt\n     hdfs dfs -rm -r /shell\n     ```\n\n10. 在hdfs文件系统中**修改文件名称**（也可以用来**移动**文件到目录）\n\n     ```shell\n     hdfs dfs -mv /xcall.sh /call.sh\n     hdfs dfs -mv /call.sh /shell\n     ```\n\n11. 在hdfs中拷贝文件到目录\n\n      ```shell\n      hdfs dfs -cp /xrsync.sh /shell\n      ```\n\n12. 递归删除目录\n\n      ```shell\n      hdfs dfs -rmr /shell\n      ```\n\n13. 列出本地文件的内容（默认是hdfs文件系统）\n\n      ```shell\n      hdfs dfs -ls file:///home/bruce/\n      ```\n\n14. 查找文件\n\n      ```shell\n      # linux find命令\n      find . -name 'edit*'\n      \n      # HDFS find命令\n      hadoop fs -find / -name part-r-00000 # 在HDFS根目录中，查找part-r-00000文件\n      ```\n\n\n> 还有许多其他命令，大家可以自己探索一下   \n\n##### 2.2.1 命令行小结\n\n- 输入hadoop fs 或hdfs dfs，回车，查看所有的HDFS命令\n\n- 许多命令与linux命令有很大的相似性，学会举一反三\n- 有用的help，如查看ls命令的使用说明：hadoop fs -help ls\n\n#####2.2.2 hdfs与getconf结合使用\n\n1. 获取NameNode的节点名称（可能有多个）\n\n      ``````shell\n      hdfs getconf -namenodes\n      ``````\n\n2. 获取hdfs最小块信息\n\n      ``````shell\n      hdfs getconf -confKey dfs.namenode.fs-limits.min-block-size\n      ``````\n\n3. 查找hdfs的NameNode的RPC地址\n\n\t``````shell\n\thdfs getconf -nnRpcAddresses\n\t``````\n\t\n\t\n\n#####2.2.3 hdfs与dfsadmin结合使用\n\n1. 同样要学会借助帮助信息\n\n      ```shell\n      hdfs dfsadmin -help safemode\n      ```\n\n2. 查看hdfs dfsadmin的帮助信息\n\n      ``````shell\n      hdfs dfsadmin\n      ``````\n\n3. 查看当前的模式\n\n      ``````shell\n      hdfs dfsadmin -safemode get\n      ``````\n\n4. 进入安全模式\n\n  ``````shell\n  hdfs dfsadmin -safemode enter\n  ``````\n\n  \n\n#####2.2.4 hdfs与fsck结合使用\n\n1. fsck指令**显示HDFS块信息**\n\n\t``````shell\n\thdfs fsck /02-041-0029.mp4 -files -blocks -locations # 查看文件02-041-0029.mp4的块信息\n\t``````\n\t\n\t\n\n#####2.2.5 其他命令\n\n1. 检查压缩库本地安装情况\n\n      ``````shell\n      hadoop checknative\n      ``````\n\n2. 格式化名称节点（**慎用**，一般只在初次搭建集群，使用一次；格式化成功后，不要再使用）\n\n      ``````shell\n      hadoop namenode -format\n      ``````\n\n3. 执行自定义jar包\n\n   ``````shell\n   hadoop jar YinzhengjieMapReduce-1.0-SNAPSHOT.jar com.kaikeba.hadoop.WordCount /world.txt /out\n   ``````\n\n#### 2.3 HDFS编程\n\n\n- 1.向hdfs中,上传一个文本文件\n\n  ```java\n   /**\n       * 上传文件到服务器\n       *  传递参数\n       *  args[0] 本地文件路径\n       *  args[1] hdoop文件系统 路径\n       */\n      public static void uploadFileToFileSystem(String source,String targetUrl){\n          System.out.println(\"文件地址：\" + source);\n          System.out.println(\"目标服务器：\" + targetUrl);\n          InputStream inputStreamSourceFile = null;\n          try {\n              // 获取文件输入流\n              inputStreamSourceFile = new BufferedInputStream(new FileInputStream(source));\n              // HDFS 读写配置文件\n              Configuration configuration = new Configuration();\n              // 通过url 返回文件系统实例\n              FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);\n              //调用Filesystem的create方法返回的是FSDataOutputStream对象\n              //该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n              // 获取文件系用的输出流\n              OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n              // 将文件输入流，写入输入流\n              IOUtils.copyBytes(inputStreamSourceFile,outputStreamTarget,4069,true);\n              System.out.println(\"上传成功\");\n          } catch (FileNotFoundException e) {\n              System.err.println(e.getMessage());\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n      }\n  \n  ```\n\n- 2.读取hdfs上的文件\n\n```java\n\n    /**\n     * 从文件系统中读取文件\n     * @param source 需要读取的文件\n     * @return 读取文件内容\n     */\n    public static String readFileFromFileSystem(String source){\n        String result = null;\n        try {\n            // HDFS 读写文件配置\n            Configuration configuration = new Configuration();\n            // HDFS文件系统\n            FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n            // 文件输入流，用于读取文件\n            InputStream inputStream = fileSystem.open(new Path(source));\n            BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream));\n            result = readBufferReader(bufferedReader).toString();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return result;\n    }\n    \n    \n    /**\n     * 获取内容\n     * @param br\n     * @return\n     */\n    private static StringBuffer readBufferReader(BufferedReader br) throws IOException {\n        StringBuffer stringBuffer = new StringBuffer();\n        String temp = \"\";\n        while ((temp = br.readLine()) != null){\n            stringBuffer.append(temp);\n        }\n        return stringBuffer;\n    }\n```\n\n\n\n- 3.列出某一个文件夹下的所有文件\n\n```java\n\n    /**\n     * 列出当前目录下所有字文件名称\n     * @param source\n     * @return\n     */\n    public static String listAllFileChildren(String source){\n        StringBuffer stringBuffer = new StringBuffer();\n        try {\n            // HDFS 读写文件配置\n            Configuration configuration = new Configuration();\n            // HDFS文件系统\n            FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n            // recursive 继续深入遍历\n            RemoteIterator<LocatedFileStatus> iterator = fileSystem.listFiles(new Path(source),true);\n            while (iterator.hasNext()){\n                LocatedFileStatus fileStatus = iterator.next();\n                stringBuffer.append(fileStatus.getPath() + \"\\n\");\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return stringBuffer.toString();\n    }\n\n```\n\n\n\n- 4.列出多级目录名称和目录下的文件名称\n\n  ```java\n  \n      /**\n       * 递归列出当前目录下所有目录和文件名称\n       * @param source\n       * @return\n       */\n      public static String listAllChildren(String source){\n          StringBuffer stringBuffer = new StringBuffer();\n          try {\n              // HDFS 读写文件配置\n              Configuration configuration = new Configuration();\n              // HDFS文件系统\n              FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n              list(stringBuffer,fileSystem,new Path(source));\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n          return stringBuffer.toString();\n      }\n  \n      /**\n       * 递归目录和文件\n       * @param stringBuffer  文件目录名称集合\n       * @param fileSystem  hdfs 文件系统\n       * @param source path 路径\n       * @throws IOException\n       */\n      private static void list(StringBuffer stringBuffer,FileSystem fileSystem, Path source) throws IOException {\n          FileStatus[] iterator = fileSystem.listStatus(source);\n          for (FileStatus status:iterator) {\n              stringBuffer.append(status.getPath() + \"\\n\");\n              if(status.isDirectory()){\n                  list(stringBuffer,fileSystem,status.getPath());\n              }\n  \n          }\n      }\n  \n  \n  ```\n\n  \n\n\n"}]