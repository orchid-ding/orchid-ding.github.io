[{"title":"解决Hash冲突的几种方法","url":"/2020/04/16/it/java/解决Hash冲突的四种方法/","content":"\n## 解决Hash冲突的几种方法\n\n### Hash冲突介绍\n\n###  开放寻址法\n\n​\t\t这种方法也称再散列法，其基本思想是：当关键字key的哈希地址p=H（key）出现冲突时，以p为基础，产生另一个哈希地址p1，如果p1仍然冲突，再以p为基础，产生另一个哈希地址p2，…，直到找出一个不冲突的哈希地址pi ，将相应元素存入其中。这种方法有一个通用的再散列函数形式：\n\nHi=（H（key）+di）% m  i=1，2，…，n\n\n其中H（key）为哈希函数，m 为表长，di\n\n#### 显性探测再散列\n\n​\t\tdi=12，-12，22，-22，…，k2，-k2  ( k<=m/2 ) 这种方法的特点是：冲突发生时，在表的左右进行跳跃式探测，比较灵活。\n\n#### 二次探测再散列\n\ndi=12，-12，22，-22，…，k2，-k2  ( k<=m/2 )\n\n这种方法的特点是：冲突发生时，在表的左右进行跳跃式探测，比较灵活。\n\n#### 伪随机探测再散列 \n\ndi=伪随机数序列。\n\n具体实现时，应建立一个伪随机数发生器，（如i=(i+p) % m），并给定一个随机数做起点。\n\n### 再Hash法\n\n​\t\t这种方法是同时构造多个不同的哈希函数：\n\nHi=RH1（key） i=1，2，…，k\n\n当哈希地址Hi=RH1（key）发生冲突时，再计算Hi=RH2（key）……，直到冲突不再产生。这种方法不易产生聚集，但增加了计算时间。\n\n### 链地址法\n\n​\t\t这种方法的基本思想是将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。\n\n### 建立公共溢出区\n\n​\t\t这种方法的基本思想是：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。\n\n###  优缺点\n\n#####  开放散列（open hashing）/ 拉链法（针对桶链结构）\n\n1）优点： ①对于记录总数频繁可变的情况，处理的比较好（也就是避免了动态调整的开销） ②由于记录存储在结点中，而结点是动态分配，不会造成内存的浪费，所以尤其适合那种记录本身尺寸（size）很大的情况，因为此时指针的开销可以忽略不计了 ③删除记录时，比较方便，直接通过指针操作即可\n\n \n\n2）缺点： ①存储的记录是随机分布在内存中的，这样在查询记录时，相比结构紧凑的数据类型（比如数组），哈希表的跳转访问会带来额外的时间开销 ②如果所有的 key-value 对是可以提前预知，并之后不会发生变化时（即不允许插入和删除），可以人为创建一个不会产生冲突的完美哈希函数（perfect hash function），此时封闭散列的性能将远高于开放散列 ③由于使用指针，记录不容易进行序列化（serialize）操作\n\n##### 封闭散列（closed hashing）/ 开放定址法\n\n1）优点： ①记录更容易进行序列化（serialize）操作 ②如果记录总数可以预知，可以创建完美哈希函数，此时处理数据的效率是非常高的\n\n \n\n2）缺点： ①存储记录的数目不能超过桶数组的长度，如果超过就需要扩容，而扩容会导致某次操作的时间成本飙升，这在实时或者交互式应用中可能会是一个严重的缺陷 ②使用探测序列，有可能其计算的时间成本过高，导致哈希表的处理性能降低 ③由于记录是存放在桶数组中的，而桶数组必然存在空槽，所以当记录本身尺寸（size）很大并且记录总数规模很大时，空槽占用的空间会导致明显的内存浪费 ④删除记录时，比较麻烦。比如需要删除记录a，记录b是在a之后插入桶数组的，但是和记录a有冲突，是通过探测序列再次跳转找到的地址，所以如果直接删除a，a的位置变为空槽，而空槽是查询记录失败的终止条件，这样会导致记录b在a的位置重新插入数据前不可见，所以不能直接删除a，而是设置删除标记。这就需要额外的空间和操作。\n\n","tags":["-hash冲突"]},{"title":"HBase Coprocessor的实现与应用","url":"/2020/04/03/it/hbase/HBase Coprocessor的实现与应用/","content":"\n## HBase Coprocessor的实现与应用\n\n*原文：叶铿\t烽火大数据平台\t研发负责人\t**转发***\n\n### Coprocessor简介\n\n- HBase协处理器的灵感来自于Jeff  Dean  09年的演讲，根据该演讲实现类似于Bigtable的协处理器，包括以下特性:每个表服务器的任意子表都可以运行代码客户端的高层调用接口(客户端能够直接访问数据表的行地址，多行读写会自动分片成多个并行的RPC调用)，提供一个非常灵活的、可用于建立分布式服务的数据模型，能够自动化扩展、负载均衡、应用请求路由。HBase的协处理器灵感来自Bigtable，但是实现细节不尽相同。HBase建立框架为用户提供类库和运行时环境，使得代码能够在HBase Region Server和Master上面进行处理。\n\n#### 实现目的\n\n- 1.HBase无法轻易建立“二级索引”；\n- 2.执行求和、计数、排序等操作比较困难，必须通过MapReduce/Spark实现，对于简单的统计或聚合计算，可能会因为网络与IO开销大而带来性能问题。\n\n#### 灵感来源\n\n- 灵感来源于Bigtable的协处理器，包含如下特性：\n  - 1.每个表服务器的任意子表都可以运行代码；\n  - 2.客户端能够直接访问数据表的行，多行读写会自动分片成多个并行的RPC调用。\n\n#### 提供接口\n\n- 1.RegionObserver：提供客户端的数据操纵事件钩子：Get、Put、Delete、Scan等\n- 2.WALObserver：提供WAL相关操作钩子；\n- 3.MasterObserver：提供DDL-类型的操作钩子。如创建、删除、修改数据表等；\n- 4.Endpoint：终端是动态RPC插件的接口，它的实现代码被安装在服务器端，能够通过HBase RPC调用唤醒。\n\n#### 应用范围\n\n- Apache HBase实战技术总结–中国HBase技术社区\n- 1.通过使用RegionObserver接口可以实现二级索引的创建和维护；\n- 2.通过使用Endpoint接口，在对数据进行简单排序和sum，count等统计操作时，能够极大提高性能。\n\n\n\n本文将通过具体实例来演示两种协处理器的开发方法的详细实现过程。\n\n### EndPoint实现\n\n- 在传统关系型数据库里面，可以随时的对某列进行求和sum，但是目前HBase目前所提供的接口，直接求和是比较困难的，所以先编写好服务端代码，并加载到对应的Table上，加载协处理器有几种方法，可以通过HTableDescriptor的add Coprocessor方法直接加载，同理也可以通过removeCoprocessor方法卸载协处理器。\n- Endpoint协处理器类似传统数据库的存储过程，客户端调用Endpoint协处理器执行一段Server端代码，并将Server端代码的结果返回给Client进一步处理，最常见的用法就是进行聚合操作。举个例子说明：如果没有协处理器，当用户需要找出一张表中的最大数据即max聚合操作，必须进行全表扫描，客户端代码遍历扫描结果并执行求max操作，这样的方法无法利用底层集群的并发能力，而将所有计算都集中到Client端统一执行，效率非常低。但是使用Coprocessor，用户将求max的代码部署到HBase Server端，HBase将利用底层Cluster的多个节点并行执行求max的操作即在每个Region范围内执行求最大值逻辑，将每个Region的最大值在Region Server端计算出，仅仅将该max值返回给客户端。客户端进一步将多个Region的max进一步处理而找到其中的max，这样整体执行效率提高很多。但是一定要注意的是Coprocessor一定要写正确，否则导致RegionServer宕机\n\n![HBase-EndPoint](/Users/dingchuangshi/Documents/net-repository/kfly-blog/source/_posts/it/hbase/assets/HBase-EndPoint.png)\n\n#### Protobuf定义\n\n- 如前所述，客户端和服务端之间需要进行RPC通信，所以两者间需要确定接口，当前版本的HBase的协处理器是通过Google  Protobuf协议来实现数据交换的，所以需要通过Protobuf来定义接口。如下所示：\n\n```protobuf\noption java_package = \"com.my.hbase.protobuf.generated\";\noption java_outer_classname = \"AggregateProtos\";\noption java_generic_services = true;\noption java_generate_equals_and_hash = true;\noption optimize_for = SPEED;\n\nimport \"Client.proto\";\n\nmessage AggregateRequest {\n\trequired string interpreter_class_name = 1;\n\trequired Scan scan = 2;\n\toptional bytes  interpreter_specific_bytes = 3;\n}\n\nmessage AggregateResponse {\n\trepeated bytes first_part = 1;\n\toptional bytes second_part = 2;\n}\n\nservice AggregateService {\n\trpc GetMax (AggregateRequest) returns (AggregateResponse);\n\trpc GetMin (AggregateRequest) returns (AggregateResponse);\n\trpc GetSum (AggregateRequest) returns (AggregateResponse);\n\trpc GetRowNum (AggregateRequest) returns (AggregateResponse);\n\trpc GetAvg (AggregateRequest) returns (AggregateResponse);\n\trpc GetStd (AggregateRequest) returns (AggregateResponse);\n\trpc GetMedian (AggregateRequest) returns (AggregateResponse);\n}\n```\n\n- 可以看到这里定义7个聚合服务RPC，名字分别叫做GetMax、GetMin、GetSum等，本文通过GetSum进行举例，其他的聚合RPC也是类似的内部实现。RPC有一个入口参数，用消息AggregateRequest表示；RPC的返回值用消息AggregateResponse表示。Service是一个抽象概念，RPC的Server端可以看作一个用来提供服务的Service。在HBaseCoprocessor中Service就是Server端需要提供的EndpointCoprocessor服务，主要用来给HBase的Client提供服务。AggregateService.java是由Protobuf软件通过终端命令`protoc filename.proto --java_out=OUT_DIR`自动生成的，其作用是将.proto文件定义的消息结构以及服务转换成对应接口的RPC实现，其中包括如何构建request消息和response响应以及消息包含的内容的处理方式，并且将AggregateService包装成一个抽象类，具体的服务以类的方法的形式提供。AggregateService.java定义Client端与Server端通信的协议，代码中包含请求信息结构AggregateRequest、响应信息结构AggregateResponse、 提 供 的 服 务 种 类AggregateService，其中AggregateRequest中的interpreter_class_name指的是column  interpreter的类名，此类的作用在于将数据格式从存储类型解析成所需类型。AggregateService.java由于代码太长，在这里就不贴出来了。\n\n- 下面我们来讲一下服务端的架构：\n\n- 首先，EndpointCoprocessor是一个Protobuf  Service的实现，因此需要它必须继承某个ProtobufService。我们在前面已经通过proto文件定义Service，命名为AggregateService，因此Server端代码需要重载该类，其次作为HBase的协处理器，Endpoint 还必须实现HBase定义的协处理器协议，用Java的接口来定义。具体来说就是CoprocessorService和Coprocessor，这些HBase接口负责将协处理器和HBase 的RegionServer等实例联系起来以便协同工作。Coprocessor接口定义两个接口函数：start和stop。\n\n- 加载Coprocessor之后Region打开的时候被RegionServer自动加载，并会调用器start 接口完成初始化工作。一般情况该接口函数仅仅需要将协处理器的运行上下文环境变量CoprocessorEnvironment保存到本地即可。\n\n- CoprocessorEnvironment保存协处理器的运行环境，每个协处理器都是在一个RegionServer进程内运行并隶属于某个Region。通过该变量获取Region的实例等HBase运行时环境对象。\n\n- Coprocessor接口还定义stop()接口函数，该函数在Region被关闭时调用，用来进行协处理器的清理工作。本文里我们没有进行任何清理工作，因此该函数什么也不干。我们的协处理器还需要实现CoprocessorService接口。该接口仅仅定义一个接口函数getService()。我们仅需要将本实例返回即可。\n\n- HBase的Region Server在接收到客户端的调用请求时，将调用该接口获取实现RPCService的实例，因此本函数一般情况下就是返回自身实例即可。完成以上三个接口函数之后，Endpoint的框架代码就已完成。每个Endpoint协处理器都必须实现这些框架代码而且写法雷同。\n\n  ![Endpoint Service端实现](/Users/dingchuangshi/Documents/net-repository/kfly-blog/source/_posts/it/hbase/assets/Endpoint Service端实现.png)\n\n- Server端的代码就是一个Protobuf  RPC的Service实现，即通过Protobuf提供的某种服务。其开发内容主要包括\n\n```\n\n```\n\n[未完待续](http://hbase.group/slides/188#page=120)\n\n","tags":["hbase","opentsdb"]},{"title":"深入理解Spark Streaming流量控制及反压机制","url":"/2019/12/12/it/spark/深入理解Spark Streaming流量控制及反压机制/","content":"\n# 反压机制\n\n> Spark Streaming 作为基于 **微批次**（micro-batch）的流处理框架，其流量的理想状态就是官方文档中所说的 “batches of data should be processed as fast as they are being generated”，即每一批次的处理时长 batchprocesstime 需要小于（但是又比较接近）我们设定的批次间隔 batchinterval。如果 batchprocesstime > batchinterval，说明程序的处理能力不足，积累的数据越来越多，最终会造成 Executor 内存溢出。如果 batchprocesstime << batch_interval ，说明系统有很长时间是空闲的，应该适当提升流量。\n\n## 接收数据的两种方式\n\n### Receiver Stream\n\n- Spark Streaming 通过 Executor 里的 Receiver 组件源源不断地接收外部数据，并通过 BlockManager 将外部数据转化为 Spark 中的块进行存储。Spark Streaming 中Receiver方式机制的简单框图如下所示。\n\n![image-20200407155624244](https://kflys.gitee.io/upic/2020/04/07/uPic/spark%E5%8F%8D%E5%8E%8B/assets/image-20200407155624244.png)\n\n- 要限制 Receiver 接收数据的速率，可以在 SparkConf 中设置配置项 `spark.streaming.receiver.maxRate`，单位为数据条数/秒。\n\n这两种方式的优点是设置非常简单，只需要通过实际业务的吞吐量估算一下使批次间隔和处理耗时基本达到平衡的速率就可以了。缺点是一旦业务量发生变化，就只能手动修改参数并重启 Streaming 程序。另外，人为估计的参数毕竟有可能不准，设置得太激进或太保守都不好\n\n### Direct Stream\n\n- 如果采用的是基于 Direct Stream 方式的 Kafka 连接，不经过 Receiver，就得设置配置项 `spark.streaming.kafka.maxRatePerPartition` 来限流，单位是每分区的数据条数/秒。\n\n## Spark Streaming 反压机制\n\n- 以上两种通过参数控制非常方便，但是一旦业务量发生改变只能手动修改配置文件并重启程序。 所以在Spark 1.5加入了动态流量控制方案。能够根据当前系统的处理速度智能地调节流量阈值，名为 **反压（back pressure）机制**。\n- 控制反压机制的配置文件如下：\n\n| 参数名称                                      | 默认值 | 说明                                                         |\n| --------------------------------------------- | ------ | ------------------------------------------------------------ |\n| spark.streaming.backpressure.enabled          | false  | 是否启用反压机制                                             |\n| spark.streaming.backpressure.initialRate      | 无     | 初始最大接收速率。只适用于Receiver Stream，不适用于Direct Stream。 |\n| spark.streaming.backpressure.rateEstimator    | pid    | 速率控制器,Spark 默认只支持此控制器，可自定义。              |\n| spark.streaming.backpressure.pid.proportional | 1.0    | 只能为非负值。当前速率与最后一批速率之间的差值对总控制信号贡献的权重。用默认值即可。 |\n| spark.streaming.backpressure.pid.integral     | 0.2    | 只能为非负值。比例误差累积对总控制信号贡献的权重。用默认值即可 |\n| spark.streaming.backpressure.pid.derived      | 0      | 只能为非负值。比例误差变化对总控制信号贡献的权重。用默认值即可 |\n| spark.streaming.backpressure.pid.minRate      | 100    | 只能为正数，最小速率                                         |\n\n### 基于PID机制的速率估算器\n\n- `org.apache.spark.streaming.scheduler.rate.RateEstimator`是一个很短的特征，其中只给出了计算流量阈值的方法 compute() 的定义。它还有一个伴生对象用于创建速率估算器的实例，其中写出了更多关于反压机制的配置参数。\n\n```scala\n// 速率估算器\nprivate[streaming] trait RateEstimator extends Serializable {\n  def compute(\n      time: Long,\n      elements: Long,\n      processingDelay: Long,\n      schedulingDelay: Long): Option[Double]\n}\n// 半生对象\nobject RateEstimator {\n  def create(conf: SparkConf, batchInterval: Duration): RateEstimator =\n    conf.get(\"spark.streaming.backpressure.rateEstimator\", \"pid\") match {\n      case \"pid\" =>\n        val proportional = conf.getDouble(\"spark.streaming.backpressure.pid.proportional\", 1.0)\n        val integral = conf.getDouble(\"spark.streaming.backpressure.pid.integral\", 0.2)\n        val derived = conf.getDouble(\"spark.streaming.backpressure.pid.derived\", 0.0)\n        val minRate = conf.getDouble(\"spark.streaming.backpressure.pid.minRate\", 100)\n        new PIDRateEstimator(batchInterval.milliseconds, proportional, integral, derived, minRate)\n      case estimator =>\n        throw new IllegalArgumentException(s\"Unknown rate estimator: $estimator\")\n    }\n}\n```\n\n- RateEstimator 的唯一实现类是 PIDRateEstimator，亦即 `spark.streaming.backpressure.rateEstimator` 配置项的值只能为 pid。\n\n```scala\nprivate[streaming] class PIDRateEstimator(\n    batchIntervalMillis: Long,\n    proportional: Double,\n    integral: Double,\n    derivative: Double,\n    minRate: Double\n  ) extends RateEstimator with Logging {\n\n  private var firstRun: Boolean = true\n  private var latestTime: Long = -1L\n  private var latestRate: Double = -1D\n  private var latestError: Double = -1L\n\n  require(\n    batchIntervalMillis > 0,\n    s\"Specified batch interval $batchIntervalMillis in PIDRateEstimator is invalid.\")\n  require(\n    proportional >= 0,\n    s\"Proportional term $proportional in PIDRateEstimator should be >= 0.\")\n  require(\n    integral >= 0,\n    s\"Integral term $integral in PIDRateEstimator should be >= 0.\")\n  require(\n    derivative >= 0,\n    s\"Derivative term $derivative in PIDRateEstimator should be >= 0.\")\n  require(\n    minRate > 0,\n    s\"Minimum rate in PIDRateEstimator should be > 0\")\n\n  logInfo(s\"Created PIDRateEstimator with proportional = $proportional, integral = $integral, \" +\n    s\"derivative = $derivative, min rate = $minRate\")\n\n  def compute(\n      time: Long, // in milliseconds\n      numElements: Long,\n      processingDelay: Long, // in milliseconds\n      schedulingDelay: Long // in milliseconds\n    ): Option[Double] = {\n    logTrace(s\"\\ntime = $time, # records = $numElements, \" +\n      s\"processing time = $processingDelay, scheduling delay = $schedulingDelay\")\n    this.synchronized {\n      if (time > latestTime && numElements > 0 && processingDelay > 0) {\n        val delaySinceUpdate = (time - latestTime).toDouble / 1000\n        val processingRate = numElements.toDouble / processingDelay * 1000\n        val error = latestRate - processingRate\n        val historicalError = schedulingDelay.toDouble * processingRate / batchIntervalMillis\n        val dError = (error - latestError) / delaySinceUpdate\n        val newRate = (latestRate - proportional * error -\n                                    integral * historicalError -\n                                    derivative * dError).max(minRate)\n        logTrace(s\"\"\"\n            | latestRate = $latestRate, error = $error\n            | latestError = $latestError, historicalError = $historicalError\n            | delaySinceUpdate = $delaySinceUpdate, dError = $dError\n            \"\"\".stripMargin)\n        latestTime = time\n        if (firstRun) {\n          latestRate = processingRate\n          latestError = 0D\n          firstRun = false\n          logTrace(\"First run, rate estimation skipped\")\n          None\n        } else {\n          latestRate = newRate\n          latestError = error\n          logTrace(s\"New rate = $newRate\")\n          Some(newRate)\n        }\n      } else {\n        logTrace(\"Rate estimation skipped\")\n        None\n      }\n    }\n  }\n}\n```\n\n- PIDRateEstimator 充分运用了工控领域中常见的 PID 控制器的思想。所谓 PID 控制器，即比例（Proportional）-积分（Integral）-微分（Derivative）控制器，本质上是一种反馈回路（loop feedback）。它把收集到的数据和一个设定值（setpoint）进行比较，然后用它们之间的差计算新的输入值，该输入值可以让系统数据尽量接近或者达到设定值。\n\n![image-20200407162210511](https://kflys.gitee.io/upic/2020/04/07/uPic/spark%E5%8F%8D%E5%8E%8B/assets/image-20200407162210511.png)\n\n![image-20200407162234229](https://kflys.gitee.io/upic/2020/04/07/uPic/spark%E5%8F%8D%E5%8E%8B/assets/image-20200407162234229.png)\n\n- 其中 e(t) 代表误差，即设定值与回授值之间的差。也就是说，比例单元对应当前误差，积分单元对应过去累积误差，而微分单元对应将来误差。控制三个单元的增益因子分别为 Kp、Ki、Kd。\n\n- 回到 PIDRateEstimator 的源码来，对应以上的式子，我们可以得知：\n  - 处理速率的设定值其实就是上一批次的处理速率 latestRate，回授值就是这一批次的速率 processingRate，误差 error 自然就是两者之差。\n  - 过去累积误差在这里体现为调度时延的过程中数据积压的速度，也就是schedulingDelay * processingRate / batchInterval。\n  - 将来误差就是上面算出的error对时间微分的结果。\n\n- 将上面三者综合起来，就可以根据 Spark Streaming 在上一批次以及这一批次的处理速率，估算出一个合适的用于下一批次的流量阈值。比例增益 Kp 由 `spark.streaming.backpressure.pid.proportional` 控制，默认值 1.0；积分增益 Ki 由 `spark.streaming.backpressure.pid.integral` 控制，默认值0.2；微分增益Kd由 `spark.streaming.backpressure.pid.derived`控制，默认值0.0。\n\n ### RateController\n\n- 动态流量控制器,类`org.apache.spark.streaming.scheduler.RateController`是动态流量控制器的核心。\n\n\n```scala\nprivate[streaming] abstract class RateController(val streamUID: Int, rateEstimator: RateEstimator) extends StreamingListener  // 抽象类继承自 StreamingListener 特征，表示它是一个 Streaming 监听器。\nwith Serializable {\n  // 监听 StreamingListenerBatchCompleted 事件，该事件表示一个批次已经处理完成。\n  override def onBatchCompleted(batchCompleted: StreamingListenerBatchCompleted) {\n    val elements = batchCompleted.batchInfo.streamIdToInputInfo\n\n    for {\n      // 处理完成的时间戳 processingEndTime\n      processingEnd <- batchCompleted.batchInfo.processingEndTime\n      // 实际处理时长 processingDelay（从批次的第一个 job 开始处理到最后一个job处理完成经过的时间）\n      workDelay <- batchCompleted.batchInfo.processingDelay\n      // 调度时延schedulingDelay（从批次被提交给 Streaming JobScheduler 到第一个 job 开始处理经过的时间）。\n      waitDelay <- batchCompleted.batchInfo.schedulingDelay\n      // 批次输入数据的条数 numRecords。\n      elems <- elements.get(streamUID).map(_.numRecords)\n    } computeAndPublish(processingEnd, elems, workDelay, waitDelay)\n  }\n}\n```\n\n#### RateController两个子类\n\n- 两个子类分别对应两种数据接收方式。Receiver 和 Direct\n\n##### ReceiverRateController\n\n- 通过 RateController 的子类 ReceiverRateController 实现的 publish() 抽象方法可知，新的流量阈值是发布给了 ReceiverTracker。\n\n  ```scala\n  /**\n     * A RateController that sends the new rate to receivers, via the receiver tracker.\n     */\n  private[streaming] class ReceiverRateController(id: Int, estimator: RateEstimator)\n  extends RateController(id, estimator) {\n    override def publish(rate: Long): Unit =\n    ssc.scheduler.receiverTracker.sendRateUpdate(id, rate)\n  }\n  ```\n\n###### 通过RPC发布流量阈值\n\n- 回来看 ReceiverTracker，顾名思义，它负责追踪 Receiver 的状态。其 sendRateUpdate() 方法如下\n\n  ```scala\n  // 1. endpoint 是 RPC 端点的引用。具体来说，是 ReceiverTrackerEndpoint 的引用\n  private var endpoint: RpcEndpointRef = null\n  \n  /** Update a receiver's maximum ingestion rate */\n  def sendRateUpdate(streamUID: Int, newRate: Long): Unit = synchronized {\n    if (isTrackerStarted) {\n      // 2. endpoint 将流 ID 与新的流量阈值包装在 UpdateReceiverRateLimit 消息中发送过去。\n      endpoint.send(UpdateReceiverRateLimit(streamUID, newRate))\n    }\n  }\n  ```\n\n- ReceiverTrackerEndpoint 收到这条消息后，会再将其包装为 UpdateRateLimit 消息并发送给 Receiver 注册时的 RPC 端点（位于 ReceiverSupervisorImpl 类中）。\n- ReceiverTrackerEndpoint 收到这条消息后，会再将其包装为 UpdateRateLimit 消息并发送给 Receiver 注册时的 RPC 端点（位于 ReceiverSupervisorImpl 类中）。\n\n```scala\n/** RpcEndpointRef for receiving messages from the ReceiverTracker in the driver */\nprivate val endpoint = env.rpcEnv.setupEndpoint(\n  \"Receiver-\" + streamId + \"-\" + System.currentTimeMillis(), new ThreadSafeRpcEndpoint {\n    override val rpcEnv: RpcEnv = env.rpcEnv\n\n    override def receive: PartialFunction[Any, Unit] = {\n      case StopReceiver =>\n      logInfo(\"Received stop signal\")\n      ReceiverSupervisorImpl.this.stop(\"Stopped by driver\", None)\n      case CleanupOldBlocks(threshTime) =>\n      logDebug(\"Received delete old batch signal\")\n      cleanupOldBlocks(threshTime)\n      case UpdateRateLimit(eps) =>\n      logInfo(s\"Received a new rate limit: $eps.\")\n      // 1. 收到该消息之后调用了 BlockGenerator.updateRate() 方法。\n      registeredBlockGenerators.asScala.foreach { bg =>\n        bg.updateRate(eps)\n      }\n    }\n  })\n```\n\n```scala\n// 2. BlockGenerator 是 RateLimiter 的子类，它负责将收到的流数据转化成块存储。updateRate() 方法是在 RateLimiter 抽象类中实现的。\nprivate[streaming] class BlockGenerator(\n    listener: BlockGeneratorListener,\n    receiverId: Int,\n    conf: SparkConf,\n    clock: Clock = new SystemClock()\n  ) extends RateLimiter(conf) with Logging {}\n```\n\n```scala\n// 这里接住了guava的RateLimiter\nimport com.google.common.util.concurrent.{RateLimiter => GuavaRateLimiter}\n\nprivate[receiver] abstract class RateLimiter(conf: SparkConf) extends Logging {\n  // treated as an upper limit\n  private val maxRateLimit = conf.getLong(\"spark.streaming.receiver.maxRate\", Long.MaxValue)\n  private lazy val rateLimiter = GuavaRateLimiter.create(getInitialRateLimit().toDouble)\n  def waitToPush() {\n    rateLimiter.acquire()\n  }\n  /**\n   * Return the current rate limit. If no limit has been set so far, it returns {{{Long.MaxValue}}}.\n   */\n  def getCurrentLimit: Long = rateLimiter.getRate.toLong\n\n  /**\n   * Set the rate limit to `newRate`. The new rate will not exceed the maximum rate configured by\n   * {{{spark.streaming.receiver.maxRate}}}, even if `newRate` is higher than that.\n   *\n   * @param newRate A new rate in records per second. It has no effect if it's 0 or negative.\n   */\n  // 3. 更细Rate\n  private[receiver] def updateRate(newRate: Long): Unit =\n    if (newRate > 0) {\n      if (maxRateLimit > 0) {\n        rateLimiter.setRate(newRate.min(maxRateLimit))\n      } else {\n        rateLimiter.setRate(newRate)\n      }\n    }\n  /**\n   * Get the initial rateLimit to initial rateLimiter\n   */\n  private def getInitialRateLimit(): Long = {\n    math.min(conf.getLong(\"spark.streaming.backpressure.initialRate\", maxRateLimit), maxRateLimit)\n  }\n}\n```\n\n![image-20200407164947975](https://kflys.gitee.io/upic/2020/04/07/uPic/spark%E5%8F%8D%E5%8E%8B/assets/image-20200407164947975.png)\n\n##### DirectKafkaRateController\n\n- DirectKafkaRateController只继承了RateController，并未在 publish() 加过多的内容，我们主要看一下`DirectKafkaInputDStream`这个类。\n\n```scala\nprivate[spark] class DirectKafkaInputDStream[K, V](\n    _ssc: StreamingContext,\n    locationStrategy: LocationStrategy,\n    consumerStrategy: ConsumerStrategy[K, V],\n    ppc: PerPartitionConfig\n  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging with CanCommitOffsets \n\n  /**\n   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n   异步，通过ReceiverTracker发送新的速率限制\n   */\n  override protected[streaming] val rateController: Option[RateController] = {\n    if (RateController.isBackPressureEnabled(ssc.conf)) {\n      Some(new DirectKafkaRateController(id,\n        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n    } else {\n      None\n    }\n  }\n\t// 主要实现在这里\n  protected[streaming] def maxMessagesPerPartition(\n    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n    //   调用RateController 的 def getLatestRate(): Long = rateLimit.get()获取rateLimiter    \n    val estimatedRateLimit = rateController.map(_.getLatestRate())\n\n    // calculate a per-partition rate limit based on current lag\n    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n      case Some(rate) =>\n      \t// offset range的消息量 totalLag\n        val lagPerPartition = offsets.map { case (tp, offset) =>\n          tp -> Math.max(offset - currentOffsets(tp), 0)\n        }\n        val totalLag = lagPerPartition.values.sum\n\n        lagPerPartition.map { case (tp, lag) =>\n          // 设置的maxRatePerPartition\n          val maxRateLimitPerPartition = ppc.maxRatePerPartition(tp)\n          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n          tp -> (if (maxRateLimitPerPartition > 0) {\n        \t // 有效速率=取设置的maxRatePerPartition和预估的速率最小值\n            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n        }\n      case None => offsets.map { case (tp, offset) => tp -> ppc.maxRatePerPartition(tp) }\n    }\n\n    if (effectiveRateLimitPerPartition.values.sum > 0) {\n      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n      Some(effectiveRateLimitPerPartition.map {\n        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n      })\n    } else {\n      None\n    }\n  }\n  /**\n   * A RateController to retrieve the rate from RateEstimator.\n   * 实现RateController抽象类，并未具体实现publish方法，主要用来取回rate数据\n   */\n  private[streaming] class DirectKafkaRateController(id: Int, estimator: RateEstimator)\n    extends RateController(id, estimator) {\n    override def publish(rate: Long): Unit = ()\n  }\n}\n```\n\n","tags":["spark","反压"]},{"title":"SparkSQL + HBase数据拉取","url":"/2019/11/03/it/spark/SparkSQL + HBase读取数据拉取/","content":"\n## SparkSQL + HBase数据拉取\n\n#### NewApiHadoop\n\n```scala\nval context: SparkContext = sparkSession.sparkContext\n// set inputformat\nconf.set(TableInputFormat.INPUT_TABLE, Constants.HTAB_ORDER)\nval scan = new Scan\n// todo scan 过滤条件\nconf.set(TableInputFormat.SCAN, convertScanToString(scan))\nval rddResult: RDD[(ImmutableBytesWritable, Result)] = \n\t\t\t\t\t\t\t\t\t\tcontext.newAPIHadoopRDD(\n                      conf, // hbase配置文件\n                      classOf[TableInputFormat],\n                      classOf[ImmutableBytesWritable],\n                      classOf[Result])\nimport sparkSession.implicits._\n// result to rdd\nval orderRDD: RDD[Order] = rddResult.mapPartitions(eachPartition => {\n  val orders: Iterator[Order] = eachPartition.map(eachResult => {\n    val value: Result = eachResult._2\n    Order(order_id, city_id, start_time, end_time)\n  })\n  orders\n})\norderRDD.toDF\n\ndef convertScanToString(scan: Scan):String = {\n    val proto = ProtobufUtil.toScan(scan)\n    Base64.encodeBytes(proto.toByteArray)\n}\n```\n\n#### DataSource V2\n\n- DataSource V1和V2的区别可以参考 [文章](https://blog.csdn.net/zjerryj/article/details/84922369)\n\n##### DataSource\n\n```scala\n/**\n \t* 自定义spark sql数据源\n  * 1.继承DataSourceV2向Spark注册数据源\n  * 2.继承ReadSupport支持读数据\n *  {@link ReadSupport }支持读取操作\n */\nclass HBaseCustomSource extends DataSourceV2 with ReadSupport{\n  // 自定义的DataSourceReader\n  override def createReader(options: DataSourceOptions): DataSourceReader = {\n      new HBaseCustomDataSourceReader(hbaseTableName,sparkSqlTableSchema,hbaseTableSchema)\n  }\n}\n```\n\n##### DataSourceReader\n\n```scala\n/**\n * 自定义的DataSourceReader\n * 继承DataSourceReader\n * 重写readSchema方法用来生成schema\n * 重写createDataReaderFactories,多分区支持。用来根据条件，创建多个工厂实例\n *\n * 注意：\n *  {@link SupportsPushDownFilters } 过滤条件\n *  {@link SupportsPushDownRequiredColumns }裁剪字段\n * 1. 下推条件必须满足数据类型 比如：string 不能判断大小\n * 2. short and bytes 比较大小必须使用显示转换 where id case (见spark ParquetFilter源码)\n */\nclass HBaseCustomDataSourceReader extends DataSourceReader \n                with SupportsPushDownFilters \n                with SupportsPushDownRequiredColumns{\n\n  var supportsFilters = Array.empty[Filter]\n  var requiredSchema:StructType = null\n\n  // 过滤条件\n  override def pushFilters(filters: Array[Filter]): Array[Filter] = {\n    val supported = ListBuffer.empty[Filter]\n    val unsupported = ListBuffer.empty[Filter]\n\n    /**\n     * 仅仅把等于大于大于等于，小于小于等于下推\n     */\n    filters.foreach{\n      case filter: EqualTo => supported +=filter\n      case filter: GreaterThan=> supported +=filter\n      case filter: GreaterThanOrEqual=> supported +=filter\n      case filter: LessThan => supported +=filter\n      case filter: LessThanOrEqual => supported +=filter\n      case filter => unsupported +=filter\n    }\n    this.supportsFilters = supported.toArray[Filter]\n    unsupported.toArray\n  }\n\n  // 下推支持的过滤条件\n  override def pushedFilters(): Array[Filter] = supportsFilters.toArray\n\n   // 获取select的列\n  override def pruneColumns(requiredSchema: StructType): Unit = {\n    this.requiredSchema = requiredSchema\n  }\n\n  // 生成schema\n  override def readSchema(): StructType = {\n    if(requiredSchema != null){\n      return requiredSchema\n    }\n    StructType.fromDDL(sparkSqlTableSchema)\n  }\n\n  // 多分区支持\n  override def createDataReaderFactories(): util.List[DataReaderFactory[Row]] = {\n    import scala.collection.JavaConverters._\n    Seq(\n      new HBaseDataReaderFactory(...).asInstanceOf[DataReaderFactory[Row]]\n    ).asJava\n  }\n}\n```\n\n##### DataReaderFactory\n\n```scala\n/**\n * 创建DataReader工厂类\n */\nclass HBaseDataReaderFactory extends DataReaderFactory[Row]{\n  override def createDataReader(): DataReader[Row] = {\n    new HBaseCustomDataReader()\n  }\n}\n```\n\n##### DataReader\n\n```scala\nclass HBaseCustomDataReaderextends DataReader[Row]{\n\t// hbase连接\n  var hbaseConnection : Connection = null\n  // 读取到的数据\n  val datas:Iterator[Result] = getIterator\n  \n  // load hbase data\n  def getIterator: Iterator[Result] = {\n    table = hbaseConnection.getTable(TableName.valueOf(hbaseTableName.trim))\n    val scan: Scan = new Scan\n    // 填充DataSourceReader下推的过滤条件,和列裁剪\n    fullScanByColumns(scan,requiredSchemaList)\n    fullScanByFilter(scan)\n    // 查询数据并解析\n    scanner = table.getScanner(scan)\n    import scala.collection.JavaConverters._\n    scanner.iterator.asScala\n  }\n}\n```\n\n###### fullScanByColumns\n\n```scala\n // 例如：拼接查询所需要的列\ndef fullScanByColumns(scan:Scan)={\n    hbaseTableSelectedFamilyAndColumn.map(tuple=>{\n    \tscan.addColumn(tuple._1getBytes,tuple._2.getBytes)\n\t\t})\n}\n```\n\n###### fullScanByFilters\n\n```scala\n//例如：拼接所需要的filter supportFilters\ndef fullScanByFilter(scan: Scan) = {\n  val filterList = new FilterList()\n  supportsFilters.foreach{\n    case filter: EqualTo => {\n      filterList.addFilter()\n    }\n    // todo \n  }\n  if(filterList.getFilters.size() > 0){\n    scan.setFilter(filterList)\n  }\n}\n\n```\n\n##### DataWriter\n\n```java\n// data writer支持事物的读写\npublic interface DataSourceWriter {\n  void commit(WriterCommitMessage[] messages);\n  void abort(WriterCommitMessage[] messages);\n}\n\npublic interface DataWriter<T> {\n  void write(T record) throws IOException;\n  WriterCommitMessage commit() throws IOException;\n  void abort() throws IOException;\n}\n```\n\n##### 使用\n\n```java\n // 数据源1\n Dataset<Row> load = spark.read()\n   .format(\"top.kfly.hbase.source.HBaseCustomSource\")\n   .option(Constants.HBASE_TABlE_NAME, \"flink:kfly_orders\")\n   .option(Constants.HBASE_TABLE_SCHEMA, \" f1:userId,f1:goodsMoney,f1:orderNo,f1:goodId\")\n   .option(Constants.SPARK_SQL_TABlE_SCHEMA, \"userId String,goodsMoney String,orderNo String,goodId int\")\n   .load()\n   .select(\"goodsMoney\",\"orderNo\",\"goodId\")\n   .filter(\"goodId <= 16\");\n```\n\n#### 注意\n\n*当字段类型出现short / byte时，将不会被下推。需要显性的转换为smallint,如下*\n\n```sql\nselect * from t where id = cast(2 as smallint) \n```\n\n\n\n","tags":["spark sql","DataSource2"]},{"title":"OpentsDB时序数据库","url":"/2019/10/03/it/hbase/opentsDB时序数据库/","content":"\n## OpentsDB时序数据库\n\n- [原理精解参见](https://www.jianshu.com/p/1cae8641109b)\n\n## 环境搭建\n\n```markdown\n- 首先确保已经安装了HBase集群\n- OpentsDB本身没有集群解决方案，而是以HBase集群为基础实现分布式集群方案。多个节点的OpentsDB访问同一个HBase集群实现。\n```\n\n1. 安装gnuplot支持，automake等依赖\n\n```shell\nyum install gnuplot automake autoconf git -y\n```\n\n2. 下载opentsDB\n\n   - 选择相应的版本下载 https://github.com/OpenTSDB/opentsdb/releases/\n\n3. 安装\n\n   ```shell\n   cd /kfly/install/opentsdb-2.2.0/\n   ./build.sh \n   \n   cd build/\n   make install\n   ```\n\n4. 启动opentsDB钱准备\n\n   ```shell\n   # 第一次启动首先要进行hbase准备,执行初始化脚本\n   env COMPRESSION=NONE HBASE_HOME=/kfly/install/hbase-0.94.27 ./src/create_table.sh \n   ```\n\n5. 配置文件 opensdb.conf\n\n   - 将配置文件放在build/目录下\n   - [详细配置信息查看](http://opentsdb.net/docs/build/html/user_guide/configuration.html)\n\n   ```properties\n   # --------- NETWORK ----------\n   # # The TCP port TSD should use for communications\n   # # *** REQUIRED ***\n    tsd.network.port = 4399\n   #\n   # # ----------- HTTP -----------\n   # # The location of static files for the HTTP GUI interface.\n   # # *** REQUIRED ***\n   tsd.http.staticroot = ./staticroot\n   #\n   # # Where TSD should write it's cache files to\n   # # *** REQUIRED ***\n   # tsd.http.cachedir = /root/openTSDB_temp\n   #\n   #\n   # # --------- CORE ----------\n   # # Whether or not to automatically create UIDs for new metric types, default\n   # # is False\n    tsd.core.auto_create_metrics = true\n   #\n   # # Name of the HBase table where data points are stored, default is \"tsdb\"\n    tsd.storage.hbase.data_table = tsdb\n   #\n   # # Path under which the znode for the -ROOT- region is located, default is \"/hbase\"\n   tsd.storage.hbase.zk_basedir = /hbase\n   #\n   # # A comma separated list of Zookeeper hosts to connect to, with or without\n   # # port specifiers, default is \"localhost\"\n   tsd.storage.hbase.zk_quorum = server4,server5,server6\n   ```\n\n6. 启动\n\n   ```shell\n   ./build/tsdb tsd\n   ```\n\n7. 日志管理\n\n   - 一般由于opentsdb默认的日志特别多，尤其以nohup启动的话，日志很有可能占满整个磁盘。所以这里需要修改opentsdb的bug级别。\n\n   ```xml\n   <!-- Per class logger levels -->\n   <logger name=\"QueryLog\" level=\"OFF\" additivity=\"false\">\n     <appender-ref ref=\"QUERY_LOG\"/>\n   </logger>\n   <!--为减少日志输出，日志级别OFF修改为ERROR-->\n     <!-- Per class logger levels -->\n   <logger name=\"QueryLog\" level=\"ERROR\" additivity=\"false\">\n     <appender-ref ref=\"QUERY_LOG\"/>\n   </logger>\n   ```\n\n## Java Api\n\n### 写入数据 [详细](http://opentsdb.net/docs/build/html/user_guide/writing/index.html)\n\n```java\nHttpClientImpl client = new HttpClientImpl(\"http://node03:4399\");\nMetricBuilder builder = MetricBuilder.getInstance();\nbuilder.addMetric(\"metric1\").setDataPoint(30L)\n  .addTag(\"tag1\", \"tab1value\").addTag(\"tag2\", \"tab2value\");\nbuilder.addMetric(\"metric2\").setDataPoint(232.34)\n  .addTag(\"tag3\", \"tab3value\");\ntry {\n  Response response = client.pushMetrics(builder,\n                                         ExpectResponse.SUMMARY);\n  System.out.println(response);\n} catch (IOException e) {\n  e.printStackTrace();\n}\n```\n\n### 查询数据 [详细](http://opentsdb.net/docs/build/html/user_guide/query/index.html)\n\n```java\nHttpClientImpl client = new HttpClientImpl(\"http://node03:4399\");\nQueryBuilder builder = QueryBuilder.getInstance();\nSubQueries subQueries = new SubQueries();\nString zimsum = Aggregator.zimsum.toString();\nsubQueries.addMetric(\"top.kfly.host2\").addTag(\"tag\", \"value\");\nsubQueries.addAggregator(zimsum);\nlong now = new Date().getTime() / 1000;\nbuilder.getQuery().addStart(126358720).addEnd(now).addSubQuery(subQueries);\nSimpleHttpResponse response = client.pushQueries(builder,ExpectResponse.STATUS_CODE);\nString content = response.getContent();\nSystem.out.println(content);\nint statusCode = response.getStatusCode();\nif (statusCode == 200) {\n  JSONArray jsonArray = JSON.parseArray(content);\n  System.out.println(jsonArray.toJSONString());\n}\n```\n\n- [opentsDB client](https://github.com/shifeng258/opentsdb-client)\n\n## [详细教程参见官网](http://opentsdb.net/docs/build/html/index.html)\n\n","tags":["hbase","opentsdb"]},{"title":"Spark or Flink中Broadcast维表更新","url":"/2019/10/03/it/spark/Spark or Flink中Broadcast维表更新/","content":"\n# 案例分析\n\n- 实际开发中维表服务是经常遇到的。例如：\n\n  - 在水务项目中，来自水源地的不同监测点（压力、温度、氯气浓度）等监测点数据的报警规则改变。\n  - 用户行为数据中需要进行大区的转码\n  - 商品信息中心只包含了商品Id等\n\n  上述案例中都可以使用维表服务解决。\n\n## 维表方案\n\n### 预加载\n\n- 在flink中，凡是继承了RichFunction的算子，都含有open方法。 可以再此方法里面实现对数据的预加载。\n\n```java\n RichMapFunction richMapFunction = new RichMapFunction<String>() {\n    \t\t\t\t@Override\n            public void open(Configuration parameters) throws Exception {\n                super.open(parameters);\n            }        \n   \t\t\t\t\t@Override\n            public Object map(String value) throws Exception {\n                return null;\n            }\n\t }\n// 如果数据变更频次较低的，可以追加每分钟频次的时间调度器。\n```\n\n- 缺点\n  - 维度更新延迟\n  - 加载到内存中，不适合数据量较大的场景\n\n### 热存储关联\n\n- IO变为异步IO，可使用cache缓存热数据(以guava为例)\n\n```java\nCache<String, String> cache = CacheBuilder\n                .newBuilder()\n                .expireAfterAccess(300,TimeUnit.MILLISECONDS) // 按照时间访问过期\n                .expireAfterWrite(300,TimeUnit.MICROSECONDS) // 写入时间过期\n                .build();\n// 维度数据加载延迟\n```\n\n### 广播维表\n\n#### spark\n\n- spark的broacast变量定义为只读，所以只能手动更新\n\n```scala\n// 监测点上报数据格式: P001,10(pointKey,value)\nval dataStream = KafkaUtils.createDirectStream[String,String](\n      sc,LocationStrategies.PreferConsistent,\n      ConsumerStrategies.Subscribe[String,String](topic1, kafkaParams))\n\n// 报警规则数据:P001,10,1,1(pointKey,max,min,isRemove)\nval broadCastStream = KafkaUtils.createDirectStream[String,String](\n    sc,LocationStrategies.PreferConsistent,\n    ConsumerStrategies.Subscribe[String,String](topic2, kafkaParams))\n\n// 首次加载所有数据\nvar broadCastDomainMap = sc.sparkContext.broadcast(InitDomainMap)\n\n// 对broadCastStream监测点报警规则进行广播\nbroadCastStream.foreachRDD(rdd=>{\n  // 获取当前广播值\n  val mapData = broadCastDomainMap.value\n  // 删除广播\n  broadCastDomainMap.unpersist()\n  // 获取规则流中的数据，转为map\n  val streamingMap = rdd.map(line => {\n    val lines = line.value().split(\",\")\n    (lines(0), new MonitorPoint(lines(0), lines(1).toLong, lines(2).toLong, lines.length >= 4 && lines(3).equals(\"1\")))\n  }).collect().toMap\n  // 表示已删除的规则，移除广播。\n  val removeKeys = streamingMap.filter(_._2.isRemove).map(_._1).toList\n  val broadcastData = mapData.filter(line => !removeKeys.contains(line._1)) ++ (streamingMap.filter(!_._2.isRemove))\n  // 重新进行广播\n  broadCastDomainMap = rdd.sparkContext.broadcast(broadcastData)\n})\n\n// 监测数据和报警规则关联\ndataStream.map(line=>{\n  val lines = line.value().split(\",\")\n  val value = broadCastDomainMap.value\n  (lines(0),lines(1),value.get(lines(0)))\n}).print()\n\n  /**\n   * 第一次加载所有数据\n   */\n  def InitDomainMap:Map[String,MonitorPoint] = {\n    val map = Map(\n      \"P001\" -> new MonitorPoint(\"P001\",10,1,false),\n      \"P002\" -> new MonitorPoint(\"P002\",10,1,false)\n    )\n    map\n  }\n```\n\n#### Flink\n\n- flink广播变量支持增量更新到State\n\n```java\n// 存储维度信息\nMapStateDescriptor<String, AlarmParam> alarmData = \n  \t\t\t\t\t\t\t\tnew MapStateDescriptor<String,AlarmParam>(\n                                                  \"alarmData\", \n                                                  Types.STRING,\n                                                  Types.POJO(AlarmParam.class)\n                                                );\n// kafka数据流\nDataStreamSource<String> dataStreamSource = env.addSource(flinkKafkaConsumer010);\n\n// 报警数据流\nDataStreamSource<String> broadcastStreamSource = env.addSource(broadcastStream);\n\n// 处理函数\nBroadcastProcessFunction processFunction = \n  new BroadcastProcessFunction<String, String, Tuple3<String, Long, AlarmParam>>() {\n        @Override\n        public void processElement(String value, ReadOnlyContext ctx, Collector out){\n          String[] split = value.split(\",\");\n          // 获取广播数据\n          ReadOnlyBroadcastState broadcastState = ctx.getBroadcastState(alarmData);\n          out.collect(1,2,broadcastState.get(1)); // 关联数据\n        }\n\n        @Override\n        public void processBroadcastElement(String value, Context ctx, Collector out){\n          BroadcastState broadcastState = ctx.getBroadcastState(alarmData);\n          String[] split = value.split(\",\");\n          boolean isRemove = split[3].equals(\"1\");\n          if (split.length >= 4 && !isRemove) {\n            // 增量更新数据\n            broadcastState.put(split[0], new AlarmParam(1,2,3,4));\n          } else {\n            // 已删除数据从广播去除\n            broadcastState.remove(split[0]);\n          }\n        }\n      };\n\n        SingleOutputStreamOperatorresult = dataStreamSource\n                .connect(broadcastStreamSource.broadcast(alarmData))\n                .process(processFunction);\n        result.print();\n```\n\n","tags":["调优","spark shuffle"]},{"title":"Spark Shuffle相关调优","url":"/2019/09/30/it/spark/Spark Shuffle及相关调优/","content":"\n### 原理分析\n\n  ```\n  \tShuffle就是对数据进行重组，由于分布式计算的特性和要求，在实现细节上更加繁琐和复杂。\n  \t在MapReduce框架，Shuffle是连接Map和Reduce之间的桥梁，Map阶段通过shuffle读取数据并输出到对应的Reduce；而Reduce阶段负责从Map端拉取数据并进行计算。在整个shuffle过程中，往往伴随着大量的磁盘和网络I/O。所以shuffle性能的高低也直接决定了整个程序的性能高低。Spark也会有自己的shuffle实现过程。 \n  ```\n\n  ```\n  \t在DAG调度的过程中，Stage阶段的划分是根据是否有shuffle过程，也就是存在wide Dependency宽依赖的时候,需要进行shuffle,这时候会将作业job划分成多个Stage，每一个stage内部有很多可以并行运行的task。\n  \t\n  \tstage与stage之间的过程就是shuffle阶段，在Spark的中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。ShuffleManager随着Spark的发展有两种实现的方式，分别为HashShuffleManager和SortShuffleManager，因此spark的Shuffle有Hash Shuffle和Sort Shuffle两种。\n  ```\n\n  #### HashShuffle机制\n\n  ```\n  \t在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。\n  \t该ShuffleManager-HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。\n  \tSortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并(merge)成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。\n  ```\n\n  - ==Hash shuffle==\n    - HashShuffleManager的运行机制主要分成两种\n      - 一种是==普通运行机制==\n      - 另一种是==合并的运行机制==。\n    - ==合并机制主要是通过复用buffer来优化Shuffle过程中产生的小文件的数量。==\n    - ==Hash shuffle是不具有排序的Shuffle。==\n\n  ##### 普通机制的Hash shuffle\n\n![未优化的HashShuffle机制](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/未优化的HashShuffle机制.png)\n\n  - ==图解==\n\n  ```\n     这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。\n  \n      图中有3个ReduceTask，从ShuffleMapTask 开始那边各自把自己进行 Hash 计算(分区器：hash/numreduce取模)，分类出3个不同的类别，每个 ShuffleMapTask 都分成3种类别的数据，想把不同的数据汇聚然后计算出最终的结果，所以ReduceTask 会在属于自己类别的数据收集过来，汇聚成一个同类别的大集合，每1个 ShuffleMapTask 输出3份本地文件，这里有4个 ShuffleMapTask，所以总共输出了4 x 3个分类文件 = 12个本地小文件。\n  ```\n\n  - ==shuffle Write阶段==\n\n  ```\n  \t主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子(比如reduceByKey，groupByKey)，而将每个task处理的数据按key进行“分区”。所谓“分区”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于reduce端的stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。\n  \n       那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢? 很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。\n  ```\n\n  - ==shuffle Read阶段==\n\n  ```\n  \tshuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给Reduce端的stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。\n  \n    shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。\n  ```\n\n  - ==注意==\n\n  ```\n  （1）buffer起到的是缓存作用，缓存能够加速写磁盘，提高计算的效率,buffer的默认大小32k。\n  （2）分区器：根据hash/numRedcue取模决定数据由几个Reduce处理，也决定了写入几个buffer中\n  （3）block file：磁盘小文件，从图中我们可以知道磁盘小文件的个数计算公式：\n                   block file=M*R\n   (4) M为map task的数量，R为Reduce的数量，一般Reduce的数量等于buffer的数量，都是由分区器决定的\n  ```\n\n  - ==Hash shuffle普通机制的问题==\n\n  ```\n  （1).Shuffle阶段在磁盘上会产生海量的小文件，建立通信和拉取数据的次数变多,此时会产生大量耗时低效的 IO 操作 (因为产生过多的小文件)\n  \n  （2).可能导致OOM，大量耗时低效的 IO 操作 ，导致写磁盘时的对象过多，读磁盘时候的对象也过多，这些对象存储在堆内存中，会导致堆内存不足，相应会导致频繁的GC，GC会导致OOM。由于内存中需要保存海量文件操作句柄和临时信息，如果数据处理的规模比较庞大的话，内存不可承受，会出现 OOM 等问题\n  ```\n\n  ##### 合并机制的Hash shuffle\n\n  ```\n  \t合并机制就是复用buffer缓冲区，开启合并机制的配置是spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。\n  ```\n\n![优化后的Shuffle机制](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/优化后的Shuffle机制.png)\n\n\n\n  - ==图解==\n\n  ```\n  \t这里有6个这里有6个shuffleMapTask，数据类别还是分成3种类型，因为Hash算法会根据你的 Key 进行分类，在同一个进程中，无论是有多少过Task，都会把同样的Key放在同一个Buffer里，然后把Buffer中的数据写入以Core数量为单位的本地文件中，(一个Core只有一种类型的Key的数据)，每1个Task所在的进程中，分别写入共同进程中的3份本地文件，这里有6个shuffleMapTasks，所以总共输出是 2个Cores x 3个分类文件 = 6个本地小文件。\n  ```\n\n  - ==注意==\n\n  ```\n  （1).启动HashShuffle的合并机制ConsolidatedShuffle的配置\n     spark.shuffle.consolidateFiles=true\n  \n  （2).block file=Core*R\n  \tCore为CPU的核数，R为Reduce的数量\n  ```\n\n  - ==Hash shuffle合并机制的问题==\n\n  ```\n  \t如果 Reducer 端的并行任务或者是数据分片过多的话则 Core * Reducer Task 依旧过大，也会产生很多小文件。\n  ```\n\n  #### Sort shuffle\n\n  - SortShuffleManager的运行机制主要分成两种，\n    - 一种是==普通运行机制==\n    - 另一种是==bypass运行机制==\n\n  ##### Sort shuffle的普通机制\n\n![sortshuffle](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/sortshuffle.png)\n\n  - ==图解==\n\n  ```\n  \t在该模式下，数据会先写入一个数据结构，聚合算子写入Map，一边通过Map局部聚合，一边写入内存。Join算子写入ArrayList直接写入内存中。然后需要判断是否达到阈值（5M），如果达到就会将内存数据结构的数据写入到磁盘，清空内存数据结构。\n  \n  在溢写磁盘前，先根据key进行排序，排序过后的数据，会分批写入到磁盘文件中。默认批次为10000条，数据会以每批一万条写入到磁盘文件。写入磁盘文件通过缓冲区溢写的方式，每次溢写都会产生一个磁盘文件，也就是说一个task过程会产生多个临时文件\n  。\n  \n  最后在每个task中，将所有的临时文件合并，这就是merge过程，此过程将所有临时文件读取出来，一次写入到最终文件。意味着一个task的所有数据都在这一个文件中。同时单独写一份索引文件，标识下游各个task的数据在文件中的索引start offset和end offset。\n  \n  \t这样算来如果第一个stage 50个task，每个Executor执行一个task，那么无论下游有几个task，就需要50*2=100个磁盘文件。\n  ```\n\n  - ==好处==\n\n  ```\n  1. 小文件明显变少了，一个task只生成一个file文件\n  2. file文件整体有序，加上索引文件的辅助，查找变快，虽然排序浪费一些性能，但是查找变快很多\n  ```\n\n  ##### bypass模式的sortShuffle\n\n  - bypass机制运行条件\n    - shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值\n    - 不是聚合类的shuffle算子（比如reduceByKey）\n\n![bypasssortshuffle](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/bypasssortshuffle.png)\n\n  - ==好处==\n\n  ```\n      该机制与sortshuffle的普通机制相比，在shuffleMapTask不多的情况下，首先写的机制是不同，其次不会进行排序。这样就可以节约一部分性能开销。\n  ```\n\n  - ==总结==\n\n  ```\n在shuffleMapTask数量小于默认值200时，启用bypass模式的sortShuffle(原因是数据量本身比较少，没必要进行sort全排序，因为数据量少本身查询速度就快，正好省了sort的那部分性能开销。)\n \n 该机制与普通SortShuffleManager运行机制的不同在于：\n    第一: 磁盘写机制不同；\n    第二: 不会进行sort排序；\n  ```\n\n\n\n### Shuffle调优\n\n#### 什么时候发生shuffle\n\n![1567231926304](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1567231926304.png)\n\n![1567232029615](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1567232029615.png)\n\n#### Shuffle核心组件\n\n碰到ShuffleDenpendency就进行stage的划分，ShuffleMapStage: 为shuffle提供数据的中间stage，ResultStage: 为一个action操作计算结果的stage。\n\n![1567232245246](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1567232245246.png)\n\n#### Shuffle组件调度\n\n![1567232408746](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1567232408746.png)\n\n\n\n\n\n#### Shuffle原理剖析\n\n##### MapOutputTracker\n\n解决的一个问题是resut task如何知道从哪个Executor去拉取Shuffle data\n\n![1567232849493](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1567232849493.png)\n\n##### ShuffleWriter\n\n**（1）HashShuffleWriter**\n\n![1567232923386](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1567232923386.png)\n\n特点：根据Hash分区，分区数是m * n 个。\n\n~~~scala\nval counts: RDD[(String, Int)] \n\t= wordCount.reduceByKey(new HashPartitioner(2), (x, y) => x + y)\n\n~~~\n\n![1567233073556](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1567233073556.png)\n\n**（2）SortShuffleWriter**\n\n![1567233308867](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1567233308867.png)\n\n特点：\n\na、文件数量为m\n\nb、如果需要排序或者需要combine，那么每一个partition数据排序要自己实现。（SortShuffleWriter里的sort指的是对partition的分区号进行排序）\n\nc、数据先放在内存,内存不够则写到磁盘中,最后再全部写到磁盘。\n\n**（3）BypassMergeSortShuffleWriter**\n\n![1567234608013](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1567234608013.png)\n\n\n\n这种模式同时具有HashShuffleWriter和SortShuffleter的特点。因为其实HashShufflerWriter的性能不错，但是如果task数太多的话，性能话下降，所以Spark在task数较少的时候自动使用了这种模式，一开始还是像HashShufflerWriter那种生成多个文件，但是最后会把多个文件合并成一个文件。然后下游来读取文件。默认map的分区需要小于spark.shuffle.sort.bypassMergeThreshold(默认是200),因为如何分区数太多，产生的小文件就会很多性能就会下降。\n\n\n\n##### ShuffleReader\n\n![1567235022607](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1567235022607.png)\n\n##### Spark Shuffle参数调优\n\n==spark.shuffle.file.buffer==\n\n- 默认值：32k\n- 参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。\n- 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。\n\n==spark.reducer.maxSizeInFlight==\n\n- 默认值：48m\n- 参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。\n- 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。\n\n==spark.shuffle.io.maxRetries==\n\n- 默认值：3\n- 参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。\n- 调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。\n\n==spark.shuffle.io.retryWait==\n\n- 默认值：5s\n- 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。\n- 调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。\n\n==spark.shuffle.memoryFraction==（Spark1.6是这个参数，1.6以后参数变了，具体参考上一讲Spark内存模型知识）\n\n- 默认值：0.2\n- 参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。\n- 调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。\n\n==spark.shuffle.manager==\n\n- 默认值：sort\n- 参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。Spark1.6以后把hash方式给移除了，tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。\n- 调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。\n\n==spark.shuffle.sort.bypassMergeThreshold==\n\n- 默认值：200\n- 参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。\n- 调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。\n\n","tags":["调优","spark shuffle"]},{"title":"Spark常见数据倾斜分析","url":"/2019/09/20/it/spark/Spark常见数据倾斜分析/","content":"\n### 数据倾斜原理和现象分析\n\n~~~\n\t有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。\n\t数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。\n~~~\n\n#### 数据倾斜现象分析\n\n* （1）绝大多数task执行得都非常快，但个别task执行极慢\n\n  ~~~\n  \t你的大部分的task，都执行的特别快，很快就执行完了，剩下几个task，执行的特别特别慢，\n  前面的task，一般10s可以执行完5个；最后发现某个task，要执行1个小时，2个小时才能执行完一个task。\n  \t\n  \t这个时候就出现数据倾斜了。\n  这种方式还算好的，因为虽然老牛拉破车一样，非常慢，但是至少还能跑。\n  ~~~\n\n* （2）绝大数task执行很快，有的task直接报OOM (Jvm Out Of Memory) 异常\n\n  ~~~\n  \t运行的时候，其他task都很快执行完了，也没什么特别的问题；但是有的task，就是会突然间报了一个OOM，JVM Out Of Memory，内存溢出了，task failed，task lost，resubmitting task等日志异常信息。反复执行几次都到了某个task就是跑不通，最后就挂掉。\n  \n  \t某个task就直接OOM，那么基本上也是因为数据倾斜了，task分配的数量实在是太大了！！！所以内存放不下，然后你的task每处理一条数据，还要创建大量的对象。内存爆掉了。\n  ~~~\n\n\n#### 数据倾斜原理剖析\n\n![数据倾斜](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/数据倾斜.png)\n\n~~~\n如上图所示：\n\t在进行任务计算shuffle操作的时候，第一个task和第二个task各分配到了1万条数据；需要5分钟计算完毕；第一个和第二个task，可能同时在5分钟内都运行完了；第三个task要98万条数据，98 * 5 = 490分钟 = 8个小时；\n\t本来另外两个task很快就运行完毕了（5分钟），第三个task数据量比较大，要8个小时才能运行完，就导致整个spark作业，也得8个小时才能运行完。最终导致整个spark任务计算特别慢。\n~~~\n\n\n\n#### 数据倾斜定位分析\n\n* 主要是根据log日志信息去定位\n\n  ~~~\n  \t数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。\n  \t出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。因为某个或者某些key对应的数据，远远的高于其他的key。\n  ~~~\n\n* 分析定位逻辑\n\n  ~~~\n  \t由于代码中有大量的shuffle操作，一个job会划分成很多个stage，首先要看的，就是数据倾斜发生在第几个stage中。\n  \t可以在任务运行的过程中，观察任务的UI界面，可以观察到每一个stage中运行的task的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。\n  \t比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完;而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。\n  \t此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。\n  ~~~\n\n![20170308091203159](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/20170308091203159.png)\n\n**某个task莫名其妙内存溢出的情况**\n\n~~~\n\t这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。\n\t但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。\n~~~\n\n**查看导致数据倾斜的key的数据分布情况**\n\n~~~\n\t知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。\n此时根据你执行操作的情况不同，可以有很多种查看key分布的方式：\n\t如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。\n\t如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。\n\t举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。\n~~~\n\n~~~scala\nval sampledPairs = pairs.sample(false, 0.1)\nval sampledWordCounts = sampledPairs.countByKey()\nsampledWordCounts.foreach(println(_))\n\n//sample算子时用来抽样用的，其有3个参数\n\n//withReplacement：表示抽出样本后是否在放回去，true表示会放回去，这也就意味着抽出的样本可能有重复\n\n//fraction ：抽出多少，这是一个double类型的参数,0-1之间，eg:0.3表示抽出30%\n\n//seed：表示一个种子，根据这个seed随机抽取，一般情况下只用前两个参数就可以，那么这个参数是干嘛的呢，这个参数一般用于调试，有时候不知道是程序出问题还是数据出了问题，就可以将这个参数设置为定值\n~~~\n\n#### 数据倾斜综述\n\n* 数据本身问题\n\n  ~~~\n  （1）、key本身分布不均衡（包括大量的key为空）\n  （2）、key的设置不合理\n  ~~~\n\n* spark使用不当的问题\n\n  ~~~\n  （1）、shuffle时的并发度不够\n  （2）、计算方式有误\t\n  ~~~\n\n\n#### 数据倾斜的后果\n\n~~~\n（1）spark中的stage的执行时间受限于最后那个执行完成的task,因此运行缓慢的任务会拖垮整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。\n\n（2）过多的数据在同一个task中运行，将会把executor内存撑爆，导致OOM内存溢出。\n~~~\n\n### spark常见数据倾斜的解决方案\n\n#### 使用Hive ETL预处理\n\n<font color=red>方案适用场景</font>：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀(比如某个key对应了100万数据，其他key才对应了10条数据)，而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。\n<font color=red>方案实现思路</font>：此时可以评估一下，是否可以通过Hive来进行数据预处理(即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join)，然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。\n<font color=red>方案实现原理</font>：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。\n<font color=red>方案优点</font>：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。\n<font color=red>方案缺点</font>：治标不治本，Hive ETL中还是会发生数据倾斜。\n<font color=red>方案实践经验</font>：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。\n<font color=red>项目实践经验</font>：有一个交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。\n\n![交互式用户行为分析系统](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/交互式用户行为分析系统.png)\n\n\n\n#### 过滤少数导致倾斜的key\n\n　　<font color=red>方案适用场景</font>：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。\n　　<font color=red>方案实现思路</font>：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。\n　　<font color=red>方案实现原理</font>：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。\n　　<font color=red>方案优点</font>：实现简单，而且效果也很好，可以完全规避掉数据倾斜。\n　　<font color=red>方案缺点</font>：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。\n　　<font color=red>方案实践经验</font>：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。\n\n\n\n#### 提高shuffle并行度(效果差)\n\n　　　<font color=red>方案适用场景</font>：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。\n　　　<font color=red>方案实现思路</font>：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。\n　　　<font color=red>方案实现原理</font>：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。\n　　　<font color=red>方案优点</font>：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。\n　　　<font color=red>方案缺点</font>：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。\n　　　<font color=red>方案实践经验</font>：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。\n\n![1570609831990](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1570609831990.png)\n\n\n\n#### 两阶段聚合（局部+全局）\n\n　　<font color=red>方案适用场景</font>：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。\n　　<font color=red>方案实现思路</font>：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。\n　　<font color=red>方案实现原理</font>：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。\n　　<font color=red>方案优点</font>：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。\n　　<font color=red>方案缺点</font>：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。\n\n~~~scala\n//案例\n//  如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作流程如下:\n//    (1) 将原始的 key 转化为  随机值 + key  (随机值 = Random.nextInt)\n//    (2) 对数据进行 reduceByKey(func)\n//    (3) 将 key + 随机值转成 key\n//    (4) 再对数据进行 reduceByKey(func)\n\nobject WordCountAggTest {\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"WordCount\")\n    val sc = new SparkContext(conf)\n    val array = Array(\"you you\",\"you you\",\"you you\",\n      \"you you\",\n      \"you you\",\n      \"you you\",\n      \"you you\",\n      \"jump jump\")\n    val rdd = sc.parallelize(array,8)\n    rdd.flatMap( line => line.split(\" \"))\n      .map(word =>{\n        val prefix = (new util.Random).nextInt(3)\n        (prefix+\"_\"+word,1)\n      }).reduceByKey(_+_)\n       .map( wc =>{\n         val newWord=wc._1.split(\"_\")(1)\n         val count=wc._2\n         (newWord,count)\n       }).reduceByKey(_+_)\n      .foreach( wc =>{\n        println(\"单词：\"+wc._1 + \" 次数：\"+wc._2)\n      })\n\n  }\n}\n注：我们这儿使用的是reduceByKey天然的有调优的效果，如果这儿是groupBykey那么发生数据倾斜的概率就会更大，更严重。\n~~~\n\n\n\n#### reduce join转为map join\n\n　　<font color=red>方案适用场景</font>：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。\n　　<font color=red>方案实现思路</font>：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。\n　　<font color=red>方案实现原理</font>：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。\n　　<font color=red>方案优点</font>：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。\n　　<font color=red>方案缺点</font>：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。\n\n![reduce joinz转换为map join ](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/reduce joinz转换为map join .png)\n\n~~~scala\nobject MapJoinTest {\n \n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"WordCount\")\n    val sc = new SparkContext(conf)\n    val lista=Array(\n      Tuple2(\"001\",\"令狐冲\"),\n      Tuple2(\"002\",\"任盈盈\")\n    )\n     //数据量小一点\n    val listb=Array(\n      Tuple2(\"001\",\"一班\"),\n      Tuple2(\"002\",\"二班\")\n    )\n    val listaRDD = sc.parallelize(lista)\n    val listbRDD = sc.parallelize(listb)\n    //val result: RDD[(String, (String, String))] = listaRDD.join(listbRDD)\n     //设置广播变量\n    val listbBoradcast = sc.broadcast(listbRDD.collect())\n    listaRDD.map(  tuple =>{\n      val key = tuple._1\n      val name = tuple._2\n      val map = listbBoradcast.value.toMap\n      val className = map.get(key)\n      (key,(name,className))\n    }).foreach( tuple =>{\n      println(\"班级号\"+tuple._1 + \" 姓名：\"+tuple._2._1 + \" 班级名：\"+tuple._2._2.get)\n    })\n  }\n}\n~~~\n\n\n\n#### 采样倾斜key并分拆join操作\n\n　　<font color=red>方案适用场景</font>：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。\n　　<font color=red>方案实现思路</font>：\n　　1、对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。\n　　2、然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。\n　　3、接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。\n　　4、再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，==此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。==\n　　5、而另外两个普通的RDD就照常join即可。\n　　6、最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。\n　　<font color=red>方案实现原理</font>：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。\n　　<font color=red>方案优点</font>：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。\n　　<font color=red>方案缺点</font>：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。\n\n![随机前缀和扩容RDD](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/随机前缀和扩容RDD.png)\n\n\n\n#### 随机前缀和扩容RDD进行join\n\n　　<font color=red>方案适用场景</font>：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用这一种方案来解决问题了。\n　　<font color=red>方案实现思路</font>：\n　　1、该方案的实现思路基本和“上一个方案”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。\n　　2、然后将该RDD的每条数据都打上一个n以内的随机前缀。\n　　3、同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。\n　　4、最后将两个处理后的RDD进行join即可。\n　　方案实现原理：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。\n　　<font color=red>方案优点</font>：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。\n　　<font color=red>方案缺点</font>：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。\n　　<font color=red>方案实践经验</font>：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。","tags":["数据倾斜"]},{"title":"Spark调优点","url":"/2019/09/05/it/spark/Spark调优点/","content":"\n# Spark调优\n\n### 分配更多的资源\n\n~~~markdown\n# 分配更多的资源：\n 它是性能优化调优的王道，就是增加和分配更多的资源，这对于性能和速度上的提升是显而易见的，\n\t基本上，在一定范围之内，增加资源与性能的提升，是成正比的；写完了一个复杂的spark作业之后，进行性能调优的时候，首先第一步，就是要来调节最优的资源配置；\n\t在这个基础之上，如果说你的spark作业，能够分配的资源达到了你的能力范围的顶端之后，无法再分配更多的资源了，公司资源有限；那么才是考虑去做后面的这些性能调优的点。\n\n#  分配哪些资源？\n\texecutor-memory、executor-cores、driver-memory\n# 在哪里可以设置这些资源？\n    spark-submit \\\n      --master spark://node1:7077 \\\n      --class cn.itcast.WordCount \\\n      --num-executors 3 \\    配置executor的数量\n      --driver-memory 1g \\   配置driver的内存（影响不大）\n      --executor-memory 1g \\ 配置每一个executor的内存大小\n      --executor-cores 3 \\   配置每一个executor的cpu个数\n \t\t\t/export/servers/wordcount.jar\n##  剖析为什么分配这些资源之后，性能可以得到提升？\n ==Standalone模式==\n    先计算出公司spark集群上的所有资源 每台节点的内存大小和cpu核数，\n    比如：一共有20台worker节点，每台节点8g内存，10个cpu。\n    实际任务在给定资源的时候，可以给20个executor、每个executor的内存8g、每个executor的使用的cpu个数10。 \n ==Yarn模式==\n    先计算出yarn集群的所有大小，比如一共500g内存，100个cpu；\n    这个时候可以分配的最大资源，比如给定50个executor、每个executor的内存大小10g,每个executor使用的cpu个数为2。\n#  使用原则\n  在资源比较充足的情况下，尽可能的使用更多的计算资源，尽量去调节到最大的大小\n~~~\n\n#### 调优分析\n\n![spark性能优化--分配资源](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/spark性能优化--分配资源.png)\n\n\n\n### 提高并行度\n\n~~~\n spark作业中，各个stage的task的数量，也就代表了spark作业在各个阶段stage的并行度！\n    当分配完所能分配的最大资源了，然后对应资源去调节程序的并行度，如果并行度没有与资源相匹配，那么导致你分配下去的资源都浪费掉了。同时并行运行，还可以让每个task要处理的数量变少（很简单的原理。合理设置并行度，可以充分利用集群资源，减少每个task处理数据量，而增加性能加快运行速度。）\n~~~\n\n#### 如何提高并行度\n\n##### task的数量\n\n~~~\n\t至少设置成与spark Application 的总cpu core 数量相同。\n\t最理想情况，150个core，分配150task，一起运行，差不多同一时间运行完毕\n\t官方推荐，task数量，设置成spark Application 总cpu core数量的2~3倍 。\n\t\n\t\n\t比如150个cpu core ，基本设置task数量为300~500. 与理想情况不同的，有些task会运行快一点，比如50s就完了，有些task 可能会慢一点，要一分半才运行完，所以如果你的task数量，刚好设置的跟cpu core 数量相同，可能会导致资源的浪费。\n\t因为比如150个task中10个先运行完了，剩余140个还在运行，但是这个时候，就有10个cpu core空闲出来了，导致浪费。如果设置2~3倍，那么一个task运行完以后，另外一个task马上补上来，尽量让cpu core不要空闲。同时尽量提升spark运行效率和速度。提升性能。\n~~~\n\n##### 如何设置task数量\n\n~~~\n设置参数spark.defalut.parallelism  \n   默认是没有值的，如果设置了值为10，它会在shuffle的过程才会起作用。\n   比如 val rdd2 = rdd1.reduceByKey(_+_) \n   此时rdd2的分区数就是10\n   \n可以通过在构建SparkConf对象的时候设置，例如：\n   new SparkConf().set(\"spark.defalut.parallelism\",\"500\")\n~~~\n\n##### RDD重新设置partition的数量\n\n~~~\n使用rdd.repartition 来重新分区，该方法会生成一个新的rdd，使其分区数变大。\n此时由于一个partition对应一个task，那么对应的task个数越多，通过这种方式也可以提高并行度。\n~~~\n\n#####  提高sparksql运行的task数量\n\n~~~\n通过设置参数 spark.sql.shuffle.partitions=500  默认为200；\n可以适当增大，来提高并行度。 比如设置为 spark.sql.shuffle.partitions=500\n~~~\n\n### RDD的重用和持久化\n\n#### 实际开发遇到的情况说明\n\n![rdd重用1](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/rdd重用1.png)\n\n~~~\n如上图所示的计算逻辑：\n（1）当第一次使用rdd2做相应的算子操作得到rdd3的时候，就会从rdd1开始计算，先读取HDFS上的文件，然后对rdd1做对应的算子操作得到rdd2,再由rdd2计算之后得到rdd3。同样为了计算得到rdd4，前面的逻辑会被重新计算。\n\n（3）默认情况下多次对一个rdd执行算子操作，去获取不同的rdd，都会对这个rdd及之前的父rdd全部重新计算一次。\n这种情况在实际开发代码的时候会经常遇到，但是我们一定要避免一个rdd重复计算多次，否则会导致性能急剧降低。\n\n总结：可以把多次使用到的rdd，也就是公共rdd进行持久化，避免后续需要，再次重新计算，提升效率。\n~~~\n\n![rdd重用2](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/rdd重用2.png)\n\n\n\n#### rdd持久化\n\n* 可以调用rdd的cache或者persist方法。\n\n~~~\n（1）cache方法默认是把数据持久化到内存中 ，例如：rdd.cache ，其本质还是调用了persist方法\n（2）persist方法中有丰富的缓存级别，这些缓存级别都定义在StorageLevel这个object中，可以结合实际的应用场景合理的设置缓存级别。例如： rdd.persist(StorageLevel.MEMORY_ONLY),这是cache方法的实现。\n~~~\n\n#### rdd持久化，采用序列化\n\n~~~\n（1）如果正常将数据持久化在内存中，那么可能会导致内存的占用过大，这样的话，也许会导致OOM内存溢出。\n（2）当纯内存无法支撑公共RDD数据完全存放的时候，就优先考虑使用序列化的方式在纯内存中存储。将RDD的每个partition的数据，序列化成一个字节数组；序列化后，大大减少内存的空间占用。\n（3）序列化的方式，唯一的缺点就是，在获取数据的时候，需要反序列化。但是可以减少占用的空间和便于网络传输\n（4）如果序列化纯内存方式，还是导致OOM，内存溢出；就只能考虑磁盘的方式，内存+磁盘的普通方式（无序列化）。\n（5）为了数据的高可靠性，而且内存充足，可以使用双副本机制，进行持久化\n\t持久化的双副本机制，持久化后的一个副本，因为机器宕机了，副本丢了，就还是得重新计算一次；\n\t持久化的每个数据单元，存储一份副本，放在其他节点上面，从而进行容错；\n\t一个副本丢了，不用重新计算，还可以使用另外一份副本。这种方式，仅仅针对你的内存资源极度充足。\n\t 比如: StorageLevel.MEMORY_ONLY_2\n~~~\n\n### 广播变量的使用\n\n~~~\n\t在实际工作中可能会遇到这样的情况，由于要处理的数据量非常大，这个时候可能会在一个stage中出现大量的task，比如有1000个task，这些task都需要一份相同的数据来处理业务，这份数据的大小为100M，该数据会拷贝1000份副本，通过网络传输到各个task中去，给task使用。这里会涉及大量的网络传输开销，同时至少需要的内存为1000*100M=100G，这个内存开销是非常大的。不必要的内存的消耗和占用，就导致了你在进行RDD持久化到内存，也许就没法完全在内存中放下；就只能写入磁盘，最后导致后续的操作在磁盘IO上消耗性能；这对于spark任务处理来说就是一场灾难。\n\n    由于内存开销比较大，task在创建对象的时候，可能会出现堆内存放不下所有对象，就会导致频繁的垃圾回收器的回收GC。GC的时候一定是会导致工作线程停止，也就是导致Spark暂停工作那么一点时间。频繁GC的话，对Spark作业的运行的速度会有相当可观的影响。\n\n~~~\n\n![task共享数据](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/task共享数据.png)\n\n\n\n#### 广播变量引入\n\n~~~\n\tSpark中分布式执行的代码需要传递到各个executor的task上运行。对于一些只读、固定的数据,每次都需要Driver广播到各个Task上，这样效率低下。广播变量允许将变量只广播给各个executor。该executor上的各个task再从所在节点的BlockManager(负责管理某个executor对应的内存和磁盘上的数据)获取变量，而不是从Driver获取变量，从而提升了效率。\n~~~\n\n![广播变量](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/广播变量.png)\n\n~~~\n广播变量，初始的时候，就在Drvier上有一份副本。通过在Driver把共享数据转换成广播变量。\n\n\ttask在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中，尝试获取变量副本；如果本地没有，那么就从Driver远程拉取广播变量副本，并保存在本地的BlockManager中；\n\t\n\t此后这个executor上的task，都会直接使用本地的BlockManager中的副本。那么这个时候所有该executor中的task都会使用这个广播变量的副本。也就是说一个executor只需要在第一个task启动时，获得一份广播变量数据，之后的task都从本节点的BlockManager中获取相关数据。\n\n\texecutor的BlockManager除了从driver上拉取，也可能从其他节点的BlockManager上拉取变量副本，网络距离越近越好。\n~~~\n\n#### 使用广播变量后的性能分析\n\n~~~\n比如一个任务需要50个executor，1000个task，共享数据为100M。\n(1)在不使用广播变量的情况下，1000个task，就需要该共享数据的1000个副本，也就是说有1000份数需要大量的网络传输和内存开销存储。耗费的内存大小1000*100=100G.\n\n(2)使用了广播变量后，50个executor就只需要50个副本数据，而且不一定都是从Driver传输到每个节点，还可能是就近从最近的节点的executor的blockmanager上拉取广播变量副本，网络传输速度大大增加；内存开销 50*100M=5G\n\n总结：\n\t不使用广播变量的内存开销为100G，使用后的内存开销5G，这里就相差了20倍左右的网络传输性能损耗和内存开销，使用广播变量后对于性能的提升和影响，还是很可观的。\n\t\n\t广播变量的使用不一定会对性能产生决定性的作用。比如运行30分钟的spark作业，可能做了广播变量以后，速度快了2分钟，或者5分钟。但是一点一滴的调优，积少成多。最后还是会有效果的。\n~~~\n\n#### 广播变量使用注意事项\n\n~~~\n（1）能不能将一个RDD使用广播变量广播出去？\n\n       不能，因为RDD是不存储数据的。可以将RDD的结果广播出去。\n\n（2）广播变量只能在Driver端定义，不能在Executor端定义。\n\n（3）在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。\n\n（4）如果executor端用到了Driver的变量，如果不使用广播变量在Executor有多少task就有多少Driver端的变量副本。\n\n（5）如果Executor端用到了Driver的变量，如果使用广播变量在每个Executor中只有一份Driver端的变量副本。\n~~~\n\n#### 示例\n\n~~~\n(1) 通过sparkContext的broadcast方法把数据转换成广播变量，类型为Broadcast，\n\tval broadcastArray: Broadcast[Array[Int]] = sc.broadcast(Array(1,2,3,4,5,6))\n\t\n(2) 然后executor上的BlockManager就可以拉取该广播变量的副本获取具体的数据。\n\t\t获取广播变量中的值可以通过调用其value方法\n\t val array: Array[Int] = broadcastArray.value\n~~~\n\n### 避免使用shuffle类算子\n\n~~~\n\tspark中的shuffle涉及到数据要进行大量的网络传输，下游阶段的task任务需要通过网络拉取上阶段task的输出数据，shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。\n\t\n\t如果有可能的话，要尽量避免使用shuffle类算子。\n\t因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。\n\t\n~~~\n\n#### 会产生shuffle的算子\n\n~~~\n\tspark程序在开发的过程中使用reduceByKey、join、distinct、repartition等算子操作，这里都会产生shuffle，由于shuffle这一块是非常耗费性能的，实际开发中尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。\n~~~\n\n#### 避免产生shuffle\n\n* 小案例\n\n~~~scala\n//错误的做法：\n// 传统的join操作会导致shuffle操作。\n// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。\nval rdd3 = rdd1.join(rdd2)\n    \n//正确的做法：\n// Broadcast+map的join操作，不会导致shuffle操作。\n// 使用Broadcast将一个数据量较小的RDD作为广播变量。\nval rdd2Data = rdd2.collect()\nval rdd2DataBroadcast = sc.broadcast(rdd2Data)\n\n// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。\n// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。\n// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。\nval rdd3 = rdd1.map(rdd2DataBroadcast...)\n\n// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。\n// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。\n~~~\n\n#### 使用map-side预聚合的shuffle操作\n\n* map-side预聚合\n\n~~~\n\t如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。\n\n\t所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。\n\tmap-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。\n\t通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。\n\t而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。\n\t\n\t比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。\n~~~\n\n* ==groupByKey进行单词计数原理==\n\n![1577080609633](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/groupByKey.png)\n\n* ==reduceByKey单词计数原理==\n\n![1577080686083](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/reduceByKey.png)\n\n### 高性能的算子\n\n* reduceByKey/aggregateByKey 可以进行预聚合操作，减少数据的传输量，提升性能\n* groupByKey 不会进行预聚合操作，进行数据的全量拉取，性能比较低\n\n~~~\n\tmapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。\n\t但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！\n~~~\n\n~~~\n\t原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。\n\t在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；\t但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。\n~~~\n\n- filter之后进行coalesce操作\n\n~~~\n\t通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。\n\t因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。\n\t因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。\n~~~\n\n- 使用repartitionAndSortWithinPartitions替代repartition与sort类操作\n\n~~~\n\trepartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。\n\t因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。\n~~~\n\n### Kryo优化序列化\n\n~~~\n\tSpark在进行任务计算的时候，会涉及到数据跨进程的网络传输、数据的持久化，这个时候就需要对数据进行序列化。Spark默认采用Java的序列化器。默认java序列化的优缺点如下:\n其好处：\n\t处理起来方便，不需要我们手动做其他操作，只是在使用一个对象和变量的时候，需要实现Serializble接口。\n其缺点：\n\t默认的序列化机制的效率不高，序列化的速度比较慢；序列化以后的数据，占用的内存空间相对还是比较大。\n\nSpark支持使用Kryo序列化机制。Kryo序列化机制，比默认的Java序列化机制，速度要快，序列化后的数据要更小，大概是Java序列化机制的1/10。所以Kryo序列化优化以后，可以让网络传输的数据变少；在集群中耗费的内存资源大大减少。\n\n~~~\n\n-  Kryo序列化启用后生效的地方\n\n~~~\nKryo序列化机制，一旦启用以后，会生效的几个地方：\n（1）算子函数中使用到的外部变量\n\t算子中的外部变量可能来着与driver需要涉及到网络传输，就需要用到序列化。\n\t    最终可以优化网络传输的性能，优化集群中内存的占用和消耗\n\t\t\n（2）持久化RDD时进行序列化，StorageLevel.MEMORY_ONLY_SER\n\t将rdd持久化时，对应的存储级别里，需要用到序列化。\n\t    最终可以优化内存的占用和消耗；持久化RDD占用的内存越少，task执行的时候，创建的对象，就不至于频繁的占满内存，频繁发生GC。\n\t\t\n（3）\t产生shuffle的地方，也就是宽依赖\n\t下游的stage中的task，拉取上游stage中的task产生的结果数据，跨网络传输，需要用到序列化。最终可以优化网络传输的性能\n\t\n\t\n~~~\n\n~~~scala\n// 创建SparkConf对象。\nval conf = new SparkConf().setMaster(...).setAppName(...)\n// 设置序列化器为KryoSerializer。\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n\n// 注册要序列化的自定义类型。\nconf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))\n~~~\n\n### fastutil优化数据格式\n\n```\nfastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue；\n\nfastutil能够提供更小的内存占用，更快的存取速度；我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set.\n```\n\n#### fastutil好处\n\n```\nfastutil集合类，可以减小内存的占用，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度；\n```\n\n#### Spark中应用fastutil的场景和使用\n\n##### 算子函数使用了外部变量\n\n```\n（1）你可以使用Broadcast广播变量优化；\n\n（2）可以使用Kryo序列化类库，提升序列化性能和效率；\n\n（3）如果外部变量是某种比较大的集合，那么可以考虑使用fastutil改写外部变量；\n\n首先从源头上就减少内存的占用(fastutil)，通过广播变量进一步减少内存占用，再通过Kryo序列化类库进一步减少内存占用。\n```\n\n##### 算子函数里使用了比较大的集合Map/List\n\n```\n在你的算子函数里，也就是task要执行的计算逻辑里面，如果有逻辑中，出现，要创建比较大的Map、List等集合，\n可能会占用较大的内存空间，而且可能涉及到消耗性能的遍历、存取等集合操作； \n那么此时，可以考虑将这些集合类型使用fastutil类库重写，\n\n使用了fastutil集合类以后，就可以在一定程度上，减少task创建出来的集合类型的内存占用。 \n避免executor内存频繁占满，频繁唤起GC，导致性能下降\n```\n\n```\n第一步：在pom.xml中引用fastutil的包\n    <dependency>\n      <groupId>fastutil</groupId>\n      <artifactId>fastutil</artifactId>\n      <version>5.0.9</version>\n    </dependency>\n    \n第二步：平时使用List （Integer）的替换成IntList即可。 \n\tList<Integer>的list对应的到fastutil就是IntList类型\n\t\n\t\n使用说明：\n基本都是类似于IntList的格式，前缀就是集合的元素类型； \n特殊的就是Map，Int2IntMap，代表了key-value映射的元素类型。\n```\n\n\n\n### 调节数据本地化等待时长\n\n~~~\n\tSpark在Driver上对Application的每一个stage的task进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先会希望每个task正好分配到它要计算的数据所在的节点，这样的话就不用在网络间传输数据；\n\n\t但是通常来说，有时事与愿违，可能task没有机会分配到它的数据所在的节点，为什么呢，可能那个节点的计算资源和计算能力都满了；所以这种时候，通常来说，Spark会等待一段时间，默认情况下是3秒（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后实在是等待不了了，就会选择一个比较差的本地化级别，比如说将task分配到距离要计算的数据所在节点比较近的一个节点，然后进行计算。\n\n~~~\n\n\n\n#### 本地化级别\n\n```\n（1）PROCESS_LOCAL：进程本地化\n\t代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好\n（2）NODE_LOCAL：节点本地化\n\t代码和数据在同一个节点中；比如说数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是数据和task在一个节点上的不同executor中；数据需要在进程间进行传输；性能其次\n（3）RACK_LOCAL：机架本地化\t\n\t数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输； 性能比较差\n（4）\tANY：无限制\n\t数据和task可能在集群中的任何地方，而且不在一个机架中；性能最差\n\t\n```\n\n#### 数据本地化等待时长\n\n```\nspark.locality.wait，默认是3s\n首先采用最佳的方式，等待3s后降级,还是不行，继续降级...,最后还是不行，只能够采用最差的。\n\n```\n\n#### 如何调节参数并且测试\n\n```\n修改spark.locality.wait参数，默认是3s，可以增加\n\n下面是每个数据本地化级别的等待时间，默认都是跟spark.locality.wait时间相同，\n默认都是3s(可查看spark官网对应参数说明，如下图所示)\nspark.locality.wait.node\nspark.locality.wait.process\nspark.locality.wait.rack\n\n```\n\n![data-local-spark](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/data-local-spark.png)\n\n\n\n```\n在代码中设置：\nnew SparkConf().set(\"spark.locality.wait\",\"10\")\n\n然后把程序提交到spark集群中运行，注意观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。 \n日志里面会显示，starting task .... PROCESS LOCAL、NODE LOCAL.....\n例如：\nStarting task 0.0 in stage 1.0 (TID 2, 192.168.200.102, partition 0, NODE_LOCAL, 5254 bytes)\n\n观察大部分task的数据本地化级别 \n如果大多都是PROCESS_LOCAL，那就不用调节了。如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长。应该是要反复调节，每次调节完以后，再来运行，观察日志 \n看看大部分的task的本地化级别有没有提升；看看整个spark作业的运行时间有没有缩短。\n\n注意注意：\n在调节参数、运行任务的时候，别本末倒置，本地化级别倒是提升了， 但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了。\n```\n\n\n\n### Spark内存模型调优\n\n#### spark中executor内存划分\n\n* Executor的内存主要分为三块\n  * 第一块是让task执行我们自己编写的代码时使用；\n  * 第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用\n  * 第三块是让RDD缓存时使用\n\n#### spark的内存模型\n\n~~~\n\t在spark1.6版本以前 spark的executor使用的静态内存模型，但是在spark1.6开始，多增加了一个统一内存模型。\n\t通过spark.memory.useLegacyMode 这个参数去配置\n\t\t\t默认这个值是false，代表用的是新的动态内存模型；\n\t\t\t如果想用以前的静态内存模型，那么就要把这个值改为true。\n~~~\n\n\n\n##### 静态内存模型\n\n![1570604272790](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1570604272790.png)\n\n~~~\n实际上就是把我们的一个executor分成了三部分，\n\t一部分是Storage内存区域，\n\t一部分是execution区域，\n\t还有一部分是其他区域。如果使用的静态内存模型，那么用这几个参数去控制：\n\t\nspark.storage.memoryFraction：默认0.6\nspark.shuffle.memoryFraction：默认0.2  \n所以第三部分就是0.2\n\n如果我们cache数据量比较大，或者是我们的广播变量比较大，\n\t那我们就把spark.storage.memoryFraction这个值调大一点。\n\t但是如果我们代码里面没有广播变量，也没有cache，shuffle又比较多，那我们要把spark.shuffle.memoryFraction 这值调大。\n~~~\n\n* 静态内存模型的缺点\n\n~~~\n我们配置好了Storage内存区域和execution区域后，我们的一个任务假设execution内存不够用了，但是它的Storage内存区域是空闲的，两个之间不能互相借用，不够灵活，所以才出来我们新的统一内存模型。\n~~~\n\n\n\n##### 统一内存模型\n\n![img](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/image2018-11-1_16-39-33.png)\n\n~~~\n\t动态内存模型先是预留了300m内存，防止内存溢出。动态内存模型把整体内存分成了两部分，\n由这个参数表示spark.memory.fraction 这个指的默认值是0.6 代表另外的一部分是0.4,\n\n然后spark.memory.fraction 这部分又划分成为两个小部分。这两小部分共占整体内存的0.6 .这两部分其实就是：Storage内存和execution内存。由spark.memory.storageFraction 这个参数去调配，因为两个共占0.6。如果spark.memory.storageFraction这个值配的是0.5,那说明这0.6里面 storage占了0.5，也就是execution占了0.3 。\n~~~\n\n* 统一内存模型有什么特点呢?\n\n  ~~~\n  Storage内存和execution内存 可以相互借用。不用像静态内存模型那样死板，但是是有规则的\n  ~~~\n\n  * ==场景一==\n\n    * Execution使用的时候发现内存不够了，然后就会把storage的内存里的数据驱逐到磁盘上。\n\n      ![1570604662552](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1570604662552.png)\n\n\n  * ==场景二==\n    * 一开始execution的内存使用得不多，但是storage使用的内存多，所以storage就借用了execution的内存，但是后来execution也要需要内存了，这个时候就会把storage的内存里的数据写到磁盘上，腾出内存空间。\n\n![1570604675176](http://kflys.gitee.io/upic/2020/03/22/uPic/Spark调优/assets/1570604675176.png)\n\n\n\n~~~\n为什么受伤的都是storage呢？\n\n是因为execution里面的数据是马上就要用的，而storage里的数据不一定马上就要用。\t\n~~~\n\n\n\n##### 任务提交脚本参考\n\n* 以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节\n\n```shell\n./bin/spark-submit \\\n  --master yarn-cluster \\\n  --num-executors 100 \\\n  --executor-memory 6G \\\n  --executor-cores 4 \\\n  --driver-memory 1G \\\n  --conf spark.default.parallelism=1000 \\\n  --conf spark.storage.memoryFraction=0.5 \\\n  --conf spark.shuffle.memoryFraction=0.3 \\\n```\n\n~~~shell\njava.lang.OutOfMemoryError\nExecutorLostFailure\nExecutor exit code 为143\nexecutor lost\nhearbeat time out\nshuffle file lost\n\n# 如果遇到以上问题，很有可能就是内存除了问题，可以先尝试增加内存。如果还是解决不了，那么请听下一次数据倾斜调优的课。\n~~~\n\n","tags":["调优","spark"]},{"title":"SparkSQL知识梳理","url":"/2019/08/25/it/spark/SparkSQL/","content":"\n## Spark SQL\n\n- Spark SQL is Apache Spark's module for working with structured data.\n- SparkSQL是apache Spark用来处理结构化数据的一个模块\n\n### 的四大特性\n\n- ==1、易整合(Integrated)==\n\n  - ```sql\n    results = spark.sql(\"SELECT * FROM people\")\n    ```\n\n- ==2、统一的数据源访问(Uniform Data Access)==\n\n  - ```scala\n    spark.read.json(\"s3n://...\")\n    spark.read.text(\"s3n://...\")\n    spark.read.parquet(\"s3n://...\")\n    ```\n\n- ==3、兼容hive(Hive Integration)==\n\n- ==4、支持标准的数据库连接(Standard Connectivity)==\n\n  - ```scala\n    spark.read.jdbc(---)\n    ```\n\n### DataFrame概述\n\n- 在Spark中，DataFrame是一种==以RDD为基础的分布式数据集==，类似于==传统数据库的二维表格==\n- DataFrame带有==Schema元信息==，即DataFrame所表示的二维表数据集的每一列都带有名称和类型，但底层做了更多的优化\n- DataFrame可以从很多数据源构建\n  - 比如：已经存在的RDD、结构化文件、外部数据库、Hive表。\n- RDD可以把它内部元素看成是一个java对象\n- DataFrame可以把内部是一个Row对象，它表示一行一行的数据\n\n#### DataFrame和RDD的优缺点\n\n- ==1、RDD==\n\n  - ==优点==\n\n    - 1、编译时类型安全\n      - 开发会进行类型检查，在编译的时候及时发现错误\n    - 2、具有面向对象编程的风格\n\n  - ==缺点==\n\n    - 1、构建大量的java对象占用了大量heap堆空间，导致频繁的GC\n\n      ```\n      由于数据集RDD它的数据量比较大，后期都需要存储在heap堆中，这里有heap堆中的内存空间有限，出现频繁的垃圾回收（GC），程序在进行垃圾回收的过程中，所有的任务都是暂停。影响程序执行的效率\n      ```\n\n    - 2、数据的序列化和反序列性能开销很大\n\n      ```\n        在分布式程序中，对象(对象的内容和结构)是先进行序列化，发送到其他服务器，进行大量的网络传输，然后接受到这些序列化的数据之后，再进行反序列化来恢复该对象\n      ```\n\n- ==2、DataFrame==\n\n  - ==DataFrame引入了schema元信息和off-heap(堆外)==\n  - ==优点==\n    - 1、DataFrame引入off-heap，大量的对象构建直接使用操作系统层面上的内存，不在使用heap堆中的内存，这样一来heap堆中的内存空间就比较充足，不会导致频繁GC，程序的运行效率比较高，它是解决了RDD构建大量的java对象占用了大量heap堆空间，导致频繁的GC这个缺点。\n    - 2、DataFrame引入了schema元信息---就是数据结构的描述信息，后期spark程序中的大量对象在进行网络传输的时候，只需要把数据的内容本身进行序列化就可以，数据结构信息可以省略掉。这样一来数据网络传输的数据量是有所减少，数据的序列化和反序列性能开销就不是很大了。它是解决了RDD数据的序列化和反序列性能开销很大这个缺点\n    - ==缺点==\n      - DataFrame引入了schema元信息和off-heap(堆外)它是分别解决了RDD的缺点，同时它也丢失了RDD的优点\n        - 1、编译时类型不安全\n          - 编译时不会进行类型的检查，这里也就意味着前期是无法在编译的时候发现错误，只有在运行的时候才会发现\n        - 2、不在具有面向对象编程的风格\n\n```SCALA\n// 1. 读取文本文件\nval personDF=spark.read.text(\"/person.txt\")\nval peopleDF=spark.read.json(\"/people.json\")\nval usersDF=spark.read.parquet(\"/users.parquet\")\n\n// 2. 加载数据\nval rdd1=sc.textFile(\"/person.txt\").map(x=>x.split(\" \"))\n    //定义一个样例类\n    case class Person(id:String,name:String,age:Int)\n    //把rdd与样例类进行关联\n    val personRDD=rdd1.map(x=>Person(x(0),x(1),x(2).toInt))\n    //把rdd转换成DataFrame\n    val personDF=personRDD.toDF\n// 3.语法风格\n// 3.1 DSL\n        personDF.select(\"name\").show\n        personDF.select($\"name\").show\n        personDF.select(col(\"name\").show\n        //实现age+1\n         personDF.select($\"name\",$\"age\",$\"age\"+1)).show   \n        //实现age大于30过滤\n         personDF.filter($\"age\" > 30).show\n         //按照age分组统计次数\n         personDF.groupBy(\"age\").count.show \n        //按照age分组统计次数降序\n         personDF.groupBy(\"age\").count().sort($\"count\".desc)show  \n// 3.2 sql\n        //DataFrame注册成表\n        personDF.createTempView(\"person\")\n\n        //使用SparkSession调用sql方法统计查询\n        spark.sql(\"select * from person\").show\n        spark.sql(\"select name from person\").show\n        spark.sql(\"select name,age from person\").show\n```\n\n### DataSet概述\n\n- DataSet是分布式的数据集合，Dataset提供了强类型支持，也是在RDD的每行数据加了类型约束。\n- DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型和可以用强大lambda函数）以及使用了Spark SQL优化的执行引擎。\n\n```properties\n1. 假设RDD中的两行数据长这样\n        1,张三,23\n        2,李四,35\n2.那么DataFrame中的数据长这样\n\t\t\t\tID:String\tName:String\tAge:int\n        \t\t1\t\t\t\t张三\t\t\t\t23\n        \t\t2\t\t\t\t李四\t\t\t\t35\n3.Dataset中的数据长这样 \n\t\t\tvalue:String\n        1,张三,23\n        2,李四,35\n  或者\n  value:People(age:bigint,id:bigint,name:string)\n        People(id=1,name=\"张三\",age=23)\n        People(id=2,name=\"李四\",age=23)\n```\n\n```properties\nDataSet包含了DataFrame的功能，Spark2.0中两者统一，DataFrame表示为DataSet[Row]，即DataSet的子集。\n（1）DataSet可以在编译时检查类型\n（2）并且是面向对象的编程接口\n```\n\nDataFrame DataSet转换 构建dataset\n\n```scala\n// 把一个DataFrame转换成DataSet\nval dataSet=dataFrame.as[强类型]\n//  2、把一个DataSet转换成DataFrame\nval dataFrame=dataSet.toDF\n\n// 补充说明,可以从dataFrame和dataSet获取得到rdd\nval rdd1=dataFrame.rdd\nval rdd2=dataSet.rdd\n\n// 1、 通过sparkSession调用createDataset方法\n  val ds=spark.createDataset(1 to 10) //scala集合\n  val ds=spark.createDataset(sc.textFile(\"/person.txt\"))  //rdd\n\n// 2、使用scala集合和rdd调用toDS方法\n  sc.textFile(\"/person.txt\").toDS\n  List(1,2,3,4,5).toDS\n\n// 3、把一个DataFrame转换成DataSet\n  val dataSet=dataFrame.as[强类型]\n\n// 4、通过一个DataSet转换生成一个新的DataSet\n   List(1,2,3,4,5).toDS.map(x=>x*10)\n\n// 5、将rdd与Row对象进行关联\n    val rowRDD: RDD[Row] = data.map(x=>Row(x(0),x(1),x(2).toInt))\n    //指定dataFrame的schema信息   \n    //这里指定的字段个数和类型必须要跟Row对象保持一致\n    val schema=StructType(\n        StructField(\"id\",StringType)::\n        StructField(\"name\",StringType)::\n        StructField(\"age\",IntegerType)::Nil\n    )\n    val dataFrame: DataFrame = spark.createDataFrame(rowRDD,schema)\n```\n\n### 示例代码\n\n```scala\n// EG\n // 1、构建SparkSession对象,开启hive支持\n    val spark: SparkSession = SparkSession.builder()\n      .appName(\"HiveSupport\")\n      .master(\"local[2]\")\n      .enableHiveSupport() //开启对hive的支持\n      .getOrCreate()\n\n// 2. 读取mysql数据\n\t\tval spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()\n        val url=\"jdbc:mysql://node03:3306/spark\"\n        val tableName=\"user\"\n        val properties = new Properties()\n      properties.setProperty(\"user\",\"root\")\n      properties.setProperty(\"password\",\"123456\")\n   val mysqlDF: DataFrame = spark.read.jdbc(url,tableName,properties)\n\n// 3. 保存数据到mysql表中\n     //mode:指定数据的插入模式\n        //overwrite: 表示覆盖，如果表不存在，事先帮我们创建\n        //append   :表示追加， 如果表不存在，事先帮我们创建\n        //ignore   :表示忽略，如果表事先存在，就不进行任何操作\n        //error    :如果表事先存在就报错（默认选项）\n    result.write.mode(\"append\").jdbc(url,\"kaikeba\",properties)\n```\n\n### 自定义函数\n\n```scala\n//小写转大写\nsparkSession.udf.register(\"low2Up\",new UDF1[String,String]() {\n  override def call(t1: String): String = {\n    t1.toUpperCase\n  }\n},StringType)\n//大写转小写\nsparkSession.udf.register(\"up2low\",(x:String)=>x.toLowerCase)\n// 把数据文件中的单词统一转换成大小写\nsparkSession.sql(\"select  value from t_udf\").show()\nsparkSession.sql(\"select  low2Up(value) from t_udf\").show()\nsparkSession.sql(\"select  up2low(value) from t_udf\").show()\n```\n\n","tags":["spark sql"]},{"title":"SparkCore知识梳理","url":"/2019/08/01/it/spark/SparkCore/","content":"\n## Spark简介\n\n* **Apache Spark™** is a unified analytics engine for large-scale data processing.\n\n* spark是针对于大规模数据处理的统一分析引擎\n\n  ~~~\n  \tspark是在Hadoop基础上的改进，是UC Berkeley AMP lab所开源的类Hadoop MapReduce的通用的并行计算框架，Spark基于map reduce算法实现的分布式计算，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是Job中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的map reduce的算法。\n  \t\n  \tspark是基于内存计算框架，计算速度非常之快，但是它仅仅只是涉及到计算，并没有涉及到数据的存储，后期需要使用spark对接外部的数据源，比如hdfs。\n  ~~~\n\n### 四大特性\n\n- 速度快\n\n```markdown\n* 运行速度提高100倍\n  * Apache Spark使用最先进的DAG调度程序，查询优化程序和物理执行引擎，实现批量和流式数据的高性能。\n* spark比mapreduce快的2个主要原因\n  * 1、==基于内存==\n    （1）mapreduce任务后期再计算的时候，每一个job的输出结果会落地到磁盘，后续有其他的job需要依赖于前面job的输出结果，这个时候就需要进行大量的磁盘io操作。性能就比较低。\n    （2）spark任务后期再计算的时候，job的输出结果可以保存在内存中，后续有其他的job需要依赖于前面job的输出结果，这个时候就直接从内存中获取得到，避免了磁盘io操作，性能比较高\n  * 2、==进程与线程==\n    （1）mapreduce任务以进程的方式运行在yarn集群中，比如程序中有100个MapTask，一个task就需要一个进程，这些task要运行就需要开启100个进程。\n    （2）spark任务以线程的方式运行在进程中，比如程序中有100个MapTask，后期一个task就对应一个线程，这里就不在是进程，这些task需要运行，这里可以极端一点：\n    只需要开启1个进程，在这个进程中启动100个线程就可以了。\n    进程中可以启动很多个线程，而开启一个进程与开启一个线程需要的时间和调度代价是不一样。 开启一个进程需要的时间远远大于开启一个线程。\n```\n\n* 易用性\n\n```markdown\n- 可以快速去编写spark程序通过 java/scala/python/R/SQL等不同语言\n```\n\n- 通用性\n\n```markdown\n- spark框架不在是一个简单的框架，可以把spark理解成一个==**生态系统**==，它内部是包含了很多模块，基于不同的应用场景可以选择对应的模块去使用\n* ==**sparksql**==\n  * 通过sql去开发spark程序做一些离线分析\n* ==**sparkStreaming**==\n  * 主要是用来解决公司有实时计算的这种场景\n* ==**Mlib**==\n  * 它封装了一些机器学习的算法库\n* ==**Graphx**==\n  * 图计算\n```\n\n- 兼容性\n\n```markdown\n- spark程序就是一个计算逻辑程序，这个任务要运行就需要计算资源（内存、cpu、磁盘），哪里可以给当前这个任务提供计算资源，就可以把spark程序提交到哪里去运行\n* ==**standAlone**==\n  * 它是spark自带的独立运行模式，整个任务的资源分配由spark集群的老大Master负责\n* ==**yarn**==\n  * 可以把spark程序提交到yarn中运行，整个任务的资源分配由yarn中的老大ResourceManager负责\n* **mesos**\n  * 它也是apache开源的一个类似于yarn的资源调度平台\n```\n\n### 集群架构\n\n![spark](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/spark.png)\n\n- **==Driver==**\n  - 它会执行客户端写好的main方法，它会构建一个名叫SparkContext对象\n    - 该对象是所有spark程序的执行入口\n\n- **==Application==**\n  - 就是一个spark的应用程序，它是包含了客户端的代码和任务运行的资源信息\n\n- **==ClusterManager==**\n  - 它是给程序提供计算资源的外部服务\n    - **standAlone**\n      - 它是spark自带的集群模式，整个任务的资源分配由spark集群的老大Master负责\n    - **yarn**\n      - 可以把spark程序提交到yarn中运行，整个任务的资源分配由yarn中的老大ResourceManager负责\n    - **mesos**\n      - 它也是apache开源的一个类似于yarn的资源调度平台。\n\n\n- **==Master==**\n  - 它是整个spark集群的主节点，负责任务资源的分配\n\n- **==Worker==**\n  - 它是整个spark集群的从节点，负责任务计算的节点\n\n- **==Executor==**\n  - 它是一个进程，它会在worker节点启动该进程（计算资源）\n\n- **==Task==**\n  - spark任务是以task线程的方式运行在worker节点对应的executor进程中\n\n### 启动停止\n\n```markdown\n(1) 如何恢复到上一次活着master挂掉之前的状态?\n\t在高可用模式下，整个spark集群就有很多个master，其中只有一个master被zk选举成活着的master，其他的多个master都处于standby，同时把整个spark集群的元数据信息通过zk中节点进行保存。\n\n\t后期如果活着的master挂掉。首先zk会感知到活着的master挂掉，下面开始在多个处于standby中的master进行选举，再次产生一个活着的master，这个活着的master会读取保存在zk节点中的spark集群元数据信息，恢复到上一次master的状态。整个过程在恢复的时候经历过了很多个不同的阶段，每个阶段都需要一定时间，最终恢复到上个活着的master的转态，整个恢复过程一般需要1-2分钟。\n\n(2) 在master的恢复阶段对任务的影响?\n   a）对已经运行的任务是没有任何影响\n   \t  由于该任务正在运行，说明它已经拿到了计算资源，这个时候就不需要master。\n   b) 对即将要提交的任务是有影响\n   \t  由于该任务需要有计算资源，这个时候会找活着的master去申请计算资源，由于没有一个活着的master,该任务是获取不到计算资源，也就是任务无法运行。\n```\n\n### 示例\n\n```shell\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://node01:7077 \\\n--executor-memory 1G \\\n--total-executor-cores 2 \\\nexamples/jars/spark-examples_2.11-2.3.3.jar \\\n10\n\n####参数说明\n--class：指定包含main方法的主类\n--master：指定spark集群master地址\n--executor-memory：指定任务在运行的时候需要的每一个executor内存大小\n--total-executor-cores： 指定任务在运行的时候需要总的cpu核数\n\n### 注意\nspark集群中有很多个master，并不知道哪一个master是活着的master，即使你知道哪一个master是活着的master，它也有可能下一秒就挂掉，这里就可以把所有master都罗列出来\n--master spark://node01:7077,node02:7077,node03:7077\n\n后期程序会轮训整个master列表，最终找到活着的master，然后向它申请计算资源，最后运行程序。\n```\n\n# SparkRDD\n\n* RDD（Resilient Distributed Dataset）叫做==弹性分布式数据集==，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合.\n  * **Dataset**:          就是一个集合，存储很多数据.\n  * **Distributed**：它内部的元素进行了分布式存储，方便于后期进行分布式计算.\n  * **Resilient**：     表示弹性，rdd的数据是可以保存在内存或者是磁盘中.\n\n* （1）A list of partitions\n  * ==一个分区（Partition）列表，数据集的基本组成单位。==\n\n~~~\n\t这里表示一个rdd有很多分区，每一个分区内部是包含了该rdd的部分数据，\nspark中任务是以task线程的方式运行， 一个分区就对应一个task线程。\n\n\t用户可以在创建RDD时指定RDD的分区个数，如果没有指定，那么就会采用默认值。\n    val rdd=sparkContext.textFile(\"/words.txt\")\n    如果该文件的block块个数小于等于2，这里生产的RDD分区数就为2\n    如果该文件的block块个数大于2，这里生产的RDD分区数就与block块个数保持一致\n~~~\n\n* （2）A function for computing each split\n  * ==一个计算每个分区的函数==\n\n~~~\n\tSpark中RDD的计算是以分区为单位的，每个RDD都会实现compute计算函数以达到这个目的.\n~~~\n\n* （3）A list of dependencies on other RDDs\n  * ==一个rdd会依赖于其他多个rdd==\n\n~~~\n  这里就涉及到rdd与rdd之间的依赖关系，spark任务的容错机制就是根据这个特性（血统）而来。\n~~~\n\n* （4）Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)\n  * ==一个Partitioner，即RDD的分区函数（可选项）==\n\n~~~\n当前Spark中实现了两种类型的分区函数，\n一个是基于哈希的HashPartitioner，(key.hashcode % 分区数= 分区号)\n另外一个是基于范围的RangePartitioner。\n只有对于key-value的RDD,并且产生shuffle，才会有Partitioner，\n\n非key-value的RDD的Parititioner的值是None。\n~~~\n\n* （5）Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)\n  * ==一个列表，存储每个Partition的优先位置(可选项)==\n\n~~~\n这里涉及到数据的本地性，数据块位置最优。\nspark任务在调度的时候会优先考虑存有数据的节点开启计算任务，减少数据的网络传输，提升计算效率。\n~~~\n\n* 流程分析\n\n![rdd的五大属性](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/rdd的五大属性.png)\n\n### RDD的算子分类\n\n* 1、==transformation（转换）==\n  - 根据已经存在的rdd转换生成一个新的rdd,  它是延迟加载，它不会立即执行\n  - 例如\n    - map / flatMap / reduceByKey 等\n* 2、==action (动作)==\n  - 它会真正触发任务的运行\n    - 将rdd的计算的结果数据返回给Driver端，或者是保存结果数据到外部存储介质中\n  - 例如\n    - collect / saveAsTextFile 等\n\n### RDD的依赖关系\n\n![rdd-dependencies](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/rdd-dependencies.png)\n\n- RDD和它依赖的父RDD的关系有两种不同的类型\n\n- 窄依赖（narrow dependency）和宽依赖（wide dependency）\n\n  - ==窄依赖==\n\n    - 窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用\n\n      - 总结：窄依赖我们形象的比喻为独生子女\n\n      ```\n      哪些算子操作是窄依赖：\n      \tmap/flatMap/filter/union等等\n      \t\n      \t所有的窄依赖不会产生shuffle\n      ```\n\n  - ==宽依赖==\n\n    - 宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition\n\n      - 总结：宽依赖我们形象的比喻为超生 \n\n      ```\n      哪些算子操作是宽依赖：\n      \treduceByKey/sortByKey/groupBy/groupByKey/join等等\n      \t\n      \t所有的宽依赖会产生shuffle\n      ```\n\n  - 补充说明\n\n    ~~~\n    由上图可知，join分为宽依赖和窄依赖，如果RDD有相同的partitioner，那么将不会引起shuffle，这种join是窄依赖，反之就是宽依赖\n    ~~~\n\n\n### lineage（血统）\n\n* RDD只支持粗粒度转换\n  * 即只记录单个块上执行的单个操作。\n* 将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区\n* ==RDD的Lineage会记录RDD的元数据信息和转换行为，lineage保存了RDD的依赖关系，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区==。\n\n### RDD的缓存机制\n\n~~~\n\t可以把一个rdd的数据缓存起来，后续有其他的job需要用到该rdd的结果数据，可以直接从缓存中获取得到，避免了重复计算。缓存是加快后续对该数据的访问操作。\n~~~\n\n* RDD通过==persist方法==或==cache方法==可以将前面的计算结果缓存。\n  * 但是并不是这两个方法被调用时立即缓存，而是==触发后面的action==时，该RDD将会被缓存在计算节点的内存中，并供后面重用。\n\n![1569036662039](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569036662039.png)\n\n* 通过查看源码发现==cache最终也是调用了persist方法==，默认的存储级别都是==仅在内存存储一份==，Spark的存储级别还有好多种，存储级别在==object StorageLevel==中定义的。\n\n![1569036703460](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569036703460.png)\n\n* 使用演示\n\n~~~scala\nval rdd1=sc.textFile(\"/words.txt\")\nval rdd2=rdd1.flatMap(_.split(\" \"))\nval rdd3=rdd2.cache\nrdd3.collect\n\nval rdd4=rdd3.map((_,1))\nval rdd5=rdd4.persist(缓存级别)\nrdd5.collect\n~~~\n\n- cache和persist区别\n\n~~~\n\t对RDD设置缓存成可以调用rdd的2个方法： 一个是cache，一个是persist\n调用上面2个方法都可以对rdd的数据设置缓存，但不是立即就触发缓存执行，后面需要有action，才会触发缓存的执行。\n\ncache方法和persist方法区别：\n    cache:   默认是把数据缓存在内存中，其本质就是调用persist方法；\n    persist：可以把数据缓存在内存或者是磁盘，有丰富的缓存级别，这些缓存级别都被定义在StorageLevel这个object中。\n~~~\n\n- 清除缓存数据\n\n* 1、==自动清除==\n\n  ~~~\n  一个application应用程序结束之后，对应的缓存数据也就自动清除\n  ~~~\n\n* 2、==手动清除==\n\n  ~~~\n  调用rdd的unpersist方法\n  ~~~\n\n### RDD的checkpoint机制\n\n* 我们可以对rdd的数据进行缓存，保存在内存或者是磁盘中。\n\n  * 后续就可以直接从内存或者磁盘中获取得到，但是它们不是特别安全。\n\n  * **cache**\n\n    ~~~\n    它是直接把数据保存在内存中，后续操作起来速度比较快，直接从内存中获取得到。但这种方式很不安全，由于服务器挂掉或者是进程终止，会导致数据的丢失。\n    ~~~\n\n  * **persist**\n\n    ~~~\n    它可以把数据保存在本地磁盘中，后续可以从磁盘中获取得到该数据，但它也不是特别安全，由于系统管理员一些误操作删除了，或者是磁盘损坏，也有可能导致数据的丢失。\n    ~~~\n\n* ==**checkpoint**（检查点）==\n\n  ~~~\n  它是提供了一种相对而言更加可靠的数据持久化方式。它是把数据保存在分布式文件系统，\n  比如HDFS上。这里就是利用了HDFS高可用性，高容错性（多副本）来最大程度保证数据的安全性。\n  ~~~\n\n####  如何设置checkpoint\n\n* 1、在hdfs上设置一个checkpoint目录\n\n  ~~~scala\n  sc.setCheckpointDir(\"hdfs://node01:8020/checkpoint\") \n  ~~~\n\n* 2、对需要做checkpoint操作的rdd调用checkpoint方法\n\n  ~~~scala\n  val rdd1=sc.textFile(\"/words.txt\")\n  rdd1.checkpoint\n  val rdd2=rdd1.flatMap(_.split(\" \")) \n  ~~~\n\n* 3、最后需要有一个action操作去触发任务的运行\n\n  ~~~scala\n  rdd2.collect\n  ~~~\n\n#### cache、persist、checkpoint三者区别\n\n* ==cache和persist==\n  * cache默认数据缓存在内存中\n  * persist可以把数据保存在内存或者磁盘中\n  * 后续要触发 cache 和 persist 持久化操作，需要有一个action操作\n  * 它不会开启其他新的任务，一个action操作就对应一个job \n  * 它不会改变rdd的依赖关系，程序运行完成后对应的缓存数据就自动消失\n\n* ==checkpoint==\n  * 可以把数据持久化写入到hdfs上\n  * 后续要触发checkpoint持久化操作，需要有一个action操作，后续会开启新的job执行checkpoint操作\n  * 它会改变rdd的依赖关系，后续数据丢失了不能够在通过血统进行数据的恢复。\n  * 程序运行完成后对应的checkpoint数据就不会消失\n\n~~~scala\n   sc.setCheckpointDir(\"/checkpoint\")\n   val rdd1=sc.textFile(\"/words.txt\")\n   val rdd2=rdd1.cache\n   rdd2.checkpoint\n   val rdd3=rdd2.flatMap(_.split(\" \"))\n   rdd3.collect\n   \n   checkpoint操作要执行需要有一个action操作，一个action操作对应后续的一个job。该job执行完成之后，它会再次单独开启另外一个job来执行 rdd1.checkpoint操作。\n   \n   对checkpoint在使用的时候进行优化，在调用checkpoint操作之前，可以先来做一个cache操作，缓存对应rdd的结果数据，后续就可以直接从cache中获取到rdd的数据写入到指定checkpoint目录中\n~~~\n\n### DAG有向无环图生成\n\n* ==DAG(Directed Acyclic Graph)== 叫做有向无环图（有方向,无闭环,代表着数据的流向），原始的RDD通过一系列的转换就形成了DAG。\n\n* 下图是基于单词统计逻辑得到的DAG有向无环图\n\n![1569047954944](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569047954944.png)\n\n### DAG划分stage\n\n- stage是什么\n  - ==一个Job会被拆分为多组Task，每组任务被称为一个stage==\n  - stage表示不同的调度阶段，一个spark job会对应产生很多个stage\n    - stage类型一共有2种\n      - ==ShuffleMapStage==\n        - 最后一个shuffle之前的所有变换的Stage叫ShuffleMapStage\n          - 它对应的task是shuffleMapTask\n      - ==ResultStage==\n        - 最后一个shuffle之后操作的Stage叫ResultStage，它是最后一个Stage。\n          - 它对应的task是ResultTask\n\n- 为什么要划分stage\n\n~~~\n根据RDD之间依赖关系的不同将DAG划分成不同的Stage(调度阶段)\n对于窄依赖，partition的转换处理在一个Stage中完成计算\n对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，\n\n由于划分完stage之后，在同一个stage中只有窄依赖，没有宽依赖，可以实现流水线计算，\nstage中的每一个分区对应一个task，在同一个stage中就有很多可以并行运行的task。\n~~~\n\n- 如何划分stage\n  - ==划分stage的依据就是宽依赖==\n\n~~~\n(1) 首先根据rdd的算子操作顺序生成DAG有向无环图，接下里从最后一个rdd往前推，创建一个新的stage，把该rdd加入到该stage中，它是最后一个stage。\n\n(2) 在往前推的过程中运行遇到了窄依赖就把该rdd加入到本stage中，如果遇到了宽依赖，就从宽依赖切开，那么最后一个stage也就结束了。\n\n(3) 重新创建一个新的stage，按照第二个步骤继续往前推，一直到最开始的rdd，整个划分stage也就结束了\n~~~\n\n![划分stage](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/划分stage.png)\n\n\n\n#### stage与stage之间的关系\n\n~~~\n\t划分完stage之后，每一个stage中有很多可以并行运行的task，后期把每一个stage中的task封装在一个taskSet集合中，最后把一个一个的taskSet集合提交到worker节点上的executor进程中运行。\n\nrdd与rdd之间存在依赖关系，stage与stage之前也存在依赖关系，前面stage中的task先运行，运行完成了再运行后面stage中的task，也就是说后面stage中的task输入数据是前面stage中task的输出结果数据。\n~~~\n\n![stage](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/stage.png)\n\n# Spark任务调度\n\n![spark任务调度](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/spark任务调度.png)\n\n\n\n~~~markdown\n- Driver端运行客户端的main方法，构建SparkContext对象，在SparkContext对象内部依次构建DAGScheduler和TaskScheduler\n- 按照rdd的一系列操作顺序，来生成DAG有向无环图\n- DAGScheduler拿到DAG有向无环图之后，按照宽依赖进行stage的划分。每一个stage内部有很多可以并行运行的task，最后封装在一个一个的taskSet集合中，然后把taskSet发送给TaskScheduler\n- 所有task运行完成，整个任务也就结束了\n~~~\n\n### spark的运行架构\n\n![spark](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/spark-7527224.png)\n\n~~~\n(1) Driver端向资源管理器Master发送注册和申请计算资源的请求\n\n(2) Master通知对应的worker节点启动executor进程(计算资源)\n\n(3) executor进程向Driver端发送注册并且申请task请求\n\n(4) Driver端运行客户端的main方法，构建SparkContext对象，在SparkContext对象内部依次构建DAGScheduler和TaskScheduler\n\n(5) 按照客户端代码洪rdd的一系列操作顺序，生成DAG有向无环图\n\n(6) DAGScheduler拿到DAG有向无环图之后，按照宽依赖进行stage的划分。每一个stage内部有很多可以并行运行的task，最后封装在一个一个的taskSet集合中，然后把taskSet发送给TaskScheduler\n\n(7) TaskScheduler得到taskSet集合之后，依次遍历取出每一个task提交到worker节点上的executor进程中运行\n\n(8) 所有task运行完成，Driver端向Master发送注销请求，Master通知Worker关闭executor进程，Worker上的计算资源得到释放，最后整个任务也就结束了。\n~~~\n\n- 基于wordcount程序剖析spark任务的提交、划分、调度流程\n\n<img src=\"http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/job-scheduler-running.png\" alt=\"job-scheduler-running\" style=\"zoom:150%;\" />\n\n\n\n### 自定义分区\n\n- 在对RDD数据进行分区时，默认使用的是==HashPartitioner==\n\n- 该函数对key进行哈希，然后对分区总数取模，取模结果相同的就会被分到同一个partition中\n\n  ```\n  HashPartitioner分区逻辑：\n  \tkey.hashcode % 分区总数 = 分区号\n  ```\n\n- 如果嫌HashPartitioner功能单一，可以自定义partitioner\n\n- 实现自定义partitioner大致分为3个步骤\n  - 1、继承==org.apache.spark.Partitioner==\n  - 2、重写==numPartitions==方法\n  - 3、重写==getPartition==方法\n\n```scala\n//1、对应上面的rdd数据进行自定义分区\n val result: RDD[(String, Int)] = wordLengthRDD.partitionBy(new MyPartitioner(3))\n\n//2、自定义分区\nclass MyPartitioner(num:Int) extends Partitioner{\n  //指定rdd的总的分区数\n  override def numPartitions: Int = {\n    num\n  }\n  //消息按照key的某种规则进入到指定的分区号中\n  override def getPartition(key: Any): Int ={\n    //这里的key就是单词\n    val length: Int = key.toString.length\n    length match {\n      case 4 =>0\n      case 5 =>1\n    }\n  }\n}\n```\n\n### 共享变量(broadcast variable)\n\n- ​\tSpark中分布式执行的代码需要==传递到各个Executor的Task上运行==。对于一些只读、固定的数据(比如从DB中读出的数据),每次都需要Driver广播到各个Task上，这样效率低下。\n- ​      广播变量允许==将变量只广播给各个Executor==。该Executor上的各个Task再从所在节点的BlockManager获取变量，而不是从Driver获取变量，以减少通信的成本，减少内存的占用，从而提升了效率。\n\n![广播变量](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/广播变量.png)\n\n##### 广播变量使用\n\n```\n(1) 通过对一个类型T的对象调用 SparkContext.broadcast创建出一个Broadcast[T]对象。\n    任何可序列化的类型都可以这么实现\n(2) 通过 value 属性访问该对象的值\n(3) 变量只会被发到各个节点一次，应作为只读值处理（修改这个值不会影响到别的节点）\n```\n\n- 使用广播变量代码示例\n\n```scala\nval word=\"spark\"\nval rddData = rdd.collect\n//通过调用sparkContext对象的broadcast方法把数据广播出去\nval broadCast = sc.broadcast(word)\nval broadRddData = sc.broadcast(rddData)\n\n//在executor中通过调用广播变量的value属性获取广播变量的值,分布式环境下广播变量通过网络传输需要序列化\nval rdd2=rdd1.flatMap(_.split(\" \")).filter(x=>x.equals(broadCast.value))\n```\n\n##### 广播变量使用注意事项\n\n```\n1、不能将一个RDD使用广播变量广播出去\n\n2、广播变量只能在Driver端定义，不能在Executor端定义\n\n3、在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值\n\n4、如果executor端用到了Driver的变量，如果不使用广播变量在Executor有多少task就有多少Driver端的变量副本\n\n5、如果Executor端用到了Driver的变量，如果使用广播变量在每个Executor中只有一份Driver端的变量副本\n```\n\n#### 累加器(accumulator)\n\n- 累加器（accumulator）是Spark中提供的一种分布式的变量机制，其原理类似于mapreduce，即分布式的改变，然后聚合这些改变\n- ==累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数。可以使用累加器来进行全局的计数\n\n```scala\nval accumulator = sc.accumulator(0); \n    val result = linesRDD.map(s => {\n      accumulator.add(1) //有一条数据就增加1\n    })\n```\n\n### 序列化问题\n\n- spark是分布式执行引擎，其核心抽象是弹性分布式数据集RDD，其代表了分布在不同节点的数据。Spark的计算是在executor上分布式执行的，故用户开发的关于RDD的map，flatMap，reduceByKey等transformation 操作（闭包）有如下执行过程：\n  - （1）代码中对象在driver本地序列化\n  - （2）对象序列化后传输到远程executor节点\n  - （3）远程executor节点反序列化对象\n  - （4）最终远程节点执行\n- 故对象在执行中需要序列化通过网络传输，则必须经过序列化过程。\n\n#### spark的任务序列化异常\n\n- 在编写spark程序中，由于在map，foreachPartition等算子==内部使用了外部定义的变量和函数==，从而引发Task未序列化问题。\n- 然而spark算子在计算过程中使用外部变量在许多情形下确实在所难免，比如在filter算子根据外部指定的条件进行过滤，map根据相应的配置进行变换。\n- 经常会出现“==org.apache.spark.SparkException: Task not serializable==”这个错误\n  - 其原因就在于这些算子使用了==外部的变量==，但是这个变量不能序列化。\n  - 当前类使用了“extends Serializable”声明支持序列化，但是由于某些字段==不支持序列化==，仍然会导致整个类序列化时出现问题，最终导致出现Task未序列化问题。\n\n#### 解决序列化的办法\n\n- (1) 如果函数中使用了该类对象，该类要实现序列化\n  - ==类  extends  Serializable==\n- (2) 如果函数中使用了该类对象的成员变量，该类除了要实现序列化之外，所有的成员变量必须要实现序列化\n- (3) 对于不能序列化的成员变量使用==“@transient”==标注，告诉编译器不需要序列化\n- (4) 也可将依赖的变量独立放到一个小的class中，让这个class支持序列化，这样做可以减少网络传输量，提高效率。\n- (5) 可以把对象的创建直接在该函数中构建这样避免需要序列化\n\n### application、job、stage、task之间的关系\n\n![application](http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/application.png)\n\n- 一个application就是一个应用程序，包含了客户端所有的代码和计算资源\n- 一个action操作对应一个DAG有向无环图，即一个action操作就是一个job \n- 一个job中包含了大量的宽依赖，按照宽依赖进行stage划分，一个job产生了很多个stage\n- 一个stage中有很多分区，一个分区就是一个task，即一个stage中有很多个task\n- ==总结==\n  - 一个application包含了很多个job\n  - 一个job包含了很多个stage\n  - 一个stage包含了很多个task\n\n# Spark内存计算框架\n\n```shell\nspark-submit --class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n# cluster / client\n--deploy-mode cluster \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 1 \\\n/kfly/install/spark-2.3.3-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.3.jar \\\n10\n```\n\n- yarn-cluster模式\n  - spark程序的==Driver程序在YARN中运行==，运行结果不能在客户端显示，并且客户端可以在启动应用程序后消失应用的。\n\n  - 最好运行那些将结果最终保存在外部存储介质（如HDFS、Redis、Mysql），客户端的终端显示的仅是作为YARN的job的简单运行状况。\n\n<img src=\"http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/yarn-cluster.png\" alt=\"yarn-cluster\" style=\"zoom:33%;\" />\n\n- yarn-client模式\n  - spark程序的==Driver运行在Client上==，应用程序运行结果会在客户端显示，所有适合运行结果有输出的应用程序（如spark-shell）\n\n<img src=\"http://kflys.gitee.io/upic/2020/03/22/uPic/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/yarn-client.png\" alt=\"yarn-client\" style=\"zoom: 33%;\" />\n\n```\n最大的区别就是Driver端的位置不一样。\n\nyarn-cluster: Driver端运行在yarn集群中，与ApplicationMaster进程在一起。\nyarn-client:  Driver端运行在提交任务的客户端,与ApplicationMaster进程没关系,经常用于进行测试\n```\n\n### collect 算子操作剖析 \n\n- collect算子操作的作用\n\n  - 1、它是一个action操作，会触发任务的运行\n\n  - 2、它会把RDD的数据进行收集之后，以数组的形式返回给Driver端\n\n  - - ==默认Driver端的内存大小为1G，由参数 spark.driver.memory 设置==\n\n    - 如果某个rdd的数据量超过了Driver端默认的1G内存，对rdd调用collect操作，这里会出现Driver端的内存溢出，所有这个collect操作存在一定的风险，实际开发代码一般不会使用。\n\n    - ==实际企业中一般都会把该参数调大，比如5G/10G等==\n\n      - 可以在代码中修改该参数，如下\n\n        ```scala\n        new SparkConf().set(\"spark.driver.memory\",\"5G\")\n        ```\n\n  ~~~\n  比如说rdd的数据量达到了10G\n  \n  rdd.collect这个操作非常危险，很有可能出现driver端的内存不足\n  ~~~\n\n### spark任务中资源参数剖析\n\n- ==--executor-memory==\n\n  - 表示每一个executor进程需要的内存大小，它决定了后期操作数据的速度\n\n  ```\n  比如说一个rdd的数据量大小为5g,这里给定的executor-memory为2g, 在这种情况下，内存是存储不下，它会把一部分数据保存在内存中，还有一部分数据保存在磁盘，后续需要用到该rdd的结果数据，可以从内存和磁盘中获取得到，这里就涉及到一定的磁盘io操作。\n  \n  ,这里给定的executor-memory为10g，这里数据就可以完全在内存中存储下，后续需要用到该rdd的数据，就可以直接从内存中获取，这样一来，避免了大量的磁盘io操作。性能得到提升。\n  \n  \n  在实际的工作，这里 --executor-memory 需要设置的大一点。\n  比如说10G/20G/30G等\n  ```\n\n- ==--total-executor-cores==\n\n  - 表示任务运行需要总的cpu核数，它决定了任务并行运行的粒度\n\n  ~~~\n  比如说要处理100个task，注意一个cpu在同一时间只能处理一个task线程。\n  \n  如果给定的总的cpu核数是5个，这里就需要100/5=20个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行20分钟。\n  \n  如果给定的总的cpu核数是20个，这里就需要100/20=5个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行5分钟。\n  \n  如果如果给定的总的cpu核数是100个，这里就需要100/100=1个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行1分钟。\n  \n  \n  在实际的生产环境中，--total-executor-cores 这个参数一般也会设置的大一点，\n  比如说 30个/50个/100个\n  ~~~\n\n\n- ==总结==\n\n  ```\n  \t后期对于spark程序的优化，可以从这2个参数入手，无论你把哪一个参数调大，对程序运行的效率来说都会达到一定程度的提升\n      加大计算资源它是最直接、最有效果的优化手段。\n      在计算资源有限的情况下，可以考虑其他方面，比如说代码层面，JVM层面等\n  ```\n\n### spark任务的调度模式\n\n* Spark中的调度模式主要有两种：==FIFO 和 FAIR==\n  * ==FIFO（先进先出）==\n    * 默认情况下Spark的调度模式是FIFO，谁先提交谁先执行，后面的任务需要等待前面的任务执行。\n  * ==FAIR（公平调度）==\n    * 支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。避免大任务运行时间长，占用了大量的资源，后面小任务无法提交运行。\n\n### spark任务的分配资源策略\n\n* 给application分配资源选择worker（executor），现在有两种策略\n  * ==尽量的打散==，即一个Application尽可能多的分配到不同的节点。这个可以通过设置spark.deploy.spreadOut来实现。默认值为true，即尽量的打散（默认）\n    * 可以充分的发挥数据的本地性，提升执行效率\n\n  * ==尽量的集中==，即一个Application尽量分配到尽可能少的节点。\n  \n  ```sh\n  # 假如集群有两个节点，worker1,worker2。各 cores 4 memory 128G，需要分配 4cores。 32g\n  # 1. 尽量的集中(尽可能分配更少的节点，worker1 4 32g)\n  # 2. 尽量打散（尽可能多的分配，worker1 worker2按照顺序依次分配，不够再次循环）\n  ```\n","tags":["spark core"]},{"title":"kafka基础知识","url":"/2019/07/07/it/kafka知识梳理/","content":"\n![Kafka架构原理图](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/Kafka架构原理图.png)\n\n## Kafka概述\n\n- 为什么有消息系统\n\n  ```markdown\n  **解耦**\n  允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。\n  \n  **冗余**\n  消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的\"插入-获取-删除\"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。\n  \n  **扩展性**\n  因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。\n  \n  **灵活性 & 峰值处理能力**\n  在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。\n  \n  **可恢复性**\n  系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。\n  \n  **顺序保证**\n  在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka 保证一个 Partition 内的消息的有序性）\n  \n  **缓冲**\n  有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。\n  \n  **异步通信**\n  很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。\n  ```\n\n  \n\n-  核心概念\n\n~~~\n\tKafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做MQ系统），常见可以用于web/nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。\n\t\n\tkafka是一个分布式消息队列。具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。Kafka就是一种发布-订阅模式。将消息保存在磁盘中，以顺序读写方式访问磁盘，避免随机读写导致性能瓶颈。\n\n~~~\n\n- Kafka集群架构\n\n![kafka集群架构 ](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/kafka集群架构 .png)\n\n```markdown\n* producer\n   消息生产者，发布消息到Kafka集群的终端或服务\n   \n* broker\n  Kafka集群中包含的服务器，一个borker表示kafka集群中的一个节点\n  \n* topic\n  每条发布到Kafka集群的消息属于的类别，即Kafka是面向 topic 的。\n  更通俗的说Topic就像一个消息队列，生产者可以向其写入消息，消费者可以从中读取消息，一个Topic支持多个生产者或消费者同时订阅它，所以其扩展性很好。\n  \n* partition\n  每个 topic 包含一个或多个partition。Kafka分配的单位是partition\n\n* replica\n  partition 的副本，保障 partition 的高可用。\n\n* consumer\n  从Kafka集群中消费消息的终端或服务\n\n* consumer group\n  每个 consumer 都属于一个 consumer group，每条消息只能被 consumer group 中的一个 Consumer 消费，但可以被多个 consumer group 消费。\n\n* leader\n  每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的partition。 producer 和 consumer 只跟 leader 交互\n\n* follower\n  Follower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower，Follower与Leader保持数据同步。如果Leader失效，则从Follower中选举出一个新的Leader。\n\n* controller\n  知道大家有没有思考过一个问题，就是Kafka集群中某个broker宕机之后，是谁负责感知到他的宕机，以及负责进行Leader Partition的选举？如果你在Kafka集群里新加入了一些机器，此时谁来负责把集群里的数据进行负载均衡的迁移？包括你的Kafka集群的各种元数据，比如说每台机器上有哪些partition，谁是leader，谁是follower，是谁来管理的？如果你要删除一个topic，那么背后的各种partition如何删除，是谁来控制？还有就是比如Kafka集群扩容加入一个新的broker，是谁负责监听这个broker的加入？如果某个broker崩溃了，是谁负责监听这个broker崩溃？这里就需要一个Kafka集群的总控组件，Controller。他负责管理整个Kafka集群范围内的各种东西。\n\n* zookeeper\n  (1)\tKafka 通过 zookeeper 来存储集群的meta元数据信息\n  (2)一旦controller所在broker宕机了，此时临时节点消失，集群里其他broker会一直监听这个临时节点，发现临时节点消失了，就争抢再次创建临时节点，保证有一台新的broker会成为controller角色。\n\n* offset\n  消费者在对应分区上已经消费的消息数（位置），offset保存的地方跟kafka版本有一定的关系。\n  kafka0.8 版本之前offset保存在zookeeper上。\n  kafka0.8 版本之后offset保存在kafka集群上。\n\n* ISR机制\n  光是依靠多副本机制能保证Kafka的高可用性，但是能保证数据不丢失吗？不行，因为如果leader宕机，但是leader的数据还没同步到follower上去，此时即使选举了follower作为新的leader，当时刚才的数据已经丢失了。\n  ISR是：in-sync replica，就是跟leader partition保持同步的follower partition的数量，只有处于ISR列表中的follower才可以在leader宕机之后被选举为新的leader，因为在这个ISR列表里代表他的数据跟leader是同步的。\n```\n\n* kafka基本命令行\n\n```shell\n# 1、启动，停止\nbin/kafka-server-start.sh config/server.properties\nbin/kafka-server-stop.sh\n\n# 2、创建topic\nkafka-topics.sh --create --partitions 3 --replication-factor 2 --topic test --zookeeper node01:2181,node02:2181,node03:2181\n\n# 3、查询所有topic\nkafka-topics.sh --list --zookeeper node01:2181,node02:2181,node03:2181 \n\n# 4、查看topic描述信息\nkafka-topics.sh --describe --topic test --zookeeper node01:2181,node02:2181,node03:2181  \n\n# 5、删除topic\nkafka-topics.sh --delete --topic test --zookeeper node01:2181,node02:2181,node03:2181 \n\n# 6、模拟生产者写入数据\n# 9092是 kafka中 conf/producer.properties  bootstrap.servers=localhost:9092地址\nkafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test \n\n# 7、模拟消费者消费数据\n#  conf/consumer.properties  bootstrap.servers=localhost:9092地址\nkafka-console-consumer.sh --bootstrap-server node01:9092,node02:9092,node03:9092 --topic test --from-beginning\n```\n\n- 生产者开发\n\n```java\n//准备配置属性\nProperties props = new Properties();\n//kafka集群地址\nprops.put(\"bootstrap.servers\", \"node01:9092,node02:9092,node03:9092\");\n//acks它代表消息确认机制\nprops.put(\"acks\", \"all\");\n//重试的次数\nprops.put(\"retries\", 0);\n//批处理数据的大小，每次写入多少数据到topic\nprops.put(\"batch.size\", 16384);\n//可以延长多久发送数据\nprops.put(\"linger.ms\", 1);\n//缓冲区的大小\nprops.put(\"buffer.memory\", 33554432);\nprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\nProducer<String, String> producer = new KafkaProducer<String, String>(props);\n//这里需要三个参数，第一个：topic的名称，第二个参数：表示消息的key,第三个参数：消息具体内容\nproducer.send(new ProducerRecord<String, String>(\"test\", Integer.toString(i), \"hello-kafka-\"+i));\nproducer.close();\n\n```\n\n- 消费者开发\n\n```java\n//准备配置属性\nProperties props = new Properties();\n//kafka集群地址\nprops.put(\"bootstrap.servers\", \"node01:9092,node02:9092,node03:9092\");\n//消费者组id\nprops.put(\"group.id\", \"test\");\n//自动提交偏移量\nprops.put(\"enable.auto.commit\", \"true\");\n//自动提交偏移量的时间间隔\nprops.put(\"auto.commit.interval.ms\", \"1000\");\n//默认是latest\n//earliest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费\n//latest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据\n//none : topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常\nprops.put(\"auto.offset.reset\",\"earliest\");\nprops.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\nprops.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\nKafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);\n//指定消费哪些topic\nconsumer.subscribe(Arrays.asList(\"test\"));\nwhile (true) {\n  //指定每个多久拉取一次数据\n  ConsumerRecords<String, String> records = consumer.poll(100);\n}\n```\n\n- 手动提交偏移量\n\n```java\n\t//关闭自动提交，改为手动提交偏移量\n  props.put(\"enable.auto.commit\", \"false\");\n  //定义一个数字，表示消息达到多少后手动提交偏移量\n  final int minBatchSize = 20;\n  if (buffer.size() >= minBatchSize) {\n    //insertIntoDb(buffer); todo  拿到数据之后，进行消费\n    consumer.commitSync();\n    buffer.clear();\n  }\n```\n\n## 分区策略 \n\n- 指定具体分区\n\n```java\n // partition(指定存储在哪个分区)\npublic ProducerRecord(String topic, Integer partition, K key, V value) {\n  this(topic, partition, null, key, value, null);\n}\n```\n\n- key.hashCode()\n\n```java\n// 不指定分区，给定key值，通过key.hashCode() 分配到指定分区\npublic ProducerRecord(String topic, K key, V value) {\n  this(topic, null, null, key, value, null);\n}\n```\n\n- 轮询方式\n\n```java\n// 不指定分区，也不指定key的值，使用轮询方式依次存储到某个分区\npublic ProducerRecord(String topic, V value) {\n  this(topic, null, null, null, value, null);\n}\n```\n\n- 自定义分区类\n\n```java\n// 4. 类似于DefaultPartitioner，自定义分区类，实现Partitioner\npublic class MyPartitions implements Partitioner {\n    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {\n        return 0; // 返回放入哪个分区\n    }\n}\n```\n\n- 源代码解析\n\n```scala\n// 下方是KafkaProducer<K, V>和DefaultPartitioner的源代码\n\t/**\n     * computes partition for given record.\n     * if the record has partition returns the value otherwise\n     * calls configured partitioner class to compute the partition.\n     */\nprivate int partition(ProducerRecord<K, V> record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) {\n  Integer partition = record.partition();\n  // 1. 如果指定了partition则为partition指定值\n  return partition != null ?\n  partition :\n  // 2. 没指定，则是调用以下方法，方法在下方\n  partitioner.partition(\n    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);\n}\n\n/**\n     * Compute the partition for the given record.\n     *\n     * @param topic The topic name\n     * @param key The key to partition on (or null if no key)\n     * @param keyBytes serialized key to partition on (or null if no key)\n     * @param value The value to partition on or null\n     * @param valueBytes serialized value to partition on or null\n     * @param cluster The current cluster metadata\n     */\n    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {\n      // 1.获取分区数量\n        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\n        int numPartitions = partitions.size();\n      // 2. key 为空时，采取轮询\n        if (keyBytes == null) {\n            int nextValue = nextValue(topic);\n            List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);\n            if (availablePartitions.size() > 0) {\n                int part = Utils.toPositive(nextValue) % availablePartitions.size();\n                return availablePartitions.get(part).partition();\n            } else {\n                // no partitions are available, give a non-available partition\n                return Utils.toPositive(nextValue) % numPartitions;\n            }\n        } else {\n            // hash the keyBytes to choose a partition\n          \t// 使用key的hash值选择一个partition\n            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;\n        }\n    }\n```\n\n## 文件存储机制\n\n- 文件结构\n\n```html\n1. Kafka文件存储在Topic中\n2. 一个Topic中包含多个分区（对应文件目录下Topic名称 + 0-partitions.size()）\n\t1） 每个partition下的文件被等分成多个数据文件，默认1G，每一个数据文件被分为一个段（segment file）\n\t2）每个segment file分为 .log 和.index timeindex两个文件\n\t\t1. log文件名称是当前数据的序号，存储数据信息\n\t\t2. index文件名称同上，记录消息的offset和所在log文件的position稀疏索引。\n\t\t3. timeindex 存储消息 timestrap和稀疏索引\n3. 一个partition有多个副本replication，分布在相同或不同的kafka节点中\n\n\n结论：一个partition中的数据是有序的吗？回答：间隔有序，不连续。\n\n针对一个topic里面的数据，只能做到partition内部有序，不能做到全局有序。特别是加入消费者的场景后，如何保证消费者的消费的消息的全局有序性，\n这是一个伪命题，只有在一种情况下才能保证消费的消息的全局有序性，那就是只有一个partition。\n```\n\n![image-20200104154450871](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/image-20200104154450871.png)\n\n- message消息结构\n\n参数说明：\n\n| 关键字              | 解释说明                                                     |\n| ------------------- | ------------------------------------------------------------ |\n| 8 byte offset       | 在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message |\n| 4 byte message size | message大小                                                  |\n| 4 byte CRC32        | 用crc32校验message                                           |\n| 1 byte “magic\"      | 表示本次发布Kafka服务程序协议版本号                          |\n| 1 byte “attributes\" | 表示为独立版本、或标识压缩类型、或编码类型。                 |\n| 4 byte key length   | 表示key的长度,当key为-1时，K byte key字段不填                |\n| K byte key          | 可选                                                         |\n| value bytes payload | 表示实际消息数据。                                           |\n\n~~~\n\t这个就需要涉及到消息的物理结构了，消息都具有固定的物理结构，包括：offset（8 Bytes）、消息体的大小（4 Bytes）、crc32（4 Bytes）、magic（1 Byte）、attributes（1 Byte）、key length（4 Bytes）、key（K Bytes）、payload(N Bytes)等等字段，可以确定一条消息的大小，即读取到哪里截止。\n~~~\n\n-  kafka优秀设计\n  - 顺序写\n\n```html\n一开始很多人质疑 kafka，大家认为一个架构在磁盘之上的系统，性能是如何保证的。这点需要跟大家解释一下，客户端写入到 Kafka 的数据首先是写入到操作系统缓存的（所以很快），然后缓存里的数据根据一定的策略再写入到磁盘，并且写入到磁盘的时候是顺序写，顺序写如果磁盘的个数和转数跟得上的话，都快赶上写内存的速度了！\n```\n\n- PageCache\n\n```html\n\t为了优化读写性能，Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存。这样做的好处有：\n\n（1）避免Object消耗：如果是使用Java堆，Java对象的内存消耗比较大，通常是所存储数据的两倍甚至更多。\n（2）避免GC问题：随着JVM中数据不断增多，垃圾回收将会变得复杂与缓慢，使用系统缓存就不会存在GC问题。\n```\n\n- 跳 表\n\n```html\n在 kafka 的代码里，我们一个的 log 文件是存储是 ConcurrentSkipListMap 里的，是一个 map 结构，key 用的是文件名（也就是 offset），value 就是 log 文件内容。而 ConcurrentSkipListMap 是基于跳表的数据结构设计的。想要消费某个大小的 offset，可以根据跳表快速的定位到这个 log 文件了。\n```\n\n-  稀疏索引\n\n```html\n.index 存储稀疏索引\n假设刚刚我们定位要消费的偏移量是在 00000000000000368769.log 文件里。\n如何根据 index 文件定位呢？\n（1）首先在 index 文件里找，index 文件存储的数据都是成对出现的，比如我们到的 1，0 代表的意思是，offset=368769+1=368770 这条信息存储的物理位置是 0 这个位置。那现在我们现在想要定位的消息是 368776 这条消息，368776 减去 368769 等于 7，我们就在 index 文件里找 offset 等于 7 对应的物理位置，但是因为是稀松索引，我们没找到，不过我们找到了 offset 等于 6 的物理值 1407。\n（2）接下来就到 log 文件里读取文件的 1407 的位置，然后遍历后面的 offset，很快就可以遍历到 offset 等于 7(368776)的数据了，然后从这儿开始消费即可\n```\n\n- 零拷贝\n  - 零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在IO读写过程中。\n\n~~~html\n Kafka利用linux操作系统的 \"零拷贝（zero-copy）\" 机制在消费端做的优化。\n~~~\n\n  - 首先来了解下数据从文件发送到socket网络连接中的常规传输路径\n\n~~~html\n比如：读取文件，再用socket发送出去\n传统方式实现：\n先读取、再发送，实际经过1~4四次copy。\nbuffer = File.read \nSocket.send(buffer)\n~~~\n\n\n```markdown\n  * 第一步：操作系统从磁盘读取数据到内核空间（kernel space）的Page Cache缓冲区\n  * 第二步：应用程序读取内核缓冲区的数据copy到用户空间（user space）的缓冲区\n  * 第三步：应用程序将用户空间缓冲区的数据copy回内核空间到socket缓冲区\n  * 第四步：操作系统将数据从socket缓冲区copy到网卡，由网卡进行网络传输\n```\n\n![传统IO](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/传统IO.png)\n\n~~~html\n传统方式，读取磁盘文件并进行网络发送，经过的四次数据copy是非常繁琐的。实际IO读写，需要进行IO中断，需要CPU响应中断(带来上下文切换)，尽管后来引入DMA来接管CPU的中断请求，但四次copy是存在“不必要的拷贝”的。\n\n重新思考传统IO方式，会注意到实际上并不需要第二个和第三个数据副本。应用程序除了缓存数据并将其传输回套接字缓冲区之外什么都不做。相反，数据可以直接从读缓冲区传输到套接字缓冲区。\n\n显然，第二次和第三次数据copy 其实在这种场景下没有什么帮助反而带来开销，这也正是零拷贝出现的意义。\n\n这种场景：是指读取磁盘文件后，不需要做其他处理，直接用网络发送出去。试想，如果读取磁盘的数据需要用程序进一步处理的话，必须要经过第二次和第三次数据copy，让应用程序在内存缓冲区处理。\n\n~~~\n\n![sendfile](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/sendfile.png)\n\n~~~html\n\t此时我们会发现用户态“空空如也”。数据没有来到用户态，而是直接在核心态就进行了传输，但这样依然还是有多次复制。首先数据被读取到read buffer中，然后发到socket buffer，最后才发到网卡。虽然减少了用户态和核心态的切换，但依然存在多次数据复制。\n\n如果可以进一步减少数据复制的次数，甚至没有数据复制是不是就会做到最快呢？\n~~~\n\n* **DMA**\n\n  * DMA，全称叫Direct Memory Access，一种可让某些硬件子系统去直接访问系统主内存，而不用依赖CPU的计算机系统的功能。听着是不是很厉害，跳过CPU，直接访问主内存。传统的内存访问都需要通过CPU的调度来完成。如下图：\n\n  ![access memory](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/access memory.png)\n\n  * DMA，则可以绕过CPU，硬件自己去直接访问系统主内存。如下图\n\n  ![1577687862577](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/1577687862577.png)\n\n  * 回到本文中的文件传输，有了DMA后，就可以实现绝对的零拷贝了，因为网卡是直接去访问系统主内存的。如下图：\t\n\n  ![零拷贝](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/零拷贝.png)\n\n~~~\n\tKafka采用顺序读写、Page Cache、零拷贝以及分区分段等这些设计，再加上在索引方面做的优化，另外Kafka数据读写也是批量的而不是单条的，使得Kafka具有了高性能、高吞吐、低延时的特点。这样Kafka提供大容量的磁盘存储也变成了一种优点\n\nJava的NIO提供了FileChannle，它的transferTo、transferFrom方法就是Zero Copy。\n~~~\n\n## 内核原理\n\n### ISR机制\n\n~~~\n\t光是依靠多副本机制能保证Kafka的高可用性，但是能保证数据不丢失吗？\n\t不行，因为如果leader宕机，但是leader的数据还没同步到follower上去，此时即使选举了follower作为新的leader，当时刚才的数据已经丢失了。\n\n\tISR是：in-sync replica，就是跟leader partition保持同步的follower partition的数量，只有处于ISR列表中的follower才可以在leader宕机之后被选举为新的leader，因为在这个ISR列表里代表他的数据跟leader是同步的。\n\n\t如果要保证写入kafka的数据不丢失，首先需要保证ISR中至少有一个follower，其次就是在一条数据写入了leader partition之后，要求必须复制给ISR中所有的follower partition，才能说代表这条数据已提交，绝对不会丢失，这是Kafka给出的承诺\n~~~\n\n- HW&LEO原理\n  - **LEO**\n\n  ~~~\n  last end offset，日志末端偏移量，标识当前日志文件中下一条待写入的消息的offset。举一个例子，若LEO=10，那么表示在该副本日志上已经保存了10条消息，位移范围是[0，9]。\n  ~~~\n\n  - **HW**\n\n  ~~~html\n  \tHighwatermark，俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个offset之前的消息。任何一个副本对象的HW值一定不大于其LEO值。\n  \t小于或等于HW值的所有消息被认为是“已提交的”或“已备份的”。HW它的作用主要是用来判断副本的备份进度.\n  \t\n  \t下图表示一个日志文件，这个日志文件中只有9条消息，第一条消息的offset（LogStartOffset）为0，最有一条消息的offset为8，offset为9的消息使用虚线表示的，代表下一条待写入的消息。日志文件的 HW 为6，表示消费者只能拉取offset在 0 到 5 之间的消息，offset为6的消息对消费者而言是不可见的。\n  ~~~\n\n  ![img](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/691225277.png)\n\n\n\n  ~~~html\n  leader持有的HW即为分区的HW,同时leader所在broker还保存了所有follower副本的leo\n  \n  （1）关系：leader的leo >= follower的leo >= leader保存的follower的leo >= leader的hw >= follower的hw\n  （2）原理：上面关系反应出各个值的更新逻辑的先后\n  ~~~\n\n\n  * ==**更新LEO的机制**==\n\n    * 注意\n      * follower副本的LEO保存在2个地方\n\n    ~~~\n    （1）follower副本所在的broker缓存里。\n    （2）leader所在broker的缓存里，也就是leader所在broker的缓存上保存了该分区所有副本的LEO。\n    ~~~\n\n    * 更新LEO的时机\n\n      * follower更新LEO\n\n      ~~~html\n      （1）follower的leo更新时间\n      \t每当follower副本写入一条消息时，leo值会被更新\n      \t\n      （2）leader端的follower副本的leo更新时间\n      \t当follower从leader处fetch消息时，leader获取follower的fetch请求中offset参数，更新保存在leader端follower的leo。\n      ~~~\n\n      * leader更新LEO\n\n      ~~~html\n      （1）leader本身的leo的更新时间：leader向log写消息时\n      ~~~\n\n\n  * ==**更新HW的机制**==\n\n    * follower更新HW\n\n      ~~~html\n      follower更新HW发生在其更新完LEO后，即follower向log写完数据，它就会尝试更新HW值。具体算法就是比较当前LEO(已更新)与fetch响应中leader的HW值，取两者的小者作为新的HW值。\n      ~~~\n\n    * leader更新HW\n\n      * leader更新HW的时机\n\n      ~~~html\n      （1）producer 向 leader 写消息时\n      （2）leader 处理 follower 的 fetch 请求时\n      （3）某副本成为leader时\n      （4）broker 崩溃导致副本被踢出ISR时\n      ~~~\n\n      * leader更新HW的方式\n\n      ~~~html\n        当尝试确定分区HW时，它会选出所有满足条件的副本，比较它们的LEO（当然也包括leader自己的LEO），并选择最小的LEO值作为HW值。\n        这里的满足条件主要是指副本要满足以下两个条件之一：\n        （1）处于ISR中\n        （2）副本LEO落后于leader LEO的时长不大于replica.lag.time.max.ms参数值（默认值是10秒）\n      ~~~\n\n\n\n![HW和LEO的更新](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/HW和LEO的更新.png)\n\n- producer消息发送原理\n  - producer核心流程概览\n\n![Producer流程分析](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/Producer流程分析.png)\n\n* 1、ProducerInterceptors是一个拦截器，对发送的数据进行拦截\n\n  ```html\n  ps：说实话这个功能其实没啥用，我们即使真的要过滤，拦截一些消息，也不考虑使用它，我们直接发送数据之前自己用代码过滤即可\n  ```\n\n* 2、Serializer 对消息的key和value进行序列化\n\n* 3、通过使用分区器作用在每一条消息上，实现数据分发进行入到topic不同的分区中\n\n* 4、RecordAccumulator收集消息，实现批量发送\n\n  ```html\n  它是一个缓冲区，可以缓存一批数据，把topic的每一个分区数据存在一个队列中，然后封装消息成一个一个的batch批次，最后实现数据分批次批量发送。\n  ```\n\n* 5、Sender线程从RecordAccumulator获取消息\n\n* 6、构建ClientRequest对象\n\n* 7、将ClientRequest交给 NetWorkClient准备发送\n\n* 8、NetWorkClient 将请求放入到KafkaChannel的缓存\n\n* 9、发送请求到kafka集群\n\n* 10、调用回调函数，接受到响应\n\n\n\n### producer核心参数\n\n- 常见异常处理\n  - 不管是异步还是同步，都可能让你处理异常，常见的异常如下：\n\n  ~~~html\n  1）LeaderNotAvailableException：这个就是如果某台机器挂了，此时leader副本不可用，会导致你写入失败，要等待其他follower副本切换为leader副本之后，才能继续写入，此时可以重试发送即可。如果说你平时重启kafka的broker进程，肯定会导致leader切换，一定会导致你写入报错，是LeaderNotAvailableException\n  \n  2）NotControllerException：这个也是同理，如果说Controller所在Broker挂了，那么此时会有问题，需要等待Controller重新选举，此时也是一样就是重试即可\n  \n  3）NetworkException：网络异常，重试即可\n  我们之前配置了一个参数，retries，他会自动重试的，但是如果重试几次之后还是不行，就会提供Exception给我们来处理了。\n  ~~~\n\n  - retries\n    - 重新发送数据的次数\n\n  - retry.backoff.ms\n    - 两次重试之间的时间间隔\n\n- 提升消息吞吐量\n  - buffer.memory\n    - 设置发送消息的缓冲区，默认值是33554432，就是32MB\n\n  ~~~html\n  如果发送消息出去的速度小于写入消息进去的速度，就会导致缓冲区写满，此时生产消息就会阻塞住，所以说这里就应该多做一些压测，尽可能保证说这块缓冲区不会被写满导致生产行为被阻塞住\n  ~~~\n\n  - compression.type\n    - producer用于压缩数据的压缩类型。默认是none表示无压缩。可以指定gzip、snappy\n    - 压缩最好用于批量处理，批量处理消息越多，压缩性能越好。\n\n  - batch.size\n    - producer将试图批处理消息记录，以减少请求次数。这将改善client与server之间的性能。\n    - 默认是16384Bytes，即16kB，也就是一个batch满了16kB就发送出去\n\n  ~~~html\n  如果batch太小，会导致频繁网络请求，吞吐量下降；如果batch太大，会导致一条消息需要等待很久才能被发送出去，而且会让内存缓冲区有很大压力，过多数据缓冲在内存里。\n  ~~~\n\n  - linger.ms\n    - 这个值默认是0，就是消息必须立即被发送\n\n  ~~~html\n  \t一般设置一个100毫秒之类的，这样的话就是说，这个消息被发送出去后进入一个batch，如果100毫秒内，这个batch满了16kB，自然就会发送出去。\n  \t但是如果100毫秒内，batch没满，那么也必须把消息发送出去了，不能让消息的发送延迟时间太长，也避免给内存造成过大的一个压力。\n  ~~~\n\n- 请求超时\n  - ==max.request.size==\n    - 这个参数用来控制发送出去的消息的大小，默认是1048576字节，也就1mb\n    - 这个一般太小了，很多消息可能都会超过1mb的大小，所以需要自己优化调整，把他设置更大一些（企业一般设置成10M）\n  - ==request.timeout.ms==\n    - 这个就是说发送一个请求出去之后，他有一个超时的时间限制，默认是30秒\n    - 如果30秒都收不到响应，那么就会认为异常，会抛出一个TimeoutException来让我们进行处理\n\n#### ACK参数\n\nacks参数，其实是控制发送出去的消息的持久化机制的。\n\n* ==acks=0==\n\n  * 生产者只管发数据，不管消息是否写入成功到broker中，数据丢失的风险最高\n\n  ~~~html\n  \tproducer根本不管写入broker的消息到底成功没有，发送一条消息出去，立马就可以发送下一条消息，这是吞吐量最高的方式，但是可能消息都丢失了。\n  你也不知道的，但是说实话，你如果真是那种实时数据流分析的业务和场景，就是仅仅分析一些数据报表，丢几条数据影响不大的。会让你的发送吞吐量会提升很多，你发送弄一个batch出去，不需要等待人家leader写成功，直接就可以发送下一个batch了，吞吐量很大的，哪怕是偶尔丢一点点数据，实时报表，折线图，饼图。\n  ~~~\n\n* ==acks=1==\n\n  * 只要leader写入成功，就认为消息成功了.\n\n  ~~~html\n  \t默认给这个其实就比较合适的，还是可能会导致数据丢失的，如果刚写入leader，leader就挂了，此时数据必然丢了，其他的follower没收到数据副本，变成leader.\n  ~~~\n\n* ==acks=all，或者 acks=-1==\n\n  * 这个leader写入成功以后，必须等待其他ISR中的副本都写入成功，才可以返回响应说这条消息写入成功了，此时你会收到一个回调通知.\n\n  ~~~html\n  这种方式数据最安全，但是性能最差。\n  ~~~\n\n* ==如果要想保证数据不丢失，得如下设置==\n\n  ~~~html\n  （1）min.insync.replicas = 2\n  \tISR里必须有2个副本，一个leader和一个follower，最最起码的一个，不能只有一个leader存活，连一个follower都没有了。\n  \n  （2）acks = -1\n  \t每次写成功一定是leader和follower都成功才可以算做成功，这样leader挂了，follower上是一定有这条数据，不会丢失。\n  \t\n  （3）retries = Integer.MAX_VALUE\n  \t无限重试，如果上述两个条件不满足，写入一直失败，就会无限次重试，保证说数据必须成功的发送给两个副本，如果做不到，就不停的重试。\n  \t除非是面向金融级的场景，面向企业大客户，或者是广告计费，跟钱的计算相关的场景下，才会通过严格配置保证数据绝对不丢失\n  ~~~\n\n#### 重试乱序\n\n* max.in.flight.requests.per.connection\n  * 每个网络连接可以忍受 producer端发送给broker 消息然后消息没有响应的个数\n\n~~~\n消息重试是可能导致消息的乱序的，因为可能排在你后面的消息都发送出去了，你现在收到回调失败了才在重试，此时消息就会乱序，所以可以使用“max.in.flight.requests.per.connection”参数设置为1，这样可以保证producer同一时间只能发送一条消息\n~~~\n\n### broker核心参数\n\n* server.properties配置文件核心参数\n\n  ~~~html\n  【broker.id】\n  每个broker都必须自己设置的一个唯一id\n  \n  【log.dirs】\n  这个极为重要，kafka的所有数据就是写入这个目录下的磁盘文件中的，如果说机器上有多块物理硬盘，那么可以把多个目录挂载到不同的物理硬盘上，然后这里可以设置多个目录，这样kafka可以数据分散到多块物理硬盘，多个硬盘的磁头可以并行写，这样可以提升吞吐量。\n  \n  【zookeeper.connect】\n  连接kafka底层的zookeeper集群的\n  \n  【Listeners】\n  broker监听客户端发起请求的端口号，默认是9092\n  \n  【unclean.leader.election.enable】\n  默认是false，意思就是只能选举ISR列表里的follower成为新的leader，1.0版本后才设为false，之前都是true，允许非ISR列表的follower选举为新的leader\n  \n  【delete.topic.enable】\n  默认true，允许删除topic\n  \n  【log.retention.hours】\n  可以设置一下，要保留数据多少个小时(默认168小时)，这个就是底层的磁盘文件，默认保留7天的数据，根据自己的需求来就行了\n  ~~~\n\n\n### consumer消费原理\n\n#### Offset管理\n\n​\t每个consumer内存里数据结构保存对每个topic的每个分区的消费offset，定期会提交offset，老版本是写入zk，但是那样高并发请求zk是不合理的架构设计，zk是做分布式系统的协调的，轻量级的元数据存储，不能负责高并发读写，作为数据存储。所以后来就是提交offset发送给内部topic：__consumer_offsets，提交过去的时候，key是group.id+topic+分区号，value就是当前offset的值，每隔一段时间，kafka内部会对这个topic进行compact。也就是每个group.id+topic+分区号就保留最新的那条数据即可。而且因为这个 __consumer_offsets可能会接收高并发的请求，所以默认分区50个，这样如果你的kafka部署了一个大的集群，比如有50台机器，就可以用50台机器来抗offset提交的请求压力，就好很多。\n\n#### Coordinator\n\n* Coordinator的作用\n\n  ~~~\n  \t每个consumer group都会选择一个broker作为自己的coordinator，他是负责监控这个消费组里的各个消费者的心跳，以及判断是否宕机，然后开启rebalance.\n  \t根据内部的一个选择机制，会挑选一个对应的Broker，Kafka总会把你的各个消费组均匀分配给各个Broker作为coordinator来进行管理的.\n  \tconsumer group中的每个consumer刚刚启动就会跟选举出来的这个fconsumer group对应的coordinator所在的broker进行通信，然后由coordinator分配分区给你的这个consumer来进行消费。coordinator会尽可能均匀的分配分区给各个consumer来消费。\n  ~~~\n\n* 如何选择哪台是coordinator\n\n  ~~~html\n  \t首先对消费组的groupId进行hash，接着对consumer_offsets的分区数量取模，默认是50，可以通过offsets.topic.num.partitions来设置，找到你的这个consumer group的offset要提交到consumer_offsets的哪个分区。\n  \t比如说：groupId，\"membership-consumer-group\" -> hash值（数字）-> 对50取模 -> 就知道这个consumer group下的所有的消费者提交offset的时候是往哪个分区去提交offset，找到consumer_offsets的一个分区，consumer_offset的分区的副本数量默认来说1，只有一个leader，然后对这个分区找到对应的leader所在的broker，这个broker就是这个consumer group的coordinator了，consumer接着就会维护一个Socket连接跟这个Broker进行通信。\n  ~~~\n\n![39 GroupCoordinator原理剖析](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/kafka知识梳理/assets/GroupCoordinator原理剖析.png)\n\n\n\n#### Rebalance策略\n\n~~~html\n比如我们消费的一个topic主题有12个分区：p0,p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11\n假设我们的消费者组里面有三个消费者。\n~~~\n\n-  range策略\n\n~~~html\nrange策略就是按照partiton的序号范围\n\tp0~3             consumer1\n\tp4~7             consumer2\n\tp8~11            consumer3\n默认就是这个策略\n~~~\n\n- round-robin策略\n\n~~~html\nconsumer1:\t0,3,6,9\nconsumer2:\t1,4,7,10\nconsumer3:\t2,5,8,11\n\n但是前面的这两个方案有个问题：\n\t假设consuemr1挂了:p0-5分配给consumer2,p6-11分配给consumer3\n\t这样的话，原本在consumer2上的的p6,p7分区就被分配到了 consumer3上\n\n~~~\n\n- sticky策略\n\n~~~html\n\t最新的一个sticky策略，就是说尽可能保证在rebalance的时候，让原本属于这个consumer\n的分区还是属于他们，然后把多余的分区再均匀分配过去，这样尽可能维持原来的分区分配的策略\n\nconsumer1： 0-3\nconsumer2:  4-7\nconsumer3:  8-11 \n\n假设consumer3挂了\nconsumer1：0-3，+8,9\nconsumer2: 4-7，+10,11\n\n~~~\n\n### consumer核心参数\n\n~~~\n【heartbeat.interval.ms】\n默认值：3000\nconsumer心跳时间，必须得保持心跳才能知道consumer是否故障了，然后如果故障之后，就会通过心跳下发rebalance的指令给其他的consumer通知他们进行rebalance的操作\n\n【session.timeout.ms】\n默认值：10000\t\nkafka多长时间感知不到一个consumer就认为他故障了，默认是10秒\n\n【max.poll.interval.ms】\n默认值：300000\n如果在两次poll操作之间，超过了这个时间，那么就会认为这个consume处理能力太弱了，会被踢出消费组，分区分配给别人去消费，一遍来说结合你自己的业务处理的性能来设置就可以了\n\n【fetch.max.bytes】\n默认值：1048576\n获取一条消息最大的字节数，一般建议设置大一些\n\n【max.poll.records】\n默认值：500条\n一次poll返回消息的最大条数，\n\n【connections.max.idle.ms】\n默认值：540000\nconsumer跟broker的socket连接如果空闲超过了一定的时间，此时就会自动回收连接，但是下次消费就要重新建立socket连接，这个建议设置为-1，不要去回收\n\n【auto.offset.reset】\n  earliest\n\t\t当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费\t\t  \n\tlatest\n\t\t当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从当前位置开始消费\n\tnone\n\t\ttopic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常\n注：我们生产里面一般设置的是latest\n\n【enable.auto.commit】\n默认值：true\n设置为自动提交offset\n\n【auto.commit.interval.ms】\n默认值：60 * 1000\n每隔多久更新一下偏移量\n~~~\n\n官网查看kafka参数<http://kafka.apache.org/10/documentation.html>\n\n","tags":["-kafka"]},{"title":"scala学习入门","url":"/2019/06/07/it/scala学习入门/","content":"\n### 1. scala简介\n\n* scala是运行在 JVM 上的多范式编程语言，同时支持==面向对象==和==面向函数编程==\n* 早期scala刚出现的时候，并没有怎么引起重视，随着==Spark==和==Kafka==这样基于scala的大数据框架的兴起，scala逐步进入大数据开发者的眼帘。scala的主要优势是它的表达性。\n* 官网地址\n  \n* http://www.scala-lang.org\n  \n### 2. 为什么用scala\n\n* 开发大数据应用程序（Spark程序、Flink程序）\n* 表达能力强，一行代码抵得上Java多行，开发速度快\n* 兼容Java，可以访问庞大的Java类库\n\n### 3. 开发环境安装\n\n* 学习如何编写scala代码之前，需要先安装scala编译器以及开发工具\n\n  * Java程序编译执行流程\n\n    ![1556551819121](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1556551819121.png)\n\n  * Scala程序编译执行流程\n\n    ![1556551904384](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1556551904384.png)\n\n* scala程序运行需要依赖于Java类库，必须要有**==Java运行环境==**，scala才能正确执行\n\n  * **要编译运行scala程序需要**\n    * ==jdk  ( jvm )==\n    * ==scala编译器（scala SDK）==\n\n### 4. scala中声明变量\n\n* **1、语法格式**\n\n~~~scala\nval/var 变量名称:变量类型 = 初始值\n~~~\n\n* 其中\n\n  * `val`定义的是**不可重新赋值**的变量(值不可修改)\n  * `var`定义的是**可重新赋值**的变量(值可以修改)\n\n* ps\n\n  * scala中声明变量是变量名称在前，变量类型在后，跟java是正好相反\n  * scala的语句最后不需要添加分号\n\n* **2、演示**\n\n  ~~~scala\n  #使用val声明变量,相当于java中的final修饰,不能在指向其他的数据了\n   val  a:Int = 10\n  #使用var声明变量,后期可以被修改重新赋值\n   var  b:Int = 20\t \n   b=100\n  #scala中的变量的类型可以显式的声明,也可以不声明,如果不显式的声明这会根据变量的值来推断出来变量的类型(scala支持类型推断)\n   val c = 20\n  ~~~\n\n![1568103524126](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1568103524126.png)\n\n\n\n* 3、**惰性变量**\n\n  * Scala中使用==关键字lazy==来定义惰性变量，实现延迟加载(懒加载)。 \n  * 惰性变量只能是不可变变量，并且只有在调用惰性变量时，才会去实例化这个变量。\n  * 语法格式\n\n  ~~~scala\n  lazy val 变量名 = 表达式\n  ~~~\n\n  ![1568104205830](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1568104205830.png)\n\n​         \n\n### 5. scala中数据类型\n\n* scala中的类型绝大多数和Java一样\n* 数据类型\n\n| 基础类型 | 类型说明                 |\n| -------- | ------------------------ |\n| Byte     | 8位带符号整数            |\n| Short    | 16位带符号整数           |\n| **Int**  | 32位带符号整数           |\n| Long     | 64位带符号整数           |\n| Char     | 16位无符号Unicode字符    |\n| String   | Char类型的序列（字符串） |\n| Float    | 32位单精度浮点数         |\n| Double   | 64位双精度浮点数         |\n| Boolean  | true或false              |\n\n* =**=注意下 scala类型与Java的区别**==\n\n~~~html\n1. scala中所有的类型都使用大写字母开头\n2. 整形使用Int而不是Integer\n3. scala中定义变量可以不写类型，让scala编译器自动推断\n~~~\n\n\n\n* scala类型层次结构\n\n![1556592270468](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1556592270468.png)\n\n\n\n| 类型    | 说明                                                         |\n| ------- | ------------------------------------------------------------ |\n| Any     | **所有类型**的父类，,它有两个子类AnyRef与AnyVal              |\n| AnyVal  | **所有数值类型**的父类                                       |\n| AnyRef  | 所有对象类型（引用类型）的父类                               |\n| Unit    | 表示空，Unit是AnyVal的子类，它只有一个的实例（），它类似于Java中的void，但scala要比Java更加面向对象 |\n| Null    | Null是AnyRef的子类，也就是说它是所有引用类型的子类。它的实例是null,   可以将null赋值给任何对象类型 |\n| Nothing | 所有类型的**子类**不能直接创建该类型实例，某个方法抛出异常时，返回的就是Nothing类型，因为Nothing是所有类的子类，那么它可以赋值为任何类型 |\n\n~~~~\n\n~~~~\n\n\n\n### 6.  scala中的条件表达式\n\n* 条件表达式就是if表达式，if表达式可以根据给定的条件是否满足，根据条件的结果（真或假）决定执行对应的操作。scala条件表达式的语法和Java一样。\n\n~~~scala\n//定义变量x\nscala> val x =1\nx: Int = 1\n\n//if表达式\nscala> val y =if(x>0) 1 else -1\ny: Int = 1\n\n//支持混合类型表达式\nscala> val z=if(x>1) 1 else \"error\"\nz: Any = error\n\n//缺失else 相当于 if(x>2) 1 else ()\nscala> val m=if(x>2) 1\nm: AnyVal = ()\n\n//scala中有个Unit类，用作不返回任何结果的方法的结果类型,相当于Java中的void，Unit只有一个实例值，写成()\nscala> val n=if(x>2) 1 else ()\nn: AnyVal = ()\n\n//if(xx) else if(xx) else \nscala> val k=if(x<0) -1 else if (x==0) 0 else 1\nk: Int = 1\n~~~\n\n![1568107951316](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1568107951316.png)\n\n\n\n### 7. scala中的块表达式\n\n* 定义变量时用 {} 包含一系列表达式，其中块的最后一个表达式的值就是块的值。\n\n~~~scala\nval x=0 \nval result={\n  val y=x+10\n  val z=y+\"-hello\"  \n  val m=z+\"-kaikeba\"\n    \"over\"\n}\n//result的值就是块表达式的结果    \n//后期一个方法的返回值不需要加上return,把要返回的结果放在方法的最后一行就可以了 \n~~~\n\n![1568108241418](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1568108241418.png)\n\n* 在scala解释器中先输入 ==:paste== ,然后写多行代码, 之后按===ctrl+d==结束输入\n\n\n\n### 8. 循环\n\n~~~html\n在scala中，可以使用for和while，但一般推荐使用for表达式，因为for表达式语法更简洁\n~~~\n\n- 8.1 for循环\n\n* 1、语法结构\n\n  ~~~scala\n  for (i <- 表达式/数组/集合){\n      //表达式\n  }\n  ~~~\n\n* 2、演示\n\n  * 简单的for循环\n\n  ~~~scala\n  //简单的for循环\n  scala> val nums= 1 to 10\n  nums: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n  \n  scala> for(i <- nums) println(i)\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  10\n  \n  ~~~\n\n  * 双重for循环\n\n  ~~~scala\n  //双重for循环\n  scala>  for(i <- 1 to 3; j <- 1 to 3) println(i*10+j)\n  11\n  12\n  13\n  21\n  22\n  23\n  31\n  32\n  33\n  \n  //双重for循环打印99乘法表\n  for(i <- 1 to 9; j <- 1 to i){\n      print(i+\"*\"+j+\"=\"+i*j+\"\\t\")\n       if(i==j){\n         println()\n      }    \n  } \n  \n  1*1=1\n  2*1=2   2*2=4\n  3*1=3   3*2=6   3*3=9\n  4*1=4   4*2=8   4*3=12  4*4=16\n  5*1=5   5*2=10  5*3=15  5*4=20  5*5=25\n  6*1=6   6*2=12  6*3=18  6*4=24  6*5=30  6*6=36\n  7*1=7   7*2=14  7*3=21  7*4=28  7*5=35  7*6=42  7*7=49\n  8*1=8   8*2=16  8*3=24  8*4=32  8*5=40  8*6=48  8*7=56  8*8=64\n  9*1=9   9*2=18  9*3=27  9*4=36  9*5=45  9*6=54  9*7=63  9*8=72  9*9=81\n  ~~~\n\n  * 守卫\n    * 在for表达式中可以添加if判断语句，这个if判断就称为守卫\n\n  ~~~scala\n  //语法结构\n  for(i <- 表达式/数组/集合 if 表达式) {\n      // 表达式\n  }\n  \n  scala> for(i <- 1 to 10 if i >5) println(i)\n  6\n  7\n  8\n  9\n  10\n  \n  ~~~\n\n  * for推导式\n    * 在for循环体中，可以使用yield表达式构建出一个集合，我们把使用yield的for表达式称之为推导式\n\n  ~~~scala\n  // for推导式：for表达式中以yield开始，该for表达式会构建出一个集合\n  \n  val v = for(i <- 1 to 5) yield i * 10\n  ~~~\n\n\n- 8.2 while循环\n\n* scala中while循环和Java中是一致的\n* 语法结构\n\n~~~scala\nwhile(返回值为布尔类型的表达式){\n    //表达式\n}\n~~~\n\n* 演示\n\n~~~scala\nscala> var x = 10\nx: Int = 10\n\nscala> while(x >5){\n     | println(x)\n     | x -= 1\n     | }\n10\n9\n8\n7\n6\n~~~\n\n### 10. 方法和函数\n\n- 10.1 方法\n\n* 语法\n\n~~~scala\ndef methodName (参数名:参数类型, 参数名:参数类型) : [return type] = {\n    // 方法体：一系列的代码\n}\n~~~\n\n![1568110629253](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1568110629253.png)\n\n* 说明\n\n  ~~~html\n  - 参数列表的参数类型不能省略\n  - 返回值类型可以省略，由scala编译器自动推断\n  - 返回值可以不写return，默认就是{}块表达式的值\n  \n  ~~~\n\n* 演示\n\n  ~~~scala\n  scala> def add(a:Int,b:Int) = a+b\n  add: (a: Int, b: Int)Int\n  \n  scala> add(1,2)\n  res8: Int = 3\n  \n  scala>\n  \n  ~~~\n\n* 注意\n\n  * 如果定义递归方法，不能省略返回值类型\n  * 示例：\n    * 定义递归方法（求阶乘）\n      * 10 * 9 * 8 * 7 * 6 * ... * 1\n\n  ~~~scala\n  scala> def m1(x:Int)={\n       | if(x==1) 1\n       | else x * m1(x-1)\n       | }\n  <console>:14: error: recursive method m1 needs result type\n         else x * m1(x-1)\n                  ^\n  \n  scala> def m1(x:Int):Int={\n       | if(x==1) 1\n       | else x * m1(x-1)\n       | }\n  m1: (x: Int)Int\n  \n  scala> m1(10)\n  res9: Int = 3628800\n  \n  ~~~\n\n\n\n* 方法的参数\n\n  * 1、默认参数\n\n    * 在定义方法时可以给参数定义一个默认值。\n\n    * 示例\n\n      ~~~scala\n      //1. 定义一个计算两个值相加的方法，这两个值默认为0\n      //2. 调用该方法\n      \n      scala> def add(x:Int = 0, y:Int = 0) = x + y\n      add: (x: Int, y: Int)Int\n      \n      scala> add(10)\n      res14: Int = 10\n      \n      scala> add(10,20)\n      res15: Int = 30\n      \n      ~~~\n\n  * 2、带名参数\n\n    * 在调用方法时，可以指定参数的名称来进行调用。\n\n    * 示例\n\n    ~~~scala\n    scala> def add(x:Int = 0, y:Int = 0) = x + y\n    add: (x: Int, y: Int)Int\n    \n    scala> add(x=1)\n    res16: Int = 1\n    \n    ~~~\n\n  * 3、变长参数\n\n    * 如果方法的参数是不固定的，可以定义一个方法的参数是变长参数。\n\n    * 语法格式：\n\n      ~~~scala\n      def 方法名(参数名:参数类型*):返回值类型 = {\n          方法体\n      }\n      \n      //在参数类型后面加一个*号，表示参数可以是0个或者多个\n      ~~~\n\n    * 示例\n\n      ~~~scala\n      scala> def add(num:Int*) = num.sum\n      add: (num: Int*)Int\n      \n      scala> add(1,2,3,4,5)\n      res17: Int = 15\n      ~~~\n\n\n- 10.2 函数\n\n* scala支持函数式编程，将来编写Spark/Flink程序中，会大量使用到函数\n* 语法\n\n~~~scala\nval 函数变量名 = (参数名:参数类型, 参数名:参数类型....) => 函数体\n~~~\n\n![1568111630788](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1568111630788.png)\n\n* 注意\n\n~~~html\n- 函数是一个对象（变量）\n- 类似于方法，函数也有输入参数和返回值\n- 函数定义不需要使用def定义\n- 无需指定返回值类型\n~~~\n\n* 演示\n\n~~~scala\nscala> val add = (x:Int, y:Int) => x + y\nadd: (Int, Int) => Int = <function2>\n\nscala> add(1,2)\nres3: Int = 3\n\n\n//一个函数没有赋予一个变量，则称为匿名函数，\n//后期再实际开发代码的时候，基本上都是使用匿名函数\n(x:Int,y:Int)=>x+y\n~~~\n\n\n\n- 10.3 方法和函数的区别\n\n* 方法是隶属于类或者对象的，在运行时，它是加载到JVM的方法区中\n* 可以将函数对象赋值给一个变量，在运行时，它是加载到JVM的堆内存中\n* ==函数是一个对象，继承自FunctionN==，函数对象有apply，curried，toString，tupled这些方法，而方法则没有\n\n\n\n- 10.4 方法转换为函数\n\n* 有时候需要将方法转换为函数，作为变量传递，就需要将方法转换为函数\n\n* 使用`_`即可将方法转换为函数\n\n* 示例\n\n  ~~~scala\n  scala> def add(x:Int,y:Int)=x+y\n  add: (x: Int, y: Int)Int\n  \n  scala> val a = add _\n  a: (Int, Int) => Int = <function2>\n  ~~~\n\n\n### 11. 数组\n\n* scala中数组的概念是和Java类似，可以用数组来存放一组数据\n* scala中，有两种数组，一种是**定长数组**，另一种是**变长数组**\n\n\n\n- 11.1 定长数组\n\n* 定长数组指的是数组的**长度**是**不允许改变**的\n\n* 数组的**元素**是**可以改变**的\n\n* 语法\n\n  ~~~scala\n  // 通过指定长度定义数组\n  val/var 变量名 = new Array[元素类型](数组长度)\n  \n  // 用元素直接初始化数组\n  val/var 变量名 = Array(元素1, 元素2, 元素3...)\n  \n  ~~~\n\n* 注意\n\n  ~~~html\n  在scala中，数组的泛型使用[]来指定\n  使用()来获取元素\n  \n  ~~~\n\n* 演示\n\n  ~~~scala\n  scala> val a=new Array[Int](10)\n  a: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n  \n  scala> a(0)\n  res19: Int = 0\n  \n  scala> a(0)=10\n  \n  scala> a\n  res21: Array[Int] = Array(10, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n  \n  //////////////////////////////////////////////////////////////////\n  scala> val b =Array(\"hadoop\",\"spark\",\"hive\")\n  b: Array[String] = Array(hadoop, spark, hive)\n  \n  scala> b(0)\n  res24: String = hadoop\n  \n  scala> b.length\n  res25: Int = 3\n  ~~~\n\n\n\n- 11.2 变长数组\n\n* 变长数组指的是数组的长度是可变的，可以往数组中添加、删除元素\n\n* 创建变长数组，需要提前导入ArrayBuffer类\n\n  ~~~scala\n  import scala.collection.mutable.ArrayBuffer\n  ~~~\n\n* 语法\n\n  * 创建空的ArrayBuffer变长数组\n\n  ~~~scala\n  val/var a = ArrayBuffer[元素类型]()\n  ~~~\n\n  * 创建带有初始元素的ArrayBuffer\n\n  ~~~scala\n  val/var a = ArrayBuffer(元素1，元素2，元素3....)\n  ~~~\n\n* 演示\n\n  ~~~scala\n  //导入ArrayBuffer类型\n  scala> import scala.collection.mutable.ArrayBuffer\n  import scala.collection.mutable.ArrayBuffer\n  \n  //定义一个长度为0的整型变长数组\n  scala> val a=ArrayBuffer[Int]()\n  a: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer()\n  \n  //定义一个有初始元素的变长数组\n  scala> val b = ArrayBuffer(\"hadoop\", \"storm\", \"spark\")\n  b: scala.collection.mutable.ArrayBuffer[String] = ArrayBuffer(hadoop, storm, spark)\n  ~~~\n\n* 变长数组的增删改操作\n\n  - 使用`+=`添加元素\n  - 使用`-=`删除元素\n  - 使用`++=`追加一个数组到变长数组\n\n* 示例\n\n  ~~~scala\n  // 定义变长数组\n  scala> val a = ArrayBuffer(\"hadoop\", \"spark\", \"flink\")\n  a: scala.collection.mutable.ArrayBuffer[String] = ArrayBuffer(hadoop, spark, flink)\n  \n  // 追加一个元素\n  scala> a += \"flume\"\n  res10: a.type = ArrayBuffer(hadoop, spark, flink, flume)\n  \n  // 删除一个元素\n  scala> a -= \"hadoop\"\n  res11: a.type = ArrayBuffer(spark, flink, flume)\n  \n  // 追加一个数组\n  scala> a ++= Array(\"hive\", \"sqoop\")\n  res12: a.type = ArrayBuffer(spark, flink, flume, hive, sqoop)\n  \n  ~~~\n\n\n\n\n\n- 11.3 遍历数组\n\n* 可以使用以下两种方式来遍历数组：\n  * 使用==for表达式== 直接遍历数组中的元素\n  * 使用 ==索引== 遍历数组中的元素\n\n* 示例\n\n~~~scala\nscala> for(i <- a)println(i)\nhadoop\nhive\nflume\nspark\n\nscala> for(i <- 0 to a.length -1 )println(a(i))\nhadoop\nhive\nflume\nspark\n\nscala> for(i <- 0 until a.length) println(a(i))\nhadoop\nhive\nflume\nspark\n\n\n//0 until n ——生成一系列的数字，包含0，不包含n\n//0 to n    ——包含0，也包含n\n\n~~~\n\n\n\n- 11.4 数组常用操作\n\n* scala中的数组封装了丰富的计算操作，将来在对数据处理的时候，不需要我们自己再重新实现。\n  * 求和——sum方法\n  * 求最大值——max方法 \n  * 求最小值——min方法 \n  * 排序——sorted方法\n* 示例\n\n~~~scala\nscala> val array=Array(1,3,4,2,5)\narray: Array[Int] = Array(1, 3, 4, 2, 5)\n\n//求和\nscala> array.sum\nres10: Int = 15\n\n//求最大值\nscala> array.max\nres11: Int = 5\n\n//求最小值\nscala> array.min\nres12: Int = 1\n\n//升序\nscala> array.sorted\nres13: Array[Int] = Array(1, 2, 3, 4, 5)\n\n//降序    reverse 反转\nscala> array.sorted.reverse\nres14: Array[Int] = Array(5, 4, 3, 2, 1)\n\n~~~\n\n\n\n### 12. 元组\n\n* 元组可以用来包含一组不同类型的值。例如：姓名，年龄，性别，出生年月。元组的元素是不可变 的。\n\n- 12.1 定义元组\n\n* 语法\n\n  * 使用括号来定义元组\n\n    ~~~scala\n    val/var 元组变量名称 = (元素1, 元素2, 元素3....)\n    ~~~\n\n  * 使用箭头来定义元素（元组只有两个元素）\n\n    ~~~scala\n    val/var 元组 = 元素1->元素2\n    ~~~\n\n- 12.2 示例\n\n~~~scala\n// 可以直接使用括号来定义一个元组 \nscala> val a = (1, \"张三\", 20, \"北京市\") \na: (Int, String, Int, String) = (1,张三,20,北京市)\n\n//使用箭头来定义元素\nscala> val b = 1->2 \nb: (Int, Int) = (1,2)\n\n~~~\n\n\n\n- 12.3 访问元组\n\n* 使用\n\n  ```html\n   _1、_2、_3....\n  \n  ```\n\n  来访问元组中的元素，_1表示访问第一个元素，依次类推\n\n* 示例\n\n~~~scala\nscala> val a = (1, \"张三\", 20, \"北京市\")\na: (Int, String, Int, String) = (1,张三,20,北京市)\n\n//获取元组中的第一个元素\nscala> a._1\nres18: Int = 1\n\n//获取元组中的第二个元素\nscala> a._2\nres19: String = 张三\n\n//获取元组中的第三个元素\nscala> a._3\nres20: Int = 20\n\n//获取元组中的第四个元素\nscala> a._4\nres21: String = 北京市\n\n//不能修改元组中的值\nscala> a._4=\"上海\"\n<console>:12: error: reassignment to val\n       a._4=\"上海\"\n           ^\n\n\n~~~\n\n\n\n### 13. 映射Map\n\n* Map可以称之为映射。它是由键值对组成的集合。scala当中的Map集合与java当中的Map类似，也是key，value对形式的。\n* 在scala中，Map也分为不可变Map和可变 Map。\n\n\n\n- 13.1 不可变Map\n\n* 定义语法\n\n~~~scala\nval/var map = Map(键->值, 键->值, 键->值...)    // 推荐，可读性更好 \nval/var map = Map((键, 值), (键, 值), (键, 值), (键, 值)...)\n~~~\n\n* 演示\n\n~~~scala\nscala> val map1 = Map(\"zhangsan\"->30, \"lisi\"->40) \nmap: scala.collection.immutable.Map[String,Int] = Map(zhangsan -> 30, lisi -> 40)\n\nscala> val map2 = Map((\"zhangsan\", 30), (\"lisi\", 30)) \nmap: scala.collection.immutable.Map[String,Int] = Map(zhangsan -> 30, lisi -> 30)\n// 根据key获取value \nscala> map1(\"zhangsan\") \nres10: Int = 30\n~~~\n\n\n\n- 13.2 可变Map\n\n* 可变Map需要手动导入==import scala.collection.mutable.Map==, 定义语法与不可变Map一致。\n\n* 演示\n\n~~~scala\n//导包\nscala> import scala.collection.mutable.Map\nimport scala.collection.mutable.Map\n\n//定义可变的map\nscala> val map3 = Map(\"zhangsan\"->30, \"lisi\"->40)\nmap3: scala.collection.mutable.Map[String,Int] = Map(lisi -> 40, zhangsan -> 30)\n\n//获取zhangsan这个key对应的value\nscala> map3(\"zhangsan\")\nres26: Int = 30\n\n//给zhangsan这个key重新赋值value\nscala> map3(\"zhangsan\")=50\n\n//显示map3\nscala> map3\nres28: scala.collection.mutable.Map[String,Int] = Map(lisi -> 40, zhangsan -> 50)\n\n\n~~~\n\n\n\n- 13.3 Map基本操作\n\n* 创建一个可变的map\n\n~~~scala\n//导包\nscala> import scala.collection.mutable.Map\nimport scala.collection.mutable.Map\n\nscala> val map = Map(\"zhangsan\"->30, \"lisi\"->40) \nmap: scala.collection.mutable.Map[String,Int] = Map(lisi -> 40, zhangsan -> 30)\n\n~~~\n\n* 按照key获取value\n\n~~~scala\n// 获取zhagnsan的年龄 \nscala> map(\"zhangsan\")\nres10: Int = 30\n\n// 获取wangwu的年龄，如果wangwu不存在，则返回-1 比较友好，避免遇到不存在的key而报错\nscala> map.getOrElse(\"wangwu\", -1) \nres11: Int = -1\n\n\n~~~\n\n* 修改key对应的value\n\n~~~scala\nscala> map(\"lisi\")=50\n\n~~~\n\n\n\n* 添加key-value键值对\n\n~~~scala\nscala> map+=(\"wangwu\" ->35)\nres12: map.type = Map(lisi -> 50, zhangsan -> 30, wangwu -> 35)\n\n\n\n~~~\n\n\n\n* 删除key-value键值对\n\n~~~scala\nscala> map -=\"wangwu\"\nres13: map.type = Map(lisi -> 50, zhangsan -> 30)\n~~~\n\n\n\n* 获取所有的key和所有的value\n\n~~~scala\n//获取所有的key\nscala> map.keys\nres36: Iterable[String] = Set(lisi, zhangsan)\n\n//获取所有的key\nscala> map.keySet\nres37: scala.collection.Set[String] = Set(lisi, zhangsan)\n\n//获取所有的value\nscala> map.values\nres38: Iterable[Int] = HashMap(50, 30)\n~~~\n\n\n\n* 遍历map\n\n~~~scala\n//第一种遍历\nscala> for(k <- map.keys) println(k+\" -> \" +map(k))\nlisi -> 50\nzhangsan -> 30\n\n\n//第二种遍历\nscala> for((k,v) <- map) println(k+\" -> \"+v)\nlisi -> 50\nzhangsan -> 30\n~~~\n\n\n\n### 14. Set集合\n\n* Set是代表没有重复元素的集合。\n* Set具备以下性质：\n  * 1、元素不重复 \n  * 2、不保证插入顺序\n* scala中的set集合也分为两种，一种是不可变集合，另一种是可变集合。\n\n\n\n- 14.1 不可变Set集合\n\n* 语法\n\n~~~scala\n//创建一个空的不可变集\nval/var 变量名 = Set[类型]()\n\n//给定元素来创建一个不可变集\nval/var 变量名 = Set[类型](元素1, 元素2, 元素3...)\n\n~~~\n\n* 演示\n\n~~~scala\n// 创建set集合 \nscala> val a = Set(1,1,2,3,4,5) \na: scala.collection.immutable.Set[Int] = Set(5, 1, 2, 3, 4)\n\n// 获取集合的大小 \nscala> a.size \nres0: Int = 5\n\n// 遍历集合\nscala> for(i <- a) println(i)\n\n//添加元素生成新的集合\nscala> a + 6\nres1: scala.collection.immutable.Set[Int] = Set(5, 1, 6, 2, 3, 4)\n\n// 删除一个元素 \nscala> a - 1 \nres2: scala.collection.immutable.Set[Int] = Set(5, 2, 3, 4)\n\n// 删除set集合中存在的元素 \nscala> a -- Set(2,3) \nres3: scala.collection.immutable.Set[Int] = Set(5, 1, 4)\n\n// 拼接两个集合 \nscala> a ++ Set(6,7,8) \nres4: scala.collection.immutable.Set[Int] = Set(5, 1, 6, 2, 7, 3, 8, 4)\n\n//求2个Set集合的交集\nscala> a & Set(3,4,5,6)\nres5: scala.collection.immutable.Set[Int] = Set(5, 3, 4)\n\n\n\n//注意：这里对不可变的set集合进行添加删除等操作，对于该集合来说是没有发生任何变化，这里是生成了新的集合，新的集合相比于原来的集合来说发生了变化。\n~~~\n\n\n\n- 14.2 可变Set集合\n\n* 要使用可变集，必须要手动导入： ==import scala.collection.mutable.Set==\n* 演示\n\n~~~scala\n//导包\nscala> import scala.collection.mutable.Set\nimport scala.collection.mutable.Set\n\n//定义可变的set集合\nscala> val set=Set(1,2,3,4,5)\nset: scala.collection.mutable.Set[Int] = Set(1, 5, 2, 3, 4)\n\n//添加单个元素\nscala> set +=6\nres10: set.type = Set(1, 5, 2, 6, 3, 4)\n\n//添加多个元素\nscala> set +=(6,7,8,9)\nres11: set.type = Set(9, 1, 5, 2, 6, 3, 7, 4, 8)\n\n//添加一个set集合中的元素\nscala> set ++=Set(10,11)\nres12: set.type = Set(9, 1, 5, 2, 6, 3, 10, 7, 4, 11, 8)\n\n//删除一个元素\nscala> set -=11\nres13: set.type = Set(9, 1, 5, 2, 6, 3, 10, 7, 4, 8)\n\n//删除多个元素\nscala> set -=(9,10)\nres15: set.type = Set(1, 5, 2, 6, 3, 7, 4, 8)\n\n//删除一个set子集\nscala> set --=Set(7,8)\nres19: set.type = Set(1,5, 2, 6, 3, 4)\n\nscala> set.remove(1)\nres17: Boolean = true\n\nscala> set\nres18: scala.collection.mutable.Set[Int] = Set(5, 2, 6, 3, 4)\n\n~~~\n\n\n\n### 15. 列表 List\n\n* List是scala中最重要的、也是最常用的数据结构。\n* List具备以下性质：\n  * 1、可以保存重复的值 \n  * 2、有先后顺序\n\n* 在scala中，也有两种列表，一种是不可变列表、另一种是可变列表\n\n\n\n- 15.1 不可变列表\n\n* 不可变列表就是列表的元素、长度都是不可变的\n* 语法\n  * 使用 List(元素1, 元素2, 元素3, ...) 来创建一个不可变列表，语法格式\n\n~~~scala\nval/var 变量名 = List(元素1, 元素2, 元素3...)\n\n//使用 Nil 创建一个不可变的空列表\nval/var 变量名 = Nil\n\n//使用 :: 方法创建一个不可变列表\nval/var 变量名 = 元素1 :: 元素2 :: Nil\n\n~~~\n\n* 演示\n\n~~~scala\n//创建一个不可变列表，存放以下几个元素（1,2,3,4）\nscala> val  list1=List(1,2,3,4)\nlist1: List[Int] = List(1, 2, 3, 4)\n\n//使用Nil创建一个不可变的空列表\nscala> val  list2=Nil\nlist2: scala.collection.immutable.Nil.type = List()\n\n//使用 :: 方法创建列表，包含1、2、3三个元素\nscala> val list3=1::2::3::Nil\nlist3: List[Int] = List(1, 2, 3)\n~~~\n\n\n\n- 15.2 可变列表\n\n* 可变列表就是列表的元素、长度都是可变的。\n\n* 要使用可变列表，先要导入 ==import scala.collection.mutable.ListBuffer==\n\n* 语法\n\n  * 使用ListBuffer[元素类型]() 创建空的可变列表，语法结构\n\n  ~~~scala\n  val/var 变量名 = ListBuffer[Int]()\n  ~~~\n\n  * 使用ListBuffer(元素1, 元素2, 元素3...)创建可变列表，语法结构\n\n  ~~~scala\n  val/var 变量名 = ListBuffer(元素1，元素2，元素3...)\n  ~~~\n\n* 演示\n\n  ~~~scala\n  //导包\n  scala> import scala.collection.mutable.ListBuffer\n  import scala.collection.mutable.ListBuffer\n  \n  //定义一个空的可变列表\n  scala> val a=ListBuffer[Int]()\n  a: scala.collection.mutable.ListBuffer[Int] = ListBuffer()\n  \n  //定义一个有初始元素的可变列表\n  scala> val b=ListBuffer(1,2,3,4)\n  b: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2, 3, 4)\n  ~~~\n\n\n\n- 15.3 列表操作\n\n~~~scala\n//导包\nscala> import scala.collection.mutable.ListBuffer\nimport scala.collection.mutable.ListBuffer\n\n//定义一个可变的列表\nscala> val list=ListBuffer(1,2,3,4)\nlist: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2, 3, 4)\n\n//获取第一个元素\nscala> list(0)\nres4: Int = 1\n//获取第一个元素\nscala> list.head\nres5: Int = 1\n\n//获取除了第一个元素外其他元素组成的列表\nscala> list.tail\nres6: scala.collection.mutable.ListBuffer[Int] = ListBuffer(2, 3, 4)\n\n//添加单个元素\nscala> list +=5\nres7: list.type = ListBuffer(1, 2, 3, 4, 5)\n\n//添加一个不可变的列表\nscala> list ++=List(6,7)\nres8: list.type = ListBuffer(1, 2, 3, 4, 5, 6, 7)\n\n//添加一个可变的列表\nscala> list ++=ListBuffer(8,9)\nres9: list.type = ListBuffer(1, 2, 3, 4, 5, 6, 7, 8, 9)\n\n//删除单个元素\nscala> list -=9\nres10: list.type = ListBuffer(1, 2, 3, 4, 5, 6, 7, 8)\n\n//删除一个不可变的列表存在的元素\nscala> list --=List(7,8)\nres11: list.type = ListBuffer(1, 2, 3, 4, 5, 6)\n\n//删除一个可变的列表存在的元素\nscala> list --=ListBuffer(5,6)\nres12: list.type = ListBuffer(1, 2, 3, 4)\n\n//可变的列表转为不可变列表\nscala> list.toList\nres13: List[Int] = List(1, 2, 3, 4)\n\n//可变的列表转为不可变数组\nscala> list.toArray\nres14: Array[Int] = Array(1, 2, 3, 4)\n~~~\n\n\n\n### 16. 函数式编程\n\n* 我们将来使用Spark/Flink的大量业务代码都会使用到函数式编程。\n* 下面的这些操作是学习的重点，先来感受下如何进行函数式编程以及它的强大\n\n\n\n- 16.1 遍历 - foreach\n\n* 方法描述\n\n  ~~~scala\n  foreach(f: (A) ⇒ Unit): Unit\n  ~~~\n\n* 方法说明\n\n  | foreach | API           | 说明                                                         |\n  | ------- | ------------- | ------------------------------------------------------------ |\n  | 参数    | f: (A) ⇒ Unit | 接收一个函数对象<br />函数的输入参数为集合的元素<br />返回值为空 |\n  | 返回值  | Unit          | 空                                                           |\n\n* 方法实操\n\n~~~scala\nscala> val list=List(1,2,3,4)\nlist: List[Int] = List(1, 2, 3, 4)\n\n//定义一个匿名函数传入到foreach方法中\nscala> list.foreach((x:Int)=>println(x))\n1\n2\n3\n4\n\n//匿名函数的输入参数类型可以省略，由编译器自动推断\nscala> list.foreach(x=>println(x))\n1\n2\n3\n4\n\n//当函数参数，只在函数体中出现一次，而且函数体没有嵌套调用时，可以使用下划线来简化函数定 义\nscala> list.foreach(println(_))\n1\n2\n3\n4\n\n//最简写，直接给定println\nscala> list.foreach(println)\n1\n2\n3\n4\n\n//很神奇的语法，别害怕，盘它就可以了，后期通过scala语言开发spark、Flink程序非常简洁方便\n~~~\n\n\n\n- 16.2 映射 - map\n\n* 集合的映射操作是将来在编写Spark/Flink用得最多的操作，是我们必须要掌握的掌握。\n\n* 方法描述\n\n~~~scala\ndef map[B](f: (A) ⇒ B): TraversableOnce[B]\n~~~\n\n* 方法说明\n\n| map方法 | API                | 说明                                                         |\n| ------- | ------------------ | ------------------------------------------------------------ |\n| 泛型    | [B]                | 指定map方法最终返回的集合泛型                                |\n| 参数    | f: (A) ⇒ B         | 传入一个函数对象<br />该函数接收一个类型A（要转换的列表元素）<br />返回值为类型B |\n| 返回值  | TraversableOnce[B] | B类型的集合                                                  |\n\n* 方法实操\n\n~~~scala\n//定义一个list集合，实现把内部每一个元素做乘以10，生成一个新的list集合\nscala> val list=List(1,2,3,4)\nlist: List[Int] = List(1, 2, 3, 4)\n\n//定义一个匿名函数\nscala> list.map((x:Int)=>x*10)\nres21: List[Int] = List(10, 20, 30, 40)\n\n//省略匿名函数参数类型\nscala> list.map(x=>x*10)\nres22: List[Int] = List(10, 20, 30, 40)\n\n//最简写   用下划线\nscala> list.map(_*10)\nres23: List[Int] = List(10, 20, 30, 40)\n~~~\n\n\n\n- 16.3 扁平化映射 - flatmap\n\n* 映射扁平化也是将来用得非常多的操作，也是必须要掌握的。\n* 方法描述\n\n~~~scala\ndef flatMap[B](f: (A) ⇒ GenTraversableOnce[B]): TraversableOnce[B]\n~~~\n\n* 方法说明\n\n| flatmap方法 | API                            | 说明                                                         |\n| ----------- | ------------------------------ | ------------------------------------------------------------ |\n| 泛型        | [B]                            | 最终要转换的集合元素类型                                     |\n| 参数        | f: (A) ⇒ GenTraversableOnce[B] | 传入一个函数对象<br />函数的参数是集合的元素<br />函数的返回值是一个集合 |\n| 返回值      | TraversableOnce[B]             | B类型的集合                                                  |\n\n* 方法实操\n\n~~~scala\n//定义一个List集合,每一个元素中就是一行数据，有很多个单词\nscala>  val list = List(\"hadoop hive spark flink\", \"hbase spark\")\nlist: List[String] = List(hadoop hive spark flink, hbase spark)\n\n//使用flatMap进行偏平化处理，获取得到所有的单词\nscala> list.flatMap(x => x.split(\" \"))\nres24: List[String] = List(hadoop, hive, spark, flink, hbase, spark)\n\n//简写\nscala> list.flatMap(_.split(\" \"))\nres25: List[String] = List(hadoop, hive, spark, flink, hbase, spark)\n\n// flatMap该方法其本质是先进行了map 然后又调用了flatten\nscala> list.map(_.split(\" \")).flatten\nres26: List[String] = List(hadoop, hive, spark, flink, hbase, spark)\n~~~\n\n\n\n- 16.4 过滤 - filter\n\n* 过滤符合一定条件的元素\n\n- 方法描述\n\n```scala\ndef filter(p: (A) ⇒ Boolean): TraversableOnce[A]\n```\n\n- 方法说明\n\n| filter方法 | API                | 说明                                                         |\n| ---------- | ------------------ | ------------------------------------------------------------ |\n| 参数       | p: (A) ⇒ Boolean   | 传入一个函数对象<br />接收一个集合类型的参数<br />返回布尔类型，满足条件返回true, 不满足返回false |\n| 返回值     | TraversableOnce[A] | 列表                                                         |\n\n* 方法实操\n\n~~~scala\n//定义一个list集合\nscala> val list=List(1,2,3,4,5,6,7,8,9,10)\nlist: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n//过滤出集合中大于5的元素\nscala> list.filter(x => x >5)\nres27: List[Int] = List(6, 7, 8, 9, 10)\n\n//把集合中大于5的元素取出来乘以10生成一个新的list集合\nscala> list.filter(_ > 5).map(_ * 10)\nres29: List[Int] = List(60, 70, 80, 90, 100)\n\n\n//通过这个案例，应该是可以感受到scala比java的强大了...\n~~~\n\n\n\n- 16.5 排序 - sort\n\n* 在scala集合中，可以使用以下几种方式来进行排序\n  * sorted默认排序 \n  * sortBy指定字段排序 \n  * sortWith自定义排序\n* ==sorted默认排序== \n\n~~~~scala\n//定义一个List集合\nscala> val list=List(5,1,2,4,3)\nlist: List[Int] = List(5, 1, 2, 4, 3)\n\n//默认就是升序\nscala> list.sorted\nres30: List[Int] = List(1, 2, 3, 4, 5)\n~~~~\n\n* ==sortBy指定字段排序==\n\n  * 根据传入的函数转换后，再进行排序\n  * 方法描述\n\n  ~~~scala\n  def sortBy[B](f: (A) ⇒ B): List[A]\n  ~~~\n\n  * 方法说明\n\n  | sortBy方法 | API        | 说明                                                         |\n  | ---------- | ---------- | ------------------------------------------------------------ |\n  | 泛型       | [B]        | 按照什么类型来进行排序                                       |\n  | 参数       | f: (A) ⇒ B | 传入函数对象<br />接收一个集合类型的元素参数<br />返回B类型的元素进行排序 |\n  | 返回值     | List[A]    | 返回排序后的列表                                             |\n\n  * 方法实操\n\n  ~~~scala\n  //定义一个List集合\n  scala> val list=List(\"1 hadoop\",\"2 spark\",\"3 flink\")\n  list: List[String] = List(1 hadoop, 2 spark, 3 flink)\n  \n  //按照单词的首字母进行排序\n  scala> list.sortBy(x=>x.split(\" \")(1))\n  res33: List[String] = List(3 flink, 1 hadoop, 2 spark)\n  \n  ~~~\n\n* ==sortWith自定义排序==\n\n  * 自定义排序，根据一个函数来进行自定义排序\n  * 方法描述\n\n  ~~~scala\n  def sortWith(lt: (A, A) ⇒ Boolean): List[A]\n  ~~~\n\n  * 方法说明\n\n  | sortWith方法 | API                  | 说明                                                         |\n  | ------------ | -------------------- | ------------------------------------------------------------ |\n  | 参数         | lt: (A, A) ⇒ Boolean | 传入一个比较大小的函数对象<br />接收两个集合类型的元素参数<br />返回两个元素大小，小于返回true，大于返回false |\n  | 返回值       | List[A]              | 返回排序后的列表                                             |\n\n  * 方法实操\n\n  ~~~scala\n  scala> val list = List(2,3,1,6,4,5)\n  a: List[Int] = List(2, 3, 1, 6, 4, 5)\n  \n  //降序\n  scala> list.sortWith((x,y)=>x>y)\n  res35: List[Int] = List(6, 5, 4, 3, 2, 1)\n  \n  //升序\n  scala> list.sortWith((x,y)=>x<y)\n  res36: List[Int] = List(1, 2, 3, 4, 5, 6)\n  ~~~\n\n\n\n- 16.6 分组 - groupBy\n\n* 我们如果要将数据按照分组来进行统计分析，就需要使用到分组方法\n* groupBy表示按照函数将列表分成不同的组\n* 方法描述\n\n```scala\ndef groupBy[K](f: (A) ⇒ K): Map[K, List[A]]\n```\n\n- 方法说明\n\n| groupBy方法 | API             | 说明                                                         |\n| ----------- | --------------- | ------------------------------------------------------------ |\n| 泛型        | [K]             | 分组字段的类型                                               |\n| 参数        | f: (A) ⇒ K      | 传入一个函数对象<br />接收集合元素类型的参数<br />返回一个K类型的key，这个key会用来进行分组，相同的key放在一组中 |\n| 返回值      | Map[K, List[A]] | 返回一个映射，K为分组字段，List为这个分组字段对应的一组数据  |\n\n* 方法实操\n\n~~~scala\nscala> val a = List(\"张三\"->\"男\", \"李四\"->\"女\", \"王五\"->\"男\")\na: List[(String, String)] = List((张三,男), (李四,女), (王五,男))\n\n// 按照性别分组\nscala> a.groupBy(_._2)\nres0: scala.collection.immutable.Map[String,List[(String, String)]] = Map(男 -> List((张三,男), (王五,男)),\n女 -> List((李四,女)))\n\n// 将分组后的映射转换为性别/人数元组列表\nscala> res0.map(x => x._1 -> x._2.size)\nres3: scala.collection.immutable.Map[String,Int] = Map(男 -> 2, 女 -> 1)\n~~~\n\n\n\n- 16.7 聚合 - reduce\n\n* reduce表示将列表，传入一个函数进行聚合计算\n* 方法描述\n\n~~~scala\ndef reduce[A1 >: A](op: (A1, A1) ⇒ A1): A1\n~~~\n\n* 方法说明\n\n| reduce方法 | API               | 说明                                                         |\n| ---------- | ----------------- | ------------------------------------------------------------ |\n| 泛型       | [A1 >: A]         | （下界）A1必须是集合元素类型的子类                           |\n| 参数       | op: (A1, A1) ⇒ A1 | 传入函数对象，用来不断进行聚合操作<br />第一个A1类型参数为：当前聚合后的变量<br />第二个A1类型参数为：当前要进行聚合的元素 |\n| 返回值     | A1                | 列表最终聚合为一个元素                                       |\n\n* 方法实操\n\n~~~scala\nscala> val a = List(1,2,3,4,5,6,7,8,9,10)\na: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\nscala> a.reduce((x,y) => x + y)\nres5: Int = 55\n\n// 第一个下划线表示第一个参数，就是历史的聚合数据结果\n// 第二个下划线表示第二个参数，就是当前要聚合的数据元素\nscala> a.reduce(_ + _)\nres53: Int = 55\n\n// 与reduce一样，从左往右计算\nscala> a.reduceLeft(_ + _)\nres0: Int = 55\n\n// 从右往左聚合计算\nscala> a.reduceRight(_ + _)\nres1: Int = 55\n~~~\n\n\n\n- 16.8 折叠 - fold\n\n* fold与reduce很像，但是多了一个指定初始值参数\n* 方法描述\n\n```scala\ndef fold[A1 >: A](z: A1)(op: (A1, A1) ⇒ A1): A1\n```\n\n- 方法说明\n\n| reduce方法 | API               | 说明                                                         |\n| ---------- | ----------------- | ------------------------------------------------------------ |\n| 泛型       | [A1 >: A]         | （下界）A1必须是集合元素类型的子类                           |\n| 参数1      | z: A1             | 初始值                                                       |\n| 参数2      | op: (A1, A1) ⇒ A1 | 传入函数对象，用来不断进行折叠操作<br />第一个A1类型参数为：当前折叠后的变量<br />第二个A1类型参数为：当前要进行折叠的元素 |\n| 返回值     | A1                | 列表最终折叠为一个元素                                       |\n\n* 方法实操\n\n~~~scala\n//定义一个List集合\nscala> val a = List(1,2,3,4,5,6,7,8,9,10)\na: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n//求和\nscala> a.sum\nres41: Int = 55\n\n//给定一个初始值，，折叠求和\nscala> a.fold(0)(_+_)\nres42: Int = 55\n\nscala> a.fold(10)(_+_)\nres43: Int = 65\n\n//从左往右\nscala> a.foldLeft(10)(_+_)\nres44: Int = 65\n\n//从右往左\nscala> a.foldRight(10)(_+_)\nres45: Int = 65\n\n\n//fold和foldLet效果一致，表示从左往右计算\n//foldRight表示从右往左计算\n\n~~~\n\n\n\n### 17. 高阶函数\n\n* 使用函数值作为参数，或者返回值为函数值的“函数”和“方法”，均称之为“高阶函数”。\n\n\n\n- 17.1 函数值作为参数\n\n~~~scala\n//定义一个数组\nscala> val array=Array(1,2,3,4,5)\narray: Array[Int] = Array(1, 2, 3, 4, 5)\n\n//定义一个函数\nscala> val func=(x:Int)=>x*10\nfunc: Int => Int = <function1>\n\n//函数作为参数传递到方法中\nscala> array.map(func)\nres0: Array[Int] = Array(10, 20, 30, 40, 50)\n~~~\n\n- 17.2 匿名函数\n\n~~~scala\n//定义一个数组\nscala> val array=Array(1,2,3,4,5)\narray: Array[Int] = Array(1, 2, 3, 4, 5)\n\n//定义一个没有名称的函数----匿名函数\nscala> array.map(x=>x*10)\nres1: Array[Int] = Array(10, 20, 30, 40, 50)\n\n~~~\n\n- 17.3 柯里化\n\n* 方法可以定义多个参数列表，当使用较少的参数列表调用多参数列表的方法时，会产生一个新的函数，该函数接收剩余的参数列表作为其参数。这被称为柯里化。\n\n~~~scala\ndef getAddress(a:String):(String,String)=>String={\n    (b:String,c:String)=>a+\"-\"+b+\"-\"+c\n}\n\nscala> val f1=getAddress(\"china\")\nf1: (String, String) => String = <function2>\n\nscala> f1(\"beijing\",\"tiananmen\")\nres5: String = china-beijing-tiananmen\n\n\n\n//这里就可以这样去定义方法\ndef getAddress(a:String)(b:String,c:String):String={ \n  \t\ta+\"-\"+b+\"-\"+c \n}\n//调用\nscala> getAddress(\"china\")(\"beijing\",\"tiananmen\")\nres0: String = china-beijing-tiananmen\n\n//之前学习使用的下面这些操作就是使用到了柯里化\nList(1,2,3,4).fold(0)(_+_)\nList(1,2,3,4).foldLeft(0)(_+_)\nList(1,2,3,4).foldRight(0)(_+_)\n\n~~~\n\n\n\n- 17.4 闭包\n\n* 函数里面引用外面类成员变量叫作闭包\n\n~~~scala\nvar factor=10\n\nval f1=(x:Int) => x*factor\n\n\n//定义的函数f1，它的返回值是依赖于不在函数作用域的一个变量\n//后期必须要要获取到这个变量才能执行\n//spark和flink程序的开发中大量的使用到函数，函数的返回值依赖的变量可能都需要进行大量的网络传输获取得到。这里就需要这些变量实现序列化进行网络传输。\n\n~~~\n\n\n\n### 18. scala面向对象编程之类\n\n- 18.1 类的定义\n\n* scala是支持面向对象的，也有类和对象的概念。\n  * 定义一个Customer类，并添加成员变量/成员方法\n  * 添加一个main方法，并创建Customer类的对象，并给对象赋值，打印对象中的成员，调用成员方法\n\n~~~scala\nclass Customer {\n  var name:String = _\n  var sex:String = _\n  val registerDate:Date = new Date\n\n  def sayHi(msg:String) = {\n    println(msg)\n  }\n}\n\nobject Main {\n  def main(args: Array[String]): Unit = {\n    val customer = new Customer\n    //给对象的成员变量赋值\n    customer.name = \"张三\"\n    customer.sex = \"男\"\n\n    println(s\"姓名: ${customer.name}, 性别：${customer.sex}, 注册时间: ${customer.registerDate}\")\n    //对象调用方法  \n    customer.sayHi(\"你好!\")\n  }\n}\n\n~~~\n\n* 说明\n\n~~~html\n(1). var name:String = _，  _表示使用默认值进行初始化\n   例如：String类型默认值是null，Int类型默认值是0，Boolean类型默认值是false...\n(2). val变量不能使用_来进行初始化，因为val是不可变的，所以必须手动指定一个默认值\n(3). main方法必须要放在一个scala的object（单例对象）中才能执行\n\n~~~\n\n\n\n- 18.2 类的构造器\n\n* 主构造器\n\n  * 主构造器是指在类名的后面跟上一系列参数，例如\n\n  ~~~scala\n  class 类名(var/val 参数名:类型 = 默认值, var/val 参数名:类型 = 默认值){\n      // 构造代码块\n  }\n  \n  ~~~\n\n* 辅助构造器\n\n  * 在类中使用this来定义，例如\n\n  ~~~scala\n  def this(参数名:类型, 参数名:类型) {\n      ...\n  }\n  ~~~\n\n* 演示\n\n  ~~~scala\n  class Student(val name:String, val age:Int) {\n      \n     val address:String=\"beijing\" \n    // 定义一个参数的辅助构造器\n    def this(name:String) {\n      // 第一行必须调用主构造器、其他辅助构造器或者super父类的构造器\n      this(name, 20)\n    }\n  \n    def this(age:Int) {\n      this(\"某某某\", age)\n    }\n  }\n  \n  ~~~\n\n\n\n### 19.scala面向对象编程之对象\n\n- 19.1 scala中的object\n\n* scala中是没有Java中的静态成员的。如果将来我们需要用到static变量、static方法，就要用到scala中的单例对象object\n\n* 定义object\n  * 定义单例对象和定义类很像，就是把class换成object\n* 演示\n  * 定义一个工具类，用来格式化日期时间\n\n~~~scala\nobject DateUtils {\n\n  // 在object中定义的成员变量，相当于Java中定义一个静态变量\n  // 定义一个SimpleDateFormat日期时间格式化对象\n  val simpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm\")\n\n  // 构造代码\n  println(\"构造代码\")\n    \n  // 相当于Java中定义一个静态方法\n  def format(date:Date) = simpleDateFormat.format(date)\n\n  // main是一个静态方法，所以必须要写在object中\n  def main(args: Array[String]): Unit = {\n      \n    println { DateUtils.format(new Date()) };\n  }\n}\n\n~~~\n\n* 说明\n\n~~~html\n(1). 使用object 单例对象名定义一个单例对象，可以用object作为工具类或者存放常量\n(2). 在单例对象中定义的变量，类似于Java中的static成员变量\n(3). 在单例对象中定义的方法，类似于Java中的static方法\n(4). object单例对象的构造代码可以直接写在花括号中\n(5). 调用单例对象的方法，直接使用单例对象名.方法名，访问单例对象的成员变量也是使用单例对象名.变量名\n(6). 单例对象只能有一个无参的主构造器，不能添加其他参数\n~~~\n\n- 19.2 scala中的伴生对象\n\n* 在==同一个scala文件，有一个class和object具有同样的名字===，那么就称这个object是class的伴生对象，class是object的伴生类；\n* 伴生类和伴生对象的最大特点是，可以相互访问；\n\n* 演示\n\n~~~scala\nclass ClassObject {\n  val id = 1\n  private var name = \"itcast\"\n  def printName(): Unit ={\n    //在Dog类中可以访问伴生对象Dog的私有属性\n    println(ClassObject.CONSTANT + name )\n  }\n\n\n}\n\nobject ClassObject{\n  //伴生对象中的私有属性\n  private val CONSTANT = \"汪汪汪 : \"\n  def main(args: Array[String]) {\n    val p = new ClassObject\n    //访问私有的字段name\n    p.name = \"123\"\n    p.printName()\n  }\n}\n~~~\n\n* 说明\n\n~~~~html\n(1). 伴生类和伴生对象的名字必须是一样的\n(2). 伴生类和伴生对象需要在一个scala源文件中\n(3). 伴生类和伴生对象可以互相访问private的属性\n\n~~~~\n\n\n\n- 19.3 scala中object的apply方法\n\n\n\n* 我们之前使用过这种方式来创建一个Array对象。\n\n~~~scala\n// 创建一个Array对象\nval a = Array(1,2,3,4)\n\n~~~\n\n* 这种写法非常简便，不需要再写一个new，然后敲一个空格，再写类名。如何直接使用类名来创建对象呢？\n* 查看scala源代码：\n\n![1568196769539](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1568196769539.png)\n\n\n\n* 答案就是：**==实现伴生对象的apply方法==**\n* 伴生对象的apply方法用来快速地创建一个伴生类的对象。\n\n* 演示\n\n~~~scala\nclass Person(var name:String, var age:Int) {\n\n  override def toString = s\"Person($name, $age)\"\n}\n\nobject Person {\n  // 实现apply方法\n  // 返回的是伴生类的对象\n  def apply(name:String, age:Int): Person = new Person(name, age)\n\n  // apply方法支持重载\n  def apply(name:String):Person = new Person(name, 20)\n\n  def apply(age:Int):Person = new Person(\"某某某\", age)\n\n  def apply():Person = new Person(\"某某某\", 20)\n}\n\nobject Main2 {\n  def main(args: Array[String]): Unit = {\n    val p1 = Person(\"张三\", 20)\n    val p2 = Person(\"李四\")\n    val p3 = Person(100)\n    val p4 = Person()\n\n    println(p1)\n    println(p2)\n    println(p3)\n    println(p4)\n  }\n}\n~~~\n\n* 说明\n\n~~~html\n（1）. 当遇到类名(参数1, 参数2...)会自动调用apply方法，在apply方法中来创建对象\n（2）. 定义apply时，如果参数列表是空，也不能省略括号()，否则引用的是伴生对象\n\n~~~\n\n\n\n- 19.4 scala中object的main方法\n\n* scala和Java一样，如果要运行一个程序，必须有一个main方法。\n\n* 而在Java中main方法是静态的，而在scala中没有静态方法。\n\n* ==在scala中，这个main方法必须放在一个object中==\n\n  * 演示1\n\n    ~~~scala\n    object Main1{\n      def main(args:Array[String]) = {\n        println(\"hello, scala\")\n      }\n    }\n    \n    ~~~\n\n* ==也可以继承自App Trait（特质==），然后将需要编写在main方法中的代码，写在object的构造方法体内。其本质是调用了Trait这个特质中的main方法。\n\n  * 演示2\n\n    ~~~scala\n    object Main2 extends App {\n      println(\"hello, scala\")\n    }\n    \n    ~~~\n\n\n\n### 20. scala面向对象编程之继承\n\n- 20.1 继承extends\n\n* scala和Java一样，使用**extends**关键字来实现继承。可以在子类中定义父类中没有的字段和方法，或者重写父类的方法。\n* ==示例1：实现简单继承==\n\n~~~scala\nclass Person1 {\n  var name = \"super\"\n\n  def getName = this.name\n}\n\nclass Student1 extends Person1\n\nobject Main1 {\n  def main(args: Array[String]): Unit = {\n    val p1 = new Person1()\n    val p2 = new Student1()\n\n    p2.name = \"张三\"\n\n    println(p2.getName)\n  }\n}\n\n~~~\n\n\n\n* ==示例2：单例对象实现继承==\n\n~~~scala\nclass Person2 {\n  var name = \"super\"\n\n  def getName = this.name\n}\n\nobject Student2 extends Person2\n\nobject Main2 {\n  def main(args: Array[String]): Unit = {\n    println(Student2.getName)\n  }\n}\n\n~~~\n\n\n\n- 20.2 override和super\n\n* 如果子类要覆盖父类中的一个非抽象方法，必须要使用override关键字\n* 可以使用override关键字来重写一个val字段\n* 可以使用super关键字来访问父类的成员\n* ==示例1：class继承class==\n\n~~~scala\nclass Person3 {\n  val name = \"super\"\n\n  def getName = name\n}\n\nclass Student3 extends Person3 {\n  // 重写val字段\n  override val name: String = \"child\"\n\n  // 重写getName方法\n  override def getName: String = \"hello, \" + super.getName\n}\n\nobject Main3 {\n  def main(args: Array[String]): Unit = {\n    println(new Student3().getName)\n  }\n}\n~~~\n\n\n\n- 20.3 isInstanceOf和asInstanceOf\n\n* 我们经常要在代码中进行类型的判断和类型的转换。在Java中，我们可以使用instanceof关键字、以及(类型)object来实现，在scala中如何实现呢？\n* scala中对象提供==isInstanceOf ==和 ==asInstanceOf==方法。\n  * isInstanceOf判断对象是否为指定类的对象\n  * asInstanceOf将对象转换为指定类型\n\n\n\n\n|                        | Java             | Scala               |\n| ---------------------- | ---------------- | ------------------- |\n| 判断对象是否是C类型    | obj instanceof C | obj.isInstanceof[C] |\n| 将对象强转成C类型      | (C ) obj         | obj.asInstanceof[C] |\n| 获取类型为T的class对象 | C.class          | classOf[C]          |\n\n* ==示例==\n\n~~~scala\nclass Person4\nclass Student4 extends Person4\n\nobject Main4 {\n  def main(args: Array[String]): Unit = {\n    val s1:Person4 = new Student4\n\n    // 判断s1是否为Student4类型\n    if(s1.isInstanceOf[Student4]) {\n      // 将s1转换为Student3类型\n      val s2 =  s1.asInstanceOf[Student4]\n      println(s2)\n    }\n\n  }\n}\n~~~\n\n\n\n- 20.4 getClass和classOf\n\n* isInstanceOf 只能判断出对象是否为指定类以及其子类的对象，而不能精确的判断出，对象就是指定类的对象。如果要求精确地判断出对象就是指定类的对象，那么就只能使用 getClass 和 classOf 。\n\n  * 对象.getClass可以精确获取对象的类型\n\n  * classOf[x]可以精确获取类型\n  * 使用==操作符就可以直接比较\n\n* ==示例==\n\n~~~scala\nclass Person5\nclass Student5 extends Person5\n\nobject Student5{\n  def main(args: Array[String]) {\n    val p:Person5=new Student5\n    //判断p是否为Person5类的实例\n    println(p.isInstanceOf[Person5])//true\n\n    //判断p的类型是否为Person5类\n    println(p.getClass == classOf[Person5])//false\n\n    //判断p的类型是否为Student5类\n    println(p.getClass == classOf[Student5])//true\n  }\n}\n~~~\n\n\n\n- 20.5 访问修饰符\n\n* Java中的访问控制，同样适用于scala，可以在成员前面添加private/protected关键字来控制成员的可见性。但在scala中，==**没有public关键字**，任何没有被标为private或protected的成员都是公共的==。\n\n  * ==**private[this]修饰符**==\n\n    * 被修饰的成员只能在当前类中被访问。或者可以理解为：`只能通过this.来访问`（在当前类中访问成员会自动添加this.）。\n\n    * ==示例==\n\n      ~~~scala\n      class Person6 {\n        // 只有在当前对象中能够访问\n        private[this] var name = \"super\"\n      \n        def getName = this.name\t// 正确！\n      \n        def sayHelloTo(p:Person6) = {\n          println(\"hello\" + p.name)     // 报错!无法访问\n        }\n      }\n      \n      object Person6 {\n        def showName(p:Person6) = println(p.name)  // 报错!无法访问\n      }\n      \n      ~~~\n\n  * ==**protected[this]修饰符**==\n\n    * ==被修饰的成员只能在当前类和当前子类中被访问==。也可以理解为：当前类通过**this.**访问或者子类通过**this.**访问\n\n    * 示例\n\n      ~~~scala\n      class Person7 {\n        // 只有在当前类以及继承该类的当前对象中能够访问\n        protected[this] var name = \"super\"\n        \n        def getName = {\n          // 正确！\n          this.name\n        }\n      \n        def sayHelloTo1(p:Person7) = {\n          // 编译错误！无法访问\n          println(p.name)\n        }\n      }\n      \n      object Person7 {\n        def sayHelloTo3(p:Person7) = {\n          // 编译错误！无法访问\n          println(p.name)\n        }\n      }\n      \n      class Student7 extends Person7 {\n        def showName = {\n          // 正确！\n          println(name)\n        }\n      \n        def sayHelloTo2(p:Person7) = {\n          // 编译错误！无法访问\n          println(p.name)\n        }\n      }\n      \n      ~~~\n\n\n\n- 20.6 调用父类的constructor\n\n* ==实例化子类对象，必须要调用父类的构造器==，在scala中，只能在子类的`主构造器`中调用父类的构造器\n* 示例\n\n~~~scala\nclass Person8(var name:String){\n    println(\"name:\"+name)\n}\n\n// 直接在父类的类名后面调用父类构造器\nclass Student8(name:String, var clazz:String) extends Person8(name)\n\nobject Main8 {\n  def main(args: Array[String]): Unit = {\n    val s1 = new Student8(\"张三\", \"三年二班\")\n    println(s\"${s1.name} - ${s1.clazz}\")\n  }\n}\n\n~~~\n\n\n\n- 20.7 抽象类\n\n* 如果类的某个成员在当前类中的定义是不包含完整的，它就是一个**抽象类**\n* 不完整定义有两种情况：\n  * 1.方法没有方法体\n  * 2.变量没有初始化\n* 没有方法体的方法称为**抽象方法**，没有初始化的变量称为**抽象字段**。定义抽象类和Java一样，在类前面加上**abstract**关键字就可以了\n\n* ==示例==\n\n~~~scala\nabstract class Person9(val name:String) {\n  //抽象方法\n  def sayHello:String\n  def sayBye:String\n  //抽象字段  \n  val address:String  \n}\nclass Student9(name:String) extends Person9(name){\n  //重写抽象方法\n  def sayHello: String = \"Hello,\"+name\n  def sayBye: String =\"Bye,\"+name\n  //重写抽象字段\n  override val address:String =\"beijing \"\n}\nobject Main9{\n  def main(args: Array[String]) {\n    val s = new Student9(\"tom\")\n    println(s.sayHello)\n    println(s.sayBye)\n    println(s.address)\n  }\n}\n\n~~~\n\n- 20.8 匿名内部类\n\n* 匿名内部类是没有名称的子类，直接用来创建实例对象。Spark的源代码中有大量使用到匿名内部类。\n\n* ==示例==\n\n~~~scala\nabstract class Person10 {\n  //抽象方法  \n  def sayHello:Unit\n}\n\nobject Main10 {\n  def main(args: Array[String]): Unit = {\n    // 直接用new来创建一个匿名内部类对象\n    val p1 = new Person10 {\n      override def sayHello: Unit = println(\"我是一个匿名内部类\")\n    }\n    p1.sayHello\n  }\n}\n\n~~~\n\n\n\n\n\n### 21. scala面向对象编程之trait特质\n\n* 特质是scala中代码复用的基础单元\n* 它可以将方法和字段定义封装起来，然后添加到类中\n* 与类继承不一样的是，类继承要求每个类都只能继承`一个`超类，而一个类可以添加`任意数量`的特质。\n* 特质的定义和抽象类的定义很像，但它是使用`trait`关键字\n\n\n\n- 21.1 作为接口使用\n\n* 使用`extends`来继承trait（scala不论是类还是特质，都是使用extends关键字）\n\n* 如果要继承多个trait，则使用`with`关键字\n\n* ==**示例一：继承单个trait**==\n\n  ~~~scala\n  trait Logger1 {\n    // 抽象方法\n    def log(msg:String)\n  }\n  \n  class ConsoleLogger1 extends Logger1 {\n    override def log(msg: String): Unit = println(msg)\n  }\n  \n  object LoggerTrait1 {\n    def main(args: Array[String]): Unit = {\n      val logger = new ConsoleLogger1\n      logger.log(\"控制台日志: 这是一条Log\")\n    }\n  }\n  ~~~\n\n\n* ==**示例二：继承多个trait**==\n\n  ~~~scala\n  trait Logger2 {\n    // 抽象方法\n    def log(msg:String)\n  }\n  \n  trait MessageSender {\n    def send(msg:String)\n  }\n  \n  class ConsoleLogger2 extends Logger2 with MessageSender {\n    \n    override def log(msg: String): Unit = println(msg)\n  \n    override def send(msg: String): Unit = println(s\"发送消息:${msg}\")\n  }\n  \n  object LoggerTrait2 {\n    def main(args: Array[String]): Unit = {\n      val logger = new ConsoleLogger2\n      logger.log(\"控制台日志: 这是一条Log\")\n      logger.send(\"你好!\")\n    }\n  }\n  ~~~\n\n\n\n- 21.2 定义具体的方法\n\n* 和类一样，trait中还可以定义具体的方法。\n\n* ==示例==\n\n  ~~~scala\n  trait LoggerDetail {\n    // 在trait中定义具体方法\n    def log(msg:String) = println(msg)\n  }\n  \n  class PersonService extends LoggerDetail {\n    def add() = log(\"添加用户\")\n  }\n  \n  object MethodInTrait {\n    def main(args: Array[String]): Unit = {\n      val personService = new PersonService\n      personService.add()\n    }\n  }\n  \n  ~~~\n\n\n\n- 21.3 定义具体方法和抽象方法\n\n* 在trait中，可以混合使用具体方法和抽象方法\n\n* 使用具体方法依赖于抽象方法，而抽象方法可以放到继承trait的子类中实现，这种设计方式也称为**模板模式**\n\n* ==示例==\n\n  ~~~scala\n  trait Logger3 {\n    // 抽象方法\n    def log(msg:String)\n    // 具体方法（该方法依赖于抽象方法log\n    def info(msg:String) = log(\"INFO:\" + msg)\n    def warn(msg:String) = log(\"WARN:\" + msg)\n    def error(msg:String) = log(\"ERROR:\" + msg)\n  }\n  \n  class ConsoleLogger3 extends Logger3 {\n    override def log(msg: String): Unit = println(msg)\n  }\n  \n  object LoggerTrait3 {\n    def main(args: Array[String]): Unit = {\n      val logger3 = new ConsoleLogger3\n  \n      logger3.info(\"这是一条普通信息\")\n      logger3.warn(\"这是一条警告信息\")\n      logger3.error(\"这是一条错误信息\")\n    }\n  }\n  ~~~\n\n\n\n- 21.4 定义具体字段和抽象字段\n\n* 在trait中可以定义具体字段和抽象字段\n\n* 继承trait的子类自动拥有trait中定义的字段\n\n* 字段直接被添加到子类中\n\n* ==示例==\n\n  ~~~scala\n  trait LoggerEx {\n    // 具体字段\n    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm\")\n    val INFO = \"信息:\" + sdf.format(new Date)\n    // 抽象字段\n    val TYPE:String\n  \n    // 抽象方法\n    def log(msg:String)\n  }\n  \n  class ConsoleLoggerEx extends LoggerEx {\n    // 实现抽象字段\n    override val TYPE: String = \"控制台\"\n    // 实现抽象方法\n    override def log(msg:String): Unit = print(s\"$TYPE$INFO $msg\")\n  }\n  \n  object FieldInTrait {\n    def main(args: Array[String]): Unit = {\n      val logger = new ConsoleLoggerEx\n  \n      logger.log(\"这是一条消息\")\n    }\n  }\n  \n  \n  ~~~\n\n\n\n- 21.5 实例对象混入trait\n\n* trait还可以混入到`实例对象`中，给对象实例添加额外的行为\n\n* 只有混入了trait的对象才具有trait中的方法，其他的类对象不具有trait中的行为\n\n* 使用with将trait混入到实例对象中\n\n* ==示例==\n\n  ~~~scala\n  trait LoggerMix {\n    def log(msg:String) = println(msg)\n  }\n  \n  class UserService\n  \n  object FixedInClass {\n    def main(args: Array[String]): Unit = {\n      // 使用with关键字直接将特质混入到对象中\n      val userService = new UserService with LoggerMix\n  \n      userService.log(\"你好\")\n    }\n  }\n  \n  ~~~\n\n\n\n### 22. 模式匹配和样例类\n\n* scala有一个十分强大的模式匹配机制，可以应用到很多场合。\n  * switch语句\n  * 类型查询\n  * 以及快速获取数据\n* 并且scala还提供了样例类，对模式匹配进行了优化，可以快速进行匹配。\n\n- 22.1 匹配字符串\n\n~~~scala\n//todo:匹配字符串\nobject CaseDemo01 extends App{\n  //定义一个数组\n  val arr=Array(\"hadoop\",\"zookeeper\",\"spark\",\"storm\")\n\n  //随机取数组中的一位，使用Random.nextInt\n  val name = arr(Random.nextInt(arr.length))\n  println(name)\n\n  name match {\n    case \"hadoop\"     => println(\"大数据分布式存储和计算框架...\")\n    case \"zookeeper\"  => println(\"大数据分布式协调服务框架...\")\n    case \"spark\"      => println(\"大数据分布式内存计算框架...\")\n      //表示以上情况都不满足才会走最后一个\n    case _            => println(\"我不认识你\")\n  }\n\n}\n\n~~~\n\n\n\n- 22.2 匹配类型\n\n~~~scala\n//todo:匹配类型\nobject CaseDemo02 extends App{\n  //定义一个数组\n  val arr=Array(\"hello\",1,-2.0,CaseDemo02)\n\n  //随机获取数组中的元素\n  val value=arr(Random.nextInt(arr.length))\n  println(value)\n\n    \n  value match {\n    case x:Int                => println(\"Int=>\"+x)\n    case y:Double if(y>=0)    => println(\"Double=>\"+y)\n    case z:String             => println(\"String=>\"+z)\n    case _                    => throw new Exception(\"not match exception\")\n  }\n\n}\n\n~~~\n\n\n\n- 22.3 匹配数组\n\n~~~scala\n//匹配数组\nobject CaseDemo03 extends App{\n\n  //匹配数组\n  val  arr=Array(1,3,5)\n  arr match{\n    case Array(1,x,y) =>println(x+\"---\"+y)\n    case Array(1,_*)  =>println(\"1...\")\n    case Array(0)     =>println(\"only 0\")\n    case _            =>println(\"something else\")\n      \n  }\n}\n\n~~~\n\n\n\n- 22.4 匹配集合\n\n~~~scala\n//匹配集合\nobject CaseDemo04 extends App{\n\n  val list=List(0,3,6)\n  list match {\n    case 0::Nil        => println(\"only 0\")\n    case 0::tail       => println(\"0....\")\n    case x::y::z::Nil  => println(s\"x:$x y:$y z:$z\")\n    case _             => println(\"something else\")\n  }\n}    \n\n\n~~~\n\n\n\n- 22.5 匹配元组\n\n~~~scala\n//匹配元组\nobject CaseDemo05 extends App{\n  \n  val tuple=(1,3,5)\n  tuple match{\n    case (1,x,y)    => println(s\"1,$x,$y\")\n    case (2,x,y)    => println(s\"$x,$y\")\n    case _          => println(\"others...\")\n  }\n}\n\n~~~\n\n\n\n- 22.6 样例类\n\n* 样例类是一种特殊类，它可以用来快速定义一个用于**保存数据**的类（类似于Java POJO类），==而且它会自动生成apply方法，允许我们快速地创建样例类实例对象==。后面在并发编程和spark、flink这些框架也都会经常使用它。\n\n* 定义样例类\n\n  * 语法结构\n\n    ~~~scala\n    case class 样例类名(成员变量名1:类型1, 成员变量名2:类型2 ...)\n    \n    ~~~\n\n* ==示例==\n\n  ~~~scala\n  // 定义一个样例类\n  // 样例类有两个成员name、age\n  case class CasePerson(name:String, age:Int)\n  \n  // 使用var指定成员变量是可变的\n  case class CaseStudent(var name:String, var age:Int)\n  \n  object CaseClassDemo {\n    def main(args: Array[String]): Unit = {\n      // 1. 使用new创建实例\n      val zhagnsan = new CasePerson(\"张三\", 20)\n      println(zhagnsan)\n  \n      // 2. 使用类名直接创建实例\n      val lisi = CasePerson(\"李四\", 21)\n      println(lisi)\n  \n      // 3. 样例类默认的成员变量都是val的，除非手动指定变量为var类型\n      //lisi.age = 22  // 编译错误！age默认为val类型\n  \n      val xiaohong = CaseStudent(\"小红\", 23)\n      xiaohong.age = 24\n      println(xiaohong)\n    }\n  }\n  ~~~\n\n\n* 样例对象\n\n  * 使用case object可以创建样例对象。样例对象是单例的，而且它**没有主构造器**。样例对象是可序列化的。格式：\n\n    ~~~scala\n    case object 样例对象名\n    ~~~\n\n  * ==示例==\n\n    ~~~scala\n    case class SendMessage(text:String)\n    \n    // 消息如果没有任何参数，就可以定义为样例对象\n    case object startTask\n    case object PauseTask\n    case object StopTask\n    \n    ~~~\n\n* 样例类和样例对象结合模式使用\n\n  * ==示例==\n\n    ~~~scala\n    case class SubmitTask(id: String, name: String)\n    case class HeartBeat(time: Long)\n    case object CheckTimeOutTask\n    \n    object CaseDemo06 extends App{\n    \n      val arr = Array(CheckTimeOutTask,\n                      HeartBeat(10000), \n                      SubmitTask(\"0001\", \"task-0001\"))\n    \n      arr(Random.nextInt(arr.length)) match {\n          \n           case SubmitTask(id, name) => println(s\"id=$id, name=$name\")\n           case HeartBeat(time) => println(s\"time=$time\")\n           case CheckTimeOutTask => println(\"检查超时\")\n    \n      }\n    }\n    \n    ~~~\n\n\n- 22.7 Option类型\n\n* 在Scala中Option类型用样例类来表示可能存在或也可能不存在的值\n\n* Option类型有2个子类\n\n  * 一个是Some\n\n    * Some包装了某个值\n\n    ![1568271621212](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1568271621212.png)\n\n  * 一个是None\n\n    * None表示没有值\n\n    ![1568271671144](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1568271671144.png)\n\n* 示例\n\n  ~~~scala\n  object TestOption {\n    def main(args: Array[String]) {\n      val map = Map(\"a\" -> 1, \"b\" -> 2)\n        \n      val value: Option[Int] = map.get(\"b\")\n      val v1 =value match {\n        case Some(i) => i\n        case None => 0\n      }\n      println(v1)\n  \n      //更好的方式\n      val v2 = map.getOrElse(\"c\", 0)\n      println(v2)\n    }\n  }\n  \n  \n  ~~~\n\n\n\n- 22.8 偏函数\n\n* 被包在花括号内==没有match的一组case语句==是一个偏函数\n\n* 它是PartialFunction[A, B]的一个实例，\n\n  * A代表输入参数类型\n  * B代表返回结果类型\n  * 可以理解为：偏函数是一个参数和一个返回值的函数。\n\n* ==示例==\n\n  ~~~scala\n  object TestPartialFunction {\n    // func1是一个输入参数为Int类型，返回值为String类型的偏函数\n    val func1: PartialFunction[Int, String] = {\n      case 1 => \"一\"\n      case 2 => \"二\"\n      case 3 => \"三\"\n      case _ => \"其他\"\n    }\n  \n    def main(args: Array[String]): Unit = {\n      println(func1(1))\n      \n      val list=List(1,2,3,4,5,6)\n  \n      //使用偏函数操作\n      val result=list.filter{\n        case x if x >3 => true\n        case _ => false\n      }\n      println(result)\n    }\n  \n  }\n  \n  \n  ~~~\n\n\n\n### 23. 异常处理\n\n- 23.1 异常场景\n\n* 来看看下面一段代码\n\n~~~scala\n  def main(args: Array[String]): Unit = {\n   val i = 10 / 0\n    \n    println(\"你好！\")\n  }\n\nException in thread \"main\" java.lang.ArithmeticException: / by zero\n\tat ForDemo$.main(ForDemo.scala:3)\n\tat ForDemo.main(ForDemo.scala)\n\n~~~\n\n* 执行程序，可以看到scala抛出了异常，而且没有打印出来\"你好\"。说明程序出现错误后就终止了。那怎么解决该问题呢？\n\n\n\n- 23.2 捕获异常\n\n* 在scala中，可以使用异常处理来解决这个问题。\n\n  * 在Scala里，借用了==模式匹配的思想来做异常的匹配==\n  * 以下为scala中try...catch异常处理的语法格式：\n\n  ~~~scala\n  try {\n      // 代码\n  }\n  catch {\n      case ex:异常类型1 => // 代码\n      case ex:异常类型2 => // 代码\n  }\n  finally {\n      // 代码\n  }\n  \n  \n  ~~~\n\n  * try中的代码是我们编写的业务处理代码\n  * 在catch中表示当出现某个异常时，需要执行的代码\n  * 在finally中，是不管是否出现异常都会执行的代码\n\n* ==示例==\n\n  ~~~scala\n  try {\n      val i = 10 / 0\n  \n  } catch {\n      case ex: Exception => println(ex.getMessage)\n  } finally {\n      println(\"我始终都会执行!\")\n  }\n  ~~~\n\n\n- 23.3 抛出异常\n\n* 我们也可以在一个方法中，抛出异常。语法格式和Java类似，使用throw new Exception...\n\n* ==示例==\n\n  ~~~scala\n    def main(args: Array[String]): Unit = {\n      throw new Exception(\"这是一个异常\")\n    }\n  \n  Exception in thread \"main\" java.lang.Exception: 这是一个异常\n  \tat ForDemo$.main(ForDemo.scala:3)\n  \tat ForDemo.main(ForDemo.scala)\n  ~~~\n\n\n\n### 24. 提取器(Extractor)\n\n* ==提取器是从传递给它的对象中提取出构造该对象的参数==。(回想样例类进行模式匹配提取参数)\n\n* scala 提取器是一个带有unapply方法的对象。\n  * ==unapply方法算是apply方法的反向操作==\n    * unapply接受一个对象，然后从对象中提取值，提取的值通常是用来构造该对象的值。\n\n![1552639637165](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1552639637165.png)\n\n![1552639674932](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1552639674932.png)\n\n* ==示例==\n\n~~~scala\nclass Student {\n  var name:String = _   // 姓名\n  var age:Int = _       // 年龄\n  \n  // 实现一个辅助构造器\n  def this(name:String, age:Int) = {\n    this()\n    \n    this.name = name\n    this.age = age\n  }\n}\n\nobject Student {\n  def apply(name:String, age:Int): Student = new Student(name, age)\n\n  // 实现一个解构器\n  def unapply(arg: Student): Option[(String, Int)] = Some(arg.name, arg.age))\n}\n\nobject extractor_DEMO {\n  def main(args: Array[String]): Unit = {\n    val zhangsan = Student(\"张三\", 20)\n\n    zhangsan match {\n      case Student(name, age) => println(s\"姓名：$name 年龄：$age\")\n      case _ => println(\"未匹配\")\n    }\n  }\n}\n~~~\n\n\n\n### 25. 泛型\n\n\n\n* scala和Java一样，类和特质、方法都可以支持泛型。我们在学习集合的时候，一般都会涉及到泛型。\n\n```scala\nscala> val list1:List[String] = List(\"1\", \"2\", \"3\")\nlist1: List[String] = List(1, 2, 3)\n\n```\n\n* 在scala中，使用方括号来定义类型参数。\n\n\n\n- 25.1 定义一个泛型方法\n\n* 不考虑泛型的支持\n\n  ~~~scala\n    def getMiddle(arr:Array[Int]) = arr(arr.length / 2)\n  \n    def main(args: Array[String]): Unit = {\n      val arr1 = Array(1,2,3,4,5)\n  \n      println(getMiddle(arr1))\n    }\n  \n  ~~~\n\n* 考虑泛型的支持\n\n  ~~~scala\n    def getMiddle[A](arr:Array[A]) = arr(arr.length / 2)\n  \n    def main(args: Array[String]): Unit = {\n      val arr1 = Array(1,2,3,4,5)\n      val arr2 = Array(\"a\", \"b\", \"c\", \"d\", \"f\")\n  \n      println(getMiddle[Int](arr1))\n      println(getMiddle[String](arr2))\n  \n      // 简写方式\n      println(getMiddle(arr1))\n      println(getMiddle(arr2))\n    }\n  \n  ~~~\n\n\n\n- 25.2 定义一个泛型类\n\n* 定义一个Pair类包含2个类型不固定的泛型\n* ==示例==\n\n ~~~scala\n// 类名后面的方括号，就表示这个类可以使用两个类型、分别是T和S\n// 这个名字可以任意取\nclass Pair[T, S](val first: T, val second: S)\n\ncase class People(var name:String, val age:Int)\n\nobject Pair {\n  def main(args: Array[String]): Unit = {\n\n  val p1 = new Pair[String, Int](\"张三\", 10)\n  val p2 = new Pair[String, String](\"张三\", \"1988-02-19\")\n  val p3 = new Pair[People, People](People(\"张三\", 20), People(\"李四\", 30))\n  }\n}\n ~~~\n\n\n\n### 26. 上下界\n\n* ==在指定泛型类型时，有时需要界定泛型类型的范围，而不是接收任意类型==。比如，要求某个泛型类型，必须是某个类的子类，这样在程序中就可以放心的调用父类的方法，程序才能正常的使用与运行.\n* scala的上下边界特性允许泛型类型是某个类的子类，或者是某个类的父类\n  * 1、 ==U >: T==\n    * 这是类型==下界==的定义，也就是U必须是类型T的父类或者是自己本身。\n  * 2、 ==U <: T==\n    - 这是类型==上界==的定义，也就是U必须是类型T的子类或者是自己本身。\n* ==示例一==\n\n~~~scala\n// 类名后面的指定泛型的范围 ----上界\nclass Pair1[T <: Person, S <:Person](val first: T, val second: S) {\n  def chat(msg:String) = println(s\"${first.name}对${second.name}说: $msg\")\n}\n\nclass Person(var name:String, val age:Int)\n\nobject Pair1 {\n  def main(args: Array[String]): Unit = {\n\n  val p3 = new Pair1[Person,Person](new Person(\"张三\", 20), new Person(\"李四\", 30))\n  p3.chat(\"你好啊！\")\n  }\n}\n\n~~~\n\n* ==示例二==\n\n  ![1552657709922](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1552657709922.png)\n\n~~~scala\n//要控制Person只能和Person、Policeman聊天，但是不能和Superman聊天。此时，还需要给泛型添加一个下界。\n\n//上下界\nclass Pair[T <: Person, S >: Policeman <:Person](val first: T, val second: S) {\n  def chat(msg:String) = println(s\"${first.name}对${second.name}说: $msg\")\n}\n\nclass Person(var name:String, val age:Int)\nclass Policeman(name:String, age:Int) extends Person(name, age)\nclass Superman(name:String) extends Policeman(name, -1)\n\nobject Pair {\n  def main(args: Array[String]): Unit = {\n\t// 编译错误：第二个参数必须是Person的子类（包括本身）、Policeman的父类（包括本身）\n   val p3 = new Pair[Person,Superman](new Person(\"张三\", 20), new Superman(\"李四\"))\n   p3.chat(\"你好啊！\")\n  }\n}\n~~~\n\n\n\n\n\n### 27. 协变、逆变、非变\n\n* 来一个类型转换的问题\n\n~~~scala\nclass Pair[T](a:T)\n\nobject Pair {\n  def main(args: Array[String]): Unit = {\n    val p1 = new Pair(\"hello\")\n    // 编译报错，无法将p1转换为p2\n    val p2:Pair[AnyRef] = p1\n\n    println(p2)\n  }\n}\n~~~\n\n* ==**协变**==\n\n  ~~~html\n  class Pair[+T]，这种情况是协变。类型B是A的子类型，Pair[B]可以认为是Pair[A]的子类型。这种情况，参数化类型的方向和类型的方向是一致的。\n  \n  ~~~\n\n* ==**逆变**==\n\n  ~~~html\n  class Pair[-T]，这种情况是逆变。类型B是A的子类型，Pair[A]反过来可以认为是Pair[B]的子类型。这种情况，参数化类型的方向和类型的方向是相反的。\n  \n  ~~~\n\n* ==**非变**==\n\n  ~~~html\n  class Pair[T]{}，这种情况就是非变（默认），类型B是A的子类型，Pair[A]和Pair[B]没有任何从属关系，这种情况和Java是一样的。\n  \n  ~~~\n\n  ![1558064807949](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/scala学习入门/assets/1558064807949.png)\n\n* ==示例==\n\n  ~~~scala\n  class Super\n  class Sub extends Super\n  \n  //非变\n  class Temp1[A](title: String)\n  //协变\n  class Temp2[+A](title: String)\n  //逆变\n  class Temp3[-A](title: String)\n  \n  object Covariance_demo {\n    def main(args: Array[String]): Unit = {\n      val a = new Sub()\n      // 没有问题，Sub是Super的子类\n      val b:Super = a\n  \n      // 非变\n      val t1:Temp1[Sub] = new Temp1[Sub](\"测试\")\n      // 报错！默认不允许转换\n      // val t2:Temp1[Super] = t1\n  \n      // 协变\n      val t3:Temp2[Sub] = new Temp2[Sub](\"测试\")\n      val t4:Temp2[Super] = t3\n      \n      // 逆变\n      val t5:Temp3[Super] = new Temp3[Super](\"测试\")\n      val t6:Temp3[Sub] = t5\n    }\n  }\n  \n  ~~~\n\n* ==总结==\n\n  ~~~html\n  C[+T]：如果A是B的子类，那么C[A]是C[B]的子类。\n  C[-T]：如果A是B的子类，那么C[B]是C[A]的子类。\n  C[T]： 无论A和B是什么关系，C[A]和C[B]没有从属关系。\n  \n  ~~~\n\n\n### 28. 隐式转换和隐式参数\n\n- 28.1 隐式转换\n\n~~~html\n\tScala提供的隐式转换和隐式参数功能，是非常有特色的功能。是Java等编程语言所没有的功能。它可以允许你手动指定，将某种类型的对象转换成其他类型的对象或者是给一个类增加方法。通过这些功能，可以实现非常强大、特殊的功能。\n\n~~~\n\n* 隐式转换其核心就是定义一个使用 ==implicit== 关键字修饰的方法 实现把一个原始类转换成目标类，进而可以调用目标类中的方法\n\n\n\n- 28.2 隐式参数\n\n~~~html\n\t所谓的隐式参数，指的是在函数或者方法中，定义一个用implicit修饰的参数，\n此时Scala会尝试找到一个指定类型的用implicit修饰的参数，即隐式值，并注入参数。\n\n~~~\n\n\n\n* ==所有的隐式转换和隐式参数必须定义在一个object中==\n\n\n\n- 28.3 案例演示\n\n* ==案例一==\n\n  * **让File类具备RichFile类中的read方法**\n\n  ~~~scala\n  package com.kaikeba.implic_demo\n  \n  import java.io.File\n  \n  import scala.io.Source\n  \n  //todo:隐式转换案例一:让File类具备RichFile类中的read方法\n  \n  object MyPredef{\n    //定义一个隐式转换的方法，实现把File转换成RichFile\n    implicit  def file2RichFile(file:File)=new RichFile(file)\n  \n  }\n  \n  class RichFile(val file:File){\n       //读取数据文件的方法\n      def read():String={\n         Source.fromFile(file).mkString\n      }\n  }\n  \n  object RichFile{\n    def main(args: Array[String]): Unit = {\n       //1、构建一个File对象\n            val file = new File(\"E:\\\\aa.txt\")\n  \n       //2、手动导入隐式转换\n        import MyPredef.file2RichFile\n  \n         val data: String = file.read\n          println(data)\n    }\n  }\n  \n  ~~~\n\n\n\n* ==案例二==\n\n  * **超人变身**\n\n  ~~~scala\n  package com.kaikeba.implic_demo\n  \n  //todo:隐式转换案例二:超人变身\n  class Man(val name:String)\n  \n  class SuperMan(val name: String) {\n    def heat=print(\"超人打怪兽\")\n  \n  }\n  \n  object SuperMan{\n    //隐式转换方法\n    implicit def man2SuperMan(man:Man)=new SuperMan(man.name)\n  \n    def main(args: Array[String]) {\n        val hero=new Man(\"hero\")\n        //Man具备了SuperMan的方法\n        hero.heat\n    }\n  \n  }\n  \n  ~~~\n\n\n\n* ==案例三==\n\n  * **一个类隐式转换成具有相同方法的多个类**\n\n  ~~~~scala\n  package com.kaikeba.implic_demo\n  \n  //todo:隐式转换案例三（一个类隐式转换成具有相同方法的多个类）\n  \n  class C\n  class A(c:C) {\n      def readBook(): Unit ={\n        println(\"A说：好书好书...\")\n      }\n  }\n  \n  class B(c:C){\n    def readBook(): Unit ={\n      println(\"B说：看不懂...\")\n    }\n    def writeBook(): Unit ={\n      println(\"B说：不会写...\")\n    }\n  }\n  \n  object AB{\n  \n    //创建一个类转换为2个类的隐式转换\n    implicit def C2A(c:C)=new A(c)\n    implicit def C2B(c:C)=new B(c)\n  }\n  \n  object B{\n    def main(args: Array[String]) {\n      //导包\n      //1. import AB._ 会将AB类下的所有隐式转换导进来\n      //2. import AB.C2A 只导入C类到A类的的隐式转换方法\n      //3. import AB.C2B 只导入C类到B类的的隐式转换方法\n      import AB._\n      val c=new C\n  \n      //由于A类与B类中都有readBook()，只能导入其中一个，否则调用共同方法时代码报错\n       //c.readBook()\n  \n      //C类可以执行B类中的writeBook()\n      c.writeBook()\n  \n    }\n  }\n  \n  ~~~~\n\n\n\n* ==案例四==\n\n  - **员工领取薪水**\n\n  ~~~scala\n  package cn.itcast.implic_demo\n  \n  //todo:隐式参数案例四:员工领取薪水\n  \n  object Company{\n    //在object中定义隐式值    注意：同一类型的隐式值只允许出现一次，否则会报错\n    implicit  val xxx=\"zhangsan\"\n    implicit  val yyy=10000.00\n  \n    //implicit  val zzz=\"lisi\"\n  \n  }\n  \n  class Boss {\n    //定义一个用implicit修饰的参数 类型为String\n    //注意参数匹配的类型   它需要的是String类型的隐式值\n    def callName(implicit name:String):String={\n      name+\" is coming !\"\n    }\n  \n    //定义一个用implicit修饰的参数，类型为Double\n    //注意参数匹配的类型    它需要的是Double类型的隐式值\n    def getMoney(implicit money:Double):String={\n      \" 当月薪水：\"+money\n    }\n  \n  \n  }\n  match\n  object Boss extends App{\n    //使用import导入定义好的隐式值，注意：必须先加载否则会报错\n    import Company.xxx\n    import Company.yyy\n  \n    val boss =new Boss\n    println(boss.callName+boss.getMoney)\n  \n  }\n  \n  ~~~\n","tags":["-scala"]},{"title":"sqoop数据迁移工具","url":"/2019/05/26/it/sqoop数据迁移工具/","content":"\n### Sqoop简介\n\n* Sqoop是apache旗下的一款 ”==Hadoop和关系数据库之间传输数据==”的工具\n  * ==导入数据== import\n    * 将MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统\n  * ==导出数据== export\n    * 从Hadoop的文件系统中导出数据到关系数据库\n\n![sqoop](https://kflys.gitee.io/upic/2020/04/02/uPic/sqoop%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E5%B7%A5%E5%85%B7/assets//sqoop.png)\n\n- Sqoop的工作机制\n  - 将导入和导出的命令翻译成mapreduce程序实现\n  - 在翻译出的mapreduce中主要是对inputformat和outputformat进行定制\n\n- Sqoop基本架构\n  - sqoop在发展中的过程中演进出来了两种不同的架构.[架构演变史](<https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop#comment-1561314193000>)\n\n\n![sqoop1](https://kflys.gitee.io/upic/2020/04/02/uPic/sqoop%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E5%B7%A5%E5%85%B7/assets//sqoop1.jpg)\n\n~~~\n版本号为1.4.x0\n~~~\n\n* ==sqoop2的架构图==\n\n![sqoop2](https://kflys.gitee.io/upic/2020/04/02/uPic/sqoop%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E5%B7%A5%E5%85%B7/assets/sqoop2.jpg)\n\n~~~\n版本号为1.99x为sqoop2 \n在架构上：sqoop2引入了sqoop server，对connector实现了集中的管理 \n访问方式：REST API、 JAVA API、 WEB UI以及CLI控制台方式进行访问 \n~~~\n\n![sqoop1 VS sqoop2](https://kflys.gitee.io/upic/2020/04/02/uPic/sqoop%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E5%B7%A5%E5%85%B7/assets//sqoop1 VS sqoop2.png)\n\n### Sqoop安装部署\n\n​\t\t[点击查看](https://kfly.top/2019/11/26/hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n\n\n### 数据导入\n\n* 导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据） \n\n~~~shell\n# 命令行查看帮助文档\nsqoop list-databases --help\n\n# 列出node03上mysql数据库中所有的数据库名称\nsqoop list-databases --connect jdbc:mysql://node03:3306/ --username root --password 123456\n\n# 查看某一个数据库下面的所有数据表\nsqoop list-tables --connect jdbc:mysql://node03:3306/hive --username root --password 123456\n\n# 导入数据库表数据到HDFS\nsqoop import \\\n  --connect jdbc:mysql://node02:3306/userdb \\\n  --username root   \\\n  --password 123456 \\\n  --table emp \\\n  --m 1\n  \n# 导出数据到指定目录,指定分隔符\nsqoop import  \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--delete-target-dir \\\n--table emp  \\\n--target-dir /sqoop/emp1 \\\n--fields-terminated-by '#' \\\n--m 1\n\n\n#参数解释\n--connect   指定mysql链接地址\n--username  连接mysql的用户名\n--password  连接mysql的密码\n--table     指定要导入的mysql表名称\n--m:        表示这个MR程序需要多少个MapTask去运行，默认为4\n--target-dir\t来指定导出目的地，\n--delete-target-dir 来判断导出目录是否存在，如果存在就删掉\n--fields-terminated-by '#' 指定数据的分隔符\n# 默认路径是/user/hadoop下\n~~~\n\n~~~sql\n--  导入关系表到Hive中\ncreate database sqooptohive;\n\ncreate external table sqooptohive.emp_hive(id int,name string,deg string,salary double ,dept string) row format delimited fields terminated by '\\001';\n~~~\n\n~~~shell\n# 导入数据到hive表\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--fields-terminated-by '\\001' \\\n--hive-import \\\n--hive-table sqooptohive.emp_hive \\\n--hive-overwrite \\\n--delete-target-dir \\\n--m 1\n\n# 导入数据库表数据到hive中(并自动创建hive表)\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--hive-database sqooptohive \\\n--hive-table emp1 \\\n--hive-import \\\n--m 1 \n\n# 导入表数据子集\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--target-dir /sqoop/emp_where \\\n--delete-target-dir \\\n--where \"dept = 'TP'\" \\\n--m 1 \n\n# sql语句查找导入hive\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--target-dir /sqoop/emp_sql \\\n--delete-target-dir \\\n--query 'select * from emp where salary >30000 and $CONDITIONS' \\\n--m 1\n\n##  $CONTITONS是linux系统的变量，如果你想通过并行的方式导入结果，每个map task需要执行sql查询后脚语句的副本，结果会根据sqoop推测的边界条件分区。query必须包含$CONDITIONS。这样每个sqoop程序都会被替换为一个独立的条件。同时你必须指定 --split-by '字段'，后期是按照字段进行数据划分，最后可以达到多个MapTask并行运行。\n\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--target-dir /sqoop/emp_sql_2 \\\n--delete-target-dir \\\n--query 'select * from emp where salary >30000 and $CONDITIONS' \\\n--split-by 'id' \\\n--m 2\n\n\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--target-dir /sqoop/emp_sql_2 \\\n--delete-target-dir \\\n--query 'select * from emp where id >1 and $CONDITIONS' \\\n--split-by 'salary' \\\n--m 2\n\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--target-dir /sqoop/emp_sql_2 \\\n--delete-target-dir \\\n--query 'select * from emp where id >1 and $CONDITIONS' \\\n--split-by 'id' \\\n--m 7\n## --split-by '字段'： 后期按照字段进行数据划分实现并行运行多个MapTask。\n~~~\n\n\n#### 增量导入\n\n* 在实际工作当中，数据的导入很多时候都是==只需要导入增量数据即可==，并不需要将表中的数据全部导入到hive或者hdfs当中去，肯定会出现重复的数据的状况，所以我们一般都是选用一些字段进行增量的导入，为了支持增量的导入，sqoop也给我们考虑到了这种情况并且支持增量的导入数据\n\n* 增量导入是仅导入新添加的表中的行的技术。\n\n* 它需要添加 ==‘incremental’, ‘check-column’, 和 ‘last-value’==选项来执行增量导入。\n\n  ~~~\n  --incremental <mode>\n  --check-column <column name>\n  --last value <last check column value>\n  ~~~\n\n* ==第一种增量导入实现==\n\n  * ==基于递增列的增量数据导入（Append方式）==\n  * 导入emp表当中id大于1202的所有数据\n    * 注意：==这里不能加上 --delete-target-dir  参数，添加就报错==\n\n  ~~~shell\n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456 \\\n  --table emp \\\n  --incremental append \\\n  --check-column id \\\n  --last-value 1202  \\\n  --target-dir /sqoop/increment \\\n  --m 1\n  \n  \n  ##参数解释\n  --incremental   这里使用基于递增列的增量数据导入\n  --check-column  递增列字段\n  --last-value    指定上一次导入中检查列指定字段最大值\n  --target-dir    数据导入的目录\n  ~~~\n\n\n* ==第二种增量导入实现==\n\n  * ==基于时间列的增量数据导入（LastModified方式）==\n\n    * 此方式要求原有表中有time字段，它能指定一个时间戳\n      * user表结构和数据\n\n    ![table_user](https://kflys.gitee.io/upic/2020/04/02/uPic/sqoop%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E5%B7%A5%E5%85%B7/assets//table_user.png)\n\n  ~~~shell\n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456  \\\n  --table user \\\n  --incremental lastmodified  \\\n  --check-column createTime  \\\n  --last-value '2019-10-01 10:30:00'  \\\n  --target-dir /sqoop/increment2 \\\n  --m 1\n  \n  ##参数解释\n  --incremental   这里使用基于时间列的增量导入\n  --check-column  时间字段\n  --last-value    指定上一次导入中检查列指定字段最大值\n  --target-dir    数据导入的目录\n  \t\t\t\t如果该目录存在(可能已经有数据)\n  \t\t\t\t再使用的时候需要添加 --merge-key or --append\n  \t\t--merge-key 指定合并key（对于有修改的）\n  \t\t--append    直接追加修改的数据\n  ~~~\n\n\n#### mysql导入hbase\n\n* 实现把一张mysql表数据导入到hbase中\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--table emp \\\n--hbase-table  mysqluser \\\n--column-family  f1 \\\n--hbase-create-table \\\n--hbase-row-key id  \\\n--m 1 \n\n\n#参数说明\n--hbase-table  \t\t\t指定hbase表名\n--column-family \t\t指定表的列族\n--hbase-create-table \t表不存在就创建\n--hbase-row-key \t\t指定hbase表的id\n--m  \t\t\t\t\t指定使用的MapTask个数\nlist\nscan 'mysqluser'\ndisable 'mysqluser'\ndrop 'mysqluser'\n\n\n# mysql导入hbase 不同的列族\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--columns id,salary,dept \\\n--table emp \\\n--hbase-table  mysqluser2 \\\n--column-family  f1 \\\n--hbase-create-table \\\n--hbase-row-key id  \\\n--m 1 \n\n\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--columns id,name,deg \\\n--table emp \\\n--hbase-table  mysqluser2 \\\n--column-family  f2 \\\n--hbase-create-table \\\n--hbase-row-key id  \\\n--m 1 \n~~~\n\n### 数据导出\n\n* 将数据从HDFS把文件导出到RDBMS数据库\n  * 导出前，目标表必须存在于目标数据库中。\n    * 默认操作是从将文件中的数据使用INSERT语句插入到表中\n    * 更新模式下，是生成UPDATE语句更新表数据\n\n\n\n#### hdfs->mysql\n\n* 1、数据是在HDFS当中的如下目录/user/hive/warehouse/hive_source，数据内容如下\n\n~~~\n1 zhangsan 20 hubei\n2 lisi 30 hunan\n3 wangwu 40 beijing\n4 xiaoming 50 shanghai\n~~~\n\n~~~sql\nCREATE TABLE  userdb.fromhdfs (\n   id INT DEFAULT NULL,\n   name VARCHAR(100) DEFAULT NULL,\n   age int DEFAULT NULL,\n   address VARCHAR(100) DEFAULT NULL\n) ENGINE=INNODB DEFAULT CHARSET=utf8;\n~~~\n\n~~~shell\nsqoop export \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table fromhdfs \\\n--export-dir /user/hive/warehouse/hive_source \\\n--input-fields-terminated-by \" \" \n\n##参数解释\n--table \t\t\t\t\t  指定导出的mysql表名\n--export-dir \t\t\t\t  指定hdfs数据文件目录\n--input-fields-terminated-by  指定文件数据字段的分隔符\n~~~\n\n### Sqoop job\n\n* 将事先定义好的数据导入导出任务按照指定流程运行\n\n\n~~~shell\nsqoop job (generic-args) (job-args)\n   [-- [subtool-name] (subtool-args)]\n~~~\n\n- 创建job\n  - ==--create==\n    - 创建一个名为myjob,实现从mysql表数据导入到hdfs上的作业\n      - 在创建job时，==命令\"-- import\" 中间有个空格==\n\n~~~shell\nsqoop job --help\n\n##创建一个sqoop作业\nsqoop job \\\n--create myjob1 \\\n-- import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--target-dir /sqoop/myjob \\\n--delete-target-dir \\\n--m 1\n\n##创建一个sqoop增量导入的作业\nsqoop  job  \\\n--create incrementJob1 \\\n-- import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--table user \\\n--target-dir /sqoop/incrementJob \\\n--incremental append  \\\n--check-column createTime  \\\n--last-value '2019-11-19 16:40:21'  \\\n--m 1\n\n# incremental.last.value\n~~~\n\n- 查看job \n\n~~~shell\nsqoop job --list\n\n最后显示：\nAvailable jobs:\n  myjob\n# 展示job\nsqoop job --show myjob1\n~~~\n\n- 执行job\n\n~~~shell\n  sqoop job --exec myjob1\n~~~\n\n  * 解决sqoop需要输入密码的问题\n  * sqoop-site.xml\n\n  ~~~xml\n  <property>\n      <name>sqoop.metastore.client.record.password</name>\n      <value>true</value>\n      <description>If true, allow saved passwords in the metastore.\n      </description>\n  </property>\n  ~~~\n\n- 删除job\n\n```shell\nsqoop job --delete myjob\n```\n\n","tags":["sqoop"]},{"title":"Flume日志采集框架","url":"/2019/05/21/it/Flume日志采集框架/","content":"\n# Flume日志采集框架\n\n![image-20191126223347760](http://kflys.gitee.io/upic/2020/04/02/uPic/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6/assets/image-20191126223347760.png)\n\n~~~\n\t在一个完整的离线大数据处理系统中，除了hdfs+mapreduce+hive组成分析系统的核心之外，还需要数据采集、结果数据导出、任务调度等不可或缺的辅助系统，而这些辅助工具在hadoop生态体系中都有便捷的开源框架。\n~~~\n\n* Flume是Cloudera提供的一个高可用的，高可靠的，分布式的==海量日志采集、聚合和传输的系统==\n* Flume支持在日志系统中定制各类数据发送方，用于收集数据；\n* Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。\n\n重构后的版本统称为 Flume NG（next generation）；改动的另一原因是将 Flume 纳入 apache 旗下，cloudera Flume 改名为 Apache Flume。\n\n　　　　官方网站： http://flume.apache.org/\n\n### 架构\n\n```markdown\n* Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。\n* Flume分布式系统中==最核心的角色是agent==，flume采集系统就是由一个个agent所连接起来形成。\n```\n\n```markdown\n- Client：\n  - Client生产数据，运行在一个独立的线程。\n- Event：\n  -  一个数据单元，消息头和消息体组成。（Events可以是日志记录、 avro 对象等。）\n- Flow：\n  -  Event从源点到达目的点的迁移的抽象。\n- Agent：\n  - 一个独立的Flume进程，包含组件Source、 Channel、 Sink。（Agent使用JVM 运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。）\n- Source：\n  -  数据收集组件。（source从Client收集数据，传递给Channel）\n- Channel：\n  -  中转Event的一个临时存储，保存由Source组件传递过来的Event。（Channel连接 sources 和 sinks ，这个有点像一个队列。）\n- Sink： \n  - 从Channel中读取并移除Event， 将Event传递到FlowPipeline中的下一个Agent（如果有的话）（Sink从Channel收集数据，运行在一个独立线程。） \n```\n\n- Agent结构\n\n　Flume 运行的核心是 Agent。Flume以agent为最小的独立运行单位。一个agent就是一个JVM。它是一个完整的数据收集工具，含有三个核心组件，分别是\n\n　　 source、 channel、 sink。通过这些组件， Event 可以从一个地方流向另一个地方，如下图所示。　　\n\n- Source\n  - Source是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入Channel中。 Flume提供了很多内置的Source， 支持 Avro， log4j， syslog 和 http post(body为json格式)。可以让应用程序同已有的Source直接打交道，如AvroSource，SyslogTcpSource。 如果内置的Source无法满足需要， Flume还支持自定义Source。\n\n\n- Channel\n  - Channel是连接Source和Sink的组件，大家可以将它看做一个数据的缓冲区（数据队列），它可以将事件暂存到内存中也可以持久化到本地磁盘上， 直到Sink处理完该事件。介绍两个较为常用的Channel， MemoryChannel和FileChannel。\n\n  - Channel类型：\n\n- Sink\n  - Sink从Channel中取出事件，然后将数据发到别处，可以向文件系统、数据库、 hadoop存数据， 也可以是其他agent的Source。在日志数据较少时，可以将数据存储在文件系统中，并且设定一定的时间间隔保存数据。\n\n### 系统结构图\n\n* 单个agent采集数据\n\n![](http://kflys.gitee.io/upic/2020/04/02/uPic/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6/assets/flume.png)\n\n* 2个agent串联\n\n![UserGuide_image03](http://kflys.gitee.io/upic/2020/04/02/uPic/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6/assets/UserGuide_image03.png)\n\n\n\n* 多个agent串联\n\n![](http://kflys.gitee.io/upic/2020/04/02/uPic/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6/assets/UserGuide_image02.png)\n\n* 多个channel\n\n![](http://kflys.gitee.io/upic/2020/04/02/uPic/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6/assets/UserGuide_image04.png)\n\n\n\n### 安装部署\n\n* [点击下载](http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.2.tar.gz)\n\n* vim flume-env.sh，添加java环境变量\n\n  ~~~shell\n  export JAVA_HOME=/kkb/install/jdk1.8.0_141\n  ~~~\n\n#### 采集文件到控制台\n\n- vim tail-memory-logger.conf\n\n```properties\n# Name the components on this agent\n#定义一个agent，分别指定source、channel、sink别名\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n#配置source\n#指定source的类型为exec，通过Unix命令来传输结果数据\na1.sources.r1.type = exec\n#监控一个文件，有新的数据产生就不断采集走\na1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n#指定source的数据流入的channel中\na1.sources.r1.channels = c1\n\n#配置channel\n#指定channel的类型为memory\na1.channels.c1.type = memory\n#指定channel的最多可以存放数据的容量\na1.channels.c1.capacity = 1000\n#指定在一个事务中source写数据到channel或者sink从channel取数据最大条数\na1.channels.c1.transactionCapacity = 100\n\n#配置sink\na1.sinks.k1.channel = c1\n#类型是日志格式，结果会打印在控制台\na1.sinks.k1.type = logger\n```\n\n- 启动agent\n\n  ```shell\nbin/flume-ng agent -n a1 -c myconf -f myconf/tail-memory-logger.conf -Dflume.root.logger=info,console\n  \n  \n  其中：\n  -n表示指定该agent名称\n  -c表示配置文件所在的目录\n  -f表示配置文件的路径名称\n  -D表示指定key=value键值对---这里指定的是启动的日志输出级别\n  ```\n\n#### 采集文件到HDFS\n\n- - vim file2Hdfs.conf\n\n  ```properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  #配置source\n  a1.sources.r1.type = exec\n  a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n  a1.sources.r1.channels = c1\n  \n  #配置channel\n  a1.channels.c1.type = file\n  #设置检查点目录--该目录是记录下event在数据目录下的位置\n  a1.channels.c1.checkpointDir=/kfly/data/flume_checkpoint\n  #数据存储所在的目录\n  a1.channels.c1.dataDirs=/kfly/data/flume_data\n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  #指定sink类型为hdfs\n  a1.sinks.k1.type = hdfs\n  #指定数据收集到hdfs目录\n  a1.sinks.k1.hdfs.path = hdfs://node01:8020/tailFile/%Y-%m-%d/%H%M\n  #指定生成文件名的前缀\n  a1.sinks.k1.hdfs.filePrefix = events-\n  #是否启用时间上的”舍弃”   -->控制目录 \n  a1.sinks.k1.hdfs.round = true\n  #时间上进行“舍弃”的值\n  # 如 12:10 -- 12:19 => 12:10\n  # 如 12:20 -- 12:29 => 12:20\n  a1.sinks.k1.hdfs.roundValue = 10\n  #时间上进行“舍弃”的单位\n  a1.sinks.k1.hdfs.roundUnit = minute\n  \n  # 控制文件个数\n  #60s或者50字节或者10条数据，谁先满足，就开始滚动生成新文件\n  a1.sinks.k1.hdfs.rollInterval = 60\n  a1.sinks.k1.hdfs.rollSize = 50\n  a1.sinks.k1.hdfs.rollCount = 10\n  #每个批次写入的数据量\n  \n  a1.sinks.k1.hdfs.batchSize = 100\n  #开始本地时间戳--开启后就可以使用%Y-%m-%d去解析时间\n  a1.sinks.k1.hdfs.useLocalTimeStamp = true\n  #生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\n    a1.sinks.k1.hdfs.fileType = DataStream\n  ```\n- 启动\n\n  ```\nbin/flume-ng agent -n a1 -c myconf -f myconf/file2Hdfs.conf -Dflume.root.logger=info,console\n  ```\n\n#### 采集目录到HDFS \n\n![Flume-HDFS](http://kflys.gitee.io/upic/2020/04/02/uPic/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6/assets/Dir-Flume-HDFS.png)\n\n* vim  dir2Hdfs.conf\n\n~~~properties\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# 配置source\n##注意：不能往监控目中重复丢同名文件\na1.sources.r1.type = spooldir\na1.sources.r1.spoolDir = /kfly/install/flumeData/files\n# 是否将文件的绝对路径添加到header\na1.sources.r1.fileHeader = true\na1.sources.r1.channels = c1\n\n\n#配置channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n\n#配置sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.channel = c1\na1.sinks.k1.hdfs.path = hdfs://node01:8020/spooldir/%Y-%m-%d/%H%M\na1.sinks.k1.hdfs.filePrefix = events-\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = minute\na1.sinks.k1.hdfs.rollInterval = 60\na1.sinks.k1.hdfs.rollSize = 50\na1.sinks.k1.hdfs.rollCount = 10\na1.sinks.k1.hdfs.batchSize = 100\na1.sinks.k1.hdfs.useLocalTimeStamp = true\n#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\na1.sinks.k1.hdfs.fileType = DataStream\n\n\n~~~\n\n\n* 启动\n\n  ```shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/dir2Hdfs.conf -Dflume.root.logger=info,console\n\n  ```\n\n\n#### 两个agent级联\n\n![](http://kflys.gitee.io/upic/2020/04/02/uPic/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6/assets/2个agent级联.png)\n\n* vim dir2avro.conf\n\n  ~~~properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  # 配置source\n  ##注意：不能往监控目中重复丢同名文件\n  a1.sources.r1.type = spooldir\n  a1.sources.r1.spoolDir = /kfly/install/flumeData/files\n  a1.sources.r1.fileHeader = true\n  a1.sources.r1.channels = c1\n  \n  \n  #配置channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  #AvroSink是用来通过网络来传输数据的,可以将event发送到RPC服务器(比如AvroSource)\n  a1.sinks.k1.type = avro\n  \n  #node02 注意修改为自己的hostname\n  a1.sinks.k1.hostname = node02\n  a1.sinks.k1.port = 5211\n  \n  ~~~\n\n* 建node02上的flume配置文件\n\n  * vim avro2Hdfs.conf\n\n  ~~~properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  #配置source\n  #通过AvroSource接受AvroSink的网络数据\n  a1.sources.r1.type = avro\n  a1.sources.r1.channels = c1\n  #AvroSource服务的ip地址\n  a1.sources.r1.bind = node02\n  #AvroSource服务的端口\n  a1.sources.r1.port = 5211\n  \n  #配置channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  a1.sinks.k1.type = hdfs\n  a1.sinks.k1.hdfs.path = hdfs://node01:8020/avro-hdfs/%Y-%m-%d/%H-%M\n  a1.sinks.k1.hdfs.filePrefix = events-\n  a1.sinks.k1.hdfs.round = true\n  a1.sinks.k1.hdfs.roundValue = 10\n  a1.sinks.k1.hdfs.roundUnit = minute\n  a1.sinks.k1.hdfs.rollInterval = 60\n  a1.sinks.k1.hdfs.rollSize = 50\n  a1.sinks.k1.hdfs.rollCount = 10\n  a1.sinks.k1.hdfs.batchSize = 100\n  a1.sinks.k1.hdfs.useLocalTimeStamp = true\n  #生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\n  a1.sinks.k1.hdfs.fileType = DataStream\n  \n  ~~~\n\n* 启动\n\n  * 先启动node02上的flume。然后在启动node01上的flume\n\n    ~~~shell\n    bin/flume-ng agent -n a1 -c myconf -f myconf/avro2Hdfs.conf -Dflume.root.logger=info,console\n    \n    ~~~\n\n    * 在node01上的flume安装目录下执行\n\n    ~~~shell\n    bin/flume-ng agent -n a1 -c myconf -f myconf/dir2avro.conf -Dflume.root.logger=info,console\n    \n    ~~~\n\n\n### 高可用配置案例\n\n#### failover故障转移\n\n![flume-failover](http://kflys.gitee.io/upic/2020/04/02/uPic/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6/assets/flume-failover.png)\n\n* 1、节点分配\n\n|    名称    | 服务器主机名 |     ip地址      |    角色    |\n| :--------: | :----------: | :-------------: | :--------: |\n|   Agent1   |    node01    | 192.168.200.200 | WebServer  |\n| Collector1 |    node02    | 192.168.200.210 | AgentMstr1 |\n| Collector2 |    node03    | 192.168.200.220 | AgentMstr2 |\n\n~~~\nAgent1数据分别流入到Collector1和Collector2，Flume NG本身提供了Failover机制，可以自动切换和恢复。\n~~~\n\n* vim flume-client-failover.conf\n\n~~~properties\n#agent name\na1.channels = c1\na1.sources = r1\n#定义了2个sink\na1.sinks = k1 k2\n\n#set gruop\n#设置一个sink组，一个sink组下可以包含很多个sink\na1.sinkgroups = g1\n\n#set sink group\n#指定g1这个sink组下有k1  k2 这2个sink\na1.sinkgroups.g1.sinks = k1 k2\n\n#set source\na1.sources.r1.channels = c1\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n\n#set channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n\n# set sink1    指定sink1的数据会传输给node02\na1.sinks.k1.channel = c1\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = node02\na1.sinks.k1.port = 52020\n\n# set sink2    指定sink2的数据会传输给node03\na1.sinks.k2.channel = c1\na1.sinks.k2.type = avro\na1.sinks.k2.hostname = node03\na1.sinks.k2.port = 52020\n\n#set failover\n#指定sink组高可用的策略---failover故障转移\na1.sinkgroups.g1.processor.type = failover\n#指定k1这个sink的优先级\na1.sinkgroups.g1.processor.priority.k1 = 10\n#指定k2这个sink的优先级\na1.sinkgroups.g1.processor.priority.k2 = 5\n#指定故障转移的最大时间，如果超时会出现异常\na1.sinkgroups.g1.processor.maxpenalty = 10000\n\n~~~\n\n~~~properties\n说明：\n#这里首先要申明一个sinkgroups,然后再设置2个sink ,k1与k2,其中2个优先级是10和5。\n#而processor的maxpenalty被设置为10秒，默认是30秒.表示故障转移的最大时间\n\n~~~\n\n* vim flume-server-failover.conf\n\n~~~properties\n#set Agent name\na1.sources = r1\na1.channels = c1\na1.sinks = k1\n\n#set channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# set source\na1.sources.r1.type = avro\na1.sources.r1.bind = 0.0.0.0\na1.sources.r1.port = 52020\na1.sources.r1.channels = c1\n\n#配置拦截器\n#指定2个拦截器  i1  i2 \na1.sources.r1.interceptors = i1 i2\n#i1的类型为时间戳拦截器  可以解析%Y-%m-%d 时间\na1.sources.r1.interceptors.i1.type = timestamp\n#i2的类型为主机拦截器，可以获取当前event中携带的主机名\na1.sources.r1.interceptors.i2.type = host\n#指定主机名变量\na1.sources.r1.interceptors.i2.hostHeader=hostname\n\n#set sink to hdfs\na1.sinks.k1.channel = c1\na1.sinks.k1.type=hdfs\na1.sinks.k1.hdfs.path=hdfs://node01:8020/failover/logs/%{hostname}\na1.sinks.k1.hdfs.filePrefix=%Y-%m-%d\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = minute\na1.sinks.k1.hdfs.rollInterval = 60\na1.sinks.k1.hdfs.rollSize = 50\na1.sinks.k1.hdfs.rollCount = 10\na1.sinks.k1.hdfs.batchSize = 100\na1.sinks.k1.hdfs.fileType = DataStream\n~~~\n\n* 先分别在node02和node03上启动flume\n\n~~~shell\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-server-failover.conf -Dflume.root.logger=info,console\n\n~~~\n\n  * 然后在node01上启动flume\n\n  ~~~shell\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-client-failover.conf -Dflume.root.logger=info,console\n\n  ~~~\n\n\n#### load balance负载均衡\n\n* vim  flume-client-loadbalance.conf\n\n```properties\n  #agent name\n  a1.channels = c1\n  a1.sources = r1\n  a1.sinks = k1 k2\n  #set gruop\n  a1.sinkgroups = g1\n  #set sink group\n  a1.sinkgroups.g1.sinks = k1 k2\n  #set source\n  a1.sources.r1.channels = c1\n  a1.sources.r1.type = exec\n  a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n  \n  #set channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  # set sink1\n  a1.sinks.k1.channel = c1\n  a1.sinks.k1.type = avro\n  a1.sinks.k1.hostname = node02\n  a1.sinks.k1.port = 52020\n  # set sink2\n  a1.sinks.k2.channel = c1\n  a1.sinks.k2.type = avro\n  a1.sinks.k2.hostname = node03\n  a1.sinks.k2.port = 52020\n  #set load-balance\n  #指定sink组高可用的策略---load_balance负载均衡\n  a1.sinkgroups.g1.processor.type =load_balance\n  # 默认是round_ robin，还可以选择random\n  a1.sinkgroups.g1.processor.selector = round_robin\n  #如果backoff被开启，则sink processor会屏蔽故障的sink\n  a1.sinkgroups.g1.processor.backoff = true\n  ~~~\n\n\n\n  ~~~\n  \n* ==创建node02和node03上的flume配置文件==vim  flume-server-loadbalance.conf\n\n\n  ~~~properties\n  #set Agent name\n  a1.sources = r1\n  a1.channels = c1\n  a1.sinks = k1\n  \n  #set channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  \n  # set source\n  a1.sources.r1.type = avro\n  a1.sources.r1.bind = 0.0.0.0\n  a1.sources.r1.port = 52020\n  a1.sources.r1.channels = c1\n  \n  #配置拦截器\n  a1.sources.r1.interceptors = i1 i2\n  a1.sources.r1.interceptors.i1.type = timestamp\n  a1.sources.r1.interceptors.i2.type = host\n  a1.sources.r1.interceptors.i2.hostHeader=hostname\n  #hostname不使用ip显示，直接就是该服务器对应的主机名\n  a1.sources.r1.interceptors.i2.useIP=false\n  \n  #set sink to hdfs\n  a1.sinks.k1.channel = c1\n  a1.sinks.k1.type=hdfs\n  a1.sinks.k1.hdfs.path=hdfs://node01:8020/loadbalance/logs/%{hostname}\n  a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d\n  a1.sinks.k1.hdfs.round = true\n  a1.sinks.k1.hdfs.roundValue = 10\n  a1.sinks.k1.hdfs.roundUnit = minute\n  a1.sinks.k1.hdfs.rollInterval = 60\n  a1.sinks.k1.hdfs.rollSize = 50\n  a1.sinks.k1.hdfs.rollCount = 10\n  a1.sinks.k1.hdfs.batchSize = 100\n  a1.sinks.k1.hdfs.fileType = DataStream\n  ~~~\n\n- 先分别在node02和node03上启动flume\n\n​```shell\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-server-loadbalance.conf -Dflume.root.logger=info,console\n\n```\n\n\n  * 然后在node01上启动flume\n\n\n  ~~~shell\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-client-loadbalance.conf -Dflume.root.logger=info,console\n\n  ~~~\n\n#### 静态拦截器使用\n\n* 1、案场景\n\n~~~\nA、B两台日志服务机器实时生产日志主要类型为access.log、nginx.log、web.log \n现在需要把A、B 机器中的access.log、nginx.log、web.log 采集汇总到C机器上然后统一收集到hdfs中。\n但是在hdfs中要求的目录为：\n/source/logs/access/20180101/**\n/source/logs/nginx/20180101/**\n/source/logs/web/20180101/**\n~~~\n\n* 2场景分析\n\n![flume采集不同的日志数据](http://kflys.gitee.io/upic/2020/04/02/uPic/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6/assets/flume采集不同的日志数据.png)\n\n* 3数据流程处理分析\n\n![](http://kflys.gitee.io/upic/2020/04/02/uPic/Flume%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6/assets/flume采集不同的日志数据流程分析.png)\n\n* ==在node01与node02服务器开发flume的配置文件==vim exec_source_avro_sink.conf\n\n  ~~~properties\n  # Name the components on this agent\n    #定义三个source\n    a1.sources = r1 r2 r3\n    a1.sinks = k1\n    a1.channels = c1\n  \n    # Describe/configure the source\n    a1.sources.r1.type = exec\n    a1.sources.r1.command = tail -F /kfly/install/flumeData/access.log\n    #指定source r1 使用拦截器i1\n    a1.sources.r1.interceptors = i1\n    #拦截器类型static静态\n    a1.sources.r1.interceptors.i1.type = static\n    ## static拦截器的功能就是往采集到的数据的header中插入自己定义的key-value对\n    # 自己进行设置,我们这里的key和value相当于键值对,k=type v=access\n    a1.sources.r1.interceptors.i1.key = type\n    a1.sources.r1.interceptors.i1.value = access\n  \n    a1.sources.r2.type = exec\n    a1.sources.r2.command = tail -F /kfly/install/flumeData/nginx.log\n    #指定source r2 使用拦截器i2\n    a1.sources.r2.interceptors = i2\n    #拦截器类型static静态\n    a1.sources.r2.interceptors.i2.type = static\n    # 自己进行设置\n    a1.sources.r2.interceptors.i2.key = type\n    a1.sources.r2.interceptors.i2.value = nginx\n  \n    a1.sources.r3.type = exec\n    a1.sources.r3.command = tail -F /kfly/install/flumeData/web.log\n    #指定source r3 使用拦截器i3\n    a1.sources.r3.interceptors = i3\n    #拦截器类型static静态\n    a1.sources.r3.interceptors.i3.type = static\n    # 自己进行设置\n    a1.sources.r3.interceptors.i3.key = type\n    a1.sources.r3.interceptors.i3.value = web\n  \n    # Use a channel which buffers events in memory\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 20000\n    a1.channels.c1.transactionCapacity = 10000\n  \n    # Describe the sink\n    a1.sinks.k1.type = avro\n    a1.sinks.k1.hostname = node03\n    a1.sinks.k1.port = 41414\n    a1.sinks.k1.channel = c1\n  \n    # Bind the source and sink to the channel\n    a1.sources.r1.channels = c1\n    a1.sources.r2.channels = c1\n    a1.sources.r3.channels = c1\n  \n  \n  ~~~\n\n\n  ~~~\n  \n* ==在node03服务器上开发flume配置文件==vim avro_source_hdfs_sink.conf\n\n  ~~~properties\na1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  #定义source\n  a1.sources.r1.type = avro\n  a1.sources.r1.bind = node03\n  a1.sources.r1.port =41414\n  \n  #定义channels\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 20000\n  a1.channels.c1.transactionCapacity = 1000\n  \n  #定义sink\n  a1.sinks.k1.type = hdfs\n  # 此处的%{type} 这里是取我们在node01和node02定义的type的值,也就是value\n  a1.sinks.k1.hdfs.path=hdfs://node01:8020/source/logs/%{type}/%Y%m%d\n  a1.sinks.k1.hdfs.filePrefix =events-\n  a1.sinks.k1.hdfs.fileType = DataStream\n  a1.sinks.k1.hdfs.writeFormat = Text\n  #时间类型\n  a1.sinks.k1.hdfs.useLocalTimeStamp = true\n  #生成的文件不按条数生成\n  a1.sinks.k1.hdfs.rollCount = 0\n  #生成的文件按时间生成\n  a1.sinks.k1.hdfs.rollInterval = 30\n  #生成的文件按大小生成\n  a1.sinks.k1.hdfs.rollSize  = 10485760\n  #批量写入hdfs的个数\n  a1.sinks.k1.hdfs.batchSize = 1000\n  #flume操作hdfs的线程数（包括新建，写入等）\n  a1.sinks.k1.hdfs.threadsPoolSize=10\n  #操作hdfs超时时间\n  a1.sinks.k1.hdfs.callTimeout=30000\n  \n  #组装source、channel、sink\n  a1.sources.r1.channels = c1\n  a1.sinks.k1.channel = c1\n  ~~~\n\n#### 自定义拦截器\n\n~~~xml\n<dependency>\n  <groupId>org.apache.flume</groupId>\n  <artifactId>flume-ng-core</artifactId>\n  <version>1.6.0-cdh5.14.2</version>\n</dependency>\n~~~\n\n~~~java\n// flume自定义拦截器对数据加密\npublic class CustomInterceptor implements Interceptor {\n    // 指定需要加密的字段下标\n    private final String encrypted_field_index;\n    // 指定不需要对应列的下标\n    private final String out_index;\n    // 提供构建方法，后期可以接受配置文件中的参数\n    public CustomInterceptor(String encrypted_field_index, String out_index) {\n    }\n\t\t// 定义拦截器规则\n    public Event intercept(Event event) {\n      String line = new String(event.getBody(), Charsets.UTF_8);\n      // TODO 处理逻辑      \n      event.setBody(newLine.getBytes(Charsets.UTF_8));\n      return event;\n    }\n\n    @Override\n    public List<Event> intercept(List<Event> events) {\n        // 遍历调用intercept\n    }\n    public static class MyBuilder  implements CustomInterceptor.Builder {\n        // 指定需要加密的字段下标\n        private String encrypted_field_index;\n\t\t\t\t// 指定不需要对应列的下标\n        private String out_index;\n        @Override\n        public CustomInterceptor build() {\n            return new CustomInterceptor(encrypted_field_index, out_index);\n        }\n\n        @Override\n        public void configure(Context context) {\n          // 后去flume配置文件中指定的内容          \n            this.encrypted_field_index = context.getString(\"encrypted_field_index\", \"\");\n            this.out_index = context.getString(\"out_index\", \"\");\n        }\n    }\n}\n\n~~~\n\n* 5打成jar包后放到flume安装目录下的lib中\n\n* 6、创建配置文件 flume-interceptor-hdfs.conf\n\n~~~properties\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n#配置source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /kfly/install/flumeData/user.txt\na1.sources.r1.channels = c1\na1.sources.r1.interceptors =i1\n# 自定义拦截器路径\na1.sources.r1.interceptors.i1.type =bigdata.flume.CustomInterceptor$MyBuilder\n#  传入自定义拦截器的参数\na1.sources.r1.interceptors.i1.encrypted_field_index=0\na1.sources.r1.interceptors.i1.out_index=3\n\n#配置channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n#配置sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.channel = c1\na1.sinks.k1.hdfs.path = hdfs://node01:8020/interceptor/files/%Y-%m-%d/%H%M\na1.sinks.k1.hdfs.filePrefix = events-\n# 时间舍弃\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = minute\n# 限定生成文件时机\na1.sinks.k1.hdfs.rollInterval = 5\na1.sinks.k1.hdfs.rollSize = 50\na1.sinks.k1.hdfs.rollCount = 10\na1.sinks.k1.hdfs.batchSize = 100\na1.sinks.k1.hdfs.useLocalTimeStamp = true\n#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\na1.sinks.k1.hdfs.fileType = DataStream\n~~~\n\n### 自定义Source\n\n~~~java\npublic class CustomSource extends AbstractSource implements Configurable, PollableSource {\n    public Status process() throws EventDeliveryException {\n        try {\n          \t// 获取 events ，传入下方\n            this.getChannelProcessor().processEventBatch(events);\n            return Status.READY;\n        } catch (InterruptedException e) {\n            return Status.BACKOFF;\n        }\n    }\n}\n\n~~~\n\n* vim mysqlsource.conf\n\n~~~properties\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = bigdata.flume.source.CustomSource\n# 老师的是node01,同学们改成自己的节点 一定要注意\na1.sources.r1.connection.url = jdbc:mysql://node03:3306/mysqlsource\na1.sources.r1.connection.user = root\na1.sources.r1.connection.password = 123456\na1.sources.r1.table = student\na1.sources.r1.columns.to.select = *\na1.sources.r1.start.from=0\na1.sources.r1.run.query.delay=3000\n\n# Describe the channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n~~~\n\n### 自定义Sink\n\n\n```java\npublic class MysqlSink extends AbstractSink implements Configurable {\n\n    @Override\n    public Status process(){\n        Status status = null;\n        // Start transaction\n        Channel ch = getChannel();\n        Transaction txn = ch.getTransaction();\n        txn.begin();\n        try\n        {\n            Event event = ch.take();\n\t\t\t\t\t\t// event取出数据，存入mysql\n            txn.commit();\n        } catch (Throwable t){\n            txn.rollback();\n            t.getCause().printStackTrace();\n            status = Status.BACKOFF;\n        } finally{\n            txn.close();\n        }\n        return status;\n    }\n}\n\n```\n\n* vim mysqlsink.conf\n\n~~~properties\n    a1.sources = r1\n    a1.sinks = k1\n    a1.channels = c1\n    \n    #配置source\n    a1.sources.r1.type = exec\n    a1.sources.r1.command = tail -F /kfly/install/flumeData/data.log\n    a1.sources.r1.channels = c1\n    \n    #配置channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    #配置sink\n    a1.sinks.k1.channel = c1\n    a1.sinks.k1.type = bigdata.flume.sink.CustomSink\n    a1.sinks.k1.mysqlurl=jdbc:mysql://node03:3306/mysqlsource?useSSL=false\n    a1.sinks.k1.username=root\n    a1.sinks.k1.password=123456\n    a1.sinks.k1.tablename=flume2mysql\n~~~\n\n### 注意事项\n\n```\ncapacity：默认该通道中最大的可以存储的event数量\ntrasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量\n注意：capacity > trasactionCapacity\n\n```\n\n#### HDFS配置说明\n\n```shell\n#定义sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.path=hdfs://node01:8020/source/logs/%{type}/%Y%m%d\na1.sinks.k1.hdfs.filePrefix =events\na1.sinks.k1.hdfs.fileType = DataStream\na1.sinks.k1.hdfs.writeFormat = Text\n#时间类型\na1.sinks.k1.hdfs.useLocalTimeStamp = true\n#生成的文件不按条数生成\na1.sinks.k1.hdfs.rollCount = 0\n#生成的文件按时间生成\na1.sinks.k1.hdfs.rollInterval = 0\n#生成的文件按大小生成\na1.sinks.k1.hdfs.rollSize  = 10485760\n#批量写入hdfs的个数\na1.sinks.k1.hdfs.batchSize = 10000\n#flume操作hdfs的线程数（包括新建，写入等）\na1.sinks.k1.hdfs.threadsPoolSize=10\n#操作hdfs超时时间\na1.sinks.k1.hdfs.callTimeout=30000\n\n```\n\n| hdfs.round          | false  | Should the timestamp be rounded down (if true, affects all time based escape sequences except %t) |\n| ------------------- | ------ | ------------------------------------------------------------ |\n| **hdfs.roundValue** | 1      | Rounded down to the highest multiple of this (in the unit configured usinghdfs.roundUnit), less than current time. |\n| **hdfs.roundUnit**  | second | The unit of the round down value - second, minute or hour.   |\n\nØ round： 默认值：false 是否启用时间上的”舍弃”，这里的”舍弃”，类似于”四舍五入”\n\nØ roundValue：默认值：1  时间上进行“舍弃”的值；\n\nØ roundUnit： 默认值：seconds时间上进行”舍弃”的单位，包含：second,minute,hour\n\n```properties\n# 案例一：\na1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d/%H:%M/%S\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = minute\n# 当时间为2015-10-16 17:38:59时候，hdfs.path依然会被解析为：\n# /flume/events/2015-10-16/17:30/00\n# /flume/events/2015-10-16/17:40/00\n# /flume/events/2015-10-16/17:50/00\n# 因为设置的是舍弃10分钟内的时间，因此，该目录每10分钟新生成一个。\n\n# 案例二：\na1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d/%H:%M/%S\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = second\n# 现象：10秒为时间梯度生成对应的目录，目录下面包括很多小文件！！！\n# 格式如下：\n# /flume/events/2016-07-28/18:45/10\n# /flume/events/2016-07-28/18:45/20\n# /flume/events/2016-07-28/18:45/30\n# /flume/events/2016-07-28/18:45/40\n# /flume/events/2016-07-28/18:45/50\n# /flume/events/2016-07-28/18:46/10\n# /flume/events/2016-07-28/18:46/20\n# /flume/events/2016-07-28/18:46/30\n# /flume/events/2016-07-28/18:46/40\n# /flume/events/2016-07-28/18:46/50\n\n```\n\n#### 实现数据的断点续传\n\n- 其本质就是记录下每一次消费的位置，把消费信息的位置保存到文件中，后续程序挂掉了再重启的时候，可以接着上一次消费的数据位置继续拉取。\n\n- ==vim taildir.conf==\n\n```properties\na1.sources = s1\na1.channels = ch1\na1.sinks = hdfs-sink1\n\n#channel\na1.channels.ch1.type = memory\na1.channels.ch1.capacity=10000\na1.channels.ch1.transactionCapacity=500\n\n#source\na1.sources.s1.channels = ch1\n#监控一个目录下的多个文件新增的内容\na1.sources.s1.type = taildir\n#通过 json 格式存下每个文件消费的偏移量，避免从头消费\na1.sources.s1.positionFile = /k/install/flumeData/index/taildir_position.json\na1.sources.s1.filegroups = f1 f2 f3 \na1.sources.s1.filegroups.f1 = /home/hadoop/taillogs/access.log\na1.sources.s1.filegroups.f2 = /home/hadoop/taillogs/nginx.log\na1.sources.s1.filegroups.f3 = /home/hadoop/taillogs/web.log\na1.sources.s1.headers.f1.headerKey = access\na1.sources.s1.headers.f2.headerKey = nginx\na1.sources.s1.headers.f3.headerKey = web\na1.sources.s1.fileHeader  = true\n\n##sink\na1.sinks.hdfs-sink1.channel = ch1\na1.sinks.hdfs-sink1.type = hdfs\na1.sinks.hdfs-sink1.hdfs.path =hdfs://node01:8020/demo/data/%{headerKey}\na1.sinks.hdfs-sink1.hdfs.filePrefix = event_data\na1.sinks.hdfs-sink1.hdfs.fileSuffix = .log\na1.sinks.hdfs-sink1.hdfs.rollSize = 1048576\na1.sinks.hdfs-sink1.hdfs.rollInterval =20\na1.sinks.hdfs-sink1.hdfs.rollCount = 10\na1.sinks.hdfs-sink1.hdfs.batchSize = 1500\na1.sinks.hdfs-sink1.hdfs.round = true\na1.sinks.hdfs-sink1.hdfs.roundUnit = minute\na1.sinks.hdfs-sink1.hdfs.threadsPoolSize = 25\na1.sinks.hdfs-sink1.hdfs.fileType =DataStream\na1.sinks.hdfs-sink1.hdfs.writeFormat = Text\na1.sinks.hdfs-sink1.hdfs.callTimeout = 60000\n\n```\n\n\n\n~~~json\n# 运行后生成的 taildir_position.json文件信息如下：\n[\n{\"inode\":102626782,\"pos\":123,\"file\":\"/home/hadoop/taillogs/access.log\"},{\"inode\":102626785,\"pos\":123,\"file\":\"/home/hadoop/taillogs/web.log\"},{\"inode\":102626786,\"pos\":123,\"file\":\"/home/hadoop/taillogs/nginx.log\"}\n]\n\n#这里inode就是标记文件的，文件名称改变，这个iNode不会变，pos记录偏移量，file就是绝对路径\n\n~~~\n\n#### 6header参数配置讲解\n\n- ==vim test-header.conf==\t\n\n```properties\n#配置信息test-header.conf\na1.channels=c1\na1.sources=r1\na1.sinks=k1\n\n#source\na1.sources.r1.channels=c1\na1.sources.r1.type= spooldir\na1.sources.r1.spoolDir= /home/hadoop/test\na1.sources.r1.batchSize= 100\na1.sources.r1.inputCharset= UTF-8\n#是否添加一个key存储目录下文件的绝对路径\na1.sources.r1.fileHeader= true\n#指定存储目录下文件的绝对路径的key\na1.sources.r1.fileHeaderKey= mm\n#是否添加一个key存储目录下的文件名称\na1.sources.r1.basenameHeader= true\n#指定存储目录下文件的名称的key\na1.sources.r1.basenameHeaderKey= nn\n\n#channel\na1.channels.c1.type= memory\na1.channels.c1.capacity=10000\na1.channels.c1.transactionCapacity=500\n\n\n#sink\na1.sinks.k1.type=logger\na1.sinks.k1.channel=c1\n\n```\n\n-  tail / tail -f /tail -F区别\n\n```markdown\n- tail -f\n\t等同于--follow=descriptor，根据文件描述符进行追踪，当文件改名或被删除，追踪停止\n- tail -F\n\t等同于--follow=name --retry，根据文件名进行追踪，并保持重试，即该文件被删除或改名后，如果再次创建相同的文件名，会继续追踪\n- tailf\n\t等同于tail -f -n 10（貌似tail -f或-F默认也是打印最后10行，然后追踪文件），与tail -f不同的是，如果文件不增长，它不会去访问磁盘文件，所以tailf特别适合那些便携机上跟踪日志文件，因为它减少了磁盘访问，\n```\n\n","tags":["flume"]},{"title":"HBase + Phoenix实现二级索引","url":"/2019/05/17/it/hbase/HBase + Phoenix实现二级索引/","content":"\n## HBase + Phoenix实现二级索引\n\n### 需求背景\n\n- 对于HBase而言，如果想精确地定位到某行记录，唯一的办法是通过rowkey来查询。如果不通过rowkey来查找数据，就必须逐行地比较每一列的值，即全表扫瞄。\n- 对于较大的表，全表扫描的代价是不可接受的。但是，很多情况下，需要从多个角度查询数据。例如，在定位某个人的时候，可以通过姓名、身份证号、学籍号等不同的角度来查询，要想把这么多角度的数据都放到rowkey中几乎不可能（业务的灵活性不允许，对rowkey长度的要求也不允许）。所以需要secondary index（二级索引）来完成这件事。secondary index的原理很简单，但是如果自己维护的话则会麻烦一些。现在，Phoenix已经提供了对HBase secondary index的支持。\n\n#### Global Indexing\n\n- Global indexing，全局索引，适用于读多写少的业务场景。\n- 使用Global indexing在写数据的时候开销很大，因为所有对数据表的更新操作（DELETE, UPSERT VALUES and UPSERT SELECT），都会引起索引表的更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。\n- 在读数据的时候Phoenix会选择索引表来降低查询消耗的时间。在默认情况下如果想查询的字段不是索引字段的话索引表不会被使用，也就是说不会带来查询速度的提升。\n\n#### Local Indexing\n\n- Local indexing，本地索引，适用于写操作频繁以及空间受限制的场景。\n- 与Global indexing一样，Phoenix会自动判定在进行查询的时候是否使用索引。使用Local indexing时，索引数据和数据表的数据存放在相同的服务器中，这样避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。使用Local indexing的时候即使查询的字段不是索引字段索引表也会被使用，这会带来查询速度的提升，这点跟Global indexing不同。对于Local Indexing，一个数据表的所有索引数据都存储在一个单一的独立的可共享的表中\n\n#### immutable index\n\n- immutable index，不可变索引，适用于数据只增加不更新并且按照时间先后顺序存储（time-series data）的场景，如保存日志数据或者事件数据等。\n- 不可变索引的存储方式是write one，append only。当在Phoenix使用create table语句时指定IMMUTABLE_ROWS = true表示该表上创建的索引将被设置为不可变索引。Phoenix默认情况下如果在create table时不指定IMMUTABLE_ROW = true时，表示该表为mutable。不可变索引分为Global immutable index和Local immutable index两种。\n\n#### mutable index\n\n- mutable index，可变索引，适用于数据有增删改的场景。\n- Phoenix默认情况创建的索引都是可变索引，除非在create table的时候显式地指定IMMUTABLE_ROWS = true。可变索引同样分为Global immutable index和Local immutable index两种。\n\n#### 修改hbase-site.xml      \n\n~~~xml\n<!-- 添加配置 -->\n  <property>\n  \t<name>hbase.regionserver.wal.codec</name>\n  \t<value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>\n  </property>\n  <property>\n     <name>hbase.region.server.rpc.scheduler.factory.class</name>\n     <value>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory</value>\n  </property>\n  <property>\n  \t\t<name>hbase.rpc.controllerfactory.class</name>\n  \t\t<value>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</value>\n  </property>\n<!--完成上述修改后重启hbase集群使配置生效。-->\n~~~\n\n### 示例\n\n```sql\n-- Phonix中创建表\ncreate  table user (\n  \"session_id\" varchar(100) not null primary key, \n  \"f\".\"cookie_id\" varchar(100), \n  \"f\".\"visit_time\" varchar(100), \n  \"f\".\"user_id\" varchar(100), \n  \"f\".\"age\" varchar(100), \n  \"f\".\"sex\" varchar(100), \n  \"f\".\"visit_url\" varchar(100), \n  \"f\".\"visit_os\" varchar(100), \n  \"f\".\"browser_name\" varchar(100),\n  \"f\".\"visit_ip\" varchar(100), \n  \"f\".\"province\" varchar(100),\n  \"f\".\"city\" varchar(100),\n  \"f\".\"page_id\" varchar(100), \n  \"f\".\"goods_id\" varchar(100),\n  \"f\".\"shop_id\" varchar(100)\n) column_encoded_bytes=0;\n\n-- 导入数据\n-- cd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/\n-- bin/psql.py -t USER node01:2181 /kfly/install/phoenixsql/user.csv\n```\n\n#### Global Indexing\n\n```sql\n-- user表f列镞的cookie_id建立全局索引 ，create local index 是创建本地索引\ncreate index USER_COOKIE_ID_INDEX on USER (\"f\".\"cookie_id\"); \n-- 正确使用索引查询，select cookie_id 和 where cookie_id = 都是索引字段\nselect \"cookie_id\" from user where \"cookie_id\" = '99738fd1-2084-44e9';\n-- 下面不会使用到索引，虽然cookie_id是索引字段，但是age不是。\nselect \"cookie_id\",\"age\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n```\n\n#### Local Indexing\n\n```sql\ncreate local index USER_USER_ID_INDEX on USER (\"f\".\"user_id\");\n```\n\n#### 确保查询使用Index\n\n##### 创建 convered index\n\n~~~sql\n-- 如果在某次查询中，查询项或者查询条件中包含除被索引列之外的列（主键MY_PK除外）。默认情况下，该查询会触发full table scan（全表扫描），但是使用covered index则可以避免全表扫描。 \n-- 创建包含某个字段的覆盖索引,创建方式如下：\ncreate index USER_COOKIE_ID_AGE_INDEX on USER (\"f\".\"cookie_id\") include(\"f\".\"age\");\n-- 查看当前所有表会发现多一张USER_COOKIE_ID_AGE_INDEX索引表，查询该表数据。\nselect \"age\" from user where  \"cookie_id\"='99738fd1-2084-44e9';\nselect \"age\",\"sex\" from user where  \"cookie_id\"='99738fd1-2084-44e9';\n~~~\n\n##### 查询中提示使用index\n\n~~~sql\n-- 在select和column_name之间加上/*+ Index(<表名> <index名>)*/，通过这种方式强制使用索引。\nselect /*+ index(user,USER_COOKIE_ID_AGE_INDEX) */ \"age\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n~~~\n\n##### 使用本地索引\n\n#### 索引重建\n\n~~~sql\n-- Phoenix的索引重建是把索引表清空后重新装配数据。\nalter index USER_COOKIE_ID_INDEX on user rebuild;\n~~~\n\n#### 删除索引\n\n~~~sql\n-- 删除某个表的某张索引：drop index 索引名称 on 表名\ndrop index USER_COOKIE_ID_INDEX on user;\n\n-- 如果表中的一个索引列被删除，则索引也将被自动删除，如果删除的是\n-- 覆盖索引上的列，则此列将从覆盖索引中被自动删除。\n~~~\n\n### 索引性能调优\n\n- ​\thbase-site.xml\n\n\n~~~\n1. index.builder.threads.max \n创建索引时，使用的最大线程数。 \n默认值: 10。\n\n2. index.builder.threads.keepalivetime \n创建索引的创建线程池中线程的存活时间，单位：秒。 \n默认值: 60\n\n3. index.writer.threads.max \n写索引表数据的写线程池的最大线程数。 \n更新索引表可以用的最大线程数，也就是同时可以更新多少张索引表，数量最好和索引表的数量一致。 \n默认值: 10\n\n4. index.writer.threads.keepalivetime \n索引写线程池中，线程的存活时间，单位：秒。\n默认值：60\n \n\n5. hbase.htable.threads.max \n每一张索引表可用于写的线程数。 \n默认值: 2,147,483,647\n\n6. hbase.htable.threads.keepalivetime \n索引表线程池中线程的存活时间，单位：秒。 \n默认值: 60\n\n7. index.tablefactory.cache.size \n允许缓存的索引表的数量。 \n增加此值，可以在写索引表时不用每次都去重复的创建htable，这个值越大，内存消耗越多。 \n默认值: 10\n\n8. org.apache.phoenix.regionserver.index.handler.count \n处理全局索引写请求时，可以使用的线程数。 \n默认值: 30\n~~~\n\n### Java Phoenix\n\n- Spring datasource config / Druid\n\n```yaml\n  datasource.driver-class-name: org.apache.phoenix.jdbc.PhoenixDriver\n  datasource.url: jdbc:phoenix:node01:2181:/HBase\n  datasource.type: com.alibaba.druid.pool.DruidDataSource\n```\n\n","tags":["phoenix","hbase二级索引"]},{"title":"Phoenix环境部署安装","url":"/2019/05/15/it/Phoenix环境部署安装/","content":"\n# Phoenix安装部署\n\n- 需要先安装好HBase集群\n- phoenix只是一个工具，只需要在一台机器上安装就可以了，这里我们选择node02服务器来进行安装一台即可\n\n# 1、下载安装包\n\n- 从对应的地址下载：http://archive.apache.org/dist/phoenix/\n- 注意版本兼容问题；这里我们使用的是\n  - apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz\n\n# 2、上传解压\n\n- 将安装包上传到node02服务器的/kfly/soft路径下，然后进行解压\n\n```shell\ncd /kfly/soft/\ntar -zxf apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz -C /kfly/install/\n```\n\n# 3、修改配置\n\n## 3.1 拷贝jar包\n\n- 将phoenix目录下的==phoenix-4.8.2-HBase-1.2-server.jar==、==phoenix-core-4.8.2-HBase-1.2.jar==拷贝到==各个 HBase节点的lib目录==下。\n\n- node02执行以下命令\n\n  ```shell\n  cd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin\n  \n  scp phoenix-4.14.0-cdh5.14.2-server.jar phoenix-core-4.14.0-cdh5.14.2.jar node01:/kfly/install/hbase-1.2.0-cdh5.14.2/lib/ \n  \n  scp phoenix-4.14.0-cdh5.14.2-server.jar phoenix-core-4.14.0-cdh5.14.2.jar node02:/kfly/install/hbase-1.2.0-cdh5.14.2/lib/ \n  \n  scp phoenix-4.14.0-cdh5.14.2-server.jar phoenix-core-4.14.0-cdh5.14.2.jar node03:/kfly/install/hbase-1.2.0-cdh5.14.2/lib/ \n  ```\n\n## 3.2 拷贝配置文件\n\n- 将HBase的配置文件==hbase-site.xml==、 hadoop下的配置文件==core-site.xml== 、==hdfs-site.xml==放到phoenix/bin/下，替换phoenix原来的配置文件。\n\n  node02执行以下命令，进行拷贝配置文件\n\n  ```shell\n  cp /kfly/install/hadoop-2.6.0/etc/hadoop/core-site.xml  /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/\n  \n  scp /kfly/install/hadoop-2.6.0/etc/hadoop/hdfs-site.xml /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/\n  \n  scp /kfly/install/hbase-1.2.0-cdh5.14.2/conf/hbase-site.xml /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/\n  ```\n\n- 重启hbase集群，使Phoenix的jar包生效。\n\n  node01执行以下命令来重启hbase的集群\n\n  ```\n  cd /kfly/install/hbase-1.2.0-cdh5.14.2/\n  bin/stop-hbase.sh \n  bin/start-hbase.sh \n  ```\n\n\n\n# 4、验证是否成功\n\n- 在phoenix/bin下输入命令, 进入到命令行，接下来就可以操作了\n\n- node02执行以下命令，进入phoenix客户端\n\n  ~~~shell\n  cd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/\n  bin/sqlline.py node01:2181\n  ~~~\n\n# 5、Phoenix使用\n\n## 5.1 批处理方式\n\n* node02执行以下命令创建user_phoenix.sql文件\n\n~~~\nmkdir -p /kfly/doc/phoenixsql\ncd /kfly/install/phoenixsql/\nvi user_phoenix.sql\n~~~\n\n​\t内容如下\n\n```sql\ncreate table if not exists user_phoenix (state varchar(10) NOT NULL,  city varchar(20) NOT NULL,  population BIGINT  CONSTRAINT my_pk PRIMARY KEY (state, city));\n```\n\n* node02执行以下命令，创建user_phoenix.csv数据文件\n\n~~~\ncd /kfly/install/phoenixsql/\nvi user_phoenix.csv\n~~~\n\n​\t内容如下\n\n```\nNY,New York,8143197\nCA,Los Angeles,3844829\nIL,Chicago,2842518\nTX,Houston,2016582\nPA,Philadelphia,1463281\nAZ,Phoenix,1461575\nTX,San Antonio,1256509\nCA,San Diego,1255540\nTX,Dallas,1213825\nCA,San Jose,912332\n```\n\n* 创建user_phoenix_query.sql文件\n\n~~~\ncd /kfly/doc/phoenixsql\nvi user_phoenix_query.sql\n~~~\n\n​\t\t内容如下\n\n```sql\nselect state as \"userState\",count(city) as \"City Count\",sum(population) as \"Population Sum\" FROM user_phoenix GROUP BY state; \n```\n\n* 执行sql语句\n\n~~~shell\ncd /kfly/doc/phoenixsql\n\n/kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/psql.py  node01:2181 user_phoenix.sql user_phoenix.csv user_phoenix_query.sql\n~~~\n\n\n\n## 5.2 命令行方式\n\n* 执行命令\n\n~~~shell\ncd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/\nbin/sqlline.py node01:2181\n~~~\n\n* 退出命令行方式\n\n  phoenix的命令都需要一个==感叹号==\n\n~~~\n!quit\n~~~\n\n- 查看phoenix的帮助文档，显示所有命令；用!help\n\n  ```\n  0: jdbc:phoenix:node01:2181> !help\n  !all                Execute the specified SQL against all the current\n                    connections\n  !autocommit         Set autocommit mode on or off\n  !batch              Start or execute a batch of statements\n  !brief              Set verbose mode off\n  !call               Execute a callable statement\n  !close              Close the current connection to the database\n  !closeall           Close all current open connections\n  !columns            List all the columns for the specified table\n  !commit             Commit the current transaction (if autocommit is off)\n  !connect            Open a new connection to the database.\n  !dbinfo             Give metadata information about the database\n  !describe           Describe a table\n  !dropall            Drop all tables in the current database\n  !exportedkeys       List all the exported keys for the specified table\n  !go                 Select the current connection\n  !help               Print a summary of command usage\n  !history            Display the command history\n  !importedkeys       List all the imported keys for the specified table\n  !indexes            List all the indexes for the specified table\n  !isolation          Set the transaction isolation for this connection\n  !list               List the current connections\n  !manual             Display the SQLLine manual\n  !metadata           Obtain metadata information\n  !nativesql          Show the native SQL for the specified statement\n  !outputformat       Set the output format for displaying results\n                    (table,vertical,csv,tsv,xmlattrs,xmlelements)\n  !primarykeys        List all the primary keys for the specified table\n  !procedures         List all the procedures\n  !properties         Connect to the database specified in the properties file(s)\n  !quit               Exits the program\n  !reconnect          Reconnect to the database\n  !record             Record all output to the specified file\n  !rehash             Fetch table and column names for command completion\n  !rollback           Roll back the current transaction (if autocommit is off)\n  !run                Run a script from the specified file\n  !save               Save the current variabes and aliases\n  !scan               Scan for installed JDBC drivers\n  !script             Start saving a script to a file\n  !set                Set a sqlline variable\n  ```\n\n  ```\n* 1、建立employee的映射表\n\n  进入hbase客户端，创建一个普通表employee，并且有两个列族 company 和family\n\n  ~~~\n  cd /kfly/install/hbase-1.2.0-cdh5.14.2/\n  bin/hbase shell\n  hbase(main):001:0> create 'employee','company','family'\n  ~~~\n\n* 2、添加数据\n\n  ~~~\n  put 'employee','row1','company:name','ted'\n  put 'employee','row1','company:position','worker'\n  put 'employee','row1','family:tel','13600912345'\n  put 'employee','row2','company:name','michael'\n  put 'employee','row2','company:position','manager'\n  put 'employee','row2','family:tel','1894225698'\n  ~~~\n\n* 3、建立hbase到phoenix的映射表\n\n  node02进入到phoenix的客户端，然后创建映射表\n\n  ~~~\n  cd kflyb/install/apache-phoenix-4.14.0-cdh5.14.2-bin\n  bin/sqlline.py node01:2181\n  ~~~\n\n  执行语句\n\n  ```sql\n  CREATE TABLE IF NOT EXISTS \"employee\" (\"no\" VARCHAR(10) NOT NULL PRIMARY KEY, \"company\".\"name\" VARCHAR(30),\"company\".\"position\" VARCHAR(20), \"family\".\"tel\" VARCHAR(20), \"family\".\"age\" INTEGER) column_encoded_bytes=0;\n\n  ```\n\n  > 说明\n  >\n  > 在建立映射表之前要说明的是，Phoenix是==大小写敏感==的，并且所有命令都是大写，如果你建的表名没有用双引号括起来，那么无论你输入的是大写还是小写，建立出来的表名都是大写的，如果你需要建立出同时包含大写和小写的表名和字段名，请把表名或者字段名用双引号括起来。 \n\n* 4、查询映射表数据\n\n~~~\n0: jdbc:phoenix:node1:2181> select * from \"employee\";\n+-------+----------+-----------+--------------+-------+\n|  no   |   name   | position  |     tel      |  age  |\n+-------+----------+-----------+--------------+-------+\n| row1  | ted      | worker    | 13600912345  | null  |\n| row2  | michael  | manager   | 1894225698   | null  |\n+-------+----------+-----------+--------------+-------+\n\n0: jdbc:phoenix:node01:2181> select * from \"employee\" where \"tel\" = '13600912345';\n+-------+-------+-----------+--------------+-------+\n|  no   | name  | position  |     tel      |  age  |\n+-------+-------+-----------+--------------+-------+\n| row1  | ted   | worker    | 13600912345  | null  |\n+-------+-------+-----------+--------------+-------+\n\n\n~~~\n\n## 5.3 GUI方式\n\n- 通过dbeaver来连接phoenix\n- [点击下载]( https://dbeaver.io/files/6.2.4/dbeaver-ce-6.2.4-macos.dmg)\n\n### 5.3.1 准备两个文件\n\n- 我们通过dbeaver来连接phoenix需要两个文件\n\n  - 第一个文件是phoenix-4.14.0-cdh5.14.2-client.jar\n  - 第二个文件是hbase-site.xml\n\n- 进入到phoenix的安装目录，获取第一个文件\n\n  node02执行以下命令，进入到以下路径，获取第一个文件\n\n  找到 phoenix-4.14.0-cdh5.14.2-client.jar  这个jar包，并将其下载下来备用\n\n```shell\ncd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin\n\n```\n\n- 然后进入到node02服务器的hbase的安装配置文件路径，获取hbase-site.xml这个文件\n\n  找到hbase-site.xml，将其下载下来备用\n\n```shell\ncd /kfly/install/hbase-1.2.0-cdh5.14.2/conf/\n\n```\n\n\n\n### 5.3.2 更新jar包\n\n- 将hbase-site.xml放到phoenix-4.14.0-cdh5.14.2-client.jar这个jar包里面去\n\n- 我们在第一步找到了hbase-site.xml和phoenix-4.14.0-cdh5.14.2-client.jar 这两个文件之后，我们需要使用解压缩工具，将phoenix-4.14.0-cdh5.14.2-client.jar 这个jar包解压开，然后将hbase-site.xml放入到phoenix-4.14.0-cdh5.14.2-client.jar 这个jar包里面去\n\n![1571282342354](/Users/dingchuangshi/Downloads/20191115-HBase第四次课/phoenix安装部署/assets/1571282342354.png)\n\n### 5.3.3 通过dbeaver连接phoenix\n\n- 注意：如果连接不上，可能不是操作配置的问题，有可能是因为dbeaver软件的问题，将dbeaver软件重启几次试试看\n\n### 5.4.4 创建数据库表，并实现sql进行操作\n\n- 直接在phoenix当中通过sql语句的方式来创建表并\n\n```sql\nCREATE TABLE IF NOT EXISTS US_POPULATION (\n      state CHAR(2) NOT NULL,\n      city VARCHAR NOT NULL,\n      population BIGINT\n      CONSTRAINT my_pk PRIMARY KEY (state, city));\n\nUPSERT INTO US_POPULATION (state, city, population) values ('NY','New York',8143197);\nUPSERT INTO US_POPULATION (state, city, population) values ('CA','Los Angeles',3844829);\n\nSELECT * FROM US_POPULATION WHERE population > 8000000;\n```\n\n## 5.4 JDBC调用方式\n\n* 创建maven工程并导入jar包\n\n~~~xml\n<dependencies>\n    <dependency>\n        <groupId>org.apache.phoenix</groupId>\n        <artifactId>phoenix-core</artifactId>\n        <version>4.14.0-cdh5.14.2</version>\n    </dependency>\n    <dependency>\n        <groupId>junit</groupId>\n        <artifactId>junit</artifactId>\n        <version>4.12</version>\n    </dependency>\n    <dependency>\n        <groupId>org.testng</groupId>\n        <artifactId>testng</artifactId>\n        <version>6.14.3</version>\n    </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <!-- 限制jdk版本插件 -->\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                    <encoding>UTF-8</encoding>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n~~~\n\n* 代码开发\n\n~~~java\nimport org.testng.annotations.BeforeTest;\nimport org.testng.annotations.Test;\nimport java.sql.*;\npublic class PhoenixSearch {\n    private Connection connection;\n    private Statement statement;\n    private ResultSet rs;\n    @BeforeTest\n    public void init() throws SQLException {\n        //定义phoenix的连接url地址\n        String url=\"jdbc:phoenix:node01:2181\";\n        connection = DriverManager.getConnection(url);\n        //构建Statement对象\n        statement = connection.createStatement();\n    }\n    @Test\n    public void queryTable() throws SQLException {\n        //定义查询的sql语句，注意大小写\n        String sql=\"select * from US_POPULATION\";\n        //执行sql语句\n        try {\n            rs=statement.executeQuery(sql);\n            while(rs.next()){\n                System.out.println(\"state:\"+rs.getString(\"state\"));\n                System.out.println(\"city:\"+rs.getString(\"city\"));\n                System.out.println(\"population:\"+rs.getInt(\"population\"));\n                System.out.println(\"-------------------------\");\n            }\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }finally {\n            if(connection!=null){\n                connection.close();\n            }\n        }\n    }\n}\n~~~\n\n","tags":["phoenix","环境搭建"]},{"title":"azkaban工作流调度器","url":"/2019/04/26/it/azkaban工作流调度器/","content":"\n## Azkaban工作流调度器\n\n* 一个完整的数据分析系统通常都是由大量任务单元组成；\n  * shell脚本程序、java程序、mapreduce程序、hive脚本等\n* 各任务单元之间存在时间先后及前后依赖关系\n\n* 为了==很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行==\n\n* Azkaban是由Linkedin开源的一个==批量工作流任务调度器==。用于在一个工作流内以一个特定的顺序运行一组工作和流程。\n* Azkaban定义了一种==KV文件(properties)格式==来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。\n  * <https://azkaban.github.io/>\n\n<img src=\"http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/azkaban.github.png\" alt=\"azkaban.github\" style=\"zoom:50%;\" />\n\n* 功能特点\n  * 提供功能清晰、简单易用的web UI界面\n  * 提供job配置文件快速建立任务和任务之间的关系\n  * 提供模块化的可插拔机制，原生支持command、java、hive、hadoop\n  * 基于java开发，代码结构清晰，易于二次开发\n\n\n\n### 基础架构\n\n![azkaban](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/azkaban.png)\n\n\n\n* Azkaban由三部分构成\n\n  * 1、==Azkaban Web Server==\n    * 提供了Web UI，是azkaban的主要管理者，包括 project 的管理，认证，调度，对工作流执行过程的监控等。\n    \n    * 2、==Azkaban Executor Server==\n      * 负责具体的工作流和任务的调度提交\n  * ==Mysql==\n\n    * 用于保存项目、日志或者执行计划之类的信息\n\n### 运行模式\n\n* 1、solo server mode(单机模式）\n\n~~~\nH2\nweb server 和 executor server运行在一个进程里\n最简单的模式，数据库内置的H2数据库，管理服务器和执行服务器都在一个进程中运行，任务量不大项目可以采用此模式。\n~~~\n\n* 2、two server mode\n\n~~~\nweb server 和 executor server运行在不同的进程\n数据库为mysql，管理服务器和执行服务器在不同进程，这种模式下，管理服务器和执行服务器互不影响。\n\n~~~\n\n* 3、multiple executor mode\n\n~~~\nweb server 和 executor server运行在不同的进程，executor server有多个\n该模式下，执行服务器和管理服务器在不同主机上，且执行服务器可以有多个。\n~~~\n\n### 示例\n\n#### command类型单一job\n\n* 1、==创建job描述文件  以.job后缀结尾==\n  * 创建 command.job 文件\n\n~~~shell\n#command.job\ntype=command\ncommand=echo 'hello azkaban......'\n~~~\n\n* 2、将job资源文件打包成zip文件\n* 例如\n  * command.zip\n\n* 3、通过azkaban的web管理平台创建project并上传job压缩包\n\n![create_project](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/create_project.png)\n\n![upload_zip](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/upload_zip.png)\n\n* 3、点击运行\n\n![execute-flow](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/execute-flow.png)\n\n![execute](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/execute.png)\n\n\n\n* 4、运行完成\n\n![success](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/success.png)\n\n#### 多job工作流\n\n* 1、创建有依赖关系的多个job描述\n\n  * 第一个job：start1.job\n\n  ~~~shell\n  #start1.job\n  type=command\n  command= echo 'start1...start1...'\n  ~~~\n\n  * 第二个job：start2.job    它依赖start1.job\n\n  ~~~shell\n  #start2.job\n  type=command\n  dependencies=start1\n  command= echo 'start2...start2...'\n  ~~~\n\n* 2、将job资源文件打包成zip文件\n\n  * start12.zip\n\n  ![start12job](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/start12job.png)\n\n* 3、创建工程，上传zip包，最后启动工作流\n\n![execute-start12](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/execute-start12.png)\n\n* ==补充==\n  * 如果一个job有多个依赖的job，可以使用逗号隔开\n\n~~~shell\n例如： \n#start1.job\ntype=command\ncommand= echo \"start1 job\"\n\n#start2.job\ntype=command\ncommand= echo \"start2 job\"\n\n#stop.job\ntype=command\ndenpendencies=start1,start2\ncommand=echo \"stop job\"\n\n注意：有多个依赖的job，用逗号隔开\n~~~\n\n\n\n#### HDFS操作任务\n\n* 1、创建job描述文件\n\n  * vim fs.job\n\n  ~~~shell\n  #fs.job\n  type=command\n  command=echo \"start execute\"\n  command.1=/kfly/install/hadoop-2.6.0-cdh5.14.2/bin/hdfs dfs -mkdir /azkaban\n  command.2=/kfly/install/hadoop-2.6.0-cdh5.14.2/bin/hdfs dfs -put /home/hadoop/source.txt /azkaban\n  ~~~\n\n* 2、将job资源文件打包成zip文件\n\n  * fs.zip\n\n* 3、创建工程，上传zip包，最后启动工作流\n\n![fs](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/fs.png)\n\n![fs-hdfs](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/fs-hdfs.png)\n\n#### MAPREDUCE任务\n\n* MR任务依然可以使用command的job类型来执行\n\n* 1、创建job描述文件，及mr程序jar包\n\n  * 示例中直接使用hadoop自带的example jar\n  * hdfs dfs -mkdir -p  /wordcount/in\n\n  ~~~shell\n  #mr.job\n  type=command\n  command=/kfly/install/hadoop-2.6.0/bin/hadoop  jar hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar wordcount /wordcount/in /wordcount/out\n  ~~~\n\n* 2、将job资源文件打包成zip文件\n\n  - mr.zip\n\n* 3、创建工程，上传zip包，最后启动工作流\n\n![mr](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/mr.png)\n\n#### HIVE脚本任务\n\n* 1、 创建job描述文件和hive脚本\n\n  * Hive脚本： test.sql\n\n  ~~~sql\n  use default;\n  create table if not exists test_azkaban(id int,name string,address string) row format delimited fields terminated by ',';\n  load data local inpath '/home/hadoop/azkaban/test.txt' into table test_azkaban;\n  create table if not exists countaddress as select address,count(*) as num from test_azkaban group by address ;\n  \n  insert overwrite local directory '/home/hadoop/azkaban/out' select * from countaddress; \n  ~~~\n\n  * 准备数据\n\n    * vim /home/hadoop/azkaban/test.txt\n\n    ~~~properties\n    1,zhangsan,shanghai\n    2,lisi,beijing\n    3,xiaoming,shanghai\n    4,xiaozhang,shanghai\n    5,xiaogang,beijing\n    ~~~\n\n* 2、创建job描述文件\n\n  * hive.job\n\n  ~~~shell\n  # hive.job\n  type=command\n  command=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive -f 'test.sql'\n  ~~~\n\n\n\n* 3、将job资源文件打包成zip文件\n  \n  * hive.zip\n  \n### 任务定时调度\n\n* 在启动工作流的时候，可以点击==Schedule==，实现定时调度一个工作流\n\n![schedule1](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/schedule1.png)\n\n\n\n![schedule2](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/schedule2.png)\n\n\n\n### webUI 传递参数\n\n* 可以通过webUI动态给job传递参数\n\n* 1、创建一个job的描述文件\n\n  * parameter.job\n\n    ~~~shell\n    #parameter.job\n    type=command\n    parameter=${param}\n    command= echo ${parameter}\n    ~~~\n\n  * 其中\n\n    * ==${param}== 表示解析页面传递的参数param的值，通过声明一个变量parameter去接受\n    * ==${parameter}==表示获取该parameter变量的值\n\n* 2、将job资源文件打包成zip文件\n\n  * parameter.zip\n\n* 3、创建工程，上传zip包，最后启动工作流，并且设置参数\n\n![1572232468448](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/1572232468448.png)\n\n* 4、运行完成后的结果\n\n  ![1572232525618](http://kflys.gitee.io/upic/2020/04/02/uPic/azkaban工作流调度器/assets/1572232525618.png)\n","tags":["-azkaban"]},{"title":"HBase深入理解及调优","url":"/2019/04/17/it/hbase/HBase深入理解及调优/","content":"\n\n\n# 大数据数据库之HBase\n\n## 1. HBase协处理器\n\n- http://hbase.apache.org/book.html#cp\n- 起源：\n  - Hbase 作为列族数据库最经常被人诟病的特性包括：无法轻易建立“二级索引”，难以执行求和、计数、排序等操作。比如，在旧版本的(<0.92)Hbase 中，统计数据表的总行数，需 要使用 Counter 方法，执行一次 MapReduce Job 才能得到。\n  - 虽然 HBase 在数据存储层中集成了 MapReduce，能够有效用于数据表的分布式计算。然而在很多情况下，做一些简单的相加或者聚合计算的时候， 如果直接将计算过程放置在 server端，能够减少通讯开销，从而获得很好的性能提升。\n  - 于是， HBase 在 0.92 之后引入了协处理器(coprocessors)，实现一些激动人心的新特性：能够轻易建立二次索引、复杂过滤器(谓词下推)以及访问控制等。\n\n### 两种协处理器\n\n#### observer\n\n- Observer 类似于传统数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。\n- Observer Coprocessor就是一些散布在 HBase Server 端代码中的 hook 钩子， 在固定的事件发生时被调用。\n  - 比如： put 操作之前有钩子函数 prePut，该函数在put操作执行前会被Region Server调用；在 put 操作之后则有 postPut 钩子函数\n\n- 以 HBase0.92 版本为例，它提供了三种观察者接口：\n  - RegionObserver：提供客户端的数据操纵事件钩子： Get、 Put、 Delete、 Scan 等。\n  - WALObserver：提供 WAL 相关操作钩子。\n  - MasterObserver：提供 DDL类型的操作钩子。如创建、删除、修改数据表等。\n  - 到 0.96 版本又新增一个 RegionServerObserver\n\n![](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/Image201911151202.png)\n\n- 下图是以 RegionObserver 为例子讲解 Observer 这种协处理器的原理：\n\n![1122015-20170511100919222-711579099](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/1122015-20170511100919222-711579099.png)\n\n####  endpoint\n\n- Endpoint协处理器类似传统数据库中的存储过程，客户端可以调用这些 Endpoint 协处理器执行一段 Server 端代码，并将 Server 端代码的结果返回给客户端进一步处理\n\n- 最常见的用法就是进行聚集操作。\n  - 如果没有协处理器，当用户需要找出一张表中的最大数据，即max 聚合操作，就必须进行全表扫描，在客户端代码内遍历扫描结果，并执行求最大值的操作。\n  - 这样的方法无法利用底层集群的并发能力，而将所有计算都集中到 Client 端统一执 行，势必效率低下。\n  - 利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，HBase将利用底层cluster 的多个节点并发执行求最大值的操作。即在每个 Region范围内执行求最大值的代码，将每个 Region 的最大值在 Region Server 端计算出，仅仅将该 max 值返回给客户端。\n  - 在客户端进一步将多个 Region 的最大值进一步处理而找到其中的最大值。这样整体的执行效率就会提高很多\n  \n  ```markdown\n  - Observer允许集群在正常的客户端操作过程中可以有不同的行为表现\n  - Endpoint 允许扩展集群的能力，对客户端应用开放新的运算命令\n  - observer 类似于 RDBMS 中的触发器，主要在服务端工作\n  - endpoint 类似于 RDBMS 中的存储过程，主要在 client 端工作\n  - observer 可以实现权限管理、优先级设置、监控、 ddl 控制、 二级索引等功能\n  - endpoint 可以实现 min、 max、 avg、 sum、 distinct、 group by 等功能\n  ```\n\n#### 加载方式  \n\n- 协处理器的加载方式有两种\n  - 静态加载方式（ Static Load）；静态加载的协处理器称之为 System Coprocessor\n  - 动态加载方式 （ Dynamic Load）；动态加载的协处理器称 之为 Table Coprocessor\n\n#### 静态加载 \n\n- 通过修改 hbase-site.xml 这个文件来实现， 如启动全局 aggregation，能过操纵所有的表数据。只需要在hbase-site.xml里面添加以下配置即可\n- ==注意==：修改完配置之后需要**重启HBase集群**\n\n```xml\n<property>\n\t<name>hbase.coprocessor.user.region.classes</name>\n\t<value>org.apache.hadoop.hbase.coprocessor.AggregateImplementation</value>\n</property>\n```\n\n- 为所有table加载了一个 cp class，可以用” ,”分割加载多个 class，修改\n\n#### 动态加载\n\n- 启用表aggregation，只对特定的表生效。\n\n```ruby\n# disable 指定表。\nhbase> disable 'mytable'\n# 添加 aggregation\nhbase> alter 'mytable', METHOD => 'table_att','coprocessor'=>'|org.apache.Hadoop.hbase.coprocessor.AggregateImplementation||'\n# 重启指定表 \nhbase> enable 'mytable'\n\n# 协处理器的卸载\ndisable 'mytable'\nalter  'mytable',METHOD=>'table_att_unset',NAME='coprocessor$1'\nenable  'mytable'\n```\n\n#### 协处理器示例\n\n<img src=\"http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/xdfsdfsdf.png\" alt=\"xdfsdfsdf\" style=\"zoom:80%;\" />\n\n- 通过协处理器Observer实现向hbase当中一张表插入数据时，通过协处理器，将数据复制一份保存到另外一张表当中去；但是只取第一张表当中的部分列数据，保存到第二张表当中去\n\n- 打开hbase shell\n\n```shell\n# 1. 创建一张表\ncreate 'proc1','info'\n\n# 2. 创建第二张表\ncreate 'proc2','info'\n```\n\n- HBase协处理器\n\n```xml\n<dependency>\n  <groupId>org.apache.hadoop</groupId>\n  <artifactId>hadoop-client</artifactId>\n  <version>2.6.0-mr1-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-client</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-server</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n```\n\n- 开发HBase的协处理器\n\n```java\npublic class MyProcessor extends BaseRegionObserver {\n    /**\n     * @param put   插入到proc1表里面的数据，都是封装在put对象里面了,就可以解析put对象，获取数据，获取到了数据之后，插入到proc2表里面去\n     */\n    @Override\n    public void prePut(ObserverContext<RegionCoprocessorEnvironment> e, Put put, WALEdit edit, Durability durability) throws IOException {\n        // 从put获取到数据，直接开启HBase连接插入到第二张表。\n      // TODO\n    }\n}\n```\n\n```shell\n# 1. jar包名称必须为processor.jar\nmv original-hbaseStudy-1.0-SNAPSHOT.jar  processor.jar\n# 2. 上传到hdfs的/processor目录即可\nhdfs dfs -mkdir -p /processor\nhdfs dfs -put processor.jar /processor\n\n# 加载协处理器\ndescribe 'proc1'\nalter 'observer:source',METHOD => 'table_att','Coprocessor'=>'hdfs://node01:8020/processor/processor.jar|top.kfly.hbasemr.MyProcessor|1001|'\n# 查看\ndescribe 'proc1'\n```\n\n## rowkey设计\n\n- 长度原则\n\n  - rowkey是一个二进制码流，可以是任意字符串，最大长度64kb，实际应用中一般为10-100bytes，以byte[]形式保存，一般设计成定长。\n\n  - 建议尽可能短；但是也不能太短，否则rowkey前缀重复的概率增大\n  - 设计过长会降低memstore内存的利用率和HFile存储数据的效率。\n\n- 散列原则\n  - 建议将rowkey的高位作为散列字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。\n  - 如果没有散列字段，首字段直接是时间信息。所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。\t\n\n- 唯一原则\n  - 必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的\n  - 因此，设计rowkey的时候，要充分利用这个排序的特点，可以将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块\n  - 电信上网详单数据，就是保存在HBase中的\n\n![2019-10-16_112336](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/2019-10-16_112336.png)\n\n## HBase表的热点\n\n```markdown\n- 检索habse的记录首先要通过row key来定位数据行。\n- 当大量的client访问hbase集群的一个或少数几个节点，造成少数region server的读/写请求过多、负载过大，而其他region server负载却很小，就造成了“热点”现象。\n```\n\n- **热点的解决方案**\n\n- 预分区\n\n  - 预分区的目的让表的数据可以均衡的分散在集群中，而不是默认只有一个region分布在集群的一个节点上。\n\n-  加盐             \n  \n- 这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同\n  \n- 哈希\n  - 哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。\n\n\t   ~~~\n   rowkey=MD5(username).subString(0,10)+时间戳\t\n     ~~~\n~~~markdown\n- 反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。\n- 这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。\n- eg: 手机号码的反转： 0120xxxx751\n- eg: rowkey= address + age + 随机数\n~~~\n\n## HBase的数据备份\n\n- 基于HBase提供的类对表进行备份\n\n~~~shell\n# 使用HBase提供的类把HBase中某张表的数据导出到HDFS，之后再导出到测试hbase表中。\nhbase org.apache.hadoop.hbase.mapreduce.Export myuser /hbase_data/myuser_bak\n  \n# hbase shell中创建备份目标表\ncreate 'myuser_bak','f1','f2'\n\n# 将HDFS上的数据导入到备份目标表中\nhbase org.apache.hadoop.hbase.mapreduce.Driver import myuser_bak /hbase_data/myuser_bak/*\n\n# 增量备份\nhbase org.apache.hadoop.hbase.mapreduce.Export test /hbase_data/test_bak_increment 开始时间戳  结束时间戳\n~~~\n\n- \n  ### 基于snapshot快照对表进行备份\n  - 通过snapshot快照的方式实现HBase数据的迁移和拷贝。这种方式比较常用，效率高，也是最为推荐的数据迁移方式。\n\n  - HBase的snapshot其实就是一组==metadata==信息的集合（文件列表），通过这些metadata信息的集合，就能将表的数据回滚到snapshot那个时刻的数据。\n    - 首先我们要了解一下所谓的HBase的LSM类型的系统结构，我们知道在HBase中，数据是先写入到Memstore中，当Memstore中的数据达到一定条件，就会flush到HDFS中，形成HFile，后面就不允许原地修改或者删除了。\n    - 如果要更新或者删除的话，只能追加写入新文件。既然数据写入以后就不会在发生原地修改或者删除，这就是snapshot做文章的地方。做snapshot的时候，只需要给快照表对应的所有文件创建好指针（元数据集合），恢复的时候只需要根据这些指针找到对应的文件进行恢复就Ok。这是原理的最简单的描述，下图是描述快照时候的简单流程：\t\n\n  ![snapshot](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/snapshot.png)\n\n~~~sql\n-- 创建表的snapshot\nsnapshot 'tableName', 'snapshotName'\n-- 查看快照\nlist_snapshots  \n-- 查看以test开头的快照\nlist_snapshots 'test.*'\n-- 恢复快照\ndisable 'tableName'\nrestore_snapshot 'snapshotName'\nenable 'tableName'\n-- 删除快照\ndelete_snapshot 'snapshotName'\n-- 迁移 snapshot\nhbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \\\n  -snapshot snapshotName  \\\n  -copy-from hdfs://src-hbase-root-dir/hbase \\\n  -copy-to hdfs://dst-hbase-root-dir/hbase \\\n  -mappers 1 \\\n  -bandwidth 1024\n  \n  -- 用于将快照迁移到另外一个集群\n  hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \\\n  -snapshot test  \\\n  -copy-from hdfs://node01:8020/hbase \\\n  -copy-to hdfs://kfly01:8020/hbase1 \\\n  -mappers 1 \\\n  -bandwidth 1024\n  -- 使用的时候记得设置好bandwidth参数，以免由于网络打满导致的线上业务故障。\n~~~\n\n  * 将snapshot使用bulkload的方式导入\n\n  ~~~shell\nhbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles \\\nhdfs://dst-hbase-root-dir/hbase/archive/datapath/tablename/filename \\\ntablename\n\n-- 创建一个新表\ncreate 'newTest','f1','f2'\nhbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles hdfs://node1:9000/hbase1/archive/data/default/test/6325fabb429bf45c5dcbbe672225f1fb newTest\n  ~~~\n\n\n\n## HBase二级索引\n\n![hbase寻址](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/hbase寻址.png)\n\n- HBase表后期按照rowkey查询性能是最高的。rowkey就相当于hbase表的一级索引\n- 但是在实际的工作中，我们做的查询基本上都是按照一定的条件进行查找，无法事先知道满足这些条件的rowkey是什么，正常是可以通过hbase过滤器去实现。但是效率非常低，这是由于查询的过程中需要在底层进行大量的文件扫描。\n\n- HBase的二级索引\n- 为了HBase的数据查询更高效、适应更多的场景，诸如使用非rowkey字段检索也能做到秒级响应，或者支持各个字段进行模糊查询和多字段组合查询等， 因此需要在HBase上面构建二级索引， 以满足现实中更复杂多样的业务需求。\n  - hbase的二级索引其本质就是建立HBase表中列与行键之间的映射关系。\n\n\n\n![](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/二级索引思想.png)\n\n- 构建hbase二级索引方案\n  - MapReduce方案 \n  - Hbase Coprocessor(协处理器)方案 \n  - Solr+hbase方案\n  - ES+hbase方案\n  - Phoenix+hbase方案\n    - [点击查看](https://kfly.top/2019/11/17/phoenix/Phoenix%E6%9E%84%E5%BB%BA%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95/) \n\n## HBase的namespace\n\n- namespace基本介绍\n  - 在HBase中，namespace命名空间指对一组表的逻辑分组，类似RDBMS中的database，方便对表在业务上划分。\n  - Apache HBase从0.98.0, 0.95.2两个版本号开始支持namespace级别的授权操作，HBase**全局管理员**能够创建、改动和回收namespace的授权。\n\n- namespace的作用\n  - 配额管理：限制一个namespace可以使用的资源，包括region和table\n  - 命名空间安全管理：提供了另一个层面的多租户安全管理\n\n  - Region服务器组：一个命名或一张表，可以被固定到一组RegionServers上，从而保证了数据隔离性\n\n- namespace的基本操作\n\n```sql\n创建namespace\nhbase>create_namespace 'nametest'  \n\n查看namespace\nhbase>describe_namespace 'nametest'  \n\n列出所有namespace\nhbase>list_namespace  \n\n在namespace下创建表\nhbase>create 'nametest:testtable', 'fm1' \n\n查看namespace下的表\nhbase>list_namespace_tables 'nametest'  \n\n删除namespace\nhbase>drop_namespace 'nametest'  \n```\n\n## 数据版本的确界以及TTL\n\n### 数据的确界\n\n- 在HBase当中，我们可以为数据设置上界和下界，其实就是定义数据的历史版本保留多少个，通过自定义历史版本保存的数量，我们可以实现数据多个历史版本的数据查询\n\n- 版本的下界\n  - 默认的版本下界是0，即禁用。row版本使用的最小数目是与生存时间（TTL Time To Live）相结合的，并且我们根据实际需求可以有0或更多的版本，使用0，即只有1个版本的值写入cell。\n\n- 版本的上界\n  - 之前默认的版本上界是3，也就是一个row保留3个副本（基于时间戳的插入）。\n  - 该值不要设计的过大，一般的业务不会超过100。如果cell中存储的数据版本号超过了3个，再次插入数据时，最新的值会将最老的值覆盖。（现版本已默认为1）\n\n### 数据的TTL\n\n- 在实际工作当中经常会遇到有些数据过了一段时间我们可能就不需要了，那么这时候我们可以使用定时任务去定时的删除这些数据\n- 或者我们也可以使用Hbase的TTL（Time  To  Live）功能，让我们的数据定期的会进行清除\n\n- 使用代码来设置数据的确界以及设置数据的TTL如下\n\n```xml\n<dependency>\n  <groupId>org.apache.hadoop</groupId>\n  <artifactId>hadoop-client</artifactId>\n  <version>2.6.0-mr1-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-client</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-server</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n```\n\n```java\n// columa family\nHColumnDescriptor column = new HColumnDescriptor(\"f1\");\n// version\ncolumn.setMaxVersions(5);\ncolumn.setMinVersions(3);\n// ttl unit s\ncolumn.setTimeToLive(30);\n```\n\n","tags":["hbase","协处理器","rowkey设计"]},{"title":"HBase原理剖析","url":"/2019/03/13/it/hbase/HBase原理剖析/","content":"\n##  数据存储原理\n\n![hbase存储架构](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/hbase%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84.png?lastModify=1573631775)\n\n![img](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/hbase_data_storage-1565601156263.png?lastModify=1573631775)\n\n- 一个HRegionServer会负责管理很多个region\n- 一个**==region==**包含很多个==store==\n  - 一个**==列族==**就划分成一个**==store==**\n  - 如果一个表中只有1个列族，那么每一个region中只有一个store\n  - 如果一个表中有N个列族，那么每一个region中有N个store\n- ==一个store==里面只有==一个memstore==\n  - memstore是一块**内存区域**，写入的数据会先写入memstore进行缓冲，然后再把数据刷到磁盘\n- 一个store里面有很多个**==StoreFile==**, 最后数据是以很多个**==HFile==**这种数据结构的文件保存在HDFS上\n  - StoreFile是HFile的抽象对象，如果说到StoreFile就等于HFile\n  - ==每次memstore刷写数据到磁盘，就生成对应的一个新的HFile文件出来==\n\n![region](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/region.png?lastModify=1573631775)\n\n\n\n## 读数据流程\n\n![img](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/hbase%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png?lastModify=1573631775)\n\n> 说明：HBase集群，只有一张meta表，此表只有一个region，该region数据保存在一个HRegionServer上\n\n- 1、客户端首先与zk进行连接；从zk找到meta表的region位置，即meta表的数据存储在某一HRegionServer上；客户端与此HRegionServer建立连接，然后读取meta表中的数据；meta表中存储了所有用户表的region信息，我们可以通过`scan  'hbase:meta'`来查看meta表信息\n- 2、根据要查询的namespace、表名和rowkey信息。找到写入数据对应的region信息\n- 3、找到这个region对应的regionServer，然后发送请求\n- 4、查找并定位到对应的region\n- 5、先从memstore查找数据，如果没有，再从BlockCache上读取\n  - HBase上Regionserver的内存分为两个部分\n    - 一部分作为Memstore，主要用来写；\n    - 另外一部分作为BlockCache，主要用于读数据；\n- 6、如果BlockCache中也没有找到，再到StoreFile上进行读取\n  - 从storeFile中读取到数据之后，不是直接把结果数据返回给客户端，而是把数据先写入到BlockCache中，目的是为了加快后续的查询；然后在返回结果给客户端。\n\n\n\n## 写数据流程\n\n![img](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/hbase%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png?lastModify=1573631775)\n\n- 1、客户端首先从zk找到meta表的region位置，然后读取meta表中的数据，meta表中存储了用户表的region信息\n- 2、根据namespace、表名和rowkey信息。找到写入数据对应的region信息\n- 3、找到这个region对应的regionServer，然后发送请求\n- 4、把数据分别写到HLog（write ahead log）和memstore各一份\n- 5、memstore达到阈值后把数据刷到磁盘，生成storeFile文件\n- 6、删除HLog中的历史数据\n\n```\n补充：\nHLog（write ahead log）：\n  也称为WAL意为Write ahead log，类似mysql中的binlog,用来做灾难恢复时用，HLog记录数据的所有变更,一旦数据修改，就可以从log中进行恢复。\n```\n\n\n\n## flush、compact机制\n\n<img src=\"http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/hbase-split-compaction.png\" style=\"zoom:50%;\" />\n\n### Flush触发条件\n\n- memstore级别限制\n\n```xml\n<!--\n\t当Region中任意一个MemStore的大小达到了上限（hbase.hregion.memstore.flush.size，默认128MB），会触发Memstore刷新。\n-->\n<property>\n\t<name>hbase.hregion.memstore.flush.size</name>\n\t<value>134217728</value>\n</property>\n```\n\n-  region级别限制\n\n```xml\n<!--\n\t当Region中所有Memstore的大小总和达到了上限（hbase.hregion.memstore.block.multiplier * hbase.hregion.memstore.flush.size，默认 2* 128M = 256M），会触发memstore刷新。\n-->\n<property>\n\t<name>hbase.hregion.memstore.flush.size</name>\n\t<value>134217728</value>\n</property>\n<property>\n\t<name>hbase.hregion.memstore.block.multiplier</name>\n\t<value>2</value>\n</property>   \n```\n\n- Region Server级别限制\n\n```xml\n<!--\n  - 当一个Region Server中所有Memstore的大小总和超过低水位阈值hbase.regionserver.global.memstore.size.lower.limit*hbase.regionserver.global.memstore.size（前者默认值0.95），RegionServer开始强制flush；\n  - 先Flush Memstore最大的Region，再执行次大的，依次执行；\n  - 如写入速度大于flush写出的速度，导致总MemStore大小超过高水位阈值hbase.regionserver.global.memstore.size（默认为JVM内存的40%），此时RegionServer会阻塞更新并强制执行flush，直到总MemStore大小低于低水位阈值\n-->\n<property>\n\t<name>hbase.regionserver.global.memstore.size.lower.limit</name>\n\t<value>0.95</value>\n</property>\n<property>\n\t<name>hbase.regionserver.global.memstore.size</name>\n\t<value>0.4</value>\n</property>\n```\n\n- HLog数量上限\n  - 当一个Region Server中HLog数量达到上限（可通过参数hbase.regionserver.maxlogs配置）时，系统会选取最早的一个 HLog对应的一个或多个Region进行flush\n\n- 定期刷新Memstore\n  - 默认周期为1小时，确保Memstore不会长时间没有持久化。为避免所有的MemStore在同一时间都进行flush导致的问题，定期的flush操作有20000左右的随机延时。\n\n- 手动flush\n  - 用户可以通过shell命令`flush ‘tablename’`或者`flush ‘region name’`分别对一个表或者一个Region进行flush。\n\n###  Flush的流程\n\n- 为了减少flush过程对读写的影响，将整个flush过程分为三个阶段：\n  - prepare阶段：遍历当前Region中所有的Memstore，将Memstore中当前数据集CellSkipListSet做一个**快照snapshot**；然后再新建一个CellSkipListSet。后期写入的数据都会写入新的CellSkipListSet中。prepare阶段需要加一把updateLock对**写请求阻塞**，结束之后会释放该锁。因为此阶段没有任何费时操作，因此持锁时间很短。\n\n  - flush阶段：遍历所有Memstore，将prepare阶段生成的snapshot持久化为**临时文件**，临时文件会统一放到目录.tmp下。这个过程因为涉及到磁盘IO操作，因此相对比较耗时。\n  - commit阶段：遍历所有Memstore，将flush阶段生成的临时文件移到指定的ColumnFamily目录下，针对HFile生成对应的storefile和Reader，把storefile添加到HStore的storefiles列表中，最后再**清空**prepare阶段生成的snapshot。\n\n### Compact合并机制\n\n- hbase为了==防止小文件过多==，以保证查询效率，hbase需要在必要的时候将这些小的store file合并成相对较大的store file，这个过程就称之为compaction。\n\n\n#### minor compaction\n\n- 在将Store中多个HFile合并为一个HFile\n\n  在这个过程中会选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，对于超过了TTL的数据、更新的数据、删除的数据仅仅只是做了标记。并没有进行物理删除，一次Minor Compaction的结果是更少并且更大的StoreFile。这种合并的触发频率很高。\n\n- minor compaction触发条件由以下几个参数共同决定：\n\n~~~xml\n<!--表示至少需要三个满足条件的store file时，minor compaction才会启动-->\n<property>\n\t<name>hbase.hstore.compactionThreshold</name>\n\t<value>3</value>\n</property>\n\n<!--表示一次minor compaction中最多选取10个store file-->\n<property>\n\t<name>hbase.hstore.compaction.max</name>\n\t<value>10</value>\n</property>\n\n<!--默认值为128m,\n表示文件大小小于该值的store file 一定会加入到minor compaction的store file中\n-->\n<property>\n\t<name>hbase.hstore.compaction.min.size</name>\n\t<value>134217728</value>\n</property>\n\n<!--默认值为LONG.MAX_VALUE，\n表示文件大小大于该值的store file 一定会被minor compaction排除-->\n<property>\n\t<name>hbase.hstore.compaction.max.size</name>\n\t<value>9223372036854775807</value>\n</property>\n~~~\n\n#### major compaction \n\n* 合并Store中所有的HFile为一个HFile\n\n  将所有的StoreFile合并成一个StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL过期数据、版本号超过设定版本号的数据。合并频率比较低，默认7天执行一次，并且性能消耗非常大，建议生产关闭(设置为0)，在应用空闲时间手动触发。一般可以是手动控制进行合并，防止出现在业务高峰期。\n\n* major compaction触发时间条件\n\n  ~~~xml\n  <!--默认值为7天进行一次大合并，-->\n  <property>\n  \t<name>hbase.hregion.majorcompaction</name>\n  \t<value>604800000</value>\n  </property>\n  ~~~\n\n* 手动触发\n\n  ~~~ruby\n  ##使用major_compact命令\n  major_compact tableName\n  ~~~\n\n## region 拆分机制\n\n- region中存储的是大量的rowkey数据 ,当region中的数据条数过多的时候,直接影响查询效率.当region过大的时候.hbase会拆分region , 这也是Hbase的一个优点 .\n\n- HBase的region split策略一共有以下几种：\n\n* 1、**ConstantSizeRegionSplitPolicy**\n\n  * 0.94版本前默认切分策略\n\n* 当region大小大于某个阈值(hbase.hregion.max.filesize=10G)之后就会触发切分，一个region等分为2个region。\n\n  * 但是在生产线上这种切分策略却有相当大的弊端：切分策略对于大表和小表没有明显的区分。阈值(hbase.hregion.max.filesize)设置较大对大表比较友好，但是小表就有可能不会触发分裂，极端情况下可能就1个，这对业务来说并不是什么好事。如果设置较小则对小表友好，但一个大表就会在整个集群产生大量的region，这对于集群的管理、资源使用、failover来说都不是一件好事。\n\n* 2、**IncreasingToUpperBoundRegionSplitPolicy**\n\n  *  0.94版本~2.0版本默认切分策略\n\n  - 切分策略稍微有点复杂，总体看和ConstantSizeRegionSplitPolicy思路相同，一个region大小大于设置阈值就会触发切分。但是这个阈值并不像ConstantSizeRegionSplitPolicy是一个固定的值，而是会在一定条件下不断调整，调整规则和region所属表在当前regionserver上的region个数有关系.\n\n  - region split的计算公式是：\n    regioncount^3 * 128M * 2，当region达到该size的时候进行split\n    例如：\n    第一次split：1^3 * 256 = 256MB \n    第二次split：2^3 * 256 = 2048MB \n    第三次split：3^3 * 256 = 6912MB \n    第四次split：4^3 * 256 = 16384MB > 10GB，因此取较小的值10GB \n    后面每次split的size都是10GB了\n\n* 3、**SteppingSplitPolicy**\n\n  * 2.0版本默认切分策略\n\n  - 这种切分策略的切分阈值又发生了变化，相比 IncreasingToUpperBoundRegionSplitPolicy 简单了一些，依然和待分裂region所属表在当前regionserver上的region个数有关系，如果region个数等于1，\n    切分阈值为flush size * 2，否则为MaxRegionFileSize。这种切分策略对于大集群中的大表、小表会比 IncreasingToUpperBoundRegionSplitPolicy 更加友好，小表不会再产生大量的小region，而是适可而止。\n\n* 4、**KeyPrefixRegionSplitPolicy**\n\n  - 根据rowKey的前缀对数据进行分组，这里是指定rowKey的前多少位作为前缀，比如rowKey都是16位的，指定前5位是前缀，那么前5位相同的rowKey在进行region split的时候会分到相同的region中。\n\n* 5、**DelimitedKeyPrefixRegionSplitPolicy**\n\n  - 保证相同前缀的数据在同一个region中，例如rowKey的格式为：userid_eventtype_eventid，指定的delimiter为 _ ，则split的的时候会确保userid相同的数据在同一个region中。\n\n\n* 6、**DisabledRegionSplitPolicy**\n  * 不启用自动拆分, 需要指定手动拆分\n\n## 表的预分区\n\n- 当一个table刚被创建的时候，Hbase默认的分配一个region给table。也就是说这个时候，所有的读写请求都会访问到同一个regionServer的同一个region中，这个时候就达不到负载均衡的效果了，集群中的其他regionServer就可能会处于比较空闲的状态。\n  - 解决这个问题可以用**pre-splitting**,在创建table的时候就配置好，生成多个region。\n- 为何要预分区？\n\n  - 增加数据读写效率\n  - 负载均衡，防止数据倾斜\n  - 方便集群容灾调度region\n  - 优化Map数量\n\n- 每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。\n\n~~~ruby\n-- 创建表，指定预分区\ncreate 'person','info1','info2',SPLITS => ['1000','2000','3000','4000']\n\n-- 根据文件内容预分区\ncat /kfly/install/split.txt\n  aaa\n  bbb\n  ccc\n  ddd\ncreate 'student','info',SPLITS_FILE => '/kfly/install/split.txt'\n~~~\n\n- HexStringSplit 算法\n  - HexStringSplit会将数据从“00000000”到“FFFFFFFF”之间的数据长度按照**n等分**之后算出每一段的其实rowkey和结束rowkey，以此作为拆分点。\n\n  ```ruby\n  create 'mytable', 'base_info',' extra_info', {NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}\n  ```\n\n\n<img src=\"http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/hbasePreSplit.png\" alt=\"hbasePreSplit\" style=\"zoom:50%;\" />\n\n## region 合并\n\n- Region的合并不是为了性能,  而是出于维护的目的 .\n- 比如删除了大量的数据 ,这个时候每个Region都变得很小 ,存储多个Region就浪费了 ,这个时候可以把Region合并起来，进而可以减少一些Region服务器节点 \n- 通过Merge类冷合并Region\n\n  - 执行合并前，==需要先关闭hbase集群==\n\n  - 创建一张hbase表：\n\n```ruby\ncreate 'test','info1',SPLITS => ['1000','2000','3000']\n```\n\n- 查看表region\n\n![testRegion](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/testRegion.png)\n\n- 需求：\n\n  \n  \n- 这里通过org.apache.hadoop.hbase.util.Merge类来实现，不需要进入hbase shell，直接执行（==**需要先关闭hbase集群**==）：\n  \n```shell\n  # 需要把test表中的2个region数据进行合并：test,,1565940912661.62d28d7d20f18debd2e7dac093bc09d8.  test,1000,1565940912661.5b6f9e8dad3880bcc825826d12e81436.\n  \n  hbase org.apache.hadoop.hbase.util.Merge test test,,1565940912661.62d28d7d20f18debd2e7dac093bc09d8. test,1000,1565940912661.5b6f9e8dad3880bcc825826d12e81436.\n```\n\n- 成功后界面观察\n\n![testMerge](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/testMerge.png)\n\n- 通过online_merge热合并Region\n  - ==不需要关闭hbase集群==，在线进行合并\n\n  - 与冷合并不同的是，online_merge的传参是Region的hash值，而Region的hash值就是Region名称的最后那段在两个.之间的字符串部分。\n\n  - 需求：需要把test表中的2个region数据进行合并：\n    test,2000,1565940912661.c2212a3956b814a6f0d57a90983a8515.\n    test,3000,1565940912661.553dd4db667814cf2f050561167ca030.\n\n  - 需要进入hbase shell：\n\n  ```ruby\n  merge_region 'c2212a3956b814a6f0d57a90983a8515','553dd4db667814cf2f050561167ca030'\n  ```\n\n  - 成功后观察界面\n\n![online_merge](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/online_merge.png)\n\n## HBase集成MapReduce\n\n* HBase表中的数据最终都是存储在HDFS上，HBase天生的支持MR的操作，我们可以通过MR直接处理HBase表中的数据，并且MR可以将处理后的结果直接存储到HBase表中。\n  * 参考地址：<http://hbase.apache.org/book.html#mapreduce\n\n* 需求：==读取HBase当中myuser这张表的数据，将数据写入到另外一张myuser2表里面去==\n\n```xml\n <dependency>\n   <groupId>org.apache.hadoop</groupId>\n   <artifactId>hadoop-client</artifactId>\n   <version>2.6.0-mr1-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-client</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-server</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n```\n\n~~~java\nclass HBaseMapper extends TableMapper<Text,Put>{\n  protected void map(ImmutableBytesWritable key, Result value, Context context){\n    // key get rowKey\n    // result -> put\n    context.write(new Text(rowkey),put);\n  }\n}\nclass HbaseReducer extends TableReducer<Text,Put,ImmutableBytesWritable>{\n  protected void reduce(Text key, Iterable<Put> values, Context context){\n    for (Put put : values) {\n      context.write(null,put);\n    }\n  }\n}\npublic static void main(String[] args){\n        Configuration conf = new Configuration();\n        Scan scan = new Scan();\n        Job job = Job.getInstance(conf);\n        job.setJarByClass(HBaseMR.class);\n        //使用TableMapReduceUtil 工具类来初始化我们的mapper\n      TableMapReduceUtil.initTableMapperJob(\n                                            TableName.valueOf(args[0]),\n                                            scan,\n                                            HBaseMapper.class,\n                                            Text.class,\n                                            Put.class,\n                                            job\n      \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t);\n        //使用TableMapReduceUtil 工具类来初始化我们的reducer\n        TableMapReduceUtil.initTableReducerJob(\n          \t\t\t\t\t\t\t\t\t\t\t\t\t\t\targs[1],\n          \t\t\t\t\t\t\t\t\t\t\t\t\t\t\tHbaseReducer.class,\n          \t\t\t\t\t\t\t\t\t\t\t\t\t\t\tjob);\n        //设置reduce task个数\n        job.setNumReduceTasks(1);\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n~~~\n\n~~~shell\n# 打成jar包提交到集群中运行\nhadoop jar hbase_java_api-1.0-SNAPSHOT.jar com.kaikeba.HBaseMR t1 t2\n~~~\n\n* 需求\n\n  * ==通过bulkload的方式批量加载数据到HBase表中==\n  * ==将我们hdfs上面的这个路径/hbase/input/user.txt的数据文件，转换成HFile格式，然后load到myuser2这张表里面去==\n\n* 知识点描述\n\n  - 加载数据到HBase当中去的方式多种多样，我们可以使用HBase的javaAPI或者使用sqoop将我们的数据写入或者导入到HBase当中去，但是这些方式不是慢就是在导入的过程的占用Region资源导致效率低下\n  - 我们也可以通过MR的程序，将我们的数据直接转换成HBase的最终存储格式HFile，然后直接load数据到HBase当中去即可\n\n* HBase数据正常写流程回顾\n\n  ![hbase-write](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/hbase-write.png)\n\n* bulkload方式的处理示意图\n\n![](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/bulkload.png)\n\n* 好处\n\n  - 导入过程不占用Region资源\n  - 能快速导入海量的数据\n  - 节省内存\n\n* ==1、开发生成HFile文件的代码==\n\n~~~java\n// 1. map阶段\ncontext.write(new ImmutableBytesWritable(Bytes.toBytes(split[0])),put);\n\n// 2. main job\nConfiguration conf = HBaseConfiguration.create();\nConnection connection = ConnectionFactory.createConnection(conf);\njob.setMapOutputKeyClass(ImmutableBytesWritable.class);\njob.setMapOutputValueClass(Put.class);\n//指定输出的类型HFileOutputFormat2\njob.setOutputFormatClass(HFileOutputFormat2.class);\nHFileOutputFormat2\n  .configureIncrementalLoad(\n                            job,\n                            table,                                       \t\t   \t \n                            conn.getRegionLocator(TableName.valueOf(\"t4\"))\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t);\n// 3. 加载数据使用java Api\nTable table = connection.getTable(tableName);\n//构建LoadIncrementalHFiles加载HFile文件\nLoadIncrementalHFiles load = new LoadIncrementalHFiles(configuration);\nload.doBulkLoad(new Path(\"hdfs://node01:8020/hbase/output_file\");\n                \n// 加载数据命令加载,先将hbase的jar包添加到hadoop的classpath路径下\nexport HBASE_HOME=/kfly/install/hbase-1.2.0-cdh5.14.2/\nexport HADOOP_HOME=/kfly/install/hadoop-2.6.0/\nexport HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase mapredcp`\n                \nyarn jar /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar   completebulkload /hbase/output_hfile myuser2\n~~~\n\n~~~shell\nhadoop jar hbase_java_api-1.0-SNAPSHOT.jar com.kaikeba.HBaseLoad\n~~~\n\n## HBase集成Hive\n\n- Hive提供了与HBase的集成，使得能够在HBase表上使用hive sql 语句进行查询、插入操作以及进行Join和Union等复杂查询，同时也可以将hive表中的数据映射到Hbase中\n\n### 对比\n\n- Hive\n  - 数据仓库\n    - ​\tHive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。\n\n  - 用于数据分析、清洗                \n    - ​\tHive适用于离线的数据分析和清洗，延迟较高\n\n  - 基于HDFS、MapReduce\n    - ​\tHive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。（不要钻不需要执行MapReduce代码的情况的牛角尖）\n\n- HBase\n  - 数据库\n    - 是一种面向列存储的非关系型数据库。\n\n  - 用于存储结构化和非结构话的数据\n    - 适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。\n\n  - 基于HDFS\n    - 数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。\n\n  - 延迟较低，接入在线业务使用\n    - 面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。\n\n- Hive和Hbase是两种基于Hadoop的不同技术，Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到HBase，或者从HBase写回Hive。\n\n#### 拷贝jar包\n\n- 将我们HBase的五个jar包拷贝到hive的lib目录下\n\n- hbase的jar包都在/kfly/install/hbase-1.2.0-cdh5.14.2/lib\n\n- 我们需要拷贝五个jar包名字如下\n\n```\nhbase-client-1.2.0-cdh5.14.2.jar                  \nhbase-hadoop2-compat-1.2.0-cdh5.14.2.jar \nhbase-hadoop-compat-1.2.0-cdh5.14.2.jar  \nhbase-it-1.2.0-cdh5.14.2.jar    \nhbase-server-1.2.0-cdh5.14.2.jar\n```\n\n- 我们直接在node03执行以下命令，通过创建软连接的方式来进行jar包的依赖\n\n```shell\nln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar              /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar   \n```\n\n#### 修改hive-site.xml\n\n-  添加以下两个属性的配置\n\n```xml\n<property>\n\t\t<name>hive.zookeeper.quorum</name>\n\t\t<value>node01,node02,node03</value>\n</property>\n <property>\n\t\t<name>hbase.zookeeper.quorum</name>\n\t\t<value>node01,node02,node03</value>\n</property>\n```\n\n#### 修改hive-env.sh\n\n```\nexport HADOOP_HOME=/export/servers/hadoop-2.6.0\nexport HBASE_HOME=/export/servers/hbase-1.2.0-cdh5.14.2\nexport HIVE_CONF_DIR=/export/servers/hive-1.1.0-cdh5.14.2/conf\n```\n\n### hive建表同步到hbase\n\n```mysql\n-- hive当中建表\ncreate external table if not exists course.score(id int,cname string,score int) row format delimited fields terminated by '\\t' stored as textfile ;\n-- 加载数据到hive\nload data local inpath '/kfly/doc/hive-hbase.txt' into table score;\n-- 创建hive管理表与HBase映射\ncreate table course.hbase_score(id int,cname string,score int) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'  with serdeproperties(\"hbase.columns.mapping\" = \"cf:name,cf:score\") tblproperties(\"hbase.table.name\" = \"hbase_score\");\n```\n\n###  hive外部表映射HBase表模型\n\n```sql\n-- 创建一张hbase表\ncreate 'hbase_hive_score',{ NAME =>'cf'}\n-- 建立hive的外部表，映射HBase当中的表以及字段\nCREATE external TABLE course.hbase2hive(id int, name string, score int) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf:name,cf:score\") TBLPROPERTIES(\"hbase.table.name\" =\"hbase_hive_score\");\n```\n","tags":["hbase","hbase与hive","hbase与mapreduce"]},{"title":"HBase基础知识认知","url":"/2019/02/11/it/hbase/HBase基础知识认知/","content":"\n[漫画学习HBase----最易懂的Hbase架构原理解析](<http://developer.51cto.com/art/201904/595698.htm>)\n\n*HBase基于Google的BigTable论文，是建立的==HDFS==之上，提供**高可靠性**、**高性能**、**列存储**、**可伸缩**、**实时读写**的分布式数据库系统。*\n\n*在需要==实时读写随机访问==超大规模数据集时，可以使用HBase。*\n\n​\t[点击跳转](https://kfly.top/2019/11/26/hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n\n## 数据模型\n\n![](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/hbase-data-model.png.png)\n\n- rowkey行键\n  - table的主键，table中的记录按照rowkey 的字典序进行排序\n  - Row key行键可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)\n\n- Column Family列族\n  - 列族或列簇\n  - HBase表中的每个列，都归属与某个列族\n  - 列族是表的schema的一部分(而列不是)，即建表时至少指定一个列族\n  - 比如创建一张表，名为`user`，有两个列族，分别是`info`和`data`，建表语句`create 'user', 'info', 'data'`\n\n- Column列\n  - 列肯定是表的某一列族下的一个列，用`列族名:列名`表示，如`info`列族下的`name`列，表示为`info:name`\n  - 属于某一个ColumnFamily,类似于我们mysql当中创建的具体的列\n\n- cell单元格\n  - 指定row key行键、列族、列，可以确定的一个cell单元格\n\n  - cell中的数据是没有类型的，全部是以字节数组进行存储\n\n![](http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/Image201911072218.png)\n\n- Timestamp时间戳\n  - 可以对表中的Cell多次赋值，每次赋值操作时的时间戳timestamp，可看成Cell值的版本号version number\n  - 即一个Cell可以有多个版本的值\n\n## 整体架构\n\n<img src=\"http://kflys.gitee.io/upic/2020/04/01/uPic/hbase/assets/hbase-Hmaster-Hregionserver.png\" style=\"zoom: 50%;\" />\n\n- Client客户端\n  - Client是操作HBase集群的入口\n    - 对于管理类的操作，如表的增、删、改操纵，Client通过RPC与HMaster通信完成\n    - 对于表数据的读写操作，Client通过RPC与RegionServer交互，读写数据\n  - Client类型：\n    - HBase shell / Java编程接口 /Thrift、Avro、Rest等等\n\n- ZooKeeper集群\n  - 实现了HMaster的高可用，多HMaster间进行主备选举\n\n  - 保存了HBase的元数据信息meta表，提供了HBase表中region的寻址入口的线索数据\n\n  - 对HMaster和HRegionServer实现了监控\n\n- HMaster\n  - HBase集群也是主从架构，HMaster是主的角色，是老大\n  - 主要负责Table表和Region的相关管理工作：\n  - 关于Table\n    - 管理Client对Table的增删改的操作\n    - 关于Region\n      - 在Region分裂后，负责新Region分配到指定的HRegionServer上\n      - 管理HRegionServer间的负载均衡，迁移region分布\n      - 当HRegionServer宕机后，负责其上的region的迁移\n\n-  HRegionServer\n  - HBase集群中从的角色，是小弟\n\n    - 响应客户端的读写数据请求\n    - 负责管理一系列的Region\n    - 切分在运行过程中变大的region\n\n- Region\n  - HBase集群中分布式存储的最小单元\n  - 一个Region对应一个Table表的部分数据\n\n> HBase使用，主要有两种形式：①命令；②Java编程\n\n## HBase Client\n\n### HBase Shell\n\n```shell\ncd /kfly/install/hbase-1.2.0-cdh5.14.2/\nbin/HBase shell\n# list table\nlist\n \n# create table\ncreate 'user', 'info', 'data'\ncreate 'user', {NAME => 'info', VERSIONS => '3'}，{NAME => 'data'}\n# 向user表中插入信息，row key为rk0001，列族info中添加名为age 、 name的列，值为zhangsan\nput 'user', 'rk0001', 'info:age', 'zhangsan'\nput 'user', 'rk0001', 'info:name', 'female'\n\n# 获取user表中row key为rk0001的所有信息（即所有cell的数据）\nget 'user', 'rk0001'\n\n# 获取user表中row key为rk0001，info列族的所有信息\nget 'user', 'rk0001', 'info'\n\n# 获取user表中row key为rk0001，info列族的name、age列的信息\nget 'user', 'rk0001', 'info:name', 'info:age'\n\n# 获取多个列镞的信息\nget 'user', 'rk0001', 'info', 'data'\nget 'user', 'rk0001', {COLUMN => ['info', 'data']}\nget 'user', 'rk0001', {COLUMN => ['info:name', 'data:pic']}\n\n# 指定rowkey与列值过滤器查询\nget 'user', 'rk0001', {FILTER => \"ValueFilter(=, 'binary:zhangsan')\"}\n# 获取user表中row key为rk0001，列标示符中含有a的信息\nget 'user', 'rk0001', {FILTER => \"(QualifierFilter(=,'substring:a'))\"}\n\nget 'user', 'rk0002', {FILTER => \"ValueFilter(=, 'binary:中国')\"}\n\n# 查询user表中的所有信息\nscan 'user'\nscan 'user', {COLUMNS => 'info'}\nscan 'user', {COLUMNS => 'info', RAW => true, VERSIONS => 5}\nscan 'user', {COLUMNS => ['info', 'data']}\nscan 'user', {COLUMNS => 'info:name'}\n\n# 指定多个列族与按照数据值模糊查询\nscan 'user', {COLUMNS => ['info', 'data'], FILTER => \"(QualifierFilter(=,'substring:a'))\"}\n\n#  指定rowkey的范围查询\nscan 'user', {COLUMNS => 'info', STARTROW => 'rk0001', ENDROW => 'rk0003'}\n\n#  指定rowkey模糊查询\nscan 'user',{FILTER=>\"PrefixFilter('rk')\"}\n\n# 指定数据版本的范围查询\nscan 'user', {TIMERANGE => [1392368783980, 1392380169184]}\n```\n\n\n\n```shell\n# 更新数据值 将user表的f1列族版本数改为5\nalter 'user', NAME => 'info', VERSIONS => 5\n\n# 指定rowkey以及列名进行删除\ndelete 'user', 'rk0001', 'info:name'\n\n#  指定rowkey，列名以及版本号进行删除\ndelete 'user', 'rk0001', 'info:name', 1392383705316\n\n# 删除一个列镞\nalter 'user', NAME => 'info', METHOD => 'delete' \n或 alter 'user', 'delete' => 'info'\n\n# 清空表数据\ntruncate 'user'\n\n# 删除表\ndisable 'user'\ndrop 'user'\n\n# 统计有多少数据\ncount 'user' \n```\n\n## 高级shell管理命令\n\n```shell\n# 显示服务器状态\nstatus 'node01'\n\n# 显示HBase当前用户，例如：\nwhoami\n\n#  显示当前所有的表\nHBase >  list\n\n#  统计指定表的记录数，例如：\nHBase> count 'user' \n\n#  展示表结构信息\nHBase> describe 'user'\n\n#  检查表是否存在，适用于表量特别多的情况\nHBase> exists 'user'\n\n#  检查表是否启用或禁用\nHBase> is_enabled 'user'\nHBase> is_disabled 'user'\n\n#  该命令可以改变表和列族的模式，例如：\n#  **为当前表增加列族：**\nHBase> alter 'user', NAME => 'CF2', VERSIONS => 2\n#  **为当前表删除列族：**\nHBase(main):002:0>  alter 'user', 'delete' => 'CF2'\n\n#  禁用一张表/启用一张表\nHBase> disable 'user'\nHBase> enable 'user'\n```\n\n## JavaAPI\n\n- HBase是一个分布式的NoSql数据库，在实际工作当中，我们一般都可以通过JavaAPI来进行各种数据的操作，包括创建表，以及数据的增删改查等等\n\n- 讲如下内容作为maven工程中pom.xml的repositories的内容\n\n```xml\n  <dependency>\n    <groupId>org.apache.hadoop</groupId>\n    <artifactId>hadoop-client</artifactId>\n    <version>2.6.0-mr1-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.HBase</groupId>\n  <artifactId>hbase-client</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.HBase</groupId>\n  <artifactId>hbase-server</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>junit</groupId>\n  <artifactId>junit</artifactId>\n  <version>4.12</version>\n  <scope>test</scope>\n</dependency>\n<dependency>\n  <groupId>org.testng</groupId>\n  <artifactId>testng</artifactId>\n  <version>6.14.3</version>\n  <scope>test</scope>\n</dependency>\n```\n### 基础Api\n```java\n// 1. 连接HBase\nConfiguration configuration = HBaseConfiguration.create();\n//连接HBase集群不需要指定HBase主节点的ip地址和端口号\nconfiguration.set(\"HBase.zookeeper.quorum\",\"node01:2181,node02:2181,node03:2181\");\n//创建连接对象\nConnection connection = ConnectionFactory.createConnection(configuration);\t\n\n\n//2. 获取管理员对象，来对手数据库进行DDL的操作\nAdmin admin = connection.getAdmin();\n\n// 3. 建表\nHTableDescriptor hTableDescriptor = new HTableDescriptor(TableName.valueOf(\"myuser\"));\n//指定列族\nHColumnDescriptor f1 = new HColumnDescriptor(\"f1\");\nhTableDescriptor.addFamily(f1);\nadmin.createTable(hTableDescriptor);\n\n// 4. 添加数据\nTable table = connection.getTable(TableName.valueOf(TABLE_NAME));\nPut put = new Put(\"0001\".getBytes());//创建put对象，并指定rowkey值\nput.addColumn(\"f1\".getBytes(),\"name\".getBytes(),\"zhangsan\".getBytes());\ntable.put(put);\n\n// 5. 查询数据\n//通过get对象，指定rowkey\nGet get = new Get(Bytes.toBytes(\"0003\"));\nget.addFamily(\"f1\".getBytes());//限制只查询f1列族下面所有列的值\n//查询f2列族 phone这个字段\nget.addColumn(\"f2\".getBytes(),\"phone\".getBytes());\n//通过get查询，返回一个result对象，所有的字段的数据都是封装在result里面了\nResult result = table.get(get);\n\n// 6. 遍历结果\nList<Cell> cells = result.listCells();  //获取一条数据所有的cell，所有数据值都是在cell里面 的\nfor (Cell cell : cells) {\n  byte[] family_name = CellUtil.cloneFamily(cell);//获取列族名\n  byte[] column_name = CellUtil.cloneQualifier(cell);//获取列名\n  byte[] rowkey = CellUtil.cloneRow(cell);//获取rowkey\n  byte[] cell_value = CellUtil.cloneValue(cell);//获取cell值\n  // 需要判断字段的数据类型，使用对应的转换的方法，才能够获取到值\n  if(\"age\".equals(Bytes.toString(column_name))  || \"id\".equals(Bytes.toString(column_name))){}else{}\n}\n\n// 7. scan查询\nScan scan = new Scan();//没有指定startRow以及stopRow  全表扫描\n//扫描f1列族\nscan.addFamily(\"f1\".getBytes());\n//扫描 f2列族 phone  这个字段\nscan.addColumn(\"f2\".getBytes(),\"phone\".getBytes());\nscan.setStartRow(\"0003\".getBytes());\nscan.setStopRow(\"0007\".getBytes());\n//通过getScanner查询获取到了表里面所有的数据，是多条数据\nResultScanner scanner = table.getScanner(scan);\n\n//遍历ResultScanner 得到每一条数据，每一条数据都是封装在result对象里面了\nfor (Result result : scanner) {\n  List<Cell> cells = result.listCells();\n  for (Cell cell : cells) {\n    byte[] family_name = CellUtil.cloneFamily(cell);\n    byte[] qualifier_name = CellUtil.cloneQualifier(cell);\n    byte[] rowkey = CellUtil.cloneRow(cell);\n    byte[] value = CellUtil.cloneValue(cell);\n  }\n}\n```\n\n### 过滤器\n\n- 过滤器的作用是在服务端判断数据是否满足条件，然后只将满足条件的数据返回给客户端\n\n- 过滤器的类型很多，但是可以分为两大类\n  - ==比较过滤器==\n  - ==专用过滤器==\n\n- HBase过滤器的**比较运算符**：\n\n```\nLESS  <\nLESS_OR_EQUAL <=\nEQUAL =\nNOT_EQUAL <>\nGREATER_OR_EQUAL >=\nGREATER >\nNO_OP 排除所有\n```\n\n- Hbase过滤器的**比较器**（指定比较机制）：\n\n```\nBinaryComparator  按字节索引顺序比较指定字节数组，采用Bytes.compareTo(byte[])\nBinaryPrefixComparator 跟前面相同，只是比较左端的数据是否相同\nNullComparator 判断给定的是否为空\nBitComparator 按位比较\nRegexStringComparator 提供一个正则的比较器，仅支持 EQUAL 和非EQUAL\nSubstringComparator 判断提供的子串是否出现在中。\n```\n\n```java\n// 比rowKey  0003小的所有值出来,获取我们比较对象\nBinaryComparator binaryComparator = new BinaryComparator(\"0003\".getBytes());\nRowFilter rowFilter = new RowFilter(CompareFilter.CompareOp.GREATER, binaryComparator);\n\n// 列镞名包含f2的数据\nSubstringComparator substringComparator = new SubstringComparator(\"f2\");\nFamilyFilter familyFilter = new FamilyFilter(CompareFilter.CompareOp.EQUAL, substringComparator);\n\n// 定义列名过滤器，只查询列名包含name的列\nSubstringComparator substringComparator = new SubstringComparator(\"name\");\nQualifierFilter qualifierFilter = new QualifierFilter(CompareFilter.CompareOp.EQUAL, substringComparator);\n\n// 列值过滤器，过滤列值当中包含数字8的所有的列\nSubstringComparator substringComparator = new SubstringComparator(\"8\");\nValueFilter valueFilter = new ValueFilter(CompareFilter.CompareOp.EQUAL, substringComparator);\n\n// 单列值过滤器，过滤  f1 列族  name  列  值为刘备的数据\nSingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter(\"f1\".getBytes(), \"name\".getBytes(), CompareFilter.CompareOp.EQUAL, \"刘备\".getBytes());\n\n// 列值排除过滤器SingleColumnValueExcludeFilter,与SingleColumnValueFilter相反，会排除掉指定的列，其他的列全部返回\n\n// 前缀过滤器，过滤rowkey以  00开头的数据\nPrefixFilter prefixFilter = new PrefixFilter(\"00\".getBytes());\n\n\nscan.setFilter(filter);\n```\n\n##### PageFilter\n\n- 通过pageFilter实现分页过滤器\n\n```java\n// 1. 获取第一页的数据\nscan.setMaxResultSize(pageSize);\nscan.setStartRow(\"\".getBytes()); // 第一页的开始row为 “”空字符串\n//使用分页过滤器来实现数据的分页\nPageFilter filter = new PageFilter(pageSize);\nscan.setFilter(filter);\nResultScanner scanner = table.getScanner(scan);\n\n// 2. 获取第n(n > 1)页的数据\nString  startRow = \"\";\n// （扫描跳过前面的数据）扫描数据的调试 扫描(pageNum - 1) * pageSize + 1条数据\nint scanDatas = (pageNum - 1) * pageSize + 1;\nscan.setMaxResultSize(scanDatas);//设置一步往前扫描多少条数据\nPageFilter filter = new PageFilter(scanDatas);\nscan.setFilter(filter);\nResultScanner scanner = table.getScanner(scan);\n// 到现在已经扫过目标值前面数据，最后一条作为目标页的startRowKey\nfor (Result result : scanner) {\n  byte[] row = result.getRow();//获取rowkey\n  //最后一次startRow的值就是目标值\n  startRow= Bytes.toString(row);//循环遍历我们多有获取到的数据的rowkey\n}\n// 现在开始获取目标页数据\nscan.setStartRow(startRow.getBytes());\nscan.setMaxResultSize(pageSize);//设置我们扫描多少条数据\nPageFilter filter1 = new PageFilter(pageSize);\nscan.setFilter(filter1);\nResultScanner scanner1 = table.getScanner(scan);\n```\n\n##### FilterList\n\n```java\nScan scan = new Scan();\nFilterList filterList = new FilterList();\nfilterList.addFilter(singleColumnValueFilter);\nfilterList.addFilter(prefixFilter);\nscan.setFilter(filterList);\nResultScanner scanner = table.getScanner(scan);\n```\n\n### 删除操作\n\n```java\n// 根据rowkey删除数据\nDelete delete = new Delete(\"0003\".getBytes());\ntable.delete(delete);\n\n//删除一张表\nadmin.disableTable(TableName.valueOf(TABLE_NAME));\nadmin.deleteTable(TableName.valueOf(TABLE_NAME));\n```\n","tags":["hbase"]},{"title":"Hive数仓构建及其数据倾斜","url":"/2019/01/21/it/hive/Hive数仓构建及数据倾斜/","content":"\n## 数据仓库\n\n英文名称为==Data Warehouse==，可简写为DW或DWH。数据仓库的目的是==构建面向分析的集成化数据环境==，为企业提供==决策支持==（Decision Support）。它出于分析性报告和决策支持目的而创建。\n\n数据仓库本身并不“生产”任何数据，同时自身也不需要“消费”任何的数据，数据来源于外部，并且开放给外部应用，这也是为什么叫“仓库”，而不叫“工厂”的原因。\n\n### 定义\n\n数据仓库是==面向主题的==（Subject-Oriented ）、==集成的==（Integrated）、==稳定性的==（Non-Volatile）和==时变的==（Time-Variant ）数据集合，用以支持管理决策。 \n\n#### 面向主题\n\n数据仓库中的数据是按照一定的主题域进行组织。\n\n主题是一个抽象的概念，是指用户使用数据仓库进行决策时所关心的重点方面，一个主题通常与多个操作型信息系统相关。\n\n> 以电商为例：\n>\n> 用户主题：主要是用于分析用户的行为\n>\n> 商品主题：针对商品进行分析    指标：昨日新增商品，昨日下架商品 最近七天流量最高的哪些商品\n>\n> 财务主题：财务分析\n>\n> 订单主题：订单分析\n>\n> 货运主题：针对快递分析\n\n#### 集成性\n\n根据决策分析的要求，将分散于各处的源数据进行抽取、筛选、清理、综合等工作，最终集成到数据仓库中。\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/2019-08-17_17-04-09.png)\n\n#### 稳定性\n\n数据的相对稳定性，数据仓库中的数据只进行新增，没有更新操作、删除操作处理。\n\n反映历史变化，以查询分析为主。\n\n#### 时变性\n\n数据仓库的数据一般都带有时间属性，随着时间的推移而发生变化，不断地生成主题的新快照\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/2019-08-17_17-09-51.png)\n\n### 数据仓库与数据库的区别\n\n数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别。\n\n**==OLTP==**： On-Line Transaction Processing  叫==联机事务处理==， 也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理 .\n\n **==OLAP==**：On-Line Analytical Processing  叫==联机分析处理==，一般针对某些主题的历史数据进行分析，支持管理决策。\n\n简而言之，==数据库是面向事务的设计，数据仓库是面向主题设计的==。 \n\n数据库一般存储在线交易数据，有很高的事务要求；数据仓库存储的一般是历史数据。 \n\n数据库设计是尽量避免冗余，一般采用符合范式的规则来设计，数据仓库在设计是有意引入冗余，采用反范式的方式来设计。 \n\n数据库是==为捕获数据而设计==，数据仓库是==为分析数据而设计==，它的两个基本的元素是维表和事实表。维是看问题的角度，比如时间，部门，维表放的就是这些东西的定义，事实表里放着要查询的数据，同时有维的ID。\n\n| **功能** | **数据仓库**                           | **数据库**                             |\n| -------- | -------------------------------------- | -------------------------------------- |\n| 数据范围 | 存储历史的、完整的、反应历史变化的     | 当前状态数据                           |\n| 数据变化 | 可添加、无删除、无变更的、反应历史变化 | 支持频繁的增、删、改、查操作           |\n| 应用场景 | 面向分析、支持战略决策                 | 面向业务交易流程                       |\n| 设计理论 | 违范式、适当冗余                       | 遵照范式(第一、二、三等范式)、避免冗余 |\n| 处理量   | 非频繁、大批量、高吞吐、有延迟         | 频繁、小批次、高并发、低延迟           |\n\n\n\n### 构建数据仓库\n\n• 传统数仓建设更多的基于成熟的商业数据集成平台，比如Teradata、Oracle、Informatica等，技术体系比较成熟完善，但相对比较封闭，对实施者技术面要求也相对专业且单一，一般更多应用于银行、保险、电信等“有钱”行业.\n\n• 基于大数据的数仓建设一般是基于非商业、开源的技术，常见的是基于hadoop生态构建，涉及技术较广泛、复杂，同时相对于商业产品，稳定性、服务支撑较弱，需要自己维护更多的技术框架。在大数据领域，==常用的数据仓库构建手段很多基于hive，sparkSQL，impala等各种技术框架==.\n\n\n\n### 仓库分层\n\n#### 数据仓库分层描述\n\n* 数据仓库更多代表的是一种对数据的管理和使用的方式，它是一整套包括了etl、调度、建模在内的完整的理论体系。现在所谓的大数据更多的是一种数据量级的增大和工具的上的更新。 两者并无冲突，相反，而是一种更好的结合。数据仓库在构建过程中通常都需要进行分层处理。业务不同，分层的技术处理手段也不同。\n* 分层是数据仓库解决方案中，数据架构设计的一种数据逻辑结构 ，通过分层理念建立的数据仓库，它的可扩展性非常好，这样设计出来的模型架构，可以任意地增减、替换数据仓库中的各个组成部分。\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/数据仓库层.png)\n\n\n\n~~~\n从整体的逻辑划分来讲，数据仓库模型实际上就是这三层架构。\n\n接入层：底层的数据源或者是操作数据层，一般在公司的话，统一都是称为ODS层\n\n中间层：是做数据仓库同学需要花费更多精力的一层，这一层包括的内容是最多的、最复杂的。\n\n应用层：对不同的应用提供对应的数据。该层主要是提供数据产品和数据分析使用的数据，\n\t   比如我们经常说的报表数据\n\t\n~~~\n\n\n\n* 针对于这三层架构，这里给出比较典型的一个做数据仓库在实施的时候，具体的层次划分。\n\n![dw](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/dw.png)\n\n\n\n* ==ODS==：\n\n  * Operation Data Store 原始数据层\n\n* ==DWD==\n* data warehouse detail 数据明细层\n  \n* 它主要是针对于接入层的数据进行数据的清洗和转换。还有就是一些维度的补充。\n  \n* ==DWS==\n  * data warehouse summary 数据汇总层\n\n  * 它是在DWD明细层之上，也有公司叫DW层\n\n  * 它是按照一定的粒度进行了汇总聚合操作。它是单业务场景。\n\n* ==DWM==\n* data warehouse market 数据集市层\n  * 它是在DWS数据汇总层之上，集市层它是多业务场景的。\n  \n* ==APP==\n\n  - Application 应用层\n  - 这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用\n\n~~~\nTMP临时表：在做一些中间层表计算的时候，大量使用tmp临时表。\nDIM维度层：基于ODS层和DWD层抽象出一些公共的维度，\n\t\t  典型的公共维度主要包括城市信息、渠道信息、个人基础属性信息。\n~~~\n\n#### 为什么要进行数据仓库分层\n\n* 分层的主要原因是在管理数据的时候，能对数据有一个更加清晰的掌控，主要有下面几个原因：\n  * **空间换时间**\n    * 通过建设多层次的数据模型供用户使用，避免用户直接使用底层操作型数据，可以更高效的访问数据。\n  * **把复杂问题简单化**\n    * 讲一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。\n  * **便于处理业务的变化**\n    * 随着业务的变化，只需要调整底层的数据，对应用层对业务的调整零感知。\n\n\n\n###  数据仓库建模\n\n​\t目前业界较为流行的数据仓库的建模方法非常多，这里主要介绍==范式建模法==，==维度建模法==，==实体建模法==等几种方法，每种方法其实从本质上讲就是从不同的角度看我们业务中的问题，不管从技术层面还是业务层面，其实代表的是哲学上的一种世界观。\n\n\n\n#### 范式建模法（Third Normal Form 3NF）\n\n~~~\n范式建模法是基于整个关系型数据库的理论基础之上发展而来的，其实是我们在构建数据模型常用的一个方法，主要解决关系型数据库得数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。\n\n从其表达的含义来看，一个符合第三范式的关系必须具有以下三个条件 :\n\n（1）每个属性值唯一，不具有多义性 ;\n（2）每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;\n（3）每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。\n\n~~~\n\n#### 维度建模法\n\n~~~\n维度建模(dimensional modeling)是专门用于分析型数据库、数据仓库、数据集市建模的方法。维度建模法简单描述就是按照事实表、维度表来构建数仓、集市。\n维度建模从分析决策的需求出发构建模型，为分析需求服务，因此它重点关注用户如何更快速地完成需求分析，同时具有较好的大规模复杂查询的相应性能。\n\n~~~\n\n* 维度表\n\n~~~\n维度表示你要对数据进行分析时所用的一个量,比如你要分析产品销售情况, \n你可以选择按类别来进行分析,或按区域来分析。\n\n通常来说维度表信息比较固定，且数据量小\n~~~\n\n- 事实表\n\n```\n表示对分析主题的度量。\n事实表包含了与各维度表相关联的外键，并通过join方式与维度表关联。事实表的度量通常是数值类型，且记录数会不断增加，表规模迅速增长。\n\n消费事实表：Prod_id(引用商品维度表), TimeKey(引用时间维度表), Place_id(引用地点维度表), Unit(销售量)。\n```\n\n~~~\n总的说来，在数据仓库中不需要严格遵守规范化设计原则。因为数据仓库的主导功能就是面向分析，以查询为主，不涉及数据更新操作。事实表的设计是以能够正确记录历史信息为准则，维度表的设计是以能够以合适的角度来聚合主题内容为准则\n~~~\n\n##### 维度建模三种模式\n\n基于事实表和维表就可以构建出多种多维模型，包括星形模型、雪花模型和星座模型。\n\n维度建模法最被人广泛知晓的名字就是星型模式。\n\n* ==星型模式==\n\n  ~~~\n  星形模式(Star Schema)是最常用的维度建模方式。星型模式是以事实表为\n  中心，所有的维度表直接连接在事实表上，像星星一样。\n  星形模式的维度建模由一个事实表和一组维表成，且具有以下特点：\n  a. 维表只和事实表关联，维表之间没有关联；\n  b. 每个维表主键为单列，且该主键放置在事实表中，作为两边连接的外键；\n  c. 以事实表为核心，维表围绕核心呈星形分布；\n  \n  ~~~\n\n  ![星型模型](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/星型模型.png)\n\n\n\n* ==雪花模式==\n\n~~~\n雪花模式是对星形模式的扩展。雪花模式的维度表可以拥有其他维度表的，虽然这种模型相比星型更规范一些，但是由于这种模型不太容易理解，维护成本比较高，而且性能方面需要关联多层维表，性能也比星型模型要低。所以一般不是很常用。\n~~~\n\n![雪花模型](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/雪花模型.png)\n\n\n\n* ==星座模式==\n\n~~~\n星座模式是星型模式延伸而来，星型模式是基于一张事实表的，而星座模式是基于多张事实表的，而且共享维度信息。\n\n前面介绍的两种维度建模方法都是多维表对应单事实表，但在很多时候维度空间内的事实表不止一个，而一个维表也可能被多个事实表用到。在业务发展后期，绝大部分维度建模都采用的是星座模式。\n~~~\n\n![星座模型](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/星座模型.png)\n\n\n\n####  实体建模法\n\n~~~\n实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。\n\n从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。\n\n那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。\n~~~\n\n参考文档：<http://www.uml.org.cn/sjjmck/201810163.asp>\n\n### 数据仓库架构\n\n\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/数据仓库架构图.png)\n\n* ==数据采集==\n\n~~~\n数据采集层的任务就是把数据从各种数据源中采集和存储到数据存储上，期间有可能会做一些ETL操作。\n\n数据源种类可以有多种：\n日志：所占份额最大，存储在备份服务器上\n业务数据库：如Mysql、Oracle\n来自HTTP/FTP的数据：合作伙伴提供的接口\n其他数据源：如Excel等需要手工录入的数据\n~~~\n\n\n\n* ==数据存储与分析==\n\n~~~\nHDFS是大数据环境下数据仓库/数据平台最完美的数据存储解决方案。\n\n离线数据分析与计算，也就是对实时性要求不高的部分，Hive是不错的选择。\n使用Hadoop框架自然而然也提供了MapReduce接口，如果真的很乐意开发Java，或者对SQL不熟，那么也可以使用MapReduce来做分析与计算。\nSpark性能比MapReduce好很多，同时使用SparkSQL操作Hive。\n~~~\n\n\n\n* ==数据共享==\n\n~~~\n　　前面使用Hive、MR、Spark、SparkSQL分析和计算的结果，还是在HDFS上，但大多业务和应用不可能直接从HDFS上获取数据，那么就需要一个数据共享的地方，使得各业务和产品能方便的获取数据。\n\n　　这里的数据共享，其实指的是前面数据分析与计算后的结果存放的地方，其实就是关系型数据库和NOSQL数据库。\n~~~\n\n\n\n* ==数据应用==\n\n~~~\n报表：报表所使用的数据，一般也是已经统计汇总好的，存放于数据共享层。\n接口：接口的数据都是直接查询数据共享层即可得到。\n即席查询：即席查询通常是现有的报表和数据共享层的数据并不能满足需求，需要从数据存储层直接查询。一般都是通过直接操作SQL得到。\n~~~\n\n## 数据倾斜\n\n~~~\n由于数据分布不均匀，造成数据大量的集中到一点，造成数据热点\n~~~\n\n~~~\n在执行任务的时候，任务进度长时间维持在99%左右，查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。\n\n单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。最长时长远大于平均时长。\n~~~\n\n![450330742](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/450330742.png)\n\n### 原因\n\n~~~\n1)、key分布不均匀\n\n2)、业务数据本身的特性\n\n3)、建表时考虑不周\n\n4)、某些SQL语句本身就有数据倾斜\n~~~\n\n\n\n### 解决方案\n\n####  map端聚合\n\n~~~sql\n--Map 端部分聚合，相当于Combiner\nhive.map.aggr = true；\n--有数据倾斜的时候进行负载均衡\nhive.groupby.skewindata=true；\n\n--有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。\n~~~\n\n#### SQL语句调节\n\n* 如何Join\n\n  ~~~\n  关于驱动表的取，用join key分布最均匀的表作为驱动表\n  做好列裁剪和filter操作，以达到两表做join的时候，数据量相对变小的效果。\n  ~~~\n\n* 大小表Join\n\n  ~~~\n  使用map join让小的维度表（1000条以下的记录条数） 先进内存。在map端完成reduce.\n  ~~~\n\n* 大表Join大表\n\n  ~~~\n  把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。\n  ~~~\n\n* count distinct大量相同特殊值\n\n  ~~~\n  count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。\n  ~~~\n\n* group by维度过小\n\n  ~~~\n  采用sum() group by的方式来替换count(distinct)完成计算。\n  ~~~\n\n* 特殊情况特殊处理\n\n  ~~~\n  在业务逻辑优化效果的不大情况下，一些时候是可以将倾斜的数据单独拿出来处理。最后union回去\n  ~~~\n\n#### 业务场景\n\n* 空值产生的数据倾斜\n\n  * 场景\n\n    ~~~\n    如日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和 用户表中的user_id 关联，会碰到数据倾斜的问题。\n    ~~~\n\n  * 解决办法\n\n  ~~~sql\n  --user_id为空的不参与关联\n  \n  select * from log a\n    join users b\n    on a.user_id is not null\n    and a.user_id = b.user_id\n  union all\n  select * from log a\n    where a.user_id is null;\n    \n    \n    \n  --赋与空值分新的key值\n  select *\n    from log a\n    left outer join users b\n    on case when a.user_id is null then concat(‘hive’,rand()) else a.user_id end = b.user_id;\n    \n  ~~~\n\n* 不同数据类型关联产生数据倾斜\n\n  * 场景\n\n  ~~~\n  用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。\n  ~~~\n\n  * 解决办法\n\n    * 把数字类型转换成字符串类型\n\n    ~~~sql\n    select * from users a\n      left outer join logs b\n      on a.usr_id = cast(b.user_id as string);\n     \n    ~~~\n\n","tags":["hive","数据倾斜","数据仓库"]},{"title":"Hive存储格式及其调优","url":"/2019/01/05/it/hive/Hive存储格式及其调优/","content":"\n### 文件存储格式\n\nHive支持的存储数的格式主要有；TEXTFILE（行式存储） 、SEQUENCEFILE(行式存储)、ORC（列式存储）、PARQUET（列式存储）。\n\n#### 列式存储和行式存储\n\n![img](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/clip_image002.jpg)\n\n上图左边为逻辑表，右边第一个为行式存储，第二个为列式存储。\n\n**行存储的特点：** 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。select  *  \n\n**列存储的特点：** 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。  select   某些字段效率更高\n\nTEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；\n\nORC和PARQUET是基于列式存储的。\n\n- TEXTFILE格式\n  - 默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。\n\n- ORC格式\n  - Orc (Optimized Row Columnar)是hive 0.11版里引入的新的存储格式。\n  - 可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB->250MB，这样能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data,Row Data,Stripe Footer：\n\n![img](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/clip_image003.png)\n\n```javascript\n一个orc文件可以分为若干个Stripe\n一个stripe可以分为三个部分\nindexData：某些列的索引数据\nrowData :真正的数据存储\nStripFooter：stripe的元数据信息\n   \t1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引只是记录某行的各字段在Row Data中的offset。\n​    2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。\n​    3）Stripe Footer：存的是各个stripe的元数据信息\n每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。\n```\n\n- PARQUET格式\n  - Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。\n  - Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。\n  - 通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。\n\n![Parquet文件格式](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/clip_image005.jpg)\n\n- ​\t上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。\n\n\n#### 文件存储格式对比实验\n\n从存储文件的压缩比和查询速度两个角度对比。\n\n| 压缩格式           | 压缩后文件大小 | 查询速度（s） |\n| ------------------ | -------------- | ------------- |\n| stored as textfile | 18.1 M         | 21.54         |\n| stored as orc      | 2.8  M         | 20.867        |\n| stored as parquet  | 13.1 M         | 22.922        |\n\n### 存储和压缩结合\n\n官网：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC\n\nORC存储方式的压缩：\n\n| Key                      | Default    | Notes                                                        |\n| ------------------------ | ---------- | ------------------------------------------------------------ |\n| orc.compress             | ZLIB       | high level   compression (one of NONE, ZLIB, SNAPPY)         |\n| orc.compress.size        | 262,144    | number of bytes in   each compression chunk                  |\n| orc.stripe.size          | 67,108,864 | number of bytes in   each stripe                             |\n| orc.row.index.stride     | 10,000     | number of rows   between index entries (must be >= 1000)     |\n| orc.create.index         | true       | whether to create row   indexes                              |\n| orc.bloom.filter.columns | \"\"         | comma separated list of column names for which bloom filter   should be created |\n| orc.bloom.filter.fpp     | 0.05       | false positive probability for bloom filter (must >0.0 and   <1.0) |\n\n| 文件格式 | 压缩格式 | 最终文件 |\n| -------- | -------- | -------- |\n| orc      | 无       | 7.7 M    |\n| orc      | snappy   | 3.8 M    |\n\n### SerDe\n\n​\tSerde是 ==Serializer/Deserializer==的简写。hive使用Serde进行行对象的序列与反序列化。最后实现把文件内容映射到 hive 表中的字段数据类型。\n\n​\t为了更好的阐述使用 SerDe 的场景，我们需要了解一下 Hive 是如何读数据的(类似于 HDFS 中数据的读写操作)：\n\n```sql\nHDFS files –> InputFileFormat –> <key, value> –> Deserializer –> Row object\n\nRow object –> Serializer –> <key, value> –> OutputFileFormat –> HDFS files\n```\n\n#### SerDe 类型\n\n- Hive 中内置==org.apache.hadoop.hive.serde2== 库，内部封装了很多不同的SerDe类型。\n\n- 你可以创建表时使用用户**自定义的Serde或者native Serde**， **如果 ROW FORMAT没有指定或者指定了 ROW FORMAT DELIMITED就会使用native Serde**。\n- [Hive SerDes](https://cwiki.apache.org/confluence/display/Hive/SerDe): \n  - Avro (Hive 0.9.1 and later) \n  - ORC (Hive 0.11 and later) \n  - RegEx \n  - Thrift \n  - Parquet (Hive 0.13 and later) \n  - CSV (Hive 0.14 and later) \n  - MultiDelimitSerDe \n\n##### 多字符分割场景\n\n```sql\n1##xiaoming\n2##xiaowang\n3##xiaozhang\n\n-- MultiDelimitSerDe\ncreate  table kfly_mul (id String, name string)\nrow format serde 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe'\nWITH SERDEPROPERTIES (\"field.delim\"=\"##\");\n\n-- RegexSerDe 解决多字符分割场景\ncreate  table t2(id int, name string)\nrow format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' \nWITH SERDEPROPERTIES (\"input.regex\" = \"^(.*)\\\\#\\\\#(.*)$\");\n```\n\n### 调优\n\n#### Fetch抓取\n\n- Fetch抓取是指，==Hive中对某些情况的查询可以不必使用MapReduce计算==\n\n  - 例如：select * from score;\n  - 在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台\n\n- 在hive-default.xml.template文件中 ==hive.fetch.task.conversion默认是more==，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。\n\n- 案例实操\n\n  - \n\n  ```sql\n  -- 把 hive.fetch.task.conversion设置成**==none==**，然后执行查询语句，都会执行mapreduce程序\n  set hive.fetch.task.fen=none;\n  select * from score;\n  select s_id from score;\n  select s_id from score limit 3;\n  \n  -- 把hive.fetch.task.conversion设置成==**more**==，然后执行查询语句，如下查询方式都不会执行mapreduce程序。\n  set hive.fetch.task.conversion=more;\n  select * from score;\n  select s_id from score;\n  select s_id from score limit 3;\n  ```\n\n\n#### 本地模式\n\n- 在Hive客户端测试时，默认情况下是启用hadoop的job模式,把任务提交到集群中运行，这样会导致计算非常缓慢；\n\n- Hive可以通过本地模式在单台机器上处理任务。对于小数据集，执行时间可以明显被缩短。\n\n- 案例实操\n\n  ```sql\n  --开启本地模式，并执行查询语句\n  set hive.exec.mode.local.auto=true;  //开启本地mr\n  \n  --设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，\n  --默认为134217728，即128M\n  set hive.exec.mode.local.auto.inputbytes.max=50000000;\n  \n  --设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，\n  --默认为4\n  set hive.exec.mode.local.auto.input.files.max=5;\n  \n  \n  --执行查询的sql语句\n  select * from student cluster by s_id\n  ```\n\n#### 表的优化\n\n##### 小表、大表 join\n\n```sql\n-- 数据量小的表放在join的左边\n-- 可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。\nselect  count(distinct s_id)  from score;\nselect count(s_id) from score group by s_id; \n- -新版的hive已经对小表 join 大表和大表 join 小表进行了优化。小表放在左边和右边已经没有明显区别。\n-- 多个表关联时，最好分拆成小段，避免大sql（无法控制中间Job）\n```\n\n##### 大表 join 大表\n\n- 1．空 key 过滤\n\n  - 有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。\n\n  - 此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。\n\n  - 测试环境准备：\n\n    ```sql\n    use myhive;\n    create table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n    \n    create table nullidtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n    \n    create table jointable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n    \n    load data local inpath '/kkb/install/hivedatas/hive_big_table/*' into table ori; \n    load data local inpath '/kkb/install/hivedatas/hive_have_null_id/*' into table nullidtable;\n    \n    ```\n\n    过滤空key与不过滤空key的结果比较\n\n    ```sql\n    不过滤：\n    INSERT OVERWRITE TABLE jointable\n    SELECT a.* FROM nullidtable a JOIN ori b ON a.id = b.id;\n    结果：\n    No rows affected (152.135 seconds)\n    \n    过滤：\n    INSERT OVERWRITE TABLE jointable\n    SELECT a.* FROM (SELECT * FROM nullidtable WHERE id IS NOT NULL ) a JOIN ori b ON a.id = b.id;\n    结果：\n    No rows affected (141.585 seconds)\n    ```\n\n- 2、空 key 转换\n\n  - 有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在 join 的结果中，此时我们可以表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上。\n\n    不随机分布：\n\n    ```sql\n    set hive.exec.reducers.bytes.per.reducer=32123456;\n    set mapreduce.job.reduces=7;\n    INSERT OVERWRITE TABLE jointable\n    SELECT a.*\n    FROM nullidtable a\n    LEFT JOIN ori b ON CASE WHEN a.id IS NULL THEN 'hive' ELSE a.id END = b.id;\n    No rows affected (41.668 seconds)  \n    \n    ```\n\n    **结果：这样的后果就是所有为null值的id全部都变成了相同的字符串，及其容易造成数据的倾斜（所有的key相同，相同key的数据会到同一个reduce当中去）**\n\n    **为了解决这种情况，我们可以通过hive的rand函数，随记的给每一个为空的id赋上一个随机值，这样就不会造成数据倾斜**\t\t\n\n  ​\t\t随机分布：\n\n  ```sql\n  set hive.exec.reducers.bytes.per.reducer=32123456;\n  set mapreduce.job.reduces=7;\n  INSERT OVERWRITE TABLE jointable\n  SELECT a.*\n  FROM nullidtable a\n  LEFT JOIN ori b ON CASE WHEN a.id IS NULL THEN concat('hive', rand()) ELSE a.id END = b.id;\n  \n  No rows affected (42.594 seconds)              \n  ```\n\n##### map  join \n\n- 如果不指定MapJoin 或者不符合 MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在map端进行join，避免reducer处理。\n\n- 1、开启MapJoin参数设置\n\n  ```sql\n   --默认为true\n  set hive.auto.convert.join = true;\n  ```\n\n- 2、大表小表的阈值设置（默认25M一下认为是小表）\n\n```sql\nset hive.mapjoin.smalltable.filesize=26214400;\n\n```\n\n- 3、MapJoin工作机制\n\n![xxx](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/xxx-1570506631515.jpg)\n\n首先是Task A，它是一个Local Task（在客户端本地执行的Task），负责扫描小表b的数据，将其转换成一个HashTable的数据结构，并写入本地的文件中，之后将该文件加载到DistributeCache中。\n\n接下来是Task B，该任务是一个没有Reduce的MR，启动MapTasks扫描大表a,在Map阶段，根据a的每一条记录去和DistributeCache中b表对应的HashTable关联，并直接输出结果。\n\n由于MapJoin没有Reduce，所以由Map直接输出结果文件，有多少个Map Task，就有多少个结果文件。\n\n**案例实操：**\n\n（1）开启Mapjoin功能\n\n```sql\nset hive.auto.convert.join = true; 默认为true\n```\n\n（2）执行小表JOIN大表语句\n\n```sql\nINSERT OVERWRITE TABLE jointable2\nSELECT b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nFROM smalltable s\nJOIN bigtable  b\nON s.id = b.id;\n\nTime taken: 31.814 seconds\n```\n\n（3）执行大表JOIN小表语句\n\n```shell\nINSERT OVERWRITE TABLE jointable2\nSELECT b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nFROM bigtable  b\nJOIN smalltable  s\nON s.id = b.id;\n\nTime taken: 28.46 seconds\n```\n\n##### group By\n\n- 默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。\n\n- 并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。\n\n- 开启Map端聚合参数设置\n\n  ```sql\n  --是否在Map端进行聚合，默认为True\n  set hive.map.aggr = true;\n  --在Map端进行聚合操作的条目数目\n  set hive.groupby.mapaggr.checkinterval = 100000;\n  --有数据倾斜的时候进行负载均衡（默认是false）\n  set hive.groupby.skewindata = true;\n  \n  当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。\n  ```\n\n\n\n#####  count(distinct) \n\n- 数据量小的时候无所谓，数据量大的情况下，由于count distinct 操作需要用一个reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般count distinct使用先group by 再count的方式替换\n\n  环境准备：\n\n  \n\n  ```sql\n  create table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n  \n  load data local inpath '/kkb/install/hivedatas/data/100万条大表数据（id除以10取整）/bigtable' into table bigtable;\n  \n  \n  --每个reduce任务处理的数据量 默认256000000（256M）\n   set hive.exec.reducers.bytes.per.reducer=32123456;\n   \n   select  count(distinct ip )  from log_text;\n   \n   转换成\n   set hive.exec.reducers.bytes.per.reducer=32123456;\n   select count(ip) from (select ip from log_text group by ip) t;\n   \n   \n   虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。\n  ```\n\n#####  笛卡尔积\n\n- 尽量避免笛卡尔积，即避免join的时候不加on条件，或者无效的on条件\n- Hive只能使用1个reducer来完成笛卡尔积。\n\n#### 分区剪裁、列剪裁\n\n- 尽可能早地过滤掉尽可能多的数据量，避免大量数据流入外层SQL。\n- **列剪裁**\n  - 只获取需要的列的数据，减少数据输入。\n- **分区裁剪**\n  - 分区在hive实质上是目录，分区裁剪可以方便直接地过滤掉大部分数据。\n  - 尽量使用分区过滤，少用select  *\n\n​\t环境准备：\n\n```sql\ncreate table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\ncreate table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\nload data local inpath '/home/admin/softwares/data/加递增id的原始数据/ori' into table ori;\n\nload data local inpath '/home/admin/softwares/data/100万条大表数据（id除以10取整）/bigtable' into table bigtable;\n\n```\n\n先关联再Where：\n\n```sql\nSELECT a.id\nFROM bigtable a\nLEFT JOIN ori b ON a.id = b.id\nWHERE b.id <= 10;\n\n```\n\n正确的写法是写在ON后面：先Where再关联\n\n```sql\nSELECT a.id\nFROM ori a\nLEFT JOIN bigtable b ON (a.id <= 10 AND a.id = b.id);\n\n```\n\n或者直接写成子查询：\n\n```sql\nSELECT a.id\nFROM bigtable a\nRIGHT JOIN (SELECT id\nFROM ori\nWHERE id <= 10\n) b ON a.id = b.id;\n\n```\n\n\n\n#### 并行执行\n\n- 把一个sql语句中没有相互依赖的阶段并行去运行。提高集群资源利用率\n\n```sql\n--开启并行执行\nset hive.exec.parallel=true;\n--同一个sql允许最大并行度，默认为8。\nset hive.exec.parallel.thread.number=16;\n```\n\n#### 严格模式\n\n- Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。\n\n- 通过设置属性hive.mapred.mode值为默认是非严格模式**nonstrict** 。开启严格模式需要修改hive.mapred.mode值为**strict**，开启严格模式可以禁止3种类型的查询。\n\n  ```sql\n  --设置非严格模式（默认）\n  set hive.mapred.mode=nonstrict;\n  \n  --设置严格模式\n  set hive.mapred.mode=strict;\n  ```\n\n\n\n- （1）对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行\n\n  ```sql\n  --设置严格模式下 执行sql语句报错； 非严格模式下是可以的\n  select * from order_partition；\n  \n  异常信息：Error: Error while compiling statement: FAILED: SemanticException [Error 10041]: No partition predicate found for Alias \"order_partition\" Table \"order_partition\" \n  ```\n\n- （2）对于使用了order by语句的查询，要求必须使用limit语句\n\n  ```sql\n  --设置严格模式下 执行sql语句报错； 非严格模式下是可以的\n  select * from order_partition where month='2019-03' order by order_price; \n  \n  异常信息：Error: Error while compiling statement: FAILED: SemanticException 1:61 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'order_price'\n  ```\n\n- （3）限制笛卡尔积的查询\n\n  - 严格模式下，避免出现笛卡尔积的查询\n\n\n\n#### JVM重用\n\n- JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。\n\n  Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。\n\n  ```xml\n  <property>\n    <name>mapreduce.job.jvm.numtasks</name>\n    <value>10</value>\n    <description>How many tasks to run per jvm. If set to -1, there is\n    no limit. \n    </description>\n  </property>\n  \n  ```\n\n  我们也可以在hive当中通过\n\n  ```sql\n   set  mapred.job.reuse.jvm.num.tasks=10;\n  ```\n\n  这个设置来设置我们的jvm重用\n\n  这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。\n\n#### 推测执行\n\n- 在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。\n\n  设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置\n\n```xml\n<property>\n  <name>mapreduce.map.speculative</name>\n  <value>true</value>\n  <description>If true, then multiple instances of some map tasks \n               may be executed in parallel.</description>\n</property>\n\n<property>\n  <name>mapreduce.reduce.speculative</name>\n  <value>true</value>\n  <description>If true, then multiple instances of some reduce tasks \n               may be executed in parallel.</description>\n</property>\n\n```\n\n不过hive本身也提供了配置项来控制reduce-side的推测执行：\n\n```xml\n  <property>\n    <name>hive.mapred.reduce.tasks.speculative.execution</name>\n    <value>true</value>\n    <description>Whether speculative execution for reducers should be turned on. </description>\n  </property>\n\n```\n\n关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。\n\n\n\n#### 压缩\n\n​\t参见数据的压缩\n\n- Hive表中间数据压缩\n\n  ```shell\n  #设置为true为激活中间数据压缩功能，默认是false，没有开启\n  set hive.exec.compress.intermediate=true;\n  #设置中间数据的压缩算法\n  set mapred.map.output.compression.codec= org.apache.hadoop.io.compress.SnappyCodec;\n  \n  ```\n\n- Hive表最终输出结果压缩\n\n  ```shell\n  set hive.exec.compress.output=true;\n  set mapred.output.compression.codec= \n  org.apache.hadoop.io.compress.SnappyCodec;\n  ```\n\n\n\n#### 数据倾斜\n\n##### 1 合理设置Map数\n\n- 1)  通常情况下，作业会通过input的目录产生一个或者多个map任务。\n\n  ```sql\n  主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。\n  \n  举例：\n  a)  假设input目录下有1个文件a，大小为780M，那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数。\n  b) 假设input目录下有3个文件a，b，c大小分别为10m，20m，150m，那么hadoop会分隔成4个块（10m，20m，128m，22m），从而产生4个map数。即，如果文件大于块大小(128m)，那么会拆分，如果小于块大小，则把该文件当成一个块。\n  \n  ```\n\n- 2） 是不是map数越多越好？\n\n  ```shell\n    答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。\n  ```\n\n- 3） 是不是保证每个map处理接近128m的文件块，就高枕无忧了？\n\n  ```shell\n  答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。\n  \n  针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；\n  \n  ```\n\n##### 小文件合并\n\n- 在map执行前合并小文件，减少map数：\n\n- CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）\n\n  ```sql\n  set mapred.max.split.size=112345600;\n  set mapred.min.split.size.per.node=112345600;\n  set mapred.min.split.size.per.rack=112345600;\n  set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n  \n  ```\n\n  这个参数表示执行前进行小文件合并，前面三个参数确定合并文件块的大小，大于文件块大小128m的，按照128m来分隔，小于128m，大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），进行合并。\n\n##### 复杂文件增加Map数\n\n- 当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。\n\n- 增加map的方法为\n\n  - 根据 ==computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))==公式\n  - ==调整maxSize最大值==。让maxSize最大值低于blocksize就可以增加map的个数。\n\n  ```shell\n  mapreduce.input.fileinputformat.split.minsize=1 默认值为1\n  \n  mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue 默认值Long.MAXValue因此，默认情况下，切片大小=blocksize \n  \n  maxsize（切片最大值): 参数如果调到比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。\n  \n  minsize(切片最小值): 参数调的比blockSize大，则可以让切片变得比blocksize还大。\n  \n  ```\n\n  - 例如\n\n  ```sql\n  --设置maxsize大小为10M，也就是说一个fileSplit的大小为10M\n  set mapreduce.input.fileinputformat.split.maxsize=10485760;\n  ```\n\n\n\n#####  合理设置Reduce数\n\n- 1、调整reduce个数方法一\n\n  - 1）每个Reduce处理的数据量默认是256MB\n\n    ```sql\n    set hive.exec.reducers.bytes.per.reducer=256000000;\n    ```\n\n  - 2) 每个任务最大的reduce数，默认为1009\n\n    ```sql\n    set hive.exec.reducers.max=1009;\n    ```\n\n  - 3) 计算reducer数的公式\n\n    ```shell\n    N=min(参数2，总输入数据量/参数1)\n    ```\n\n- 2、调整reduce个数方法二\n\n  ```sql\n  --设置每一个job中reduce个数\n  set mapreduce.job.reduces=3;\n  ```\n\n\n\n- 3、reduce个数并不是越多越好\n- 过多的启动和初始化reduce也会消耗时间和资源；\n  \n- 同时过多的reduce会生成很多个文件，也有可能出现小文件问题\n\n","tags":["hive","调优"]},{"title":"zookeeper实现hadoop高可用环境","url":"/2018/12/28/it/zookeeper实现hadoop高可用环境/","content":"\n# zookeeper实现hadoop高可用环境\n\n>  说明：\n>\n>  - 集群共5个节点，主机名分别是node01、node02、node03、node04、node05\n>\n>  - 初始启动集群\n>    - node01上运行active namenode即主namenode；node02上运行standby namenode即从namenode\n>    - node04上运行主resourcemanager；node05上运行从resourcemanager\n\n- 每个节点运行的进程如下表\n\n| 机器名 | 运行进程                                                    |\n| ------ | ----------------------------------------------------------- |\n| node01 | NameNode/zkfc/Zookeeper/Journalnode/DataNode/NodeManager    |\n| node02 | NameNode/zkfc/Zookeeper/Journalnode/DataNode/NodeManager    |\n| node03 | Zookeeper/Journalnode/DataNode/NodeManager/JobHistoryServer |\n| node04 | ResourceManager                                             |\n| node05 | ResourceManager                                             |\n\n\n\n# Hadoop HA搭建\n\n## 1. 虚拟机环境准备\n\n- 准备**5台**虚拟机\n- 在做五节点hadoop HA集群搭建之前，要求先完成**每台**虚拟机的**基本环境准备**\n  - 每个节点都要做好“在node01上开始解压hadoop的tar.gz包之前的环境配置”\n  - 主要包括如下步骤（三节点Hadoop集群搭建时已讲解过，不再赘述）\n    - windows|mac安装VMWare虚拟化软件\n    - VMWare下安装CenoOS7\n    - 虚拟机关闭防火墙\n    - 禁用selinux\n    - 配置虚拟网卡\n    - 配置虚拟机网络\n    - 安装JDK\n    - 配置时间同步\n    - 修改主机名\n    - 修改ip地址\n    - 修改/etc/hosts\n    - 各节点免密钥登陆\n    - 重启虚拟机\n\n\n\n## 2. 安装ZooKeeper集群\n\n> Hadoop高可用集群需要使用ZooKeeper集群做分布式协调；所以先安装ZooKeeper集群\n\n- 在node01、node02、node03上安装ZooKeeper集群（详见三节点ZooKeeper集群搭建，不再赘述）\n\n\n\n## 3. 五节点Hadoop HA搭建\n\n> **注意：**\n>\n> ①3.1到3.8在**node01**上操作\n>\n> ②**此文档使用<font color=red>普通用户</font>操作，如hadoop**\n>\n> ③**hadoop安装到用户主目录下，如/kfly/install**\n>\n> <font color=red>**请根据自己的实际情况修改**</font>\n\n\n\n### 3.1 解压hadoop压缩包\n\n- hadoop压缩包hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz上传到node01的/kfly/soft路径中\n\n- 解压hadoop压缩包到/kfly/install\n\n```shell\n#解压hadoop压缩包到/kfly/install\n[hadoop@node01 ~]$ cd\n[hadoop@node01 ~]$ cd /kfly/soft/\n[hadoop@node01 soft]$ tar -xzvf hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz -C /kfly/install/\n```\n\n\n### 3.2 修改hadoop-env.sh\n\n- 进入hadoop配置文件路径$HADOOP_HOME/etc/hadoop\n\n```shell\n[hadoop@node01 soft]$ cd /kfly/install/hadoop-2.6.0-cdh5.14.2/\n[hadoop@node01 hadoop-2.6.0-cdh5.14.2]$ cd etc/hadoop/\n```\n\n- 修改hadoop-env.sh，修改JAVA_HOME值为jdk解压路径；保存退出\n\n```shell\nexport JAVA_HOME=/kfly/install/jdk1.8.0_141\n```\n\n> 注意：JAVA_HOME值修改为<font color=red>**自己jdk的实际目录**</font>\n\n### 3.3 修改core-site.xml\n\n> **注意：**\n>\n> **情况一：值/kfly/install/hadoop-2.6.0-cdh5.14.2/tmp根据实际情况修改**\n>\n> **情况二：值node01:2181,node02:2181,node03:2181根据实际情况修改，修改成安装了zookeeper的虚拟机的主机名**\n\n```xml\n<configuration>\n\t<!-- 指定hdfs的nameservice id为ns1 -->\n\t<property>\n\t\t<name>fs.defaultFS</name>\n\t\t<value>hdfs://ns1</value>\n\t</property>\n\t<!-- 指定hadoop临时文件存储的基目录 -->\n\t<property>\n\t\t<name>hadoop.tmp.dir</name>\n\t\t<value>/kfly/install/hadoop-2.6.0-cdh5.14.2/tmp</value>\n\t</property>\n\t<!-- 指定zookeeper地址，ZKFailoverController使用 -->\n\t<property>\n\t\t<name>ha.zookeeper.quorum</name>\n\t\t<value>node01:2181,node02:2181,node03:2181</value>\n\t</property>\n</configuration>\n```\n\n### 3.4 修改hdfs-site.xml\n\n> **注意：**\n>\n> **情况一：属性值qjournal://node01:8485;node02:8485;node03:8485/ns1中的主机名，修改成实际安装zookeeper的虚拟机的主机名**\n>\n> **情况二：属性值/kfly/install/hadoop-2.6.0-cdh5.14.2/journal中”/kfly/install/hadoop-2.6.0-cdh5.14.2”替换成实际hadoop文件夹的路径**\n>\n> **情况三：属性值/home/hadoop/.ssh/id_rsa中/home/hadoop根据实际情况替换**\n\n```xml\n<configuration>\n\t<!--指定hdfs的nameservice列表，多个之前逗号分隔；此处只有一个ns1，需要和core-site.xml中的保持一致 -->\n\t<property>\n\t\t<name>dfs.nameservices</name>\n\t\t<value>ns1</value>\n\t</property>\n\t<!-- ns1下面有两个NameNode，分别是nn1，nn2 -->\n\t<property>\n\t\t<name>dfs.ha.namenodes.ns1</name>\n\t\t<value>nn1,nn2</value>\n\t</property>\n\t<!-- nn1的RPC通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.ns1.nn1</name>\n\t\t<value>node01:8020</value>\n\t</property>\n\t<!-- nn1的http通信地址,web访问地址 -->\n\t<property>\n\t\t<name>dfs.namenode.http-address.ns1.nn1</name>\n\t\t<value>node01:50070</value>\n\t</property>\n\t<!-- nn2的RPC通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.ns1.nn2</name>\n\t\t<value>node02:8020</value>\n\t</property>\n\t<!-- nn2的http通信地址,web访问地址 -->\n\t<property>\n\t\t<name>dfs.namenode.http-address.ns1.nn2</name>\n\t\t<value>node02:50070</value>\n\t</property>\n\t<!-- 指定NameNode的元数据在JournalNode上的存放位置 -->\n\t<property>\n\t\t<name>dfs.namenode.shared.edits.dir</name>\n\t\t<value>qjournal://node01:8485;node02:8485;node03:8485/ns1</value>\n\t</property>\n\t<!-- 指定JournalNode在本地磁盘存放数据的位置 -->\n\t<property>\n\t\t<name>dfs.journalnode.edits.dir</name>\n\t\t<value>/kfly/install/hadoop-2.6.0-cdh5.14.2/journal</value>\n\t</property>\n\t<!-- 开启NameNode失败自动切换 -->\n\t<property>\n\t\t<name>dfs.ha.automatic-failover.enabled</name>\n\t\t<value>true</value>\n\t</property>\n\t<!-- 此类决定哪个namenode是active，切换active和standby -->\n\t<property>\n\t\t<name>dfs.client.failover.proxy.provider.ns1</name>\n\t\t<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n\t</property>\n\t<!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行-->\n\t<property>\n\t\t<name>dfs.ha.fencing.methods</name>\n\t\t<value>\n\t\tsshfence\n\t\tshell(/bin/true)\n\t\t</value>\n\t</property>\n\t<!-- 使用sshfence隔离机制时需要ssh免密登陆到目标机器 -->\n\t<property>\n\t\t<name>dfs.ha.fencing.ssh.private-key-files</name>\n\t\t<value>/home/hadoop/.ssh/id_rsa</value>\n\t</property>\n\t<!-- 配置sshfence隔离机制超时时间 -->\n\t<property>\n\t\t<name>dfs.ha.fencing.ssh.connect-timeout</name>\n\t\t<value>30000</value>\n\t</property>\n</configuration>\n```\n\n### 3.5 修改mapred-site.xml\n\n- 重命名文件\n\n```shell\n[hadoop@node01 hadoop]$ mv mapred-site.xml.template mapred-site.xml\n```\n\n- 修改mapred-site.xml\n\n```xml\n<configuration>\n\t<!-- 指定运行mr job的运行时框架为yarn -->\n\t<property>\n\t\t<name>mapreduce.framework.name</name>\n\t\t<value>yarn</value>\n\t</property>\n    <!-- MapReduce JobHistory Server IPC host:port -->\n\t<property>\n\t\t<name>mapreduce.jobhistory.address</name>\n\t\t<value>node03:10020</value>\n\t</property>\n\t<!-- MapReduce JobHistory Server Web UI host:port -->\n\t<property>\n\t\t<name>mapreduce.jobhistory.webapp.address</name>\n\t\t<value>node03:19888</value>\n\t</property>\n</configuration>\n```\n\n### 3.6 修改yarn-site.xml\n\n> **注意：**\n>\n> **情况一：属性yarn.resourcemanager.hostname.rm1的值node04根据实际情况替换**\n>\n> **情况二：属性yarn.resourcemanager.hostname.rm2的值node05根据实际情况替换**\n>\n> **情况三：属性值node01:2181,node02:2181,node03:2181根据实际情况替换；替换成实际安装zookeeper的虚拟机的主机名**\n\n```xml\n<configuration>\n    <!-- 是否启用日志聚合.应用程序完成后,日志汇总收集每个容器的日志,这些日志移动到文件系统,例如HDFS. -->\n\t<!-- 用户可以通过配置\"yarn.nodemanager.remote-app-log-dir\"、\"yarn.nodemanager.remote-app-log-dir-suffix\"来确定日志移动到的位置 -->\n\t<!-- 用户可以通过应用程序时间服务器访问日志 -->\n\t<!-- 启用日志聚合功能，应用程序完成后，收集各个节点的日志到一起便于查看 -->\n\t<property>\n\t\t\t<name>yarn.log-aggregation-enable</name>\n\t\t\t<value>true</value>\n\t</property>\n\t<!-- 开启RM高可靠 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.ha.enabled</name>\n\t\t<value>true</value>\n\t</property>\n\t<!-- 指定RM的cluster id为yrc，意为yarn cluster -->\n\t<property>\n\t\t<name>yarn.resourcemanager.cluster-id</name>\n\t\t<value>yrc</value>\n\t</property>\n\t<!-- 指定RM的名字 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.ha.rm-ids</name>\n\t\t<value>rm1,rm2</value>\n\t</property>\n\t<!-- 指定第一个RM的地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.hostname.rm1</name>\n\t\t<value>node04</value>\n\t</property>\n    <!-- 指定第二个RM的地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.hostname.rm2</name>\n\t\t<value>node05</value>\n\t</property>\n    <!-- 配置第一台机器的resourceManager通信地址 -->\n\t<!--客户端通过该地址向RM提交对应用程序操作-->\n\t<property>\n\t\t<name>yarn.resourcemanager.address.rm1</name>\n\t\t<value>node04:8032</value>\n\t</property>\n\t<!--向RM调度资源地址--> \n\t<property>\n\t\t<name>yarn.resourcemanager.scheduler.address.rm1</name>\n\t\t<value>node04:8030</value>\n\t</property>\n\t<!--NodeManager通过该地址交换信息-->\n\t<property>\n\t\t<name>yarn.resourcemanager.resource-tracker.address.rm1</name>\n\t\t<value>node04:8031</value>\n\t</property>\n\t<!--管理员通过该地址向RM发送管理命令-->\n\t<property>\n\t\t<name>yarn.resourcemanager.admin.address.rm1</name>\n\t\t<value>node04:8033</value>\n\t</property>\n\t<!--RM HTTP访问地址,查看集群信息-->\n\t<property>\n\t\t<name>yarn.resourcemanager.webapp.address.rm1</name>\n\t\t<value>node04:8088</value>\n\t</property>\n\t<!-- 配置第二台机器的resourceManager通信地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.address.rm2</name>\n\t\t<value>node05:8032</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.scheduler.address.rm2</name>\n\t\t<value>node05:8030</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.resource-tracker.address.rm2</name>\n\t\t<value>node05:8031</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.admin.address.rm2</name>\n\t\t<value>node05:8033</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.webapp.address.rm2</name>\n\t\t<value>node05:8088</value>\n\t</property>\n    <!--开启resourcemanager自动恢复功能-->\n\t<property>\n\t\t<name>yarn.resourcemanager.recovery.enabled</name>\n\t\t<value>true</value>\n\t</property>\t\n    <!--在node4上配置rm1,在node5上配置rm2,注意：一般都喜欢把配置好的文件远程复制到其它机器上，但这个在YARN的另一个机器上一定要修改，其他机器上不配置此项-->\n\t<!--\n    <property>       \n\t\t<name>yarn.resourcemanager.ha.id</name>\n\t\t<value>rm1</value>\n\t   <description>If we want to launch more than one RM in single node, we need this configuration</description>\n\t</property>\n\t-->\n\t<!--用于持久存储的类。尝试开启-->\n\t<property>\n\t\t<name>yarn.resourcemanager.store.class</name>\n\t\t<!-- 基于zookeeper的实现 -->\n\t\t<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\n\t</property>\n    <!-- 单个任务可申请最少内存，默认1024MB -->\n\t<property>\n\t\t<name>yarn.scheduler.minimum-allocation-mb</name>\n\t\t<value>512</value>\n\t</property>\n\t<!--多长时间聚合删除一次日志 此处-->\n\t<property>\n\t\t<name>yarn.log-aggregation.retain-seconds</name>\n\t\t<value>2592000</value><!--30 day-->\n\t</property>\n\t<!--时间在几秒钟内保留用户日志。只适用于如果日志聚合是禁用的-->\n\t<property>\n\t\t<name>yarn.nodemanager.log.retain-seconds</name>\n\t\t<value>604800</value><!--7 day-->\n\t</property>\n\t<!-- 指定zk集群地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.zk-address</name>\n\t\t<value>node01:2181,node02:2181,node03:2181</value>\n\t</property>\n    <!-- 逗号隔开的服务列表，列表名称应该只包含a-zA-Z0-9_,不能以数字开始-->\n\t<property>\n\t\t<name>yarn.nodemanager.aux-services</name>\n\t\t<value>mapreduce_shuffle</value>\n\t</property>\n</configuration>\n```\n\n### 3.7 修改slaves\n\n> node01、node02、node03上运行了datanode、nodemanager，所以修改slaves内容**替换**为：\n\n```shell\nnode01\nnode02\nnode03\n\n```\n\n### 3.8 远程拷贝hadoop文件夹\n\n> 拷贝到node02~node05\n\n```shell\n[hadoop@node01 hadoop]$ scp -r /kfly/install/hadoop-2.6.0-cdh5.14.2/ node02:/kfly/install/\n[hadoop@node01 hadoop]$ scp -r /kfly/install/hadoop-2.6.0-cdh5.14.2/ node03:/kfly/install/\n[hadoop@node01 hadoop]$ scp -r /kfly/install/hadoop-2.6.0-cdh5.14.2/ node04:/kfly/install/\n[hadoop@node01 hadoop]$ scp -r /kfly/install/hadoop-2.6.0-cdh5.14.2/ node05:/kfly/install/\n\n```\n\n### 3.9 修改两个RM的yarn-site.xml\n\n- 在**node04**上，找到属性`yarn.resourcemanager.ha.id`去除注释①、②\n\n```shell\n[hadoop@node04 ~]$ cd /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop\n[hadoop@node04 hadoop]$ vim yarn-site.xml \n\n```\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909232016.png)\n\n- 在**node05**上\n  - 找到属性`yarn.resourcemanager.ha.id`去除注释**①、②**\n  - **③**修改成rm2\n\n```shell\n[hadoop@node05 ~]$ cd /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/\n[hadoop@node05 hadoop]$ vim yarn-site.xml\n\n```\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909232022.png)\n\n- 修改后，结果如下\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909232024.png)\n\n### 3.10 配置环境变量\n\n- **node01到node05<font color='red'>五个节点都配置环境变量</font>**\n\n```shell\n#将hadoop添加到环境变量中\nvim /etc/profile\n\n```\n\n- 添加内容如下（注意：若HADOOP_HOME已经存在，则修改）：\n\n```shell\nexport HADOOP_HOME=/kfly/install/hadoop-2.6.0-cdh5.14.2/\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n\n```\n\n- 编译文件，使新增环境变量生效\n\n```shell\nsource /etc/profile\n\n```\n\n## 4. 启动与初始化hadoop集群\n\n>  **注意：**严格按照下面的步骤 先检查各台hadoop环境变量是否设置好\n\n### 4.1 启动zookeeper集群\n\n>  注意：根据zookeeper实际安装情况，启动zookeeper\n\n分别在node01、node02、node03上启动zookeeper\n\n```shell\nzkServer.sh start\n\n```\n\n#查看状态：一个为leader，另外两个为follower\n\n```shell\nzkServer.sh status\n\n```\n\n### 4.2 启动HDFS\n\n#### 4.2.1 格式化ZK\n\n> 在**node01**上执行即可\n>\n> - 集群有两个namenode，分别在node01、node02上\n>\n> - 每个namenode对应一个zkfc进程；\n>\n> - 在主namenode node01上格式化zkfc\n\n```shell\nhdfs zkfc -formatZK\n\n```\n\n#### 4.2.2 启动journalnode\n\n- 在**node01**上执行\n  - 会启动node01、node02、node03上的journalnode\n  - 因为使用的是hadoop-daemon**s**.sh\n\n```shell\nhadoop-daemons.sh start journalnode\n\n```\n\n- 运行jps命令检验，node01、node02、node03上多了JournalNode进程\n\n####  4.2.3 格式化HDFS\n\n- 在node01上执行\n- 根据集群规划node01、node02上运行namenode；所以<font color='red'>**只在主namenode节点**</font>即node01上执行命令:\n  - 此命令慎用；只在集群搭建（初始化）时使用一次；\n  - 一旦再次使用，会将HDFS上之前的数据格式化删除掉\n\n```shell\nhdfs namenode -format\n\n```\n\n- 下图表示格式化成功\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909241056.png)\n\n#### 4.2.4 初始化元数据、启动主NN\n\n- node01上执行（主namenode）\n\n```shell\nhdfs namenode -initializeSharedEdits -force\n#启动HDFS\nstart-dfs.sh\n\n```\n\n#### 4.2.5 同步元数据信息、启动从NN\n\n- **node02**上执行（从namenode）\n- 同步元数据信息，并且设置node02上namenode为standBy状态\n\n```shell\nhdfs namenode -bootstrapStandby\nhadoop-daemon.sh start namenode\n\n```\n\n#### 4.2.5 JPS查看进程\n\n- node01上\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909241118.png)\n\n- node02上\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909241119.png)\n\n- node03上\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909241120.png)\n\n### 4.3 启动YARN\n\n#### 4.6.1 **主resourcemanager**\n\n- **node04**上执行（**<font color='red'>主resourcemanager</font>**）\n  - 把namenode和resourcemanager部署在不同节点，是因为性能问题，因为他们都要占用大量资源\n  - <font color='red'>坑</font>：在node04上启动yarn之前，先依次从node04 ssh远程连接到node01、node02、node03、node04、node05；因为初次ssh时，需要交互，输入yes，回车\n\n```shell\nstart-yarn.sh\n\n```\n\n#### 4.6.2 从resourcemanager\n\n- 在<font color='red'>从resourcemanager</font>即**node05**上启动rm\n\n```shell\nyarn-daemon.sh start resourcemanager\n\n```\n\n#### 4.6.3 查看resourceManager状态\n\n- node04上，它的resourcemanager的Id是rm1\n\n```shell\nyarn rmadmin -getServiceState rm1\n\n```\n\n- node05上，它的resourcemanager的Id是rm2\n\n```shell\nyarn rmadmin -getServiceState rm2\n\n```\n\n### 4.4 启动JobHistory\n\n- **node03**上执行\n\n```shell\nmr-jobhistory-daemon.sh start historyserver\n\n```\n\n\n\n## 5. 验证集群是否可用\n\n### 5.1 验证HDFS HA\n\n#### 5.1.1 访问WEB UI\n\n> node01、node02一主一备\n\n```html\nhttp://node01:50070\n\n```\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271415.png)\n\n```\nhttp://node02:50070\n\n```\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271416.png)\n\n#### 5.1.2 模拟主备切换\n\n- 在主namenode节点，运行\n\n```shell\nhadoop-daemon.sh stop namenode\n\n```\n\n- 访问之前为\"备namenode\"的WEB UI；发现状态更新为active\n\n- 或者使用命令查看状态\n\n```shell\nhdfs haadmin -getServiceState nn2\n\n```\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271417.png)\n\n- 启动刚才手动停掉的namenode\n\n```shell\nhadoop-daemon.sh start namenode\n\n```\n\n- 访问它的WEB UI，发现状态更新为standby\n\n- 或者使用命令查看状态\n\n```\nhdfs haadmin -getServiceState nn1\n\n```\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271419.png)\n\n### 5.2 验证Yarn HA\n\n> node04、node05主备切换\n\n#### 5.2.1 访问WEB UI\n\n- node04浏览器访问\n\n```\nhttp://node04:8088/cluster/cluster\n\n```\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271519.png)\n\n- node05浏览器访问\n\n```\nhttp://node05:8088/cluster/cluster\n\n```\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271520.png)\n\n#### 5.2.2 模拟主备切换\n\n- 在主resourcemanager节点，运行\n\n```shell\nyarn-daemon.sh stop resourcemanager\n\n```\n\n- 访问之前为\"备resourcemanager\"的WEB UI；发现状态更新为active\n\n- 或者命令查看状态\n\n```shell\nyarn rmadmin -getServiceState rm2\n```\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271524.png)\n\n- 启动刚才手动停掉的resourcemanager\n\n```shell\nyarn-daemon.sh start resourcemanager\n```\n\n- 访问它的WEB UI，发现状态更新为standby\n\n- 或者命令查看状态\n\n```shell\nyarn rmadmin -getServiceState rm1\n```\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271526.png)\n\n#### 5.2.3 运行MR示例\n\n- 运行一下hadoop示例中的WordCount程序：\n\n```shell\nhadoop fs -put /kfly/install/hadoop-2.6.0-cdh5.14.2/LICENSE.txt /\nhadoop jar /kfly/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar wordcount /LICENSE.txt /w0727\n\n```\n\n\n\n## 6. 集群常用命令\n\n### 6.1 关闭Hadoop HA集群\n\n> 正确指令执行顺序如下\n\n- 主namenode上运行\n\n```shell\nstop-dfs.sh\n```\n\n- 主resoucemanager上运行\n\n```shell\nstop-yarn.sh\n```\n\n- 从resoucemanager上运行\n\n```shell\nyarn-daemon.sh stop resourcemanager\n```\n\n- 关闭zookeeper集群；每个zk服务器运行\n\n```shell\nzkServer.sh stop\n```\n\n### 6.2 常用命令\n\n- 单独启动namenode\n\n```shell\nhadoop-daemon.sh start namenode\n```\n\n- 单独启动datanode\n\n```shell\nhadoop-daemon.sh start datanode\n```\n\n- 单独启动journalnode\n\n```shell\nhadoop-daemon.sh start journalnode\n```\n\n- 启动zookeeper\n\n```shell\n./zkServer.sh start\n```\n\n- 启动hdfs\n\n```shell\nstart-dfs.sh\n```\n\n- 启动yarn\n\n```shell\nstart-yarn.sh\n```\n\n- 单独启动resorucemanager\n\n```shell\nyarn-daemon.sh start resouremanger\n```\n\n- 查看namenode状态（namenode1）\n\n```shell\nhdfs haadmin -getServiceState nn1\n```\n\n- 查看resourcemanager状态（resourcemanager2）\n\n```shell\nyarn rmadmin -getServiceState rm2\n```\n\n","tags":["环境搭建","zookeeper","zookeeper ha","hadoop"]},{"title":"hive常用函数","url":"/2018/12/13/it/hive/Hive常用函数/","content":"\n# Hive常用函数\n\n[UDF官网 : https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)\n\n[github FunctionRegistry.java 查看具体注册代码](https://github.com/apache/hive/blob/18d0b5a46c23056d3fe60032e00de4534a5be533/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java)\n\n### Hive的参数传递\n\n```shell\nhive [-hiveconf x=y]* [<-i filename>]* [<-f filename>|<-e query-string>] [-S]\n1、   -i 从文件初始化HQL。\n2、   -e从命令行执行指定的HQL \n3、   -f 执行HQL脚本 \n4、   -v 输出执行的HQL语句到控制台 \n5、   -p <port> connect to Hive Server on port number \n6、   -hiveconf x=y Use this to set hive/hadoop configuration variables.  设置hive运行时候的参数配置\n```\n\nHive参数大全：\n\nhttps://cwiki.apache.org/confluence/display/Hive/Configuration+Properties\n\n开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么设定的参数没有起作用？这通常是错误的设定方式导致的。\n\n**对于一般参数，有以下三种设定方式：**\n\n```sh\n# 1. 配置文件 ,以及读取的hadoop配置\nhive-site.xml > hive-default.xml > hadoop配置\n\n# 2. 命令行参数  启动hive客户端的时候可以设置参数\nbin/hive -hiveconf hive.root.logger=INFO,console\n\n# 3. 参数声明   进入客户端以后设置的一些参数  set \nbin/hive\nset mapred.reduce.tasks=100;\n\n# 优先级： 参数声明  >   命令行参数   >  配置文件参数（hive）\n```\n\n##### hiveconf使用说明\n\nhiveconf用于定义HIVE执行上下文的属性(配置参数)，可覆盖覆盖hive-site.xml（hive-default.xml）中的参数值，如用户执行目录、日志打印级别、执行队列等。例如我们可以使用hiveconf来覆盖我们的hive属性配置，\n\nhiveconf变量取值必须要使用hiveconf作为前缀参数，具体格式如下:\n\n```sql\n${hiveconf:key} \nbin/hive --hiveconf \"mapred.job.queue.name=root.default\"\n```\n\n##### hivevar使用说明\n\nhivevar用于定义HIVE运行时的变量替换，类似于JAVA中的“PreparedStatement”，与\\${key}配合使用或者与 ${hivevar:key}\n\n对于hivevar取值可以不使用前缀hivevar，具体格式如下：\n\n```sql\n使用前缀:\n ${hivevar:key}\n不使用前缀:\n ${key}\n--hivevar  name=zhangsan    ${hivevar:name}  \n也可以这样取值  ${name}\n```\n\n##### define使用说明\n\n```sql\ndefine与hivevar用途完全一样，还有一种简写“-d\nbin/hive --hiveconf \"mapred.job.queue.name=root.default\" -d my=\"201809\" --database mydb\n执行SQL\nselect * from mydb where concat(year, month) = ${my} limit 10;\n```\n\n##### hiveconf与hivevar使用实战\n\n```shell\n# 脚本文件如下 /kfly/sql/query.sql\nselect * from student left join score on student.s_id = score.s_id where score.month = ${hiveconf:month} and score.s_score > ${hivevar:s_score} and score.c_id = ${c_id};   \n# 执行脚本文件\nbin/hive --hiveconf month=201807 --hivevar s_score=80 --hivevar c_id=03  -f /kfly/sql/query.sql\n```\n\n### 常用函数\n\n```shell\n# 1．查看系统自带的函数\nhive> show functions;\n\n# 2．显示自带的函数的用法\nhive> desc function upper;\n\n# 3．详细显示自带的函数的用法\nhive> desc function extended upper;\n```\n\n\n\n#### 1、数值计算\n\n##### 1、取整函数: round \n\n**语法**: round(double a)\n **返回值**: BIGINT\n **说明**: 返回double类型的整数值部分 （遵循四舍五入）\n\n```\nhive> select round(3.1415926) from tableName;\n3\nhive> select round(3.5) from tableName;\n4\nhive> create table tableName as select round(9542.158) from tableName;\n\n```\n\n\n\n##### 2、指定精度取整函数: round \n\n**语法**: round(double a, int d)\n **返回值**: DOUBLE\n **说明**: 返回指定精度d的double类型\n\n```\nhive> select round(3.1415926,4) from tableName;\n3.1416\n```\n\n##### 3、向下取整函数: floor \n\n**语法**: floor(double a)\n **返回值**: BIGINT\n **说明**: 返回等于或者小于该double变量的最大的整数\n\n```\nhive> select floor(3.1415926) from tableName;\n3\nhive> select floor(25) from tableName;\n25\n\n\n```\n\n##### 4、向上取整函数: ceil \n\n**语法**: ceil(double a)\n **返回值**: BIGINT\n **说明**: 返回等于或者大于该double变量的最小的整数\n\n```\nhive> select ceil(3.1415926) from tableName;\n4\nhive> select ceil(46) from tableName;\n46\n\n\n```\n\n##### 5、向上取整函数: ceiling \n\n**语法**: ceiling(double a)\n **返回值**: BIGINT\n **说明**: 与ceil功能相同\n\n\n\n```\nhive> select ceiling(3.1415926) from tableName;\n4\nhive> select ceiling(46) from tableName;\n46\n\n```\n\n##### 6、取随机数函数: rand \n\n**语法**: rand(),rand(int seed)\n **返回值**: double\n **说明**: 返回一个0到1范围内的随机数。如果指定种子seed，则会等到一个稳定的随机数序列\n\n```\nhive> select rand() from tableName;\n0.5577432776034763\nhive> select rand() from tableName;\n0.6638336467363424\nhive> select rand(100) from tableName;\n0.7220096548596434\nhive> select rand(100) from tableName;\n0.7220096548596434\n```\n\n\n\n#### 2、日期函数\n\n##### 1、UNIX时间戳转日期函数: from_unixtime  \n\n**语法**: from_unixtime(bigint unixtime[, string format])\n **返回值**: string\n **说明**: 转化UNIX时间戳（从1970-01-01 00:00:00 UTC到指定时间的秒数）到当前时区的时间格式\n\n```sql\nhive> select from_unixtime(1323308943,'yyyyMMdd') from tableName;\n20111208\n\n\n```\n\n##### 2、获取当前UNIX时间戳函数: unix_timestamp\n\n**语法**: unix_timestamp()\n **返回值**: bigint\n **说明**: 获得当前时区的UNIX时间戳\n\n```sql\nhive> select unix_timestamp() from tableName;\n1323309615\n\n```\n\n##### 3、日期转UNIX时间戳函数: unix_timestamp \n\n**语法**: unix_timestamp(string date)\n **返回值**: bigint\n **说明**: 转换格式为\"yyyy-MM-dd HH:mm:ss\"的日期到UNIX时间戳。如果转化失败，则返回0。\n\n```   sql\nhive> select unix_timestamp('2011-12-07 13:01:03') from tableName;\n1323234063\n```\n\n##### 4、指定格式日期转UNIX时间戳函数: unix_timestamp \n\n**语法**: unix_timestamp(string date, string pattern)\n **返回值**: bigint\n **说明**: 转换pattern格式的日期到UNIX时间戳。如果转化失败，则返回0。\n\n```sql\nhive> select unix_timestamp('20111207 13:01:03','yyyyMMdd HH:mm:ss') from tableName;\n1323234063\n\n```\n\n##### 5、日期时间转日期函数: to_date  \n\n**语法**: to_date(string timestamp)\n **返回值**: string\n **说明**: 返回日期时间字段中的日期部分。\n\n```sql\nhive> select to_date('2011-12-08 10:03:01') from tableName;\n2011-12-08\n```\n\n##### 6、日期转年函数: year \n\n**语法**: year(string date)\n **返回值**: int\n **说明**: 返回日期中的年。\n\n```sql\nhive> select year('2011-12-08 10:03:01') from tableName;\n2011\nhive> select year('2012-12-08') from tableName;\n2012\n\n\n```\n\n##### 7、日期转月函数: month \n\n**语法**: month (string date)\n **返回值**: int\n **说明**: 返回日期中的月份。\n\n```sql\nhive> select month('2011-12-08 10:03:01') from tableName;\n12\nhive> select month('2011-08-08') from tableName;\n8\n\n\n```\n\n##### 8、日期转天函数: day \n\n**语法**: day (string date)\n **返回值**: int\n **说明**: 返回日期中的天。\n\n```sql\nhive> select day('2011-12-08 10:03:01') from tableName;\n8\nhive> select day('2011-12-24') from tableName;\n24\n\n\n```\n\n##### 9、日期转小时函数: hour \n\n**语法**: hour (string date)\n **返回值**: int\n **说明**: 返回日期中的小时。\n\n```sql\nhive> select hour('2011-12-08 10:03:01') from tableName;\n10\n\n```\n\n##### 10、日期转分钟函数: minute\n\n**语法**: minute (string date)\n **返回值**: int\n **说明**: 返回日期中的分钟。\n\n```sql\nhive> select minute('2011-12-08 10:03:01') from tableName;\n3\n\nhive> select second('2011-12-08 10:03:01') from tableName;\n1\n```\n\n\n\n##### 12、日期转周函数: weekofyear\n\n**语法**: weekofyear (string date)\n **返回值**: int\n **说明**: 返回日期在当前的周数。\n\n```sql\nhive> select weekofyear('2011-12-08 10:03:01') from tableName;\n49\n\n```\n\n##### 13、日期比较函数: datediff \n\n**语法**: datediff(string enddate, string startdate)\n **返回值**: int\n **说明**: 返回结束日期减去开始日期的天数。\n\n```sql\nhive> select datediff('2012-12-08','2012-05-09') from tableName;\n213\n\n```\n\n##### 14、日期增加函数: date_add \n\n**语法**: date_add(string startdate, int days)\n **返回值**: string\n **说明**: 返回开始日期startdate增加days天后的日期。\n\n```\nhive> select date_add('2012-12-08',10) from tableName;\n2012-12-18\n\n```\n\n##### 15、日期减少函数: date_sub \n\n**语法**: date_sub (string startdate, int days)\n **返回值**: string\n **说明**: 返回开始日期startdate减少days天后的日期。\n\n```\nhive> select date_sub('2012-12-08',10) from tableName;\n2012-11-28\n\n```\n\n\n\n#### 3、条件函数\n\n##### 1、If函数: if \n\n**语法**: if(boolean testCondition, T valueTrue, T valueFalseOrNull)\n **返回值**: T\n **说明**: 当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull\n\n```\nhive> select if(1=2,100,200) from tableName;\n200\nhive> select if(1=1,100,200) from tableName;\n100\n\n\n```\n\n##### 2、非空查找函数: COALESCE\n\n**语法**: COALESCE(T v1, T v2, …)\n **返回值**: T\n **说明**: 返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL\n\n```\nhive> select COALESCE(null,'100','50') from tableName;\n100\n\n\n```\n\n##### 3、条件判断函数：CASE \n\n**语法**: CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END\n **返回值**: T\n **说明**：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f\n\n```\nhive> Select case 100 when 50 then 'tom' when 100 then 'mary' else 'tim' end from tableName;\nmary\nhive> Select case 200 when 50 then 'tom' when 100 then 'mary' else 'tim' end from tableName;\ntim\n\n\n```\n\n##### 4、条件判断函数：CASE  \n\n**语法**: CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END\n **返回值**: T\n **说明**：如果a为TRUE,则返回b；如果c为TRUE，则返回d；否则返回e\n\n```\nhive> select case when 1=2 then 'tom' when 2=2 then 'mary' else 'tim' end from tableName;\nmary\nhive> select case when 1=1 then 'tom' when 2=2 then 'mary' else 'tim' end from tableName;\ntom\n\n\n```\n\n\n\n#### 4、字符串函数\n\n##### 1、字符串长度函数：length\n\n**语法**: length(string A)\n **返回值**: int\n **说明**：返回字符串A的长度\n\n```\nhive> select length('abcedfg') from tableName;\n\n```\n\n##### 2、字符串反转函数：reverse\n\n**语法**: reverse(string A)\n **返回值**: string\n **说明**：返回字符串A的反转结果\n\n```\nhive> select reverse('abcedfg') from tableName;\ngfdecba\n```\n\n##### 3、字符串连接函数：concat\n\n**语法**: concat(string A, string B…)\n **返回值**: string\n **说明**：返回输入字符串连接后的结果，支持任意个输入字符串\n\n```\nhive> select concat('abc','def','gh') from tableName;\nabcdefgh\n\n```\n\n##### 4、字符串连接并指定字符串分隔符：concat_ws\n\n**语法**: concat_ws(string SEP, string A, string B…)\n **返回值**: string\n **说明**：返回输入字符串连接后的结果，SEP表示各个字符串间的分隔符\n\n```\nhive> select concat_ws(',','abc','def','gh')from tableName;\nabc,def,gh\n\n```\n\n##### 5、字符串截取函数：substr\n\n**语法**: substr(string A, int start),substring(string A, int start)\n **返回值**: string\n **说明**：返回字符串A从start位置到结尾的字符串\n\n```\nhive> select substr('abcde',3) from tableName;\ncde\nhive> select substring('abcde',3) from tableName;\ncde\nhive>  select substr('abcde',-1) from tableName;  （和ORACLE相同）\ne\n\n```\n\n\n\n##### 6、字符串截取函数：substr,substring \n\n**语法**: substr(string A, int start, int len),substring(string A, int start, int len)\n **返回值**: string\n **说明**：返回字符串A从start位置开始，长度为len的字符串\n\n```\nhive> select substr('abcde',3,2) from tableName;\ncd\nhive> select substring('abcde',3,2) from tableName;\ncd\nhive>select substring('abcde',-2,2) from tableName;\nde\n\n```\n\n\n\n##### 7、字符串转大写函数：upper,ucase  \n\n**语法**: upper(string A) ucase(string A)\n **返回值**: string\n **说明**：返回字符串A的大写格式\n\n```\nhive> select upper('abSEd') from tableName;\nABSED\nhive> select ucase('abSEd') from tableName;\nABSED\n\n```\n\n##### 8、字符串转小写函数：lower,lcase  \n\n**语法**: lower(string A) lcase(string A)\n **返回值**: string\n **说明**：返回字符串A的小写格式\n\n```\nhive> select lower('abSEd') from tableName;\nabsed\nhive> select lcase('abSEd') from tableName;\nabsed\n\n```\n\n\n\n##### 9、去空格函数：trim \n\n**语法**: trim(string A)\n **返回值**: string\n **说明**：去除字符串两边的空格\n\n```\nhive> select trim(' abc ') from tableName;\nabc\n\n```\n\n##### 10、url解析函数  parse_url\n\n**语法**:\nparse_url(string urlString, string partToExtract [, string keyToExtract])\n**返回值**: string\n**说明**：返回URL中指定的部分。partToExtract的有效值为：HOST, PATH,\nQUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO.\n\n```\nhive> select parse_url\n('https://www.tableName.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST') \nfrom tableName;\nwww.tableName.com \nhive> select parse_url\n('https://www.tableName.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k1')\n from tableName;\nv1\n\n```\n\n\n\n##### 11、json解析  get_json_object \n\n**语法**: get_json_object(string json_string, string path)\n **返回值**: string\n **说明**：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。\n\n```\nhive> select  get_json_object('{\"store\":{\"fruit\":\\[{\"weight\":8,\"type\":\"apple\"},{\"weight\":9,\"type\":\"pear\"}], \"bicycle\":{\"price\":19.95,\"color\":\"red\"} },\"email\":\"amy@only_for_json_udf_test.net\",\"owner\":\"amy\"}','$.owner') from tableName;\n\n```\n\n\n\n##### 12、重复字符串函数：repeat \n\n**语法**: repeat(string str, int n)\n **返回值**: string\n **说明**：返回重复n次后的str字符串\n\n```\nhive> select repeat('abc',5) from tableName;\nabcabcabcabcabc\n\n```\n\n##### 13、分割字符串函数: split   \n\n**语法**: split(string str, string pat)\n **返回值**: array\n **说明**: 按照pat字符串分割str，会返回分割后的字符串数组\n\n```\nhive> select split('abtcdtef','t') from tableName;\n[\"ab\",\"cd\",\"ef\"]\n\n```\n\n#### 5、集合统计函数\n\n##### 1、个数统计函数: count  \n\n**语法**: count(*), count(expr), count(DISTINCT expr[, expr_.])\n**返回值**：Int\n\n**说明**: count(*)统计检索出的行的个数，包括NULL值的行；count(expr)返回指定字段的非空值的个数；count(DISTINCT\nexpr[, expr_.])返回指定字段的不同的非空值的个数\n\n```\nhive> select count(*) from tableName;\n20\nhive> select count(distinct t) from tableName;\n10\n\n\n```\n\n##### 2、总和统计函数: sum \n\n**语法**: sum(col), sum(DISTINCT col)\n **返回值**: double\n **说明**: sum(col)统计结果集中col的相加的结果；sum(DISTINCT col)统计结果中col不同值相加的结果\n\n```\nhive> select sum(t) from tableName;\n100\nhive> select sum(distinct t) from tableName;\n70\n\n\n```\n\n##### 3、平均值统计函数: avg   \n\n**语法**: avg(col), avg(DISTINCT col)\n **返回值**: double\n **说明**: avg(col)统计结果集中col的平均值；avg(DISTINCT col)统计结果中col不同值相加的平均值\n\n```\nhive> select avg(t) from tableName;\n50\nhive> select avg (distinct t) from tableName;\n30\n\n\n```\n\n##### 4、最小值统计函数: min \n\n**语法**: min(col)\n **返回值**: double\n **说明**: 统计结果集中col字段的最小值\n\n```\nhive> select min(t) from tableName;\n20\n\n\n```\n\n##### 5、最大值统计函数: max  \n\n**语法**: maxcol)\n **返回值**: double\n **说明**: 统计结果集中col字段的最大值\n\n```\nhive> select max(t) from tableName;\n120\n\n```\n\n#### 6、复合类型构建函数\n\n##### 1、Map类型构建: map  \n\n**语法**: map (key1, value1, key2, value2, …)\n **说明**：根据输入的key和value对构建map类型\n\n```\ncreate table score_map(name string, score map<string,int>)\nrow format delimited fields terminated by '\\t' \ncollection items terminated by ',' map keys terminated by ':';\n\n创建数据内容如下并加载数据\ncd /kfly/install/hivedatas/\nvim score_map.txt\n\nzhangsan\t数学:80,语文:89,英语:95\nlisi\t语文:60,数学:80,英语:99\n\n加载数据到hive表当中去\nload data local inpath '/kfly/install/hivedatas/score_map.txt' overwrite into table score_map;\n\nmap结构数据访问：\n获取所有的value：\nselect name,map_values(score) from score_map;\n\n获取所有的key：\nselect name,map_keys(score) from score_map;\n\n按照key来进行获取value值\nselect name,score[\"数学\"]  from score_map;\n\n查看map元素个数\nselect name,size(score) from score_map;\n\n```\n\n##### 2、Struct类型构建: struct\n\n**语法**: struct(val1, val2, val3, …)\n **说明**：根据输入的参数构建结构体struct类型，似于C语言中的结构体，内部数据通过X.X来获取，假设我们的数据格式是这样的，电影ABC，有1254人评价过，打分为7.4分\n\n```\n创建struct表\nhive> create table movie_score( name string,  info struct<number:int,score:float> )row format delimited fields terminated by \"\\t\"  collection items terminated by \":\"; \n\n加载数据\ncd /kfly/install/hivedatas/\nvim struct.txt\n\nABC\t1254:7.4  \nDEF\t256:4.9  \nXYZ\t456:5.4\n\n加载数据\nload data local inpath '/kfly/install/hivedatas/struct.txt' overwrite into table movie_score;\n\n\nhive当中查询数据\nhive> select * from movie_score;  \nhive> select info.number,info.score from movie_score;  \nOK  \n1254    7.4  \n256     4.9  \n456     5.4  \n\n```\n\n##### 3、array类型构建: array\n\n**语法**: array(val1, val2, …)\n **说明**：根据输入的参数构建数组array类型\n\n```sql\nhive> create table  person(name string,work_locations array<string>)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nCOLLECTION ITEMS TERMINATED BY ',';\n\n加载数据到person表当中去\ncd /kfly/install/hivedatas/\nvim person.txt\n\n数据内容格式如下\nbiansutao\tbeijing,shanghai,tianjin,hangzhou\nlinan\tchangchu,chengdu,wuhan\n\n加载数据\nhive > load  data local inpath '/kfly/install/hivedatas/person.txt' overwrite into table person;\n\n查询所有数据数据\nhive > select * from person;\n\n按照下表索引进行查询\nhive > select work_locations[0] from person;\n\n查询所有集合数据\nhive  > select work_locations from person; \n\n查询元素个数\nhive >  select size(work_locations) from person;   \n```\n\n#### 7、复杂类型长度统计函数\n\n##### 1.Map类型长度函数: size(Map<k .V>)\n\n**语法**: size(Map<k .V>)\n **返回值**: int\n **说明**: 返回map类型的长度\n\n```\nhive> select size(t) from map_table2;\n2\n```\n\n##### 2.array类型长度函数: size(Array<T>)\n\n**语法**: size(Array<T>)\n **返回值**: int\n **说明**: 返回array类型的长度\n\n```\nhive> select size(t) from arr_table2;\n4\n```\n\n##### 3.类型转换函数  \n\n**类型转换函数**: cast\n **语法**: cast(expr as <type>)\n **返回值**: Expected \"=\" to follow \"type\"\n **说明**: 返回转换后的数据类型\n\n```\nhive> select cast('1' as bigint) from tableName;\n1\n\n```\n\n#### 8、hive当中的lateral view 与 explode以及reflect和分析函数\n\n##### 1、使用explode函数将hive表中的Map和Array字段数据进行拆分\n\nlateral view用于和split、explode等UDTF一起使用的，能将一行数据拆分成多行数据，在此基础上可以对拆分的数据进行聚合，lateral view首先为原始表的每行调用UDTF，UDTF会把一行拆分成一行或者多行，lateral view在把结果组合，产生一个支持别名表的虚拟表。\n其中explode还可以用于将hive一列中复杂的array或者map结构拆分成多行\n\n```\n需求：现在有数据格式如下\nzhangsan\tchild1,child2,child3,child4\tk1:v1,k2:v2\nlisi\tchild5,child6,child7,child8\t k3:v3,k4:v4\n\n字段之间使用\\t分割，需求将所有的child进行拆开成为一列\n \n+----------+--+\n| mychild  |\n+----------+--+\n| child1   |\n| child2   |\n| child3   |\n| child4   |\n| child5   |\n| child6   |\n| child7   |\n| child8   |\n+----------+--+\n\n将map的key和value也进行拆开，成为如下结果\n\n+-----------+-------------+--+\n| mymapkey  | mymapvalue  |\n+-----------+-------------+--+\n| k1        | v1          |\n| k2        | v2          |\n| k3        | v3          |\n| k4        | v4          |\n+-----------+-------------+--+\n```\n\n###### 第一步：创建hive数据库\n\n创建hive数据库\n\n```\nhive (default)> create database hive_explode;\nhive (default)> use hive_explode;\n```\n\n###### 第二步：创建hive表，然后使用explode拆分map和array\n\n```\nhive (hive_explode)> create  table hive_explode.t3(name string,children array<string>,address Map<string,string>) row format delimited fields terminated by '\\t'  collection items    terminated by ','  map keys terminated by ':' stored as textFile;\n```\n\n###### 第三步：加载数据\n\nnode03执行以下命令创建表数据文件\n\n```\ncd  /kfly/install/hivedatas/\n\nvim maparray\n数据内容格式如下\n\nzhangsan\tchild1,child2,child3,child4\tk1:v1,k2:v2\nlisi\tchild5,child6,child7,child8\tk3:v3,k4:v4\n```\n\nhive表当中加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kfly/install/hivedatas/maparray' into table hive_explode.t3;\n\n```\n\n###### 第四步：使用explode将hive当中数据拆开\n\n将array当中的数据拆分开\n\n```\nhive (hive_explode)> SELECT explode(children) AS myChild FROM hive_explode.t3;\n\n```\n\n将map当中的数据拆分开\n\n```\nhive (hive_explode)> SELECT explode(address) AS (myMapKey, myMapValue) FROM hive_explode.t3;\n\n```\n\n##### 2、使用explode拆分json字符串\n\n需求：现在有一些数据格式如下：\n\n```\na:shandong,b:beijing,c:hebei|1,2,3,4,5,6,7,8,9|[{\"source\":\"7fresh\",\"monthSales\":4900,\"userCount\":1900,\"score\":\"9.9\"},{\"source\":\"jd\",\"monthSales\":2090,\"userCount\":78981,\"score\":\"9.8\"},{\"source\":\"jdmart\",\"monthSales\":6987,\"userCount\":1600,\"score\":\"9.0\"}]\n\n```\n\n其中字段与字段之间的分隔符是 | \n\n我们要解析得到所有的monthSales对应的值为以下这一列（行转列）\n\n```\n4900\n2090\n6987\n\n```\n\n###### 第一步：创建hive表\n\n```\nhive (hive_explode)> create table hive_explode.explode_lateral_view  (area string, goods_id string, sale_info string)  ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS textfile;\n```\n\n###### 第二步：准备数据并加载数据\n\n准备数据如下\n\n```\ncd /kfly/install/hivedatas\nvim explode_json\n\na:shandong,b:beijing,c:hebei|1,2,3,4,5,6,7,8,9|[{\"source\":\"7fresh\",\"monthSales\":4900,\"userCount\":1900,\"score\":\"9.9\"},{\"source\":\"jd\",\"monthSales\":2090,\"userCount\":78981,\"score\":\"9.8\"},{\"source\":\"jdmart\",\"monthSales\":6987,\"userCount\":1600,\"score\":\"9.0\"}]\n\n \n\n```\n\n加载数据到hive表当中去\n\n```\nhive (hive_explode)> load data local inpath '/kfly/install/hivedatas/explode_json' overwrite into table hive_explode.explode_lateral_view;\n```\n\n###### 第三步：使用explode拆分Array\n\n```\nhive (hive_explode)> select explode(split(goods_id,',')) as goods_id from hive_explode.explode_lateral_view;\n```\n\n \n\n###### 第四步：使用explode拆解Map\n\n```\nhive (hive_explode)> select explode(split(area,',')) as area from hive_explode.explode_lateral_view;\n```\n\n\n\n###### 第五步：拆解json字段\n\n```\nhive (hive_explode)> select explode(split(regexp_replace(regexp_replace(sale_info,'\\\\[\\\\{',''),'}]',''),'},\\\\{')) as  sale_info from hive_explode.explode_lateral_view;\n```\n\n \n\n然后我们想用get_json_object来获取key为monthSales的数据：\n\n```\nhive (hive_explode)> select get_json_object(explode(split(regexp_replace(regexp_replace(sale_info,'\\\\[\\\\{',''),'}]',''),'},\\\\{')),'$.monthSales') as  sale_info from hive_explode.explode_lateral_view;\n\n\n然后出现异常FAILED: SemanticException [Error 10081]: UDTF's are not supported outside the SELECT clause, nor nested in expressions\nUDTF explode不能写在别的函数内\n如果你这么写，想查两个字段，select explode(split(area,',')) as area,good_id from explode_lateral_view;\n会报错FAILED: SemanticException 1:40 Only a single expression in the SELECT clause is supported with UDTF's. Error encountered near token 'good_id'\n使用UDTF的时候，只支持一个字段，这时候就需要LATERAL VIEW出场了\n```\n\n##### 3、配合LATERAL  VIEW使用\n\n配合lateral view查询多个字段\n\n```\nhive (hive_explode)> select goods_id2,sale_info from explode_lateral_view LATERAL VIEW explode(split(goods_id,','))goods as goods_id2;\n\n```\n\n其中LATERAL VIEW explode(split(goods_id,','))goods相当于一个虚拟表，与原表explode_lateral_view笛卡尔积关联。\n\n也可以多重使用\n\n```\nhive (hive_explode)> select goods_id2,sale_info,area2 from explode_lateral_view  LATERAL VIEW explode(split(goods_id,','))goods as goods_id2 LATERAL VIEW explode(split(area,','))area as area2;\n\n```\n\n也是三个表笛卡尔积的结果\n\n最终，我们可以通过下面的句子，把这个json格式的一行数据，完全转换成二维表的方式展现\n\n```\nhive (hive_explode)> select get_json_object(concat('{',sale_info_1,'}'),'$.source') as source, get_json_object(concat('{',sale_info_1,'}'),'$.monthSales') as monthSales, get_json_object(concat('{',sale_info_1,'}'),'$.userCount') as monthSales,  get_json_object(concat('{',sale_info_1,'}'),'$.score') as monthSales from explode_lateral_view   LATERAL VIEW explode(split(regexp_replace(regexp_replace(sale_info,'\\\\[\\\\{',''),'}]',''),'},\\\\{'))sale_info as sale_info_1;\n\n```\n\n总结：\n\nLateral View通常和UDTF一起出现，为了解决UDTF不允许在select字段的问题。 \n Multiple Lateral View可以实现类似笛卡尔乘积。 \n Outer关键字可以把不输出的UDTF的空结果，输出成NULL，防止丢失数据。\n\n\n\n#### 9、列转行\n\n##### 1．相关函数说明\n\nCONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;\n\nCONCAT_WS(separator, str1, str2,...)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;\n\nCOLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。\n\n##### 2．数据准备\n\n表6-6 数据准备\n\n| name   | constellation | blood_type |\n| ------ | ------------- | ---------- |\n| 孙悟空 | 白羊座        | A          |\n| 老王   | 射手座        | A          |\n| 宋宋   | 白羊座        | B          |\n| 猪八戒 | 白羊座        | A          |\n| 冰冰   | 射手座        | A          |\n\n##### 3．需求\n\n把星座和血型一样的人归类到一起。结果如下：\n\n```\n射手座,A            老王|冰冰\n白羊座,A            孙悟空|猪八戒\n白羊座,B            宋宋\n\n```\n\n\n\n##### 4．创建本地constellation.txt，导入数据\n\nnode03服务器执行以下命令创建文件，注意数据使用\\t进行分割\n\n```\ncd /kfly/install/hivedatas\nvim constellation.txt\n```\n\n```\n孙悟空\t白羊座\tA\n老王\t射手座\tA\n宋宋\t白羊座\tB       \n猪八戒\t白羊座\tA\n凤姐\t射手座\tA\n```\n\n##### 5．创建hive表并导入数据\n\n创建hive表并加载数据\n\n```\nhive (hive_explode)> create table person_info(  name string,  constellation string,  blood_type string)  row format delimited fields terminated by \"\\t\";\n```\n\n加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kfly/install/hivedatas/constellation.txt' into table person_info;\n```\n\n##### 6．按需求查询数据\n\n```sql\nselect t1.base, concat_ws('|', collect_set(t1.name)) name from    (select name, concat(constellation, \",\" , blood_type) base from person_info) t1 group by  t1.base;\n```\n\n\n\n#### 10、行转列\n\n##### 1．函数说明\n\nEXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。\n\nLATERAL VIEW\n\n用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias\n\n解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。\n\n##### 2．数据准备\n\n数据内容如下，字段之间都是使用\\t进行分割\n\n```\ncd /kfly/install/hivedatas\n\nvim movie.txt\n《疑犯追踪》\t悬疑,动作,科幻,剧情\n《Lie to me》\t悬疑,警匪,动作,心理,剧情\n《战狼2》\t战争,动作,灾难\n```\n\n\n\n##### 3．需求\n\n将电影分类中的数组数据展开。结果如下：\n\n```\n《疑犯追踪》\t悬疑\n《疑犯追踪》\t动作\n《疑犯追踪》\t科幻\n《疑犯追踪》\t剧情\n《Lie to me》\t悬疑\n《Lie to me》\t警匪\n《Lie to me》\t动作\n《Lie to me》\t心理\n《Lie to me》\t剧情\n《战狼2》\t战争\n《战狼2》\t动作\n《战狼2》\t灾难\n```\n\n\n\n##### 4．创建hive表并导入数据\n\n创建hive表\n\n```\nhive (hive_explode)> create table movie_info(movie string, category array<string>) row format delimited fields terminated by \"\\t\" collection items terminated by \",\";\n```\n\n加载数据\n\n```\nload data local inpath \"/kfly/install/hivedatas/movie.txt\" into table movie_info;\n```\n\n \n\n##### 5．按需求查询数据\n\n```\nhive (hive_explode)>  select movie, category_name  from  movie_info lateral view explode(category) table_tmp as category_name;\n```\n\n#### 11、reflect函数\n\nreflect函数可以支持在sql中调用java中的自带函数，秒杀一切udf函数。\n\n##### 使用java.lang.Math当中的Max求两列中最大值\n\n创建hive表\n\n```\nhive (hive_explode)>  create table test_udf(col1 int,col2 int) row format delimited fields terminated by ',';\n```\n\n准备数据并加载数据\n\n```\ncd /kfly/install/hivedatas\n\nvim test_udf\n\n1,2\n4,3\n6,4\n7,5\n5,6\n```\n\n加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kfly/install/hivedatas/test_udf' overwrite into table test_udf;\n```\n\n使用java.lang.Math当中的Max求两列当中的最大值\n\n```\nhive (hive_explode)> select reflect(\"java.lang.Math\",\"max\",col1,col2) from test_udf;\n```\n\n##### 不同记录执行不同的java内置函数\n\n创建hive表\n\n```\nhive (hive_explode)> create table test_udf2(class_name string,method_name string,col1 int , col2 int) row format delimited fields terminated by ',';\n```\n\n准备数据\n\n```\ncd /export/servers/hivedatas\n\nvim test_udf2\n\njava.lang.Math,min,1,2\njava.lang.Math,max,2,3\n\n```\n\n加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kfly/install/hivedatas/test_udf2' overwrite into table test_udf2;\n```\n\n执行查询\n\n```\nhive (hive_explode)> select reflect(class_name,method_name,col1,col2) from test_udf2;\n```\n\n##### 判断是否为数字\n\n使用apache commons中的函数，commons下的jar已经包含在hadoop的classpath中，所以可以直接使用。\n\n使用方式如下：\n\n```\nhive (hive_explode)> select reflect(\"org.apache.commons.lang.math.NumberUtils\",\"isNumber\",\"123\");\n```\n\n\n\n#### 12、hive当中的分析函数—分组求topN\n\n##### 1、分析函数的作用介绍\n\n对于一些比较复杂的数据求取过程，我们可能就要用到分析函数，分析函数主要用于分组求topN，或者求取百分比，或者进行数据的切片等等，我们都可以使用分析函数来解决\n\n\n\n##### 2、常用的分析函数介绍\n\n1、ROW_NUMBER()：\n\n从1开始，按照顺序，生成分组内记录的序列,比如，按照pv降序排列，生成分组内每天的pv名次,ROW_NUMBER()的应用场景非常多，再比如，获取分组内排序第一的记录;获取一个session中的第一条refer等。 \n\n2、RANK() ：\n\n生成数据项在分组中的排名，排名相等会在名次中留下空位 \n\n3、DENSE_RANK() ：\n\n生成数据项在分组中的排名，排名相等会在名次中不会留下空位 \n\n4、CUME_DIST ：\n\n小于等于当前值的行数/分组内总行数。比如，统计小于等于当前薪水的人数，所占总人数的比例 \n\n5、PERCENT_RANK ：\n\n分组内当前行的RANK值/分组内总行数\n\n6、NTILE(n) ：\n\n用于将分组数据按照顺序切分成n片，返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。NTILE不支持ROWS BETWEEN，比如 NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)。\n\n##### 3、需求描述\n\n现有数据内容格式如下，分别对应三个字段，cookieid，createtime ，pv，求取每个cookie访问pv前三名的数据记录，其实就是分组求topN，求取每组当中的前三个值\n\n```\ncookie1,2015-04-10,1\ncookie1,2015-04-11,5\ncookie1,2015-04-12,7\ncookie1,2015-04-13,3\ncookie1,2015-04-14,2\ncookie1,2015-04-15,4\ncookie1,2015-04-16,4\ncookie2,2015-04-10,2\ncookie2,2015-04-11,3\ncookie2,2015-04-12,5\ncookie2,2015-04-13,6\ncookie2,2015-04-14,3\ncookie2,2015-04-15,9\ncookie2,2015-04-16,7g\n\n```\n\n###### 第一步：创建数据库表\n\n在hive当中创建数据库表\n\n```\nCREATE EXTERNAL TABLE cookie_pv (\ncookieid string,\ncreatetime string, \npv INT\n) ROW FORMAT DELIMITED \nFIELDS TERMINATED BY ',' ;\n```\n\n###### 第二步：准备数据并加载\n\nnode03执行以下命令，创建数据，并加载到hive表当中去\n\n```\ncd /kfly/install/hivedatas\nvim cookiepv.txt\n\ncookie1,2015-04-10,1\ncookie1,2015-04-11,5\ncookie1,2015-04-12,7\ncookie1,2015-04-13,3\ncookie1,2015-04-14,2\ncookie1,2015-04-15,4\ncookie1,2015-04-16,4\ncookie2,2015-04-10,2\ncookie2,2015-04-11,3\ncookie2,2015-04-12,5\ncookie2,2015-04-13,6\ncookie2,2015-04-14,3\ncookie2,2015-04-15,9\ncookie2,2015-04-16,7\n```\n\n加载数据到hive表当中去\n\n```\nload  data  local inpath '/kfly/install/hivedatas/cookiepv.txt'  overwrite into table  cookie_pv \n```\n\n###### 第三步：使用分析函数来求取每个cookie访问PV的前三条记录\n\n```sql\nSELECT * FROM (\n\tSELECT \n  cookieid,\n  createtime,\n  pv,\n  RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn1,\n  DENSE_RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn2,\n  ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY pv DESC) AS rn3 \n  FROM cookie_pv) temp \nWHERE temp.rn1 <=  3 ;\n```\n\n#### 13、hive自定义函数\n\n##### 1、自定义函数的基本介绍\n\n1）Hive 自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。\n\n2）当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。\n\n3）根据用户自定义函数类别分为以下三种：\n\n        （1）UDF（User-Defined-Function）\n    \n                一进一出\n    \n        （2）UDAF（User-Defined Aggregation Function）\n    \n                聚集函数，多进一出\n    \n                类似于：count/max/min\n    \n        （3）UDTF（User-Defined Table-Generating Functions）\n    \n                一进多出\n    \n                如lateral view explode()\n\n4）官方文档地址\n\nhttps://cwiki.apache.org/confluence/display/Hive/HivePlugins\n\n5）编程步骤：\n\n        （1）继承org.apache.hadoop.hive.ql.UDF\n    \n        （2）需要实现evaluate函数；evaluate函数支持重载；\n\n6）注意事项\n\n        （1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void；\n    \n        （2）UDF中常用Text/LongWritable等类型，不推荐使用java类型；\n\n \n\n\n\n\n\n\n#####  2、自定义函数开发\n\n###### 第一步：创建maven java 工程，并导入jar包\n\n```\n<repositories>\n    <repository>\n        <id>cloudera</id>\n <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n    </repository>\n</repositories>\n<dependencies>\n    <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-common</artifactId>\n        <version>2.6.0-cdh5.14.2</version>\n    </dependency>\n    <dependency>\n        <groupId>org.apache.hive</groupId>\n        <artifactId>hive-exec</artifactId>\n        <version>1.1.0-cdh5.14.2</version>\n    </dependency>\n</dependencies>\n<build>\n<plugins>\n    <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>3.0</version>\n        <configuration>\n            <source>1.8</source>\n            <target>1.8</target>\n            <encoding>UTF-8</encoding>\n        </configuration>\n    </plugin>\n     <plugin>\n         <groupId>org.apache.maven.plugins</groupId>\n         <artifactId>maven-shade-plugin</artifactId>\n         <version>2.2</version>\n         <executions>\n             <execution>\n                 <phase>package</phase>\n                 <goals>\n                     <goal>shade</goal>\n                 </goals>\n                 <configuration>\n                     <filters>\n                         <filter>\n                             <artifact>*:*</artifact>\n                             <excludes>\n                                 <exclude>META-INF/*.SF</exclude>\n                                 <exclude>META-INF/*.DSA</exclude>\n                                 <exclude>META-INF/*/RSA</exclude>\n                             </excludes>\n                         </filter>\n                     </filters>\n                 </configuration>\n             </execution>\n         </executions>\n     </plugin>\n</plugins>\n</build>\n```\n\n \n\n \n\n###### 第二步：开发java类继承UDF，并重载evaluate 方法\n\n```\npublic class MyUDF extends UDF {\n     public Text evaluate(final Text s) {\n         if (null == s) {\n             return null;\n         }\n         //**返回大写字母         return new Text(s.toString().toUpperCase());\n     }\n }\n```\n\n###### 第三步：将我们的项目打包，并上传到hive的lib目录下\n\n使用maven的package进行打包，将我们打包好的jar包上传到node03服务器的/kfly/install/hive-1.1.0-cdh5.14.2/lib 这个路径下\n\n###### 第四步：添加我们的jar包\n\n重命名我们的jar包名称\n\n```\ncd /kfly/install/hive-1.1.0-cdh5.14.2/lib\nmv original-day_hive_udf-1.0-SNAPSHOT.jar udf.jar\n```\n\nhive的客户端添加我们的jar包\n\n```\n0: jdbc:hive2://node03:10000> add jar /kfly/install/hive-1.1.0-cdh5.14.2/lib/udf.jar;\n```\n\n \n\n###### 第五步：设置函数与我们的自定义函数关联\n\n```\n0: jdbc:hive2://node03:10000> create temporary function tolowercase as 'com.kfly.udf.MyUDF';\n```\n\n###### 第六步：使用自定义函数\n\n```\n0: jdbc:hive2://node03:10000>select tolowercase('abc');\n```\n\n \n\nhive当中如何创建永久函数\n\n在hive当中添加临时函数，需要我们每次进入hive客户端的时候都需要添加以下，退出hive客户端临时函数就会失效，那么我们也可以创建永久函数来让其不会失效\n\n创建永久函数\n\n```\n1、指定数据库，将我们的函数创建到指定的数据库下面\n0: jdbc:hive2://node03:10000>use myhive;\n\n2、使用add jar添加我们的jar包到hive当中来\n0: jdbc:hive2://node03:10000>add jar /kfly/install/hive-1.1.0-cdh5.14.2/lib/udf.jar;\n\n3、查看我们添加的所有的jar包\n0: jdbc:hive2://node03:10000>list  jars;\n\n4、创建永久函数，与我们的函数进行关联\n0: jdbc:hive2://node03:10000>create  function myuppercase as 'com.kfly.udf.MyUDF';\n\n5、查看我们的永久函数\n0: jdbc:hive2://node03:10000>show functions like 'my*';\n\n6、使用永久函数\n0: jdbc:hive2://node03:10000>select myhive.myuppercase('helloworld');\n\n7、删除永久函数\n0: jdbc:hive2://node03:10000>drop function myhive.myuppercase;\n\n8、查看函数\n show functions like 'my*';\n```\n\n\n\n### 3. hive表的数据压缩 \n\n#### 1、数据的压缩说明\n\n- 压缩模式评价\n  - 可使用以下三种标准对压缩方式进行评价\n    - 1、压缩比：压缩比越高，压缩后文件越小，所以压缩比越高越好\n    - 2、压缩时间：越快越好\n    - 3、已经压缩的格式文件是否可以再分割：可以分割的格式允许单一文件由多个Mapper程序处理，可以更好的并行化\n\n- 常见压缩格式\n\n| 压缩方式 | 压缩比 | 压缩速度 | 解压缩速度 | 是否可分割 |\n| :------: | :----: | :------: | :--------: | :--------: |\n|   gzip   | 13.4%  | 21 MB/s  |  118 MB/s  |     否     |\n|  bzip2   | 13.2%  | 2.4MB/s  |  9.5MB/s   |     是     |\n|   lzo    | 20.5%  | 135 MB/s |  410 MB/s  |     是     |\n|  snappy  | 22.2%  | 172 MB/s |  409 MB/s  |     否     |\n\n- Hadoop编码/解码器方式\n\n| 压缩格式 |             对应的编码/解码器              |\n| :------: | :----------------------------------------: |\n| DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |\n|   Gzip   |  org.apache.hadoop.io.compress.GzipCodec   |\n|  BZip2   |  org.apache.hadoop.io.compress.BZip2Codec  |\n|   LZO    |     com.hadoop.compress.lzo.LzopCodec      |\n|  Snappy  | org.apache.hadoop.io.compress.SnappyCodec  |\n\n \t压缩性能的比较\n\n| 压缩算法 | 原始文件大小 | 压缩文件大小 | 压缩速度 | 解压速度 |\n| -------- | ------------ | ------------ | -------- | -------- |\n| gzip     | 8.3GB        | 1.8GB        | 17.5MB/s | 58MB/s   |\n| bzip2    | 8.3GB        | 1.1GB        | 2.4MB/s  | 9.5MB/s  |\n| LZO      | 8.3GB        | 2.9GB        | 49.3MB/s | 74.6MB/s |\n\nhttp://google.github.io/snappy/\n\nOn a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.\n\n#### 2、压缩配置参数\n\n要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：\n\n| 参数                                                 | 默认值                                                       | 阶段        | 建议                                         |\n| ---------------------------------------------------- | ------------------------------------------------------------ | ----------- | -------------------------------------------- |\n| io.compression.codecs      （在core-site.xml中配置） | org.apache.hadoop.io.compress.DefaultCodec,   org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,   org.apache.hadoop.io.compress.Lz4Codec | 输入压缩    | Hadoop使用文件扩展名判断是否支持某种编解码器 |\n| mapreduce.map.output.compress                        | false                                                        | mapper输出  | 这个参数设为true启用压缩                     |\n| mapreduce.map.output.compress.codec                  | org.apache.hadoop.io.compress.DefaultCodec                   | mapper输出  | 使用LZO、LZ4或snappy编解码器在此阶段压缩数据 |\n| mapreduce.output.fileoutputformat.compress           | false                                                        | reducer输出 | 这个参数设为true启用压缩                     |\n| mapreduce.output.fileoutputformat.compress.codec     | org.apache.hadoop.io.compress. DefaultCodec                  | reducer输出 | 使用标准工具或者编解码器，如gzip和bzip2      |\n| mapreduce.output.fileoutputformat.compress.type      | RECORD                                                       | reducer输出 | SequenceFile输出使用的压缩类型：NONE和BLOCK  |\n\n#### 3、开启Map输出阶段压缩\n\n开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：\n\n**案例实操：**\n\n```\n1）开启hive中间传输数据压缩功能\nhive (default)>set hive.exec.compress.intermediate=true;\n\n2）开启mapreduce中map输出压缩功能\nhive (default)>set mapreduce.map.output.compress=true;\n\n3）设置mapreduce中map输出数据的压缩方式\nhive (default)>set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec;\n\n4）执行查询语句\n   select count(1) from score;\n```\n\n\n\n#### 4、 开启Reduce输出阶段压缩\n\n当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。\n\n**案例实操：**\n\n```\n1）开启hive最终输出数据压缩功能\nhive (default)>set hive.exec.compress.output=true;\n\n2）开启mapreduce最终输出数据压缩\nhive (default)>set mapreduce.output.fileoutputformat.compress=true;\n\n3）设置mapreduce最终数据输出压缩方式\nhive (default)> set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;\n\n4）设置mapreduce最终数据输出压缩为块压缩\nhive (default)>set mapreduce.output.fileoutputformat.compress.type=BLOCK;\n\n5）测试一下输出结果是否是压缩文件\ninsert overwrite local directory '/kfly/install/hivedatas/snappy' select * from score distribute by s_id sort by s_id desc;\n```\n\n","tags":["hive","function"]},{"title":"Hive基础知识","url":"/2018/12/01/it/hive/Hive基础知识/","content":"\n## Hive基础知识\n\n### Hive简介\n\n- Hive是什么\n\nHive是基于Hadoop的一个数据仓库工具，==可以将结构化的数据文件映射为一张数据库表==，并提供类SQL查询功能。其本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据的存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更进一步可以说hive就是一个MapReduce的客户端\n\n<img src=\"http://kflys.gitee.io/upic/2020/03/31/uPic/hive/Snipaste_2019-07-10_23-23-31.png\" style=\"zoom:67%;\" />\n\n- Hive与数据库的区别\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/2018040319335283.png)\n\n* Hive 具有 SQL 数据库的外表，但应用场景完全不同。\n* ==Hive 只适合用来做海量离线数据统计分析，也就是数据仓库==。\n\n#### 优缺点\n\n* ==优点==\n  * **操作接口采用类SQL语法**，提供快速开发的能力（简单、容易上手）。\n\n  * **避免了去写MapReduce**，减少开发人员的学习成本。\n\n  * **Hive支持用户自定义函数**，用户可以根据自己的需求来实现自己的函数。\n\n* ==缺点==\n  * **Hive 不支持记录级别的增删改操作**\n  * **Hive 的查询延迟很严重**\n    * hadoop jar  xxxx.jar  xxx.class /input /output\n      * 进行任务的划分，然后进行计算资源的申请\n      * map 0%  reduce 0%\n      * map 10%  reduce 0%\n  * **Hive 不支持事务**\n\n\n\n#### 架构原理\n\n<img src=\"http://kflys.gitee.io/upic/2020/03/31/uPic/hive/2019-07-11_11-08-35.png\" style=\"zoom:50%;\" />\n\n* 1、用户接口：Client\n\n```shell\n# CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）\n# 第一种 hive Client\nbin/hive\n\n# 第二种\nnohup  bin/hive --service hiveserver2  &\n# \nbin/beeline\nbeeline> !connect jdbc:hive2://node03:10000\n\teg:sql\n\t\tbin/hive -e \"show databases\"\n\t\tbin/hive -f hive.sql  # 执行文件 \n```\n\n* 2、元数据：Metastore\n  * 元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；\n\n    * 默认存储在自带的derby数据库中，==推荐使用MySQL存储Metastore==\n\n* 3、Hadoop集群\n  * 使用HDFS进行存储，使用MapReduce进行计算。\n\n* 4、Driver：驱动器\n  * 解析器（SQL Parser） \n    * 将SQL字符串转换成抽象语法树AST\n    * 对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误\n  * 编译器（Physical Plan）：将AST编译生成逻辑执行计划\n  * 优化器（Query Optimizer）：对逻辑执行计划进行优化\n  * 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说默认就是mapreduce任务\n\n![hive1](http://kflys.gitee.io/upic/2020/03/31/uPic/hive/hive1.png)\n\n### 数据类型\n\n#### 基本数据类型\n\n|    类型名称    |              描述               |    举例    |\n| :------------: | :-----------------------------: | :--------: |\n|    boolean     |           true/false            |    true    |\n|    tinyint     |        1字节的有符号整数        |     1      |\n|    smallint    |        2字节的有符号整数        |     1      |\n|  ==**int**==   |        4字节的有符号整数        |     1      |\n| **==bigint==** |        8字节的有符号整数        |     1      |\n|     float      |        4字节单精度浮点数        |    1.0     |\n| **==double==** |        8字节单精度浮点数        |    1.0     |\n| **==string==** |        字符串(不设长度)         |   “abc”    |\n|    varchar     | 字符串（1-65355长度，超长截断） |   “abc”    |\n|   timestamp    |             时间戳              | 1563157873 |\n|      date      |              日期               |  20190715  |\n\n#### 复合数据类型\n\n| 类型名称 |                             描述                             |       举例        |\n| :------: | :----------------------------------------------------------: | :---------------: |\n|  array   | 一组有序的字段，字段类型必须相同 array(元素1，元素2)  ： array[0] |  Array（1,2,3）   |\n|   map    |        一组无序的键值对 map(k1,v1,k2,v2)  ：map['a']         | Map(‘a’,1,'b',2)  |\n|  struct  | 一组命名的字段, col3 struct\\<a:string,b:int,c:double\\> : c.a | Struct('a',1,2,0) |\n\n### Hive的DDL操作\n\n#### hive的数据库DDL操作\n\n```sql\n-- 1. 创建数据库\nhive > create database db_hive;\n# 或者\nhive > create database if not exists db_hive;\n\n-- 查询数据库\t\nshow databases like 'db_hive*';\n\n-- 查看数据库详情\ndesc database db_hive;\n\n-- 显示数据库详细信息\ndesc database extended db_hive;\n\n-- 删除为空的数据库\nhive> drop database db_hive;\n\n-- 如果删除的数据库不存在，最好采用if exists 判断数据库是否存在\nhive> drop database if exists db_hive;\n\n-- #如果数据库中有表存在，这里需要使用cascade强制删除数据库\nhive> drop database if exists db_hive cascade;\n```\n\n#### hive的表DDL操作\n\n官网地址：<https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL>\n\n```sql\n-- 通过AS 查询语句完成建表：将子查询的结果存在新表里，有数据 \ncreate table if not exists myhive.stu1 as select id, name from stu;\n\n-- 根据已经存在的表结构创建表\ncreate table if not exists myhive.stu2 like stu;\n\n-- 查询表的类型\ndesc formatted myhive.stu;\n\n-- eg:external 加上为外部表\ncreate  [external] table if not exists myhive.stu3(id int ,name string)\nrow format delimited fields terminated by '\\t' stored as textfile location       '/user/stu2';\n\n-- 加载数据 [local]：加上代表本地路径，不加代表hdfs路径\nload data [local] inpath '/kfly/install/hivedatas/teacher.csv' into table myhive.teacher;\n\n-- 内部表转换为外部表\nalter table stu set tblproperties('EXTERNAL'='TRUE');\n\n-- 注意： 内部表由于删除表的时候会同步删除HDFS的数据文件，所以确定如果一个表仅仅是你独占使用，其他人不适用的时候就可以创建内部表，如果一个表的文件数据，其他人也要使用，那么就创建外部表\n-- 一般外部表都是用在数据仓库的ODS层\n-- 内部表都是用在数据仓库的DW层\n```\n\n##### hive的分区表\n\n```sql\n-- 创建分区表\ncreate table score(s_id string,c_id string, s_score int) partitioned by (month string) row -format delimited fields terminated by '\\t';\n\n-- 创建一个表带多个分区\ncreate table score2 (s_id string,c_id string, s_score int) partitioned by (year string,month string,day string) row format delimited fields terminated by '\\t';\n\n-- 加载数据到分区表\nload data  local inpath '/kfly/install/hivedatas/score.csv' into table score partition  (month='201806');\n\n-- 展示分区\nshow  partitions  score;\n-- 添加一个 / 多个分区\nalter table score add partition(month='201805');\nalter table score add partition(month='201804') partition(month = '201803');\n-- 删除分区\nalter table score drop partition(month = '201806');\n\n-- 进行表的修复,建立我们表与我们数据文件之间的一个关系映射，本地文件数据与元数据之间不同步时执行\nmsck  repair   table  score4;\n```\n\n\n\n<img src=\"http://kflys.gitee.io/upic/2020/03/31/uPic/hive/2019-07-15_11-35-37.png\" alt=\"2019-07-15_11-35-37\" style=\"zoom:67%;\" />\n\n##### hive的分桶表\n\n<img src=\"http://kflys.gitee.io/upic/2020/03/31/uPic/hive/2019-07-16_17-01-51.png\" style=\"zoom:67%;\" />\n\n- 分桶是相对分区进行更细粒度的划分。\n\n- ==分桶将整个数据内容安装某列属性值取hash值进行区分，具有相同hash值的数据进入到同一个文件中==\n\n  - 比如按照name属性分为3个桶，就是对name属性值的hash值对3取摸，按照取模结果对数据分桶。\n    - 取模结果为==0==的数据记录存放到一个文件\n    - 取模结果为==1==的数据记录存放到一个文件\n    - 取模结果为==2==的数据记录存放到一个文件\n    - 取模结果为==3==的数据记录存放到一个文件\n\n- **==作用==**\n\n  - 取样sampling更高效。没有分区的话需要扫描整个数据集。\n  - 提升某些查询操作效率，例如map side join\n\n  - 在创建分桶表之前要执行的命令\n    - ==set hive.enforce.bucketing=true;==  开启对分桶表的支持\n    - ==set mapreduce.job.reduces=4;==      设置与桶相同的reduce个数（默认只有一个reduce）\n\n    ```sql\n    -- 进入hive客户端然后执行以下命令\n    use myhive;\n    set mapreduce.job.reduces=4;  \n    set hive.enforce.bucketing=true; \n    --分桶表\n    create table myhive.user_buckets_demo(id int, name string)\n    clustered by(id) \n    into 4 buckets \n    row format delimited fields terminated by '\\t';\n    \n    -- 加载数据到普通表 user_demo 中\n    load data local inpath '/user_bucket.txt'  overwrite into table user_demo; \n    \n    -- 在hive客户端当中加载数据\n    load data local inpath '/user_bucket.txt' into table user_demo;\n    \n    -- 加载数据到桶表user_buckets_demo中\n    insert into table user_buckets_demo select * from user_demo;\n    ```\n\n\n- 抽样查询桶表的数据\n  - tablesample抽样语句，语法：tablesample(bucket  x  out  of  y)\n    - x表示从第几个桶开始取数据（x 为start y为步长）\n    - y表示桶数的倍数，一共需要从 ==桶数/y==  个桶中取数据\n\n```sql\nselect * from user_buckets_demo tablesample(bucket 1 out of 2)\n\n-- 需要的总桶数=4/2=2个\n-- 先从第1个桶中取出数据\n-- 再从第1+2=3个桶中取出数据\n```\n\n### Hive修改表结构 \n\n```sql\n-- 修改表名称语法\nalter table  old_table_name  rename to  new_table_name;\nalter table stu3 rename to stu4;\n-- 增加列\nalter table stu4 add columns(address string);\n-- 修改列\nalter table stu4 change column address address_id int;\n\n```\n\n### 数据导入\n\n```sql\n-- 数据导入\nload data [local] inpath 'dataPath' overwrite | into table student [partition (partcol1=val1,…)]; \n\ninsert overwrite table score5 partition(month = '201806') select s_id,c_id,s_score from score;\n\n-- 查询语句创建表，并加载数据\ncreate table score6 as select * from score;\n\n-- 查询语句中创建表并加载数据（as select）\ncreate external table score (s_id string,c_id string,s_score int) row format delimited fields terminated by '\\t' location '/myscore7';\nmsck repair table score;\n\n-- 数据导入导出\nhive (myhive)> create table teacher2 like teacher;\nhive (myhive)> import table teacher2 from '/kfly/teacher';\n```\n\n### 数据导出\n\n~~~sql\n-- 将查询的结果导出到本地\ninsert overwrite local directory '/kfly/install/hivedatas/stu' select * from stu;\n-- 将查询的结果格式化导出到本地\ninsert overwrite local directory '/kkb/install/hivedatas/stu2' row format delimited fields terminated by  ',' select * from stu;\n-- 将查询的结果导出到HDFS上==(没有local)==\ninsert overwrite  directory '/kfly/hivedatas/stu'  row format delimited fields terminated by  ','  select * from stu;\n\n-- shell 命令导出\nhive -e \"sql语句\" >   file\nhive -f  sql文件   >    file\nbin/hive -e 'select * from myhive.stu;' > /kfly/install/hivedatas/student1.txt\n-- 导出到hdfs上\nexport table  myhive.stu to '/kfly/install/hivedatas/stuexport';\n~~~\n\n### 静态、动态分区\n\n~~~sql\n-- 静态分区\n  create table order_partition(order_number string,order_price  double,order_time string)\n  partitioned BY(month string)\n  row format delimited fields terminated by '\\t';\n  \n  load data local inpath '/kfly/install/hivedatas/order.txt' overwrite into table order_partition partition(month='2019-03');\n\n-- 动态分区\n--创建普通表\ncreate table t_order(\n    order_number string,\n    order_price  double, \n    order_time   string\n)row format delimited fields terminated by '\\t';\nload data local inpath '/kkb/install/hivedatas/order_partition.txt' overwrite into table t_order;\n\n--创建目标分区表\ncreate table order_dynamic_partition(\n    order_number string,\n    order_price  double    \n)partitioned BY(order_time string)\nrow format delimited fields terminated by '\\t';\n\n\n-- 要想进行动态分区，需要设置参数\n-- 开启动态分区功能\nhive> set hive.exec.dynamic.partition=true; \n-- 设置hive为非严格模式\nhive> set hive.exec.dynamic.partition.mode=nonstrict; \nhive> insert into table order_dynamic_partition partition(order_time) select order_number,order_price,order_time from t_order;\n-- 查看分区\nbin/hive>  show partitions order_dynamic_partition;\n~~~\n\n#### 排序\n\n```sql\n -- 全局排序\n order by \n \n --局部排序，每个reduce内部进行排序\n sort by\t\n set mapreduce.job.reduces=3;\n \n -- 分区排序,类似MR中partition，==采集hash算法，在map端将查询的结果中hash值相同的结果分发到对应的reduce文件中==。\n distribute by\n -- 当distribute by和sort by字段相同时，可以使用cluster by方式\n cluster by\n```\n\n### Hive java Api\n\n```xml\n <dependency>\n   <groupId>org.apache.hive</groupId>\n   <artifactId>hive-exec</artifactId>\n   <version>1.1.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hive</groupId>\n  <artifactId>hive-jdbc</artifactId>\n  <version>1.1.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hive</groupId>\n  <artifactId>hive-cli</artifactId>\n  <version>1.1.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hadoop</groupId>\n  <artifactId>hadoop-common</artifactId>\n  <version>2.6.0-cdh5.14.2</version>\n</dependency>\n```\n\n```java\nClass.forName(\"org.apache.hive.jdbc.HiveDriver\");\n//获取数据库连接\nConnection connection = DriverManager.getConnection(url, \"hadoop\",\"\");\n//定义查询的sql语句\nString sql=\"select * from stu\";\nPreparedStatement ps = connection.prepareStatement(sql);\nResultSet rs = ps.executeQuery();\n```\n","tags":["hive"]},{"title":"zookeeper分布式协调框架","url":"/2018/11/27/it/zookeeper分布式协调框架/","content":"# zookeeper分布式协调框架\n\n- ZooKeeper简单易用，能够很好的解决分布式框架在运行中，出现的各种协调问题。比如集群master主备切换、节点的上下线感知、统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等等\n\n- 是Google的Chubby的一个开源实现版\n- ZooKeeper\n  - 一个分布式的，开源的，用于分布式应用程序的协调服务（service）\n  - 主从架构\n- Zookeeper 作为一个分布式的服务框架\n  - 主要用来解决分布式集群中应用系统的一致性问题\n  - 它能提供基于类似于文件系统的**目录节点树**方式的数据存储，\n  - Zookeeper 作用主要是用来维护和监控存储的数据的状态变化，通过监控这些数据状态的变化，从而达到基于数据的集群管理\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201906091839.png)\n\n### 命令行\n\n- 集群命令（**每个节点运行此命令**）\n\n```shell\n# 启动ZooKeeper集群；在ZooKeeper集群中的每个节点执行此命令\n${ZK_HOME}/bin/zkServer.sh start\n# 停止ZooKeeper集群（每个节点执行以下命令）\n${ZK_HOME}/bin/zkServer.sh stop\n# 查看集群状态（每个节点执行此命令）\n${ZK_HOME}/bin/zkServer.sh status\n```\n\n```shell\n# 使用ZooKeeper自带的脚本，连接ZooKeeper的服务器\nzkCli.sh -server node01:2181,node02:2181,node03:2181\n```\n\n```shell\n#查看ZooKeeper根目录/下的文件列表\nls /\n```\n\n```shell\n#创建节点，并指定数据\ncreate /kfly\tkfly\n```\n\n```shell\n#获得某节点的数据\nget /kfly\n```\n\n```shell\n#修改节点的数据\nset /kfly kfly_top\n#删除节点\ndelete /kfly\n```\n\n### Java API\n\n- [Curator官网](< http://curator.apache.org/ >)\n- Curator编程\n\n ```java\n// 重试连接策略，失败重试次数；每次休眠5000毫秒\n// RetryPolicy policy = new ExponentialBackoffRetry(3000, 3);\nRetryNTimes retryPolicy = new RetryNTimes(10, 5000);\n// 设置客户端参数，参数1：指定连接的服务器集端口列表；参数2：重试策略\nclient = CuratorFrameworkFactory.newClient(ZK_ADDRESS, retryPolicy);\n// 启动客户端，连接到zk集群\nclient.start();\n\n///a/b/c\nclient.create().\n  creatingParentsIfNeeded().\n  withMode(CreateMode.PERSISTENT).\n  forPath(\"/kfly/top/orchid\", zNodeData.getBytes());\n\n// 查询节点列表\nclient.getChildren().forPath(\"/\")\n// 删除节点\nclient.delete().forPath(ZK_PATH);\n// 查询节点数据\nclient.getData().forPath(ZK_PATH)\n// 修改节点数据\nclient.setData().forPath(ZK_PATH, data2.getBytes())\n\t\t\n//设置节点的cache\nTreeCache treeCache = new TreeCache(client, \"/zk_test\");\n//设置监听器和处理过程\ntreeCache.getListenable().addListener(new TreeCacheListener(){\n\n})\n ```\n\n## 基本概念和操作\n\n> 分布式通信有几种方式\n>\n> 1、直接通过网络连接的方式进行通信；\n>\n> 2、通过共享存储的方式，来进行通信或数据的传输\n>\n> ZooKeeper使用第二种方式，提供分布式协调服务\n\n### 数据结构\n\n**ZooKeeper=①简版文件系统(Znode)+②原语+③通知机制(Watcher)。**\n\n- ZK文件系统\n  - 基于类似于文件系统的**目录节点树**方式的数据存储\n- 原语\n  - 可简单理解成ZooKeeper的基本的命令\n- Watcher（监听器）\n\n![img](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/fcfaaf51f3deb48f36625a57fa1f3a292df57834.jpg)\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201909181739.png)\n\n\n\n### **数据节点**ZNode\n\n|            | 持久节点  | 临时节点     |\n| ---------- | --------- | ------------ |\n| 非有序节点 | create    | create -e    |\n| 有序节点   | create -s | create -s -e |\n\n\n### 会话（Session) \n\n- 客户端要对ZooKeeper集群进行读写操作，得先与某一ZooKeeper服务器建立TCP长连接；此TCP长连接称为建立一个会话Session。\n\n- 每个会话有超时时间：SessionTimeout\n  - 当客户端与集群建立会话后，如果超过SessionTimeout时间，两者间没有通信，会话超时\n\n**特点**\n\n- 客户端打开一个Session中的请求以FIFO（先进先出）的顺序执行；\n  - 如客户端client01与集群建立会话后，先发出一个create请求，再发出一个get请求；\n  - 那么在执行时，会先执行create，再执行get\n- 若打开两个Session，无法保证Session间，请求FIFO执行；只能保证一个session中请求的FIFO\n\n**生命周期**\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201905311514.png)\n\n### 事务zxid\n\n- 事务\n  - 客户端的写请求，会对ZooKeeper中的数据做出更改；如增删改的操作\n  - 每次写请求，会生成一次事务\n  - 每个事务有一个全局唯一的事务ID，用 ZXID 表示；全局自增\n\n- 事务特点\n  - ACID：\n  - 原子性atomicity | 一致性consistency | 隔离性isolation | 持久性durability\n\n- ZXID结构：\n  - 通常是一个64位的数字。由**epoch+counter**组成\n  - epoch、counter各32位\n  \n  ```shell\n  cd /kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/version-2\n  cat currentEpoch \n  4 # 记录当前leader是选举出来的第几任\n  \n  get /kfly kfly\n  cZxid = 0x400000007 # 第四任leader通过的第七个事务\n  # zxid = epoch(32位) + counter(32位)\n  ```\n\n### Watcher监视与通知 \n\n- 客户端获取ZooKeeper服务器上的最新数据\n\n  - **方式一**轮询：ZooKeeper以远程服务的方式，被客户端访问；客户端以轮询的方式获得znode数据，效率会比较低（代价比较大）\n\n  ![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201905291811.png)\n\n  - **方式二**基于通知的机制：\n    - 客户端在znode上注册一个Watcher监视器\n    - 当znode上数据出现变化，watcher监测到此变化，通知客户端\n\n  ![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201905291818.png)\n\n\n- 客户端在服务器端，注册的事件监听器；\n- watcher用于监听znode上的某些事件\n  - 比如znode数据修改、节点增删等；\n  - 当监听到事件后，watcher会触发通知客户端\n\n> 注意：**Watcher是一个<font color='red'>单次触发的操作</font>**\n\n```shell\n# stat path [watch] 查看path节点状态\n# ls path [watch]  查看节点有哪些子节点\n# ls2 path [watch] 查看节点有哪些子节点、状态、相当于ls+stat\n# get path [watch] 获得节点各种数据\n```\n\n## 工作原理\n\n- ZooKeeper使用原子广播协议叫做Zab(ZooKeeper Automic Broadcast)协议\n- Zab协议有两种模式\n  - **恢复模式（选主）**：因为ZooKeeper也是主从架构；当ZooKeeper集群没有主的角色leader时，从众多服务器中选举leader时，处于此模式\n  - **广播模式（同步）**：当集群有了leader后，客户端向ZooKeeper集群读写数据时，集群处于此模式\n- 为了保证事务的顺序一致性，ZooKeeper采用了递增的事务id号（zxid）来标识事务，所有提议（proposal）都有zxid\n\n## 应用场景\n\n- ZooKeeper应用场景\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/20170221224856838.png)\n\n1. NameNode使用ZooKeeper实现高可用.\n\n2. Yarn ResourceManager使用ZooKeeper实现高可用.\n\n3. 利用ZooKeeper对HBase集群做高可用配置\n\n4. kafka使用ZooKeeper\n\n   - 保存消息消费信息比如offset.\n   \n   - 用于检测崩溃\n   \n- 主题topic发现\n  \n   - 保持主题的生产和消费状态\n   \n## ACL访问控制列表\n\nzk做为分布式架构中的重要中间件，通常会在上面以节点的方式存储一些关键信息，默认情况下，所有应用都可以读写任何节点，在复杂的应用中，这不太安全，ZK通过ACL机制来解决访问权限问题\\\n\nACL(Access Control List)可以设置某些客户端，对zookeeper服务器上节点的权限，如增删改查等\n\nZooKeeper 采用 ACL（Access Control Lists）策略来进行权限控制。ZooKeeper 定义了如下5种权限。\n\n- CREATE: 创建**子节点**的权限。\n\n- READ: 获取节点数据和子节点列表的权限。\n\n- WRITE：更新节点数据的权限。\n\n- DELETE: 删除**子节点**的权限。\n\n- ADMIN: 设置节点ACL的权限。\n\n>  注意：CREATE 和 DELETE 都是针对子节点的权限控制。\n\n### 设置ACL\n\n1. 五种权限简称\n\n   ```shell\n   CREATE -> 增 -> c\n   READ -> 查 -> r\n   WRITE -> 改 -> w\n   DELETE -> 删 -> d\n   ADMIN -> 管理 -> a\n   这5种权限简写为**crwda**\n   ```\n\n2. 鉴权模式\n\n```shell\n- world：默认方式，相当于全世界都能访问\n- auth：代表已经认证通过的用户(cli中可以通过addauth digest user:pwd 来添加当前上下文中的授权用户)\n- digest：即用户名:密码这种方式认证，这也是业务系统中最常用的\n- ip：使用Ip地址认证\n```\n\n3. 演示auth方式\n\n```shell\n# 1）增加一个认证用户\n# addauth digest 用户名:密码明文\naddauth digest kfly:kflt\n\n# 2）设置权限\n# setAcl /path auth:用户名:密码明文:权限\nsetAcl /zk_test auth:kfly:kfly:rw\n\n# 3）查看ACL设置\ngetAcl /zk_test\n```\n\n## HDFS HA方案\n\n- 关于ZooKeeper监听器有三个重要的逻辑：\n\n   - **注册**：客户端向ZooKeeper集群注册监听器\n\n   - **监听事件**：监听器负责监听特定的事件\n\n   - **回调函数**：当监听器监听到事件的发生后，调用注册监听器时定义的回调函数\n\n### 原理\n\n> 关键逻辑：\n>\n> ①监听器：**注册、监听事件、回调函数**\n>\n> ②共享存储：JournalNode\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201905211519.png)   \n\n- 在Hadoop 1.x版本，HDFS集群的NameNode一直存在单点故障问题：\n  - 集群只存在一个NameNode节点，它维护了HDFS所有的元数据信息\n  - 当该节点所在服务器宕机或者服务不可用，整个HDFS集群处于不可用状态\n  \n- Hadoop 2.x版本提出了高可用 (High Availability, HA) 解决方案\n  \n> HDFS HA方案，主要分两部分：\n  >\n> ①元数据同步\n  >\n  > ②主备切换\n\n- 元数据同步\n- 在同一个HDFS集群，运行两个互为主备的NameNode节点。\n  - 一台为主Namenode节点，处于Active状态，一台为备NameNode节点，处于Standby状态。\n  - 其中只有Active NameNode对外提供读写服务，Standby NameNode会根据Active NameNode的状态变化，在必要时**切换**成Active状态。\n  - **JournalNode集群**\n    - 在主备切换过程中，新的Active NameNode必须确保与原Active NamNode元数据同步完成，才能对外提供服务\n    - 所以用JournalNode集群作为共享存储系统；\n    - 当客户端对HDFS做操作，会在Active NameNode中edits.log文件中作日志记录，同时日志记录也会写入JournalNode集群；负责存储HDFS新产生的元数据\n    - 当有新数据写入JournalNode集群时，Standby NameNode能监听到此情况，将新数据同步过来\n    - Active NameNode(写入)和Standby NameNode(读取)实现元数据同步\n    - 另外，所有datanode会向两个主备namenode做block report\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201909200732.png)\n\n- <font color='blue'>②主备切换</font>\n\n- **ZKFC涉及角色**\n\n  - 每个NameNode节点上各有一个ZKFC进程\n  - ZKFC即ZKFailoverController，作为独立进程存在，负责控制NameNode的主备切换\n  - ZKFC会监控NameNode的健康状况，当发现Active NameNode异常时，通过Zookeeper集群进行namenode主备选举，完成Active和Standby状态的切换\n    - ZKFC在启动时，同时会初始化HealthMonitor和ActiveStandbyElector服务\n    - ZKFC同时会向HealthMonitor和ActiveStandbyElector注册相应的回调方法（如上图的①回调、②回调）\n    - **HealthMonitor**定时调用NameNode的HAServiceProtocol RPC接口(monitorHealth和getServiceStatus)，监控NameNode的健康状态并向ZKFC反馈\n    - **ActiveStandbyElector**接收ZKFC的选举请求，通过Zookeeper自动完成namenode主备选举\n    - 选举完成后回调ZKFC的主备切换方法对NameNode进行Active和Standby状态的切换\n  \n- **主备选举过程：**两个ZKFC通过各自ActiveStandbyElector发起NameNode的主备选举，这个过程利用Zookeeper的写一致性和临时节点机制实现\n\n  - 当发起一次**主备**选举时，ActiveStandbyElector会尝试在Zookeeper创建临时节点`/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock`，Zookeeper的写一致性保证最终只会有一个ActiveStandbyElector创建成功\n- ActiveStandbyElector从ZooKeeper获得选举结果\n  \n  - 创建成功的 ActiveStandbyElector回调ZKFC的回调方法②，将对应的NameNode切换为Active NameNode状态\n- 而创建失败的ActiveStandbyElector回调ZKFC的回调方法②，将对应的NameNode切换为Standby NameNode状态\n  \n- 不管是否选举成功，所有ActiveStandbyElector都会在临时节点ActiveStandbyElectorLock上注册一个Watcher监听器，来监听这个节点的状态变化事件\n  \n  - 如果Active NameNode对应的HealthMonitor检测到NameNode状态异常时，通知对应ZKFC\n- ZKFC会调用 ActiveStandbyElector 方法，删除在Zookeeper上创建的临时节点ActiveStandbyElectorLock\n  \n  - 此时，Standby NameNode的ActiveStandbyElector注册的Watcher就会监听到此节点的 NodeDeleted事件。\n- 收到这个事件后，此ActiveStandbyElector发起主备选举，成功创建临时节点ActiveStandbyElectorLock，如果创建成功，则Standby NameNode被选举为Active NameNode（过程同上）\n  \n- **如何防止脑裂**\n\n  - 脑裂\n\n    在分布式系统中双主现象又称为脑裂，由于Zookeeper的“假死”、长时间的垃圾回收或其它原因都可能导致双Active NameNode现象，此时两个NameNode都可以对外提供服务，无法保证数据一致性\n\n  - 隔离\n\n    对于生产环境，这种情况的出现是毁灭性的，必须通过自带的**隔离（Fencing）**机制预防此类情况\n\n  - 原理\n    - ActiveStandbyElector成功创建ActiveStandbyElectorLock临时节点后，会创建另一个ActiveBreadCrumb持久节点\n  \n    - ActiveBreadCrumb持久节点保存了Active NameNode的地址信息\n  \n    - 当Active NameNode在正常的状态下断开Zookeeper Session，会一并删除临时节点ActiveStandbyElectorLock、持久节点ActiveBreadCrumb\n  \n    - 但是如果ActiveStandbyElector在异常的状态下关闭Zookeeper Session，那么持久节点ActiveBreadCrumb会保留下来（此时有可能由于active NameNode与ZooKeeper通信不畅导致，所以此NameNode**还处于active状态**）\n  \n    - 当另一个NameNode要由standy变成active状态时，会发现上一个Active NameNode遗留下来的ActiveBreadCrumb节点，那么会回调ZKFailoverController的方法对旧的Active NameNode进行    fencing\n  \n      ①首先ZKFC会尝试调用旧Active NameNode的HAServiceProtocol RPC接口的transitionToStandby方法，看能否将其状态切换为Standby\n  \n      ②如果transitionToStandby方法切换状态失败，那么就需要执行Hadoop自带的隔离措施，Hadoop目前主要提供两种隔离措施：\n      sshfence：SSH to the Active NameNode and kill the process；\n      shellfence：run an arbitrary shell command to fence the Active NameNode\n  \n      ③只有成功地fencing之后，选主成功的ActiveStandbyElector才会回调ZKFC的becomeActive方法将对应的NameNode切换为Active，开始对外提供服务\n  \n\n## ZooKeeper读写\n\n### ZooKeeper集群架构图\n\n- ZooKeeper集群也是主从架构\n  - 主角色：leader\n  - 从角色：follower或observer；统称为learner\n\n### 读操作\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201910251149.png)\n\n- 常见的读取操作，如ls /查看目录；get /zktest查询ZNode数据\n\n- 读操作\n\n  - 客户端先与某个ZK服务器建立Session\n\n  - 然后，直接从此ZK服务器读取数据，并返回客户端即可\n\n  - 关闭Session\n\n### 写操作\n\n- 写操作比较复杂；为了便于理解，先举个生活中的例子：去银行存钱\n  - 银行柜台共有5个桂圆姐姐，编程从①到⑤，其中③是**领导leader**\n  - 有两个客户\n  - 客户①找到桂圆①，说：昨天少给我存了1000万，现在需要给我加进去\n  - 桂圆①说，对不起先生，我没有这么大的权限，请你稍等一下，我向领导**leader**③汇报一下\n  - 领导③收到消息后，为了做出英明的决策，要征询下属的意见(**proposal**)①②④⑤\n  - 只要有**过半数quorum**（5/2+1=3，包括leader自己）同意，则leader做出决定(**commit**)，同意此事\n  - leader告知所有下属follower，你们都记下此事生效\n  - 桂圆①答复客户①，说已经给您账号里加了1000万\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image2019061212537.png)\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201910251203.png)\n\n- 客户端写操作\n  \n  - ①客户端向zk集群写入数据，如create /kfly；与一个follower建立Session连接，从节点follower01\n  \n  - ②follower将写请求转发给leader\n  \n  - ③leader收到消息后，发出**proposal提案**（创建/kfly），每个follower先**记录下**要创建/kfly\n  \n  - ④超过**半数quorum**（包括leader自己）同意提案，则leader提交**commit提案**，leader本地创建/kfly节点ZNode\n  \n  - ⑤leader通知所有follower，也commit提案；follower各自在本地创建/kfly\n  \n  - ⑥follower01响应client\n  \n\n### ZooKeeper状态同步\n\n完成leader选举后，zk就进入ZooKeeper之间状态同步过程\n\n1. leader构建NEWLEADER封包，包含leader中最大的zxid值；广播给其它follower\n2. follower收到后，如果自己的最大zxid小于leader的，则需要与leader状态同步；否则不需要\n3. leader给需要同步的每个follower创建LearnerHandler线程，负责数据同步请求\n4. leader主线程等待LearnHandler线程处理结果\n5. 只有多数follower完成同步，leader才开始对外服务，响应写请求\n6. LearnerHandler线程处理逻辑\n   1. 接收follower封包FOLLOWERINFO，包含此follower最大zxid（代称f-max-zxid）\n   2. f-max-zxid与leader最大zxid（代称l-max-zxid）比较\n   3. 若相等，说明当前follower是最新的\n   4. 另外，若在判断期间，有没有新提交的proposal\n      1. 如果有那么会发送DIFF封包将有差异的数据同步过去.同时将follower没有的数据逐个发送COMMIT封包给follower要求记录下来.\n      2. 如果follower数据id更大,那么会发送TRUNC封包告知截除多余数据.\n      3. 如果这一阶段内没有提交的提议值,直接发送SNAP封包将快照同步发送给follower.\n   5. 以上消息完毕之后,发送UPTODATE封包告知follower当前数据就是最新的了\n   6. 再次发送NEWLEADER封包宣称自己是leader,等待follower的响应.\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201906140856.png)\n\n\n\n## ZooKeeper服务\n\n### **架构问题**\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/zkservice.jpg)\n\n- leader很重要？\n- 如果没有leader怎么办？\n  - 开始选举新的leader\n\n- **ZooKeeper服务器四种状态：**\n    - looking：服务器处于寻找Leader群首的状态\n\n    - leading：服务器作为群首时的状态\n\n    - following：服务器作为follower跟随者时的状态\n\n    - observing：服务器作为观察者时的状态\n\n> leader选举分**两种情况**\n>\n> - 全新集群leader选举\n>\n> - 非全新集群leader选举\n\n### 全新集群leader选举\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201906130749.png)\n\n  - 以3台机器组成的ZooKeeper集群为例 \n\n  - 原则：集群中过**半数**（多数派quorum）Server启动后，才能选举出Leader；\n\n      - 此处quorum数是多少？3/2+1=2\n      - 即quorum=集群服务器数除以2，再加1\n\n  - 理解leader选举前，先了解几个概念\n\n        - 选举过程中，每个server需发出投票；投票信息**vote信息**结构为(sid, zxid)\n\n            全新集群，server1~3初始投票信息分别为：\n      \n            ​\tserver1 ->  **(1, 0)**\n          ​\t​server2 ->  **(2, 0)**\n          ​\tserver3 ->  **(3, 0)**\n        \n    - **leader选举公式**：\n    \n      ​\tserver1 vote信息 (sid1,zxid1)\n    \n      ​\tserver2 vote信息 (sid2,zxid2)\n    \n      ​\t**①zxid大的server胜出；**\n    \n      ​\t**②若zxid相等，再根据判断sid判断，sid大的胜出**\n  \n  - 选举leader流程：\n\n    > 假设按照ZK1、ZK2、ZK3的依次启动\n    \n    - 启动ZK1后，投票给自己，vote信息(1,0)，没有过半数，选举不出leader\n    \n    - 再启动ZK2；ZK1和ZK2票投给自己及其他服务器；ZK1的投票为(1, 0)，ZK2的投票为(2, 0)\n    \n    - 处理投票。每个server将收到的多个投票做处理\n      - 如ZK1投给自己的票(1,0)与ZK2传过来的票(2,0)比较；\n      - 利用leader选举公式，因为zxid都为0，相等；所以判断sid最大值；2>1；ZK1更新自己的投票为(2, 0)\n      - ZK2也是如此逻辑，ZK2更新自己的投票为(2,0)\n    \n    - 再次发起投票\n      - ZK1、ZK2上的投票都是(2,0)\n      - 发起投票后，ZK1上有一个自己的票(2,0)和一票来自ZK2的票(2,0)，这两票都选ZK2为leader\n      - ZK2上有一个自己的票(2,0)和一票来自ZK1的票(2,0)，这两票都选ZK2为leader\n      - 统计投票。server统计投票信息，是否有半数server投同一个服务器为leader；\n        - ZK2当选2票；多数\n      - 改变服务器状态。确定Leader后，各服务器更新自己的状态\n        - 更改ZK2状态从looking到leading，为Leader\n        - 更改ZK1状态从looking到following，为Follower\n    \n    - 当K3启动时，发现已有Leader，不再选举，直接从LOOKING改为FOLLOWING\n\n- 选举原理同上比较zxid、sid\n\n\n\n## ZAB算法\n\n### 仲裁quorum\n\n- 什么是仲裁quorum？\n\n  - 发起proposal时，只要多数派同意，即可生效\n\n- 为什么要仲裁？\n\n  - 多数据派不需要所有的服务器都响应，proposal就能生效\n  - 且能提高集群的响应速度\n\n- quorum数如何选择？\n\n  -    **集群节点数 / 2 + 1**\n  - 如3节点的集群：quorum数=3/2+1=2\n\n### 网络分区、脑裂\n\n  - 网络分区：网络通信故障，集群被分成了2部分\n\n  - 脑裂：\n\n    - 原leader处于一个分区；\n    - 另外一个分区选举出新的leader \n    - 集群出现2个leader\n\n### ZAB算法\n\n> [raft算法动图地址](<http://thesecretlivesofdata.com/raft/#replication>)\n\n- **ZAB与RAFT相似，区别如下：**\n\n  1、zab心跳从follower到leader；raft相反\n\n  2、zab任期叫epoch\n\n- 一下以RAFT算法动图为例，分析ZAB算法\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/脑裂.gif)\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/脑裂-1560463867696.png)\n\n\n\n###  ZooKeeper服务器个数\n\n- 仲裁模式下，服务器个数最好为奇数个。\n\n```table\nquorum数=3/2+1=2\n集群数\t\t\t\tquorum数\t\t\t最大可容灾数\n5 / 2 + 1 = 3\t\t\t\t\t\t\t5-3=2\n6 / 2 +1 =  4\t\t\t\t\t\t\t6-4=2\n```\n\n## 分布式锁\n\n![](http://kflys.gitee.io/upic/2020/03/31/uPic/kfly-top/zookeeper分布式协调框架/assets/Image201906121639.png)\n\n```shell\ncreate -s -e /locker/node_ ndata\n```\n","tags":["zookeeper","zookeeper ha"]},{"title":"Hadoop架构原理之Yarn","url":"/2018/11/01/it/hadoop/Hadoop架构原理之Yarn/","content":"\n## YARN\n\n![img](http://kflys.gitee.io/upic/2020/03/30/uPic/Hadoop架构原理之Yarn/assets/a19a61bc-9378-3e38-944a-899a09f37908.jpg)\n\n- Apache Hadoop YARN(Yet Another Resource Negotiator)是Hadoop的子项目，为分离Hadoop2.0资源管理和计算组件而引入\n- YRAN具有足够的通用性，可以支持其它的分布式计算模式\n\n![img](http://kflys.gitee.io/upic/2020/03/30/uPic/Hadoop架构原理之Yarn/assets/99b59921-9a97-3199-8c39-d3b77dfdceaf.jpg)\n\n\n\n## YARN架构\n\n- 类似HDFS，YARN也是经典的**主从（master/slave）架构**\n  - YARN服务由一个ResourceManager（RM）和多个NodeManager（NM）构成\n  - ResourceManager为主节点（master）\n  - NodeManager为从节点（slave）\n\n![yarn的体系结构](http://kflys.gitee.io/upic/2020/03/30/uPic/Hadoop架构原理之Yarn/assets/Figure3Architecture-of-YARN.png)\n\n\n\n\n\n- ApplicationMaster可以在容器内运行任何类型的任务。例如，MapReduce ApplicationMaster请求容器启动map或reduce任务，而Giraph ApplicationMaster请求容器运行Giraph任务。\n\n| 组件名                 | 作用                                                         |\n| :--------------------- | ------------------------------------------------------------ |\n| **ApplicationManager** | 相当于这个Application的监护人和管理者，负责监控、管理这个Application的所有Attempt在cluster中各个节点上的具体运行，同时负责向Yarn ResourceManager申请资源、返还资源等； |\n| **NodeManager**        | 是Slave上一个独立运行的进程，负责上报节点的状态(磁盘，内存，cpu等使用信息)； |\n| **Container**          | 是yarn中分配资源的一个单位，包涵内存、CPU等等资源，YARN以Container为单位分配资源； |\n\nResourceManager 负责对各个 NodeManager 上资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的 ApplicationMaster，它负责向 ResourceManager 申请资源，并要求 NodeManger 启动可以占用一定资源的任务。由于不同的 ApplicationMaster 被分布到不同的节点上，因此它们之间不会相互影响。\n\nClient 向 ResourceManager 提交的每一个应用程序都必须有一个 ApplicationMaster，它经过 ResourceManager 分配资源后，运行于某一个 Slave 节点的 Container 中，具体做事情的 Task，同样也运行与某一个 Slave 节点的 Container 中。\n\n### **ResourceManager**\n\n- RM是一个全局的资源管理器，集群只有一个\n  - 负责整个系统的资源管理和分配\n  - 包括处理客户端请求\n  - 启动/监控 ApplicationMaster\n  - 监控 NodeManager、资源的分配与调度\n- 它主要由两个组件构成：\n  - 调度器（Scheduler）\n  - 应用程序管理器（Applications Manager，ASM）\n\n- 调度器\n  - 调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。\n  - 需要注意的是，该调度器是一个“纯调度器”\n    - 它不从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。\n    - 调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。\n\n- 应用程序管理器\n  - 应用程序管理器主要负责管理整个系统中所有应用程序\n  - 接收job的提交请求\n  - 为应用分配第一个 Container 来运行 ApplicationMaster，包括应用程序提交、与调度器协商资源以启动 ApplicationMaster、监控 ApplicationMaster 运行状态并在失败时重新启动它等\n\n### **NodeManager**\n\n![nodemanager架构](http://kflys.gitee.io/upic/2020/03/30/uPic/Hadoop架构原理之Yarn/assets/20190103113256851.png)\n\n- NodeManager 是一个 slave 服务，整个集群有多个\n\n- NodeManager ：\n  - 它负责接收 ResourceManager 的资源分配请求，分配具体的 Container 给应用。\n  - 负责监控并报告 Container 使用信息给 ResourceManager。\n\n- 功能：\n\n  - NodeManager 本节点上的资源使用情况和各个 Container 的运行状态（cpu和内存等资源）\n  - 接收及处理来自 ResourceManager 的命令请求，分配 Container 给应用的某个任务；\n  - 定时地向RM汇报以确保整个集群平稳运行，RM 通过收集每个 NodeManager 的报告信息来追踪整个集群健康状态的，而 NodeManager 负责监控自身的健康状态；\n  - 处理来自 ApplicationMaster 的请求；\n  - 管理着所在节点每个 Container 的生命周期；\n  - 管理每个节点上的日志；\n\n  - 当一个节点启动时，它会向 ResourceManager 进行注册并告知 ResourceManager 自己有多少资源可用。\n  - 在运行期，通过 NodeManager 和 ResourceManager 协同工作，这些信息会不断被更新并保障整个集群发挥出最佳状态。\n\n  - NodeManager 只负责管理自身的 Container，它并不知道运行在它上面应用的信息。负责管理应用信息的组件是 ApplicationMaster\n\n### Container\n\n- Container 是 YARN 中的资源抽象\n  - 它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等\n  - 当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。\n  - YARN 会为每个任务分配一个 Container，且该任务只能使用该 Container 中描述的资源。\n\n- Container 和集群NodeManager节点的关系是：\n  - 一个NodeManager节点可运行多个 Container\n  - 但一个 Container 不会跨节点。\n  - 任何一个 job 或 application 必须运行在一个或多个 Container 中\n  - 在 Yarn 框架中，ResourceManager 只负责告诉 ApplicationMaster 哪些 Containers 可以用\n  - ApplicationMaster 还需要去找 NodeManager 请求分配具体的 Container。\n\n- 需要注意的是\n  - Container 是一个动态资源划分单位，是根据应用程序的需求动态生成的\n  - 目前为止，YARN 仅支持 CPU 和内存两种资源，且使用了轻量级资源隔离机制 Cgroups 进行资源隔离。\n\n- 功能：\n  - 对task环境的抽象；\n\n  - 描述一系列信息；\n\n  - 任务运行资源的集合（cpu、内存、io等）；\n\n  - 任务运行环境\n\n### **ApplicationMaster**\n\n- 功能：\n  - 数据切分；\n  - 为应用程序申请资源并进一步分配给内部任务（TASK）；\n  - 任务监控与容错；\n  - 负责协调来自ResourceManager的资源，并通过NodeManager监视容器的执行和资源使用情况。\n\n- ApplicationMaster 与 ResourceManager 之间的通信\n  - 是整个 Yarn 应用从提交到运行的最核心部分，是 Yarn 对整个集群进行动态资源管理的根本步骤\n  - Yarn 的动态性，就是来源于多个Application 的 ApplicationMaster 动态地和 ResourceManager 进行沟通，不断地申请、释放、再申请、再释放资源的过程。\n\n### Resource Request\n\n[引用连接](https://www.jianshu.com/p/f50e85bdb9ce)\n\n- Yarn的设计目标\n  - 允许我们的各种应用以共享、安全、多租户的形式使用整个集群。\n  - 并且，为了保证集群资源调度和数据访问的高效性，Yarn还必须能够感知整个集群拓扑结构。\n\n- 为了实现这些目标，ResourceManager的调度器Scheduler为应用程序的资源请求定义了一些灵活的协议，**Resource Request**和**Container**。\n  - 一个应用先向ApplicationMaster发送一个满足自己需求的资源请求\n  - 然后ApplicationMaster把这个资源请求以resource-request的形式发送给ResourceManager的Scheduler\n  - Scheduler再在这个原始的resource-request中返回分配到的资源描述Container。\n\n- 每个ResourceRequest可看做一个可序列化Java对象，包含的字段信息如下：\n\n```xml\n<!--\n- resource-name：资源名称，现阶段指的是资源所在的host和rack，后期可能还会支持虚拟机或者更复杂的网络结构\n- priority：资源的优先级\n- resource-requirement：资源的具体需求，现阶段指内存和cpu需求的数量\n- number-of-containers：满足需求的Container的集合\n-->\n<resource-name, priority, resource-requirement, number-of-containers>\n```\n\n### JobHistoryServer \n\n```shell\nmr-jobhistory-daemon.sh start historyserver\nnode01:19888\n```\n\n### Timeline Server \n\n- 用来写日志服务数据 , 一般来写与第三方结合的日志服务数据(比如spark等)\n- 它是对jobhistoryserver功能的有效补充，jobhistoryserver只能对mapreduce类型的作业信息进行记录\n- 它记录除了jobhistoryserver能够进行对作业运行过程中信息进行记录之外\n- 还记录更细粒度的信息，比如任务在哪个队列中运行，运行任务时设置的用户是哪个用户。\n\n- 根据官网的解释jobhistoryserver只能记录mapreduce应用程序的记录，timelineserver功能更强大,但不是替代jobhistory两者是功能间的互补关系.\n\n![1563006522419](http://kflys.gitee.io/upic/2020/03/30/uPic/Hadoop架构原理之Yarn/assets/1563006522419.png)\n\n- [官网教程](<http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/TimelineServer.html>)\n\n\n\n## YARN应用运行原理\n\n![yarn架构图](http://kflys.gitee.io/upic/2020/03/30/uPic/Hadoop架构原理之Yarn/assets/yarn_architecture.gif)\n\n\n###  YARN应用提交过程\n\n- Application在Yarn中的执行过程，整个执行过程可以总结为三步：\n\n  - 应用程序提交\n  - 启动应用的ApplicationMaster实例\n  - ApplicationMaster 实例管理应用程序的执行\n\n- **具体提交过程为：**\n\n  ![](http://kflys.gitee.io/upic/2020/03/30/uPic/Hadoop架构原理之Yarn/assets/Image201909161351.png)\n\n  - 客户端程序向 ResourceManager 提交应用，并请求一个 ApplicationMaster 实例；\n  - ResourceManager 找到一个可以运行一个 Container 的 NodeManager，并在这个 Container 中启动 ApplicationMaster 实例；\n  - ApplicationMaster 向 ResourceManager 进行注册，注册之后客户端就可以查询 ResourceManager 获得自己 ApplicationMaster 的详细信息，以后就可以和自己的 ApplicationMaster 直接交互了（这个时候，客户端主动和 ApplicationMaster 交流，应用先向 ApplicationMaster 发送一个满足自己需求的资源请求）；\n  - ApplicationMaster 根据 resource-request协议 向 ResourceManager 发送 resource-request请求；\n  - 当 Container 被成功分配后，ApplicationMaster 通过向 NodeManager 发送 **container-launch-specification**信息 来启动Container，container-launch-specification信息包含了能够让Container 和 ApplicationMaster 交流所需要的资料；\n  - 应用程序的代码以 task 形式在启动的 Container 中运行，并把运行的进度、状态等信息通过 **application-specific**协议 发送给ApplicationMaster；\n  - 在应用程序运行期间，提交应用的客户端主动和 ApplicationMaster 交流获得应用的运行状态、进度更新等信息，交流协议也是 **application-specific**协议；\n  - 应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster 向 ResourceManager 取消注册然后关闭，用到所有的 Container 也归还给系统。\n\n- **精简版的：**\n\n  - 步骤1：用户将应用程序提交到 ResourceManager 上；\n  - 步骤2：ResourceManager为应用程序 ApplicationMaster 申请资源，并与某个 NodeManager 通信启动第一个 Container，以启动ApplicationMaster；\n  - 步骤3：ApplicationMaster 与 ResourceManager 注册进行通信，为内部要执行的任务申请资源，一旦得到资源后，将于 NodeManager 通信，以启动对应的 Task；\n  - 步骤4：所有任务运行完成后，ApplicationMaster 向 ResourceManager 注销，整个应用程序运行结束。\n\n### MapReduce on YARN\n\n![img](http://kflys.gitee.io/upic/2020/03/30/uPic/Hadoop架构原理之Yarn/assets/820234-20160604233916133-2026396104.jpg)\n\n- 提交作业\n\n  - ①程序打成jar包，在客户端运行hadoop jar命令，提交job到集群运行\n  - job.waitForCompletion(true)中调用Job的submit()，此方法中调用JobSubmitter的submitJobInternal()方法；\n    - ②submitClient.getNewJobID()向resourcemanager请求一个MR作业id\n    - 检查输出目录：如果没有指定输出目录或者目录已经存在，则报错\n    - 计算作业分片；若无法计算分片，也会报错\n    - ③运行作业的相关资源，如作业的jar包、配置文件、输入分片，被上传到HDFS上一个以作业ID命名的目录（jar包副本默认为10，运行作业的任务，如map任务、reduce任务时，可从这10个副本读取jar包）\n    - ④调用resourcemanager的submitApplication()提交作业\n  - 客户端**每秒**查询一下作业的进度（map 50% reduce 0%），进度如有变化，则在控制台打印进度报告；\n  - 作业如果成功执行完成，则打印相关的计数器\n  - 但如果失败，在控制台打印导致作业失败的原因（要学会查看日志，定位问题，分析问题，解决问题）\n\n- **初始化作业**\n\n  - 当ResourceManager(一下简称RM)收到了submitApplication()方法的调用通知后，请求传递给RM的scheduler（调度器）；调度器分配container（容器）\n  - ⑤a RM与指定的NodeManager通信，通知NodeManager启动容器；NodeManager收到通知后，创建占据特定资源的container；\n  - ⑤b 然后在container中运行MRAppMaster进程\n  - ⑥MRAppMaster需要接受任务（各map任务、reduce任务的）的进度、完成报告，所以appMaster需要创建多个簿记对象，记录这些信息\n  - ⑦从HDFS获得client计算出的输入分片split\n    - 每个分片split创建一个map任务\n    - 通过 mapreduce.job.reduces 属性值(编程时，jog.setNumReduceTasks()指定)，知道当前MR要创建多少个reduce任务\n    - 每个任务(map、reduce)有task id\n\n- **Task 任务分配**\n\n  - 如果小作业，appMaster会以uberized的方式运行此MR作业；appMaster会决定在它的JVM中顺序此MR的任务；\n\n    - 原因是，若每个任务运行在一个单独的JVM时，都需要单独启动JVM，分配资源（内存、CPU），需要时间；多个JVM中的任务再在各自的JVM中并行运行\n\n    - 若将所有任务在appMaster的JVM中顺序执行的话，更高效，那么appMaster就会这么做 ，任务作为uber任务运行\n\n    - 小作业判断依据：①小于10个map任务；②只有一个reduce任务；③MR输入大小小于一个HDFS块大小\n\n    - 如何开启uber?设置属性 mapreduce.job.ubertask.enable 值为true\n\n      ```java\n      configuration.set(\"mapreduce.job.ubertask.enable\", \"true\");\n      ```\n\n    - 在运行任何task之前，appMaster调用setupJob()方法，创建OutputCommitter，创建作业的最终输出目录（一般为HDFS上的目录）及任务输出的临时目录（如map任务的中间结果输出目录）\n\n  - ⑧若作业不以uber任务方式运行，那么appMaster会为作业中的每一个任务（map任务、reduce任务）向RM请求container\n\n    - 由于reduce任务在进入排序阶段之前，所有的map任务必须执行完成；所以，为map任务申请容器要优先于为reduce任务申请容器\n    - 5%的map任务执行完成后，才开始为reduce任务申请容器\n    - 为map任务申请容器时，遵循数据本地化，调度器尽量将容器调度在map任务的输入分片所在的节点上（移动计算，不移动数据）\n\n    - reduce任务能在集群任意计算节点运行\n    - 默认情况下，为每个map任务、reduce任务分配1G内存、1个虚拟内核，由属性决定mapreduce.map.memory.mb、mapreduce.reduce.memory.mb、mapreduce.map.cpu.vcores、mapreduce.reduce.reduce.cpu.vcores\n\n- **Task 任务执行**\n\n  - 当调度器为当前任务分配了一个NodeManager（暂且称之为NM01）的容器，并将此信息传递给appMaster后；appMaster与NM01通信，告知NM01启动一个容器，并此容器占据特定的资源量（内存、CPU）\n  - NM01收到消息后，启动容器，此容器占据指定的资源量\n  - 容器中运行YarnChild，由YarnChild运行当前任务（map、reduce）\n  - ⑩在容器中运行任务之前，先将运行任务需要的资源拉取到本地，如作业的JAR文件、配置文件、分布式缓存中的文件\n\n- **作业运行进度与状态更新**\n\n  - 作业job以及它的每个task都有状态（running、successfully completed、failed），当前任务的运行进度、作业计数器\n  - 任务在运行期间，每隔3秒向appMaster汇报执行进度、状态（包括计数器）\n  - appMaster汇总目前运行的所有任务的上报的结果\n  - 客户端每个1秒，轮询访问appMaster获得作业执行的最新状态，若有改变，则在控制台打印出来\n\n- 完成作业\n\n  - appMaster收到最后一个任务完成的报告后，将作业状态设置为成功\n  - 客户端轮询appMaster查询进度时，发现作业执行成功，程序从waitForCompletion()退出\n  - 作业的所有统计信息打印在控制台\n  - appMaster及运行任务的容器，清理中间的输出结果\n  - 作业信息被历史服务器保存，留待以后用户查询\n\n\n\n### yarn应用生命周期\n\n- RM: Resource Manager\n- AM: Application Master\n- NM: Node Manager\n\n1. Client向RM提交应用，包括AM程序及启动AM的命令。\n\n2. RM为AM分配第一个容器，并与对应的NM通信，令其在容器上启动应用的AM。\n\n3. AM启动时向RM注册，允许Client向RM获取AM信息然后直接和AM通信。\n\n4. AM通过资源请求协议，为应用协商容器资源。\n\n5. 如容器分配成功，AM要求NM在容器中启动应用，应用启动后可以和AM独立通信。\n\n6. 应用程序在容器中执行，并向AM汇报。\n\n7. 在应用执行期间，Client和AM通信获取应用状态。\n\n8. 应用执行完成，AM向RM注销并关闭，释放资源。\n\n   **申请资源->启动appMaster->申请运行任务的container->分发Task->运行Task->Task结束->回收container**\n\n## 如何使用YARN\n\n### 配置文件\n\n```xml\n<!-- $HADOOP_HOME/etc/hadoop/mapred-site.xml -->\n<configuration>\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n</configuration>\n```\n\n```xml\n<!-- $HADOOP_HOME/etc/hadoop/yarn-site.xml -->\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n</configuration>\n```\n\n### YARN启动停止\n\n- 启动 ResourceManager 和 NodeManager （以下分别简称RM、NM）\n\n```shell\n#主节点运行命令\n$HADOOP_HOME/sbin/start-yarn.sh\n```\n\n- 停止 RM 和 NM \n\n```shell\n#主节点运行命令\n$HADOOP_HOME/sbin/stop-yarn.sh\n```\n\n- 若RM没有启动起来，可以单独启动\n\n```shell\n#若RM没有启动，在主节点运行命令\n$HADOOP_HOME/sbin/yarn-daemon.sh start resouremanager\n#相反，可单独关闭\n$HADOOP_HOME/sbin/yarn-daemon.sh stop resouremanager\n\n```\n\n- 若NM没有启动起来，可以单独启动\n\n```shell\n#若NM没有启动，在相应节点运行命令\n$HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager\n#相反，可单独关闭\n$HADOOP_HOME/sbin/yarn-daemon.sh stop nodemanager\n\n```\n\n### YARN常用命令\n\n```shell\n#1.查看正在运行的任务\nyarn application -list\n\n#2.杀掉正在运行任务\nyarn application -kill 任务id\n\n#3.查看节点列表\nyarn node -list\n\n#4.查看节点状况；所有端口号与上图中端口号要一致（随机分配）\nyarn node -status node-03:45568\n\n#5.查看yarn依赖jar的环境变量\nyarn classpath\n```\n\n## YARN调度器\n\n**yarn分为一级调度管理和二级调度管理**\n一级调度管理(更近底层,更接近于操作资源, 更偏向于应用层和底层结合)\n    计算资源管理(cpu,内存等,计算复杂消耗的cpu多)\n    App生命周期管理\n二级调度管理(自己代码的算法等, 更偏向于应用层)\n    App内部的计算模型管理\n    多样化的计算模型\n\n- 在YARN中有三种调度器可以选择：FIFO Scheduler ，Capacity Scheduler，FairS cheduler\n\n![三种调度模型](http://kflys.gitee.io/upic/2020/03/30/uPic/Hadoop架构原理之Yarn/assets/20180912140209122.png)\n\n### FIFO Scheduler\n\n- FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。\n\n- FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。在共享集群中，更适合采用Capacity Scheduler或Fair Scheduler，这两个调度器都允许大任务和小任务在提交的同时获得一定的系统资源。\n\n- 上图展示了这几个调度器的区别，从图中可以看出，在FIFO 调度器中，小任务会被大任务阻塞。\n\n### Capacity Scheduler\n\n- CDH版本默认使用Fair Scheduler公平调度器\n\n![](http://kflys.gitee.io/upic/2020/03/30/uPic/Hadoop架构原理之Yarn/assets/Image201909241610.png)\n\n- 若要使用capacity scheduler，需要修改yarn-site.xml文件；\n\n  ```xml\n  <property>\n  \t<name>yarn.resourcemanager.scheduler.class</name>\n  \t\t\t<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>\n  </property>\n  ```\n  \n- 而对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间\n\n- 如何配置容量调度器\n\n  - 队列层级结构如下\n\n    ```\n    root \n    ├── prod \n    └── dev \n    \t├── spark \n    \t└── hdp\n    ```\n    \n- 主节点上，将$HADOOP_HOME/etc/hadoop/中的对应capacity-scheduler.xml配置文件备份到其它目录\n  \n- 目录$HADOOP_HOME/etc/hadoop/中建立一个新的capacity-scheduler.xml；内容如下\n  \n  ```xml\n    <?xml version=\"1.0\" encoding=\"utf-8\"?>\n    <configuration> \n      <property> \n        <name>yarn.scheduler.capacity.root.queues</name>  \n        <value>prod,dev</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.queues</name>  \n        <value>hdp,spark</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.prod.capacity</name>  \n        <value>40</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.capacity</name>  \n        <value>60</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.maximum-capacity</name>  \n        <value>75</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.hdp.capacity</name>  \n        <value>50</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.spark.capacity</name>  \n        <value>50</value> \n      </property> \n    </configuration>\n    \n    ```\n  \n    ```java\n  // 指定程序提交到哪个队列，默认使用“default”队列\n    configuration.set(\"mapreduce.job.queuename\",\"hdn\")\n  ```\n\n- 动态更新配置：容量调度器的配置在运行时，可以随时重新加载，调整资源分配参数；你需要编辑conf/capacity-scheduler.xml 并在yarn主节点运行命令让配置文件生效\n  - 另外，除非重启resourcemanager，否则队列只能添加不能删除；但允许关闭\n\n```shell\n[hadoop@node01 hadoop]$ yarn rmadmin -refreshQueues\n```\n\n### Fair Scheduler\n\n![](http://kflys.gitee.io/upic/2020/03/30/uPic/Hadoop架构原理之Yarn/assets/Image201907171437 (38).png)\n\n- Apache Hadoop默认使用Capacity Scheduler容量调度器\n\n- CDH版本默认使用Fair Scheduler公平调度器\n\n- 若要用Fair Scheduler的话，需要配置yarn-site.xml，将属性\"yarn.resourcemanager.scheduler.class\"的值修改成\"org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler\"，如下\n\n```xml\n<property>\n\t<name>yarn.resourcemanager.scheduler.class</name>\n\t<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>\n</property>\n```\n\n- 在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如下图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。\n\n- 需要注意的是，在下图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成.\n\n- 支持资源抢占\n\n在yarn-site.xml中设置yarn.scheduler.fair.preemption为true\n\n- 可通过一个名为fair-scheduler.xml文件对公平调度器进行配置\n- 此文件可放置在${HADOOP_HOME}/etc/hadoop/目录下\n- 当没有设置此配置文件时，每个应用放置在以当前用户名命名的队列中\n- 队列是用户提交第一个应用时动态创建的\n\n## YARN应用状态\n\n我们在yarn 的web ui上能够看到yarn 应用程序分为如下几个状态:\n\n- NEW -----新建状态\n- NEW_SAVING-----新建保存状态\n- SUBMITTED-----提交状态\n- ACCEPTED-----接受状态\n- RUNNING-----运行状态\n- FINISHED-----完成状态\n- FAILED-----失败状态\n- KILLED-----杀掉状态\n\n## 扩展\n\n[查看官网capacity scheduler内容](<https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html>)\n\n[capacity scheduler参考资料](<https://blog.csdn.net/u014589856/article/details/78119504>)\n\n[官网查看fair scheduler内容](<https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/FairScheduler.html>)","tags":["hadoop","Yarn"]},{"title":"MapReduce编程模型","url":"/2018/10/01/it/hadoop/MapReduce编程模型/","content":"\n# MapReduce编程模型\n\n### MapReduce编程模型\n\n- Hadoop架构图\n\n  Hadoop由HDFS分布式存储、**MapReduce分布式计算**、Yarn资源调度三部分组成\n\n<img src=\"http://kflys.gitee.io/upic/2020/03/29/uPic/hadoop/mapreduce/assets/Image201906191834-1562922704761.png\" style=\"zoom:50%;\" />\n\n- MapReduce是采用一种**分而治之**的思想设计出来的分布式计算框架\n- MapReduce由两个阶段组成：\n  - Map阶段（切分成一个个小的任务）\n  - Reduce阶段（汇总小任务的结果）\n- 那什么是分而治之呢？\n  - 比如一复杂、计算量大、耗时长的的任务，暂且称为“大任务”；\n  - 此时使用单台服务器无法计算或较短时间内计算出结果时，可将此大任务切分成一个个小的任务，小任务分别在不同的服务器上**并行**的执行\n  - 最终再汇总每个小任务的结果\n\n![](http://kflys.gitee.io/upic/2020/03/29/uPic/hadoop/mapreduce/assets/Image201906251747.png)\n\n#### Map阶段\n\n- map阶段有一个关键的map()函数；\n- 此函数的输入是**键值对**\n- 输出是一系列**键值对**，输出写入**本地磁盘**。\n\n####  Reduce阶段\n\n- reduce阶段有一个关键的函数reduce()函数\n\n- 此函数的输入也是键值对（即map的输出（kv对））\n\n- 输出也是一系列键值对，结果最终写入HDFS\n\n#### Map&Reduce\n\n![](http://kflys.gitee.io/upic/2020/03/29/uPic/hadoop/mapreduce/assets/Image201906251807.png)\n\n### MapReduce编程示例\n\n- 以**MapReduce的词频统计**为例：统计一批英文文章当中，每个单词出现的总次数\n\n#### MapReduce原理图\n\n![](http://kflys.gitee.io/upic/2020/03/29/uPic/hadoop/mapreduce/assets/Image201906271715.png)\n\n- Map阶段\n  - 假设MR的输入文件“**Gone With The Wind**”有三个block；block1、block2、block3 \n  - MR编程时，每个block对应一个分片split\n  - 每一个split对应一个map任务（map task）\n  - 如图共3个map任务（map1、map2、map3）；这3个任务的逻辑一样，所以以第一个map任务（map1）为例分析 \n  - map1读取block1的数据；一次读取block1的一行数据；\n    - 产生键值对(key/value)，作为map()的参数传入，调用map()；\n    - 假设当前所读行是第一行\n    - 将当前所读行的行首相对于当前block开始处的字节偏移量作为key（0）\n    - 当前行的内容作为value（Dear Bear River）\n  - map()内\n    - (按需求，写业务代码)，将value当前行内容按空格切分，得到三个单词Dear | Bear | River\n    - 将每个单词变成键值对，输出出去(Dear, 1) | (Bear, 1) | (River, 1)；最终结果写入map任务所在节点的本地磁盘中（内里还有细节，讲到shuffle时，再细细展开）\n    - block的第一行的数据被处理完后，接着处理第二行；逻辑同上\n    - 当map任务将当前block中所有的数据全部处理完后，此map任务即运行结束\n  - 其它的每一个map任务都是如上逻辑，不再赘述\n- Reduce阶段\n  - reduce任务（reduce task）的个数由自己写的程序编程指定，main()内的job.setNumReduceTasks(4)指定reduce任务是4个（reduce1、reduce2、reduce3、reduce4）\n  - 每一个reduce任务的逻辑一下，所以以第一个reduce任务（reduce1）为例分析\n  - map1任务完成后，reduce1通过网络，连接到map1，将map1输出结果中属于reduce1的分区的数据，通过网络获取到reduce1端（拷贝阶段）\n  - 同样也如此连接到map2、map3获取结果\n  - 最终reduce1端获得4个(Dear, 1)键值对；由于key键相同，它们分到同一组；\n  - 4个(Dear, 1)键值对，转换成[Dear, Iterable(1, 1, 1, )]，作为两个参数传入reduce()\n  - 在reduce()内部，计算Dear的总数为4，并将(Dear, 4)作为键值对输出\n  - 每个reduce任务最终输出文件（内里还有细节，讲到shuffle时，再细细展开），文件写入到HDFS\n\n#### MR中key的作用\n\n- <font color='red'>**MapReduce编程中，key有特殊的作用**</font>\n\n  - ①数据中，若要针对某个值进行分组、聚合时，需将此值作为MR中的reduce的输入的key\n\n  - 如当前的词频统计例子，按单词进行分组，每组中对出现次数做聚合（计算总和）；所以需要将每个单词作为reduce输入的key，MapReduce框架自动按照单词分组，进而求出每组即每个单词的总次数\n\n  - ②另外，key还具有可排序的特性，因为MR中的key类需要实现WritableComparable接口；而此接口又继承Comparable接口\n\n  - MR编程时，要充分利用以上两点；结合实际业务需求，设置合适的key\n\n    ![](http://kflys.gitee.io/upic/2020/03/29/uPic/hadoop/mapreduce/assets/Image201908221717.png)\n\n    ![](http://kflys.gitee.io/upic/2020/03/29/uPic/hadoop/mapreduce/assets/Image201908221718.png)\n\n#### map - reduce代码\n\n Mapper代码\n\n```java\n/**\n * 类Mapper<LongWritable, Text, Text, IntWritable>的四个泛型分别表示\n * map方法的输入的键的类型kin、值的类型vin；输出的键的类型kout、输出的值的类型vout\n * kin指的是当前所读行行首相对于split分片开头的字节偏移量,所以是long类型，对应序列化类型LongWritable\n * vin指的是当前所读行，类型是String，对应序列化类型Text\n * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text\n * vout根据需求，输出值指的是单词的个数，1，类型是int，对应序列化类型是IntWritable\n */\npublic class WordCountMap extends Mapper<LongWritable, Text, Text, IntWritable> {\n    /**\n     * 处理分片split中的每一行的数据；针对每行数据，会调用一次map方法\n     * 在一次map方法调用时，从一行数据中，获得一个个单词word，再将每个单词word变成键值对形式(word, 1)输出出去\n     * 输出的值最终写到本地磁盘中\n     * @param key 当前所读行行首相对于split分片开头的字节偏移量\n     * @param value  当前所读行\n     */\n    public void map(LongWritable key, Text value, Context context)\n            throws IOException, InterruptedException {\n            context.write(new Text(word), new IntWritable(1));\n        }\n    }\n}\n\n```\n\nReducer代码\n\n```java\n/**\n * Reducer<Text, IntWritable, Text, IntWritable>的四个泛型分别表示\n * reduce方法的输入的键的类型kin、输入值的类型vin；输出的键的类型kout、输出的值的类型vout\n * 注意：因为map的输出作为reduce的输入，所以此处的kin、vin类型分别与map的输出的键类型、值类型相同\n * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text\n * vout根据需求，输出值指的是每个单词的总个数，类型是int，对应序列化类型是IntWritable\n */\npublic class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {\n    public void reduce(Text key, Iterable<IntWritable> values,\n                          Context context) throws IOException, InterruptedException {\n        //定义变量，用于累计当前单词出现的次数\n        int sum = 0;\n        for (IntWritable count : values) {\n            //从count中获得值，累加到sum中\n            sum += count.get();\n        }\n        //将单词、单词次数，分别作为键值对，输出\n        context.write(key, new IntWritable(sum));// 输出最终结果\n    };\n}\n```\n\n**2.4.3 Main程序入口**\n\n```java\nJob job = Job.getInstance(configuration, WordCountMain.class.getSimpleName());\n//设置job的jar包，如果参数指定的类包含在一个jar包中，则此jar包作为job的jar包； 参数class跟主类在一个工程即可；一般设置成主类\njob.setJarByClass(WordCountMain.class);\n\n//通过job设置输入/输出格式\n//MR的默认输入格式是TextInputFormat，输出格式是TextOutputFormat；所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n//设置输入/输出路径\nFileInputFormat.setInputPaths(job, new Path(args[0]));\nFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n//设置处理Map阶段的自定义的类\njob.setMapperClass(WordCountMap.class);\n//设置map combine类，减少网路传出量\njob.setCombinerClass(WordCountReduce.class);\n//设置处理Reduce阶段的自定义的类\njob.setReducerClass(WordCountReduce.class);\n\n//注意：如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n//注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n// job.setMapOutputKeyClass(Text.class);\n// job.setMapOutputValueClass(IntWritable.class);\n\n//设置reduce task最终输出key/value的类型\n//注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\njob.setOutputKeyClass(Text.class);\njob.setOutputValueClass(IntWritable.class);\n\n// 提交作业\njob.waitForCompletion(true);\n```\n\n\n#### 运行 / 查看\n\n```shell\n# 查看运行情况 -> job： http://node01:8088\n# outpath -> http://node01:50070\nhadoop jar [jar path] [main class path] /inpath /outpath\n```\n\n### Shuffle\n\n![](http://kflys.gitee.io/upic/2020/03/29/uPic/hadoop/mapreduce/assets/Image201906280906.png)\n\n#### map端\n\n  - 每个map任务都有一个对应的环形内存缓冲区；输出是kv对，先写入到环形缓冲区（默认大小100M），当内容占据80%缓冲区空间后，由一个后台线程将缓冲区中的数据溢出写到一个磁盘文件\n  - 在溢出写的过程中，map任务可以继续向环形缓冲区写入数据；但是若写入速度大于溢出写的速度，最终造成100m占满后，map任务会暂停向环形缓冲区中写数据的过程；只执行溢出写的过程；直到环形缓冲区的数据全部溢出写到磁盘，才恢复向缓冲区写入\n  - 后台线程溢写磁盘过程，有以下几个步骤：\n    - 先对每个溢写的kv对做分区；分区的个数由MR程序的reduce任务数决定；默认使用HashPartitioner计算当前kv对属于哪个分区；计算公式：(key.hashCode() & Integer.MAX_VALUE) % numReduceTasks\n    - 每个分区中，根据kv对的key做内存中排序；\n    - 若设置了map端本地聚合combiner，则对每个分区中，排好序的数据做combine操作；\n    - 若设置了对map输出压缩的功能，会对溢写数据压缩\n  - 随着不断的向环形缓冲区中写入数据，会多次触发溢写（每当环形缓冲区写满100m），本地磁盘最终会生成多个溢出文件\n  - 合并溢写文件：在map task完成之前，所有溢出文件会被合并成一个大的溢出文件；且是已分区、已排序的输出文件\n  - 小细节：\n    - 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作；\n    - 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销）\n\n#### reduce端\n\n- reduce task会在每个map task运行完成后，通过HTTP获得map task输出中，属于自己的分区数据（许多kv对）\n\n- 如果map输出数据比较小，先保存在reduce的jvm内存中，否则直接写入reduce磁盘\n\n- 一旦内存缓冲区达到阈值（默认0.66）或map输出数的阈值（默认1000），则触发**归并merge**，结果写到本地磁盘\n\n- 若MR编程指定了combine，在归并过程中会执行combine操作\n\n- 随着溢出写的文件的增多，后台线程会将它们合并大的、排好序的文件\n\n- reduce task将所有map task复制完后，将合并磁盘上所有的溢出文件\n\n- 默认一次合并10个\n\n- 最后一批合并，部分数据来自内存，部分来自磁盘上的文件\n\n- 进入“归并、排序、分组阶段”\n\n- 每组数据调用一次reduce方法\n\n\n### 自定义Partitioner\n\n- HashPartitioner\n\n```java\npublic class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n  public int getPartition(K2 key, V2 value, int numReduceTasks) {\n    // numReduceTasks : reduce个数，可设置\n    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n  }\n}\n```\n\n### 自定义Combiner\n\n```java\n// 实际上Combiner就是reduce操作，需要设置 \njob.setReducerClass(CustomReduce.class);\njob.setCombinerClass(CustomReduce.class);  // open combiner\n```\n\n- map端combine本地聚合（**本质是reduce**）\n\n\n- 不论运行多少次Combine操作，都不能影响最终的结果\n- 并非所有的mr都适合combine操作，比如求平均值 \n\n![](http://kflys.gitee.io/upic/2020/03/29/uPic/hadoop/mapreduce/assets/Image201909091014.png)\n\n- 当每个map任务的环形缓冲区添满80%，开始溢写磁盘文件\n\n- 此过程会分区、每个分区内按键排序、再combine操作（若设置了combine的话）、若设置map输出压缩的话则再压缩\n\n  - 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作；\n  - 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销）\n\n- combine本质上也是reduce；因为自定义的combine类继承自Reducer父类\n\n- map: (K1, V1) -> list(K2, V2)\n\n- combiner: (K2, list(V2)) -> (K2, V2)\n\n- reduce: (K2, list(V2)) -> (K3, V3)\n\n  - reduce函数与combine函数通常是一样的\n  - K3与K2类型相同；\n  - V3与V2类型相同\n  - 即reduce的输入的kv类型分别与输出的kv类型相同\n\n\n### mr设置压缩\n\n\n```java\n//开启map输出进行压缩的功能\nconfiguration.set(\"mapreduce.map.output.compress\", \"true\");\n//设置map输出的压缩算法是：BZip2Codec，它是hadoop默认支持的压缩算法，且支持切分\nconfiguration.set(\"mapreduce.map.output.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n//开启job输出压缩功能\nconfiguration.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n//指定job输出使用的压缩算法\nconfiguration.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n```\n\n### 自定义InputFormat\n\n#### MapReduce执行过程\n\n![](http://kflys.gitee.io/upic/2020/03/29/uPic/hadoop/mapreduce/assets/Image201905211621.png)\n\n- 上图也描述了mapreduce的一个完整的过程；我们主要看map任务是如何从hdfs读取分片数据的部分\n\n  - 涉及3个关键的类\n\n  - ①InputFormat输入格式类\n\n    ②InputSplit输入分片类：getSplits()\n\n    - InputFormat输入格式类将输入文件分成一个个分片InputSplit\n    - 每个Map任务对应一个split分片\n\n    ③RecordReader记录读取器类：createRecordReader()\n\n    - RecordReader（记录读取器）读取分片数据，一行记录生成一个键值对\n    - 传入map任务的map()方法，调用map()\n\n- 详细流程：\n\n  - 客户端调用InputFormat的**getSplits()**方法，获得输入文件的分片信息\n\n    ```java\n    public abstract class InputFormat<K, V> {\n        public abstract List<InputSplit> getSplits(JobContext var1);\n    }\n    ```\n\n  - 针对每个MR job会生成一个相应的app master，负责map 、 reduce任务的调度及监控执行情况\n\n  - 将分片信息传递给MR job的app master\n\n  - app master根据分片信息，尽量将map任务尽量调度在split分片数据所在节点（**移动计算不移动数据**）\n\n    ```java\n    public abstract class InputSplit {\n        public abstract String[] getLocations() ;\n    }\n    ```\n\n  - 有几个分片，就生成几个map任务\n\n  - 每个map任务将split分片传递给createRecordReader()方法，生成此分片对应的RecordReader\n\n  - RecordReader用来读取分片的数据，生成记录的键值对\n\n    - nextKeyValue()判断是否有下一个键值对，如果有，返回true；否则，返回false\n    - 如果返回true，调用getCurrentKey()获得当前的键\n    - 调用getCurrentValue()获得当前的值\n\n  - map任务运行过程\n\n    ```java\n    // mapper\n    public class Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {\n      \t// 1. map任务运行时，会调用run()\n        public void run(Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>.Context context) throws IOException, InterruptedException {\n          // 2. 首先运行一次setup()方法；只在map任务启动时，运行一次；一些初始化的工作可以在setup方法中完成；如要连接数据库之类的操作\n            this.setup(context);\n          // 3. while循环，调用context.nextKeyValue()；会委托给RecordRecord的nextKeyValue()，判断是否有下一个键值对\n          // 当读取分片尾，context.nextKeyValue()返回false；退出循环\n            while(context.nextKeyValue()) {\n              \t//4.  如果有下一个键值对，调用context.getCurrentKey()、context.getCurrentValue()获得当前的键、值的值（也是调用RecordReader的同名方法[见5]）\n                this.map(context.getCurrentKey(), context.getCurrentValue(), context);\n            }\n          \t//6. 调用cleanup()方法，只在map任务结束之前，调用一次；所以，一些回收资源的工作可在此方法中实现，如关闭数据库连接\n            this.cleanup(context);\n        }\n      // 5. - 作为参数传入map(key, value, context)，调用一次map()\n      protected void map(KEYIN key, VALUEIN value, Mapper.Context context){\n            context.write(key, value);\n        }\n    }\n    \n    // recordReader\n    public abstract class RecordReader<KEYIN, VALUEIN> implements Closeable {\n        public abstract void initialize(InputSplit var1, TaskAttemptContext var2);\n        public abstract boolean nextKeyValue();\n        public abstract KEYIN getCurrentKey();\n        public abstract VALUEIN getCurrentValue();\n        public abstract float getProgress();\n        public abstract void close();\n    }\n    ```\n\n#### 示例代码\n\n- 小文件的优化无非以下几种方式：\n  - 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS(SequenceFile方案)\n  - 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并；可使用**自定义InputFormat**实现\n  - 在mapreduce处理时，可采用**CombineFileInputFormat**提高效率\n\n- 自定义InputFormat\n\n  ```java\n  /**\n   * 自定义InputFormat类；\n   * 泛型：\n   *  键：因为不需要使用键，所以设置为NullWritable\n   *  值：值用于保存小文件的内容，此处使用BytesWritable\n   */\n  public class WholeFileInputFormat extends FileInputFormat<NullWritable, BytesWritable> {\n    \t // 返回false，表示输入文件不可切割\n      protected boolean isSplitable(JobContext context, Path file) {\n          return false;\n      }\n      // 生成读取分片split的RecordReader\n      public RecordReader<NullWritable, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException,InterruptedException {\n          WholeFileRecordReader reader = new WholeFileRecordReader();\n        \t// split传如WholeFileRecordReader进行读取，组装value\n          reader.initialize(split, context);\n      }\n  }\n  ```\n\n- 自定义RecordReader\n\n  ```java\n  public class WholeFileRecordReader extends RecordReader<NullWritable, BytesWritable> {\n      private BytesWritable value = new BytesWritable();\n      @Override\n      public boolean nextKeyValue(){\n         value.set(splitBytes, 0, splitBytes.length);\n      }\n  }\n  ```\n\n\n### 自定义OutputFormat\n\n- 输出结果到不同**目录**\n\n```java\npublic class MyOutPutFormat extends FileOutputFormat<Text, NullWritable> {\n    public RecordWriter getRecordWriter(TaskAttemptContext context){\n        // 两个输出文件路径\n        FSDataOutputStream badOut = fs.create(badPath);\n        FSDataOutputStream goodOut = fs.create(goodPath);\n        return new MyRecordWriter(badOut,goodOut);\n    }\n    static class MyRecordWriter extends RecordWriter<Text, NullWritable>{\n        public void write(Text key, NullWritable value){\n            if\n             \tgoodOut.write();\n            else\n              badOut.write();\n        }\n    }\n}\n```\n\n```java\n// 设置自定义的输出类\njob.setOutputFormatClass(MyOutPutFormat.class);\n// 设置一个输出目录，这个目录会输出一个success的成功标志的文件\nMyOutPutFormat.setOutputPath(job, new Path(args[1]));\n```\n\n#### 二次排序\n\n- hadoop自带的key类型无法满足需求，自定义key\n\n  - 实现WritableComparable接口\n  - 实现compareTo比较方法\n  - 实现write序列化方法\n  - 实现readFields反序列化方法\n- 示例代码\n\n```java\n//根据输入文件格式，定义JavaBean，作为MR时，Map的输出key类型；要求此类可序列化、可比较\npublic class Person implements WritableComparable<Person> {\n    private String name;\n    private int age;\n    private int salary;\n\n    public Person() {}\n\n    //两个Person对象的比较规则：①先比较salary，高的排序在前；②若相同，age小的在前\n    public int compareTo(Person other) {}\n\n    //序列化，将NewKey转化成使用流传送的二进制\n    public void write(DataOutput dataOutput) throws IOException {}\n\n    //使用in读字段的顺序，要与write方法中写的顺序保持一致：name、age、salary\n    public void readFields(DataInput dataInput) throws IOException {}\n}\n```\n\n```java\njob.setOutputKeyClass(Person.class);\n```\n\n### 知识点小例子\n\n- 现有一个淘宝用户订单历史记录文件；每条记录有6个字段，分别表示\n\n  - userid、datetime、title商品标题、unitPrice商品单价、purchaseNum购买量、productId商品ID\n\n- 现使用MR编程，求出每个用户、每个月消费金额最多的两笔订单，花了多少钱\n\n  - 所以得相同用户、同一个年月的数据，分到同一组\n\n#### 逻辑分析\n\n- 根据文件格式，自定义JavaBean类OrderBean\n  - 实现WritableComparable接口\n  - 包含6个字段分别对应文件中的6个字段\n  - 重点实现compareTo方法\n    - 先比较userid是否相等；若不相等，则userid升序排序\n    - 若相等，比较两个Bean的日期是否相等；若不相等，则日期升序排序\n    - 若相等，再比较总开销，降序排序\n  - 实现序列化方法write()\n  - 实现反序列化方法readFields()\n- 自定义分区类\n  - 继承Partitioner类\n  - getPartiton()实现，userid相同的，处于同一个分区\n- 自定义Mapper类\n  - 输出key是当前记录对应的Bean对象\n  - 输出的value对应当前下单的总开销\n- 自定义分组类\n  - 决定userid相同、日期（年月）相同的记录，分到同一组中，调用一次reduce()\n- 自定义Reduce类\n  - reduce()中求出当前一组数据中，开销头两笔的信息\n- main方法\n  - job.setMapperClass\n  - job.setPartitionerClass\n  - job.setReducerClass\n  - job.setGroupingComparatorClass\n\n#### 示例代码\n\n- OrderBean\n\n```java\npublic class OrderBean implements WritableComparable<OrderBean> {\n\n    //用户ID 等字段\n    private String userid;\n    public OrderBean() {}\n   \n    //key的比较规则\n    public int compareTo(OrderBean other) {}\n    // 序列化\n    public void write(DataOutput dataOutput) throws IOException {}\n\t\t// 反序列化\n    public void readFields(DataInput dataInput) throws IOException { }\n\n    /**\n     * 使用默认分区器，那么userid相同的，落入同一分区；\n     * 另外一个方案：此处不覆写hashCode方法，而是自定义分区器，getPartition方法中，对OrderBean的userid求hashCode值%reduce任务数\n     */\n//    public int hashCode() {\n//        return this.userid.hashCode();\n//    }\n}\n\n```\n\n- MyPartitioner\n\n```java\n//mapper的输出key类型是自定义的key类型OrderBean；输出value类型是单笔订单的总开销double -> DoubleWritable\npublic class MyPartitioner extends Partitioner<OrderBean, DoubleWritable> {\n    @Override\n    public int getPartition{\n        //userid相同的，落入同一分区\n        return (orderBean.getUserid().hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n}\n\n```\n\n- MyMapper\n\n```java\npublic class MyMapper extends Mapper<LongWritable, Text, OrderBean, DoubleWritable> {\n    protected void map(LongWritable key, Text value, Context context){\n            // 生成OrderBean对象\n            OrderBean orderBean = getOrderBean();\n            context.write(orderBean, valueOut);\n        }\n    }\n}\n```\n\n- MyReducer\n\n```java\npublic class MyReducer extends Reducer<OrderBean, DoubleWritable, Text, DoubleWritable> {\n    /**\n     * ①由于自定义分组逻辑，相同用户、相同年月的订单是一组，调用一次reduce()；\n     * ②由于自定义的key类OrderBean中，比较规则compareTo规定，相同用户、相同年月的订单，按总金额降序排序\n     * 所以取出头两笔，就实现需求\n     */\n    @Override\n    protected void reduce(OrderBean key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException {\n        //求每个用户、每个月、消费金额最多的两笔多少钱\n        int num = 0;\n        for(DoubleWritable value: values) {\n            if(num < 2) {\n                String keyOut = key.getUserid() + \"  \" + key.getDatetime();\n                context.write(new Text(keyOut), value);\n                num++;\n            } else {\n                break;\n            }\n        }\n\n    }\n}\n```\n\n- MyGroup\n\n```java\n//自定义分组类：reduce端调用reduce()前，对数据做分组；每组数据调用一次reduce()\npublic class MyGroup extends WritableComparator {\n  \t// 注意： 分组实现的方法是这个\n    public int compare(WritableComparable a, WritableComparable b) {\n        //userid、年、月相同的，作为一组\n        int ret1 = aUserId.compareTo(bUserId);\n        if(ret1 == 0) {//同一用户\n            //年月也相同返回0，在同一组；\n            return aOrderBean.getDatetime().compareTo(bOrderBean.getDatetime());\n        } else {\n            return ret1;\n        }\n    }\n}\n\n```\n\n- CustomGroupingMain\n\n```java\n//设置处理Map阶段的自定义的类\njob.setMapperClass(MyMapper.class);\n//设置map combine类，减少网路传出量\n//job.setCombinerClass(MyReducer.class);\njob.setPartitionerClass(MyPartitioner.class);\n//设置处理Reduce阶段的自定义的类\njob.setReducerClass(MyReducer.class);\njob.setGroupingComparatorClass(MyGroup.class);\n```\n\n### MapReduce数据倾斜\n\n- 什么是数据倾斜？\n  - 数据中不可避免地会出现离群值（outlier），并导致数据倾斜。这些离群值会显著地拖慢MapReduce的执行。\n- 常见的数据倾斜有以下几类：\n  - 数据频率倾斜——某一个区域的数据量要远远大于其他区域。比如某一个key对应的键值对远远大于其他键的键值对。\n  - 数据大小倾斜——部分记录的大小远远大于平均值。\n\n- 在map端和reduce端都有可能发生数据倾斜\n  - 在map端的数据倾斜可以考虑使用combine\n  - 在reduce端的数据倾斜常常来源于MapReduce的默认分区器\n\n- 数据倾斜会导致map和reduce的任务执行时间大为延长，也会让需要缓存数据集的操作消耗更多的内存资源\n\n#### 诊断是否存在数据倾斜\n\n- 发现倾斜数据之后，有必要诊断造成数据倾斜的那些键。有一个简便方法就是在代码里实现追踪每个键的**最大值**。\n- 为了减少追踪量，可以设置数据量阀值，只追踪那些数据量大于阀值的键，并输出到日志中。实现代码如下\n- 运行作业后就可以从日志中判断发生倾斜的键以及倾斜程度；跟踪倾斜数据是了解数据的重要一步，也是设计MapReduce作业的重要基础\n\n```java\npublic class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {\n   \n  private int maxValueThreshold;\n\n  @Override\n  protected void setup(Context context) throws IOException, InterruptedException {\n\n    //一个键达到多少后，会做数据倾斜记录\n    maxValueThreshold = 10000;\n  }\n\n  public void reduce(Text key, Iterable<IntWritable> values,\n                     Context context) throws IOException, InterruptedException {\n    int sum = 0;\n    //用于记录键出现的次数\n    int i = 0;\n\n    for (IntWritable count : values) {\n      sum += count.get();\n      i++;\n    }\n\n    //如果当前键超过10000个，则打印日志\n    if(i > maxValueThreshold) {\n      LOGGER.info(\"Received \" + i + \" values for key \" + key);\n    }\n\n    context.write(key, new IntWritable(sum));// 输出最终结果\n  };\n}\n```\n\n\n#### 减缓数据倾斜\n\n- Reduce数据倾斜一般是指map的输出数据中存在数据频率倾斜的状况，即部分输出键的数据量远远大于其它的输出键\n\n- 如何减小reduce端数据倾斜的性能损失？常用方式有：\n  - 自定义分区\n\n    - 基于输出键的背景知识进行自定义分区。\n\n    - 例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的一部分reduce实例。而将其他的都发送给剩余的reduce实例。\n\n  - Combine\n\n    - 使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。\n    - combine的目的就是聚合并精简数据。\n\n  - 抽样和范围分区\n\n    - Hadoop默认的分区器是HashPartitioner，基于map输出键的哈希值分区。这仅在数据分布比较均匀时比较好。**在有数据倾斜时就很有问题**。\n\n    - 使用分区器需要首先了解数据的特性。**TotalOrderPartitioner**中，可以通过对原始数据进行抽样得到的结果集来**预设分区边界值**。\n    - TotalOrderPartitioner中的范围分区器可以通过预设的分区边界值进行分区。因此它也可以很好地用在矫正数据中的部分键的数据倾斜问题。\n\n  - 数据大小倾斜的自定义策略\n\n    - 在map端或reduce端的数据大小倾斜都会对缓存造成较大的影响，乃至导致OutOfMemoryError异常。处理这种情况并不容易。可以参考以下方法。\n\n    - 设置mapreduce.input.linerecordreader.line.maxlength来限制RecordReader读取的最大长度。\n    - RecordReader在TextInputFormat和KeyValueTextInputFormat类中使用。默认长度没有上限。\n\n### 抽样分区案例\n\n- > 使用全排序分区器TotalOrderPartitioner\n\n```java\n//分区器：全局排序分区器\njob.setPartitionerClass(TotalOrderPartitioner.class);\n\n/**\n     * 随机采样器从所有的分片中采样\n     * 每一个参数：采样率；\n     * 第二个参数：总的采样数\n     * 第三个参数：采样的最大分区数；\n     * 只要numSamples和maxSplitSampled（第二、第三参数）任一条件满足，则停止采样\n     */\nInputSampler.Sampler<IntWritable, Text> sampler =\n  new InputSampler.RandomSampler<IntWritable, Text>(0.1, 5000, 10);\n//    TotalOrderPartitioner.setPartitionFile();\n/**\n     * 存储定义分区的键；即整个数据集中温度的大致分布情况；\n     * 由TotalOrderPartitioner读取，作为全排序的分区依据，让每个分区中的数据量近似\n     */\nInputSampler.writePartitionFile(job, sampler);\n\n// 根据上边的SequenceFile文件（包含键的近似分布情况），创建分区\nString partitionFile = TotalOrderPartitioner.getPartitionFile(job.getConfiguration());\nURI partitionUri = new URI(partitionFile);\n\n//与所有map任务共享此文件，添加到分布式缓存中\nDistributedCache.addCacheFile(partitionUri, job.getConfiguration());\n// job.addCacheFile(partitionUri);\n```\n\n[示例代码](https://github.com/orchid-ding/myself-learning/tree/master/hadoop/hadoop/src/main/java/bigdata/hadoop/mapreduces)","tags":["hadoop","MapReduce"]},{"title":"HDFS文件系统","url":"/2018/09/01/it/hadoop/hadoop之-hdfs知识详解/","content":"\n#  HDFS分布式文件系统\n\n### HDFS读写流程\n\n#### 数据写流程\n\n![HDFS写入文件流程](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/HDFS写入文件流程.png)\n\n**详细流程**\n\n- 创建文件：\t\n\n  - HDFS客户端向HDFS写数据，先调用DistributedFileSystem.create()方法，在HDFS创建新的空文件\n  - RPC（ClientProtocol.create()）远程过程调用NameNode（NameNodeRpcServer）的create()，首先在HDFS目录树指定路径添加新文件\n  - 然后将创建新文件的操作记录在editslog中\n  - NameNode.create方法执行完后，DistributedFileSystem.create()返回FSDataOutputStream，它本质是封装了一个DFSOutputStream对象\n\n- 建立数据流管道：\n\n  - 客户端调用DFSOutputStream.write()写数据\n  - DFSOutputStream调用ClientProtocol.addBlock()，首先向NameNode申请一个空的数据块\n  - addBlock()返回LocatedBlock对象，对象包含当前数据块的所有datanode的位置信息\n  - 根据位置信息，建立数据流管道\n\n- 向数据流管道pipeline中写当前块的数据：\n\n  - 客户端向流管道中写数据，先将数据写入一个检验块chunk中，大小512Byte，写满后，计算chunk的检验和checksum值（4Byte）\n  - 然后将chunk数据本身加上checksum，形成一个带checksum值的chunk（516Byte）\n  - 保存到一个更大一些的结构**packet数据包**中，packet为64kB大小\n- packet写满后，先被写入一个**dataQueue**队列中\n  - packet被从队列中取出，向pipeline中写入，先写入datanode1，再从datanoe1传到datanode2，再从datanode2传到datanode3中\n- 一个packet数据取完后，后被放入到**ackQueue**中等待pipeline关于该packet的ack的反馈\n  - 每个packet都会有ack确认包，逆pipeline（dn3 -> dn2 -> dn1）传回输出流\n- 若packet的ack是SUCCESS成功的，则从ackQueue中，将packet删除；否则，将packet从ackQueue中取出，重新放入dataQueue，重新发送\n  - 如果当前块写完后，文件还有其它块要写，那么再调用addBlock方法（**流程同上**）\n- 文件最后一个block块数据写完后，会再发送一个空的packet，表示当前block写完了，然后关闭pipeline\n  - 所有块写完，close()关闭流\n- ClientProtocol.complete()通知namenode当前文件所有块写完了\n\n**容错**\n\n- 在写的过程中，pipeline中的datanode出现故障（如网络不通），输出流如何恢复\n  - 输出流中ackQueue缓存的所有packet会被重新加入dataQueue\n  - 输出流调用ClientProtocol.updateBlockForPipeline()，为block申请一个新的时间戳，namenode会记录新时间戳\n  - 确保故障datanode即使恢复，但由于其上的block时间戳与namenode记录的新的时间戳不一致，故障datanode上的block进而被删除\n  - 故障的datanode从pipeline中删除\n  - 输出流调用ClientProtocol.getAdditionalDatanode()通知namenode分配新的datanode到数据流pipeline中，并使用新的时间戳建立pipeline\n  - 新添加到pipeline中的datanode，目前还没有存储这个新的block，HDFS客户端通过DataTransferProtocol通知pipeline中的一个datanode复制这个block到新的datanode中\n  - pipeline重建后，输出流调用ClientProtocol.updatePipeline()，更新namenode中的元数据\n  - 故障恢复完毕，完成后续的写入流程\n\n#### 数据读流程\n\n![HDFS文件读取流程](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/HDFS文件读取流程.png)\n\n- 1、client端读取HDFS文件，client调用文件系统对象DistributedFileSystem的open方法\n- 2、返回FSDataInputStream对象（对DFSInputStream的包装）\n- 3、构造DFSInputStream对象时，调用namenode的getBlockLocations方法，获得file的开始若干block（如blk1, blk2, blk3, blk4）的存储datanode（以下简称dn）列表；针对每个block的dn列表，会根据网络拓扑做排序，离client近的排在前；\n- 4、调用DFSInputStream的read方法，先读取blk1的数据，与client最近的datanode建立连接，读取数据\n- 5、读取完后，关闭与dn建立的流\n- 6、读取下一个block，如blk2的数据（重复步骤4、5、6）\n- 7、这一批block读取完后，再读取下一批block的数据（重复3、4、5、6、7）\n- 8、完成文件数据读取后，调用FSDataInputStream的close方法\n\n**容错**\n\n- 情况一：读取block过程中，client与datanode通信中断\n\n  - client与存储此block的第二个datandoe建立连接，读取数据\n  - 记录此有问题的datanode，不会再从它上读取数据\n\n- 情况二：client读取block，发现block数据有问题\n  -  client读取block数据时，同时会读取到block的校验和，若client针对读取过来的block数据，计算检验和，其值与读取过来的校验和不一样，说明block数据损坏\n  -  client从存储此block副本的其它datanode上读取block数据（也会计算校验和）\n  -  同时，client会告知namenode此情况；\n\n### Hadoop HA高可用\n\n#### HDFS高可用原理\n\n![](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201905211519.png)\n\n- 对于HDFS ，NN存储元数据在内存中，并负责管理文件系统的命名空间和客户端对HDFS的读写请求。但是，如果只存在一个NN，一旦发生“单点故障”，会使整个系统失效。\n- 虽然有个SNN，但是它并不是NN的热备份\n- 因为SNN无法提供“热备份”功能，在NN故障时，无法立即切换到SNN对外提供服务，即HDFS处于停服状态。\n- HDFS2.x采用了HA（High Availability高可用）架构。\n  - 在HA集群中，可设置两个NN，一个处于“活跃（Active）”状态，另一个处于“待命（Standby）”状态。\n  - 由zookeeper确保一主一备（讲zookeeper时具体展开）\n  - 处于Active状态的NN负责响应所有客户端的请求，处于Standby状态的NN作为热备份节点，保证与active的NN的元数据同步\n  - Active节点发生故障时，zookeeper集群会发现此情况，通知Standby节点立即切换到活跃状态对外提供服务\n  - 确保集群一直处于可用状态\n- 如何热备份元数据：\n  - Standby NN是Active NN的“热备份”，因此Active NN的状态信息必须实时同步到StandbyNN。\n  - 可借助一个共享存储系统来实现状态同步，如NFS(NetworkFile System)、QJM(Quorum Journal Manager)或者Zookeeper。\n  - Active NN将更新数据写入到共享存储系统，Standby NN一直监听该系统，一旦发现有新的数据写入，就立即从公共存储系统中读取这些数据并加载到Standby NN自己内存中，从而保证元数据与Active NN状态一致。\n- 块报告：\n  - NN保存了数据块到实际存储位置的映射信息，为了实现故障时的快速切换，必须保证StandbyNN中也包含最新的块映射信息\n  - 因此需要给所有DN配置Active和Standby两个NN的地址，把块的位置和心跳信息同时发送到两个NN上。\n\n### Hadoop联邦\n\n#### 为什么需要联邦\n\n- 虽然HDFS HA解决了“单点故障”问题，但HDFS在扩展性、整体性能和隔离性方面仍有问题\n  - 系统扩展性方面，元数据存储在NN内存中，受限于内存上限（每个文件、目录、block占用约150字节）\n  - 整体性能方面，吞吐量受单个NN的影响\n  - 隔离性方面，一个程序可能会影响其他程序的运行，如果一个程序消耗过多资源会导致其他程序无法顺利运行\n  - HDFS HA本质上还是单名称节点\n\n#### 联邦\n\n<img src=\"http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201909041239.png\" style=\"zoom: 67%;\" />\n\n\n- HDFS联邦可以解决以上三个问题\n  - HDFS联邦中，设计了多个命名空间；每个命名空间有一个NN或一主一备两个NN，使得HDFS的命名服务能够水平扩展\n  - 这些NN分别进行各自命名空间namespace和块的管理，相互独立，不需要彼此协调\n  - 每个DN要向集群中所有的NN注册，并周期性的向所有NN发送心跳信息和块信息，报告自己的状态\n  - HDFS联邦每个相互独立的NN对应一个独立的命名空间\n  - 每一个命名空间管理属于自己的一组块，这些属于同一命名空间的块对应一个“块池”的概念。\n  - 每个DN会为所有块池提供块的存储，块池中的各个块实际上是存储在不同DN中的\n\n[联邦-官网](<https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/Federation.html>)\n\n### 文件压缩\n\n#### 压缩算法\n\n- 文件压缩好处：\n\n  - 减少数据所占用的磁盘空间\n  - 加快数据在磁盘、网络上的IO\n\n- 常用压缩格式\n\n  | 压缩格式 | UNIX工具 | 算      法 | 文件扩展名 | 可分割 |\n  | -------- | -------- | ---------- | ---------- | ------ |\n  | DEFLATE  | 无       | DEFLATE    | .deflate   | No     |\n  | gzip     | gzip     | DEFLATE    | .gz        | No     |\n  | zip      | zip      | DEFLATE    | .zip       | YES    |\n  | bzip     | bzip2    | bzip2      | .bz2       | YES    |\n  | LZO      | lzop     | LZO        | .lzo       | No     |\n  | Snappy   | 无       | Snappy     | .snappy    | No     |\n\n- Hadoop的压缩实现类；均实现CompressionCodec接口\n\n  | 压缩格式 | 对应的编码/解码器                          |\n  | -------- | ------------------------------------------ |\n  | DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |\n  | gzip     | org.apache.hadoop.io.compress.GzipCodec    |\n  | bzip2    | org.apache.hadoop.io.compress.BZip2Codec   |\n  | LZO      | com.hadoop.compression.lzo.LzopCodec       |\n  | Snappy   | org.apache.hadoop.io.compress.SnappyCodec  |\n\n- 查看集群是否支持本地压缩（所有节点都要确认）\n\n  ```\n  [hadoop@node01 ~]$ hadoop checknative\n  ```\n\n\n- 编程：案例\n\n\n```java\n// 压缩类型\nBZip2Codec codec = new BZip2Codec();\ncodec.setConf(configuration);\n//调用Filesystem的create方法返回的是FSDataOutputStream对象\n//该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n// 获取文件系用的输出流\nOutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n// 对输出流进行压缩\nCompressionOutputStream compressionOut = codec.createOutputStream(outputStreamTarget);\n// 将文件输入流，写入输入流\nIOUtils.copyBytes(inputStreamSourceFile,compressionOut,4069,true);\n```\n\n- [HDFS文件压缩](<https://blog.csdn.net/qq_38262266/article/details/79171524>)\n\n### 小文件治理\n\n- NameNode存储着文件系统的元数据，每个文件、目录、块大概有150字节的元数据；\n- 因此文件数量的限制也由NN内存大小决定，如果小文件过多则会造成NN的压力过大\n- 且HDFS能存储的数据总量也会变小\n\n#### HAR文件方案\n\n- 本质启动mr程序，所以需要启动yarn(手动压缩文件)\n\n![1558004541101](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/1558004541101.png)\n\n用法：\n\n```sh\narchive -archiveName <NAME>.har -p <parent path> [-r <replication factor>]<src>* <dest>\n```\n\n```shell\n# 创建archive文件；/testhar有两个子目录th1、th2；两个子目录中有若干文件\nhadoop archive -archiveName test.har -p /testhar -r 3 th1 th2 /outhar # 原文件还存在，需手动删除\n\n# 查看archive文件\nhdfs dfs -ls -R har:///outhar/test.har\n\n# 解压archive文件\n# 方式一\nhdfs dfs -cp har:///outhar/test.har/th1 hdfs:/unarchivef # 顺序\nhadoop fs -ls /unarchivef\t\n# 方式二\nhadoop distcp har:///outhar/test.har/th1 hdfs:/unarchivef2 # 并行，启动MR\n```\n\n#### Sequence Files方案\n\n- SequenceFile文件，主要由一条条record记录组成；每个record是键值对形式的\n- SequenceFile文件可以作为小文件的存储容器；\n  - 每条record保存一个小文件的内容\n  - 小文件名作为当前record的键；\n  - 小文件的内容作为当前record的值；\n  - 如10000个100KB的小文件，可以编写程序将这些文件放到一个SequenceFile文件。\n- 一个SequenceFile是**可分割**的，所以MapReduce可将文件切分成块，每一块独立操作。\n- 具体结构（如下图）：\n  - 一个SequenceFile首先有一个4字节的header（文件版本号）\n  - 接着是若干record记录\n  - 记录间会随机的插入一些同步点sync marker，用于方便定位到记录边界\n- 不像HAR，SequenceFile**支持压缩**。记录的结构取决于是否启动压缩\n  - 支持两类压缩：\n    - 不压缩NONE，如下图\n    - 压缩RECORD，如下图\n    - 压缩BLOCK，①一次性压缩多条记录；②每一个新块Block开始处都需要插入同步点；如下图\n  - 在大多数情况下，以block（注意：指的是SequenceFile中的block）为单位进行压缩是最好的选择\n  - 因为一个block包含多条记录，利用record间的相似性进行压缩，压缩效率更高\n  - 把已有的数据转存为SequenceFile比较慢。比起先写小文件，再将小文件写入SequenceFile，一个更好的选择是直接将数据写入一个SequenceFile文件，省去小文件作为中间媒介.\n\n![](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201907101934.png)\n\n\n\n![](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201907101935.png)\n\n- 向SequenceFile写入数据\n\n```java\n//1. 创建向SequenceFile文件写入数据时的一些选项\n//2. 要写入的SequenceFile的路径\nSequenceFile.Writer.Option pathOption = SequenceFile.Writer.file(path);\n//3. record的key类型选项\nSequenceFile.Writer.Option keyOption = SequenceFile.Writer.keyClass(IntWritable.class);\n//4. record的value类型选项\nSequenceFile.Writer.Option valueOption = SequenceFile.Writer.valueClass(Text.class);\n// SequenceFile压缩方式：NONE | RECORD | BLOCK三选一\n// 方案一：RECORD、不指定压缩算法\nSequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD);\nSequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);\n\n\n// 方案二：BLOCK、不指定压缩算法\nSequenceFile.Writer.Option compressOption = SequenceFile.Writer.compression(SequenceFile.CompressionType.BLOCK);\nSequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);\n\n\n\n// 方案三：使用BLOCK、压缩算法BZip2Codec；压缩耗时间 再加压缩算法\nBZip2Codec codec = new BZip2Codec();\ncodec.setConf(conf);\nSequenceFile.Writer.Option compressAlgorithm = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD, codec);\n// 创建写数据的Writer实例\nSequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressAlgorithm);\n\n\n// 填充小文件数据\nfor (int i = 0; i < 100000; i++) {\n  //分别设置key、value值\n  key.set(100 - i);\n  value.set(DATA[i % DATA.length]);\n  writer.append(key, value);\n}\n\n// 关闭流\nIOUtils.closeStream(writer);\n```\n\n- 命令查看SequenceFile内容\n\n```shell\n hadoop fs -text /writeSequenceFile\n```\n\n- 读取SequenceFile文件\n\n```java\n//1. 读取SequenceFile的Reader的路径选项\nSequenceFile.Reader.Option pathOption = SequenceFile.Reader.file(path);\n//2. 实例化Reader对象\nreader = new SequenceFile.Reader(conf, pathOption);\n//3. 根据反射，求出key类型\nWritable key = (Writable)\n  ReflectionUtils.newInstance(reader.getKeyClass(), conf);\n//根据反射，求出value类型\nWritable value = (Writable)\n  ReflectionUtils.newInstance(reader.getValueClass(), conf);\n\nlong position = reader.getPosition();\n\nwhile (reader.next(key, value)) {\n  String syncSeen = reader.syncSeen() ? \"*\" : \"\";\n  System.out.printf(\"[%s%s]\\t%s\\t%s\\n\", position, syncSeen, key, value);\n  position = reader.getPosition(); // beginning of next record\n}\n```\n\n###  文件快照\n\n####  什么是快照\n\n- 快照比较常见的应用场景是数据备份，以防一些用户错误或灾难恢复\n- 快照snapshots是HDFS文件系统的，只读的、某时间点的拷贝\n- 可以针对**某个目录**，或者**整个文件系统**做快照\n- 创建快照时，block块并不会被拷贝。快照文件中只是记录了block列表和文件大小，**不会做任何数据拷贝**\n\n####  快照操作\n\n- 允许快照\n\n  允许一个快照目录被创建。如果这个操作成功完成，这个目录就变成snapshottable\n\n  用法：hdfs dfsadmin -allowSnapshot <snapshotDir>\n\n  ```shell\n  hdfs dfsadmin -allowSnapshot /wordcount\n  ```\n\n- 禁用快照\n\n  用法：hdfs dfsadmin -disallowSnapshot <snapshotDir>\n\n  ```shell\n  hdfs dfsadmin -disallowSnapshot /wordcount\n  ```\n\n- 创建快照\n\n  用法：hdfs dfs -createSnapshot <snapshotDir> [<snapshotName>]\n\n  ```shell\n  #注意：先将/wordcount目录变成允许快照的\n  hdfs dfs -createSnapshot /wordcount wcSnapshot\n  ```\n\n- 查看快照\n\n  ```shell\n  hdfs dfs -ls /wordcount/.snapshot\n  ```\n  \n- 重命名快照\n\n  这个操作需要拥有snapshottabl目录所有者权限\n\n  用法：hdfs dfs -renameSnapshot <snapshotDir> <oldName> <newName>\n\n  ```shell\n  hdfs dfs -renameSnapshot /wordcount wcSnapshot newWCSnapshot\n  ```\n  \n- 用快照恢复误删除数据\n\n  HFDS的/wordcount目录，文件列表如下\n\n  ![](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201909041356.png)\n\n  误删除/wordcount/edit.xml文件\n\n  ```shell\n  hadoop fs -rm /wordcount/edit.xml\n  ```\n  \n![](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201909041400.png)\n  \n恢复数据\n  \n```shell\n  hadoop fs -cp /wordcount/.snapshot/newWCSnapshot/edit.xml /wordcount\n  ```\n  \n- 删除快照\n\n  这个操作需要拥有snapshottabl目录所有者权限\n\n  用法：hdfs dfs -deleteSnapshot <snapshotDir> <snapshotName>\n\n  ```shell\n  hdfs dfs -deleteSnapshot /wordcount newWCSnapshot\n  ```\n\n\n##  计算机知识\n\n1. HDFS存储地位\n\n2. **block块为什么设置的比较大**\n\n- [磁盘基础知识](<https://www.cnblogs.com/jswang/p/9071847.html>)\t\n\n  - 盘片platter、磁头head、磁道track、扇区sector、柱面cylinder\n  - 为了最小化寻址开销；从磁盘传输数据的时间明显大于定位这个块开始位置所需的时间\n\n- 问：块的大小是不是设置的越大越好呢？\n\n  1、 不是，寻址的时间大概是 100ms，设计一般设置为寻址时间占用十分之一，也就是一秒。 硬盘的传输速录大概是100m/s 一秒大概为100M，最接近100的大小为128M。 \n\n![](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E7%9F%A5%E8%AF%86%E8%AF%A6%E8%A7%A3/img/Image201906211143.png)\n","tags":["hadoop","hdfs"]},{"title":"大数据概论-HDFS理论基础","url":"/2018/08/01/it/hadoop/hadoop之-hdfs基础/","content":"## 大数据概论\n\n> 概念： 大数据（big data）是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产\n\n| 数据单位 | B    | KB   | MB   | GB   | PE   | PB   | EB   | ZB   | YB   |\n| -------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| 基数     |      | 2    | 2    | 2    | 2    | 2    | 2    | 10   | 10   |\n| 次方     | 0    | 10   | 20   | 30   | 40   | 50   | 60   | 21   | 24   |\n\n### 大数据特性\n\n1. 数据量大（Volume） \n2. 类型繁多（Variety） \n3. 价值密度低（Value） \n4. 速度快时效高（Velocity）\n\n### 大数据的挑战\n\n1. 存储： 每天几TB、GB的数据增量，并且还在持续的增长中。\n2. 分析： 如何从巨大的数据中挖掘出隐藏的商业价值。\n3. 管理： 如何快速构建并且保证系统的安全、简单可用。\n\n## 分布式文件系统\n\n### Hadoop简介\n\n1. Hadoop架构\n\n   <img src=\"http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E5%9F%BA%E7%A1%80/img/Image201906191834.png\" alt=\"Image201906191834\" style=\"zoom:50%;\" />\n\n2. Hadoop历史\n\n   ![Image201906202055](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E5%9F%BA%E7%A1%80/img/Image201906202055.png)\n\n### HDFS\n\n- HDFS是Hadoop中的一个存储子模块\n- HDFS (全称Hadoop Distributed File System)，即hadoop的分布式文件系统\n- File System**文件系统**：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读出、修改、转储、删除文件等\n- 当数据集大小超出一台计算机的存储能力时，就有必要将它拆分成若干部分，然后分散到不同的计算机中存储。管理网络中跨多台计算机存储的文件系统称之为**分布式文件系统**（distributed filesystem）\n\n#### HDFS特点\n\n**2.1.1 优点：**\n\n- 适合存储大文件，能用来存储管理PB级的数据；不适合存储小文件\n- 处理非结构化数据\n- 流式的访问数据，一次写入、多次读写\n- 运行于廉价的商用机器集群上，成本低\n- 高容错：故障时能继续运行且不让用户察觉到明显的中断\n- 可扩展\n\n<img src=\"http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E5%9F%BA%E7%A1%80/img/Image201907081216.png\" style=\"zoom:50%;\" />\n\n**2.1.2 局限性**\n\n- 不适合处理低延迟数据访问\n  - HDFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的\n  - 对于低延时的访问需求，HBase是更好的选择\n- 无法高效存储大量的小文件\n  - 小文件会给Hadoop的扩展性和性能带来严重问题\n  - 利用SequenceFile、MapFile等方式归档小文件\n- 不支持多用户写入及任意修改文件\n  - 文件有一个写入者，只能执行追加操作\n  - 不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改\n\n#### HDFS常用命令\n\n```shell\n# HDFS两种命令风格，两种命令效果等同\nhadoop fs / hdfs dfs\n\n# 如何查看hdfs或hadoop子命令的**帮助信息**，如ls子命令\nhadoop fs -help ls\n\n# 查看hdfs文件系统中已经存在的文件。对比linux命令ls\nhdfs dfs -ls /\n\n# 在hdfs文件系统中创建文件\nhdfs dfs -touchz /edits.txt\n\n#将本地磁盘当前目录的edit1.xml内容追加到HDFS根目录 的edits.txt文件\nhadoop fs -appendToFile edit1.xml /edits.txt \n\n# 查看HDFS文件内容\nhdfs dfs -cat /edits.txt\n\n#用法：hdfs dfs -put /本地路径 /hdfs路径\nhdfs dfs -put hadoop-2.7.3.tar.gz /\n\n#根put作用一样\nhdfs dfs -copyFromLocal hadoop-2.7.3.tar.gz /  \n\n#根put作用一样，只不过，源文件被拷贝成功后，会被删除\nhdfs dfs -moveFromLocal hadoop-2.7.3.tar.gz / \n\n# 在hdfs文件系统中下载文件\nhdfs dfs -get /hdfs路径 /本地路径\nhdfs dfs -copyToLocal /hdfs路径 /本地路径  #根get作用一样\n\n# 在hdfs文件系统中**创建目录**\nhdfs dfs -mkdir /shell\n\n# 在hdfs文件系统中**删除**文件\nhdfs dfs -rm /edits.txt\nhdfs dfs -rm -r /shell\n# 递归删除目录\nhdfs dfs -rmr /shell\n\n# 在hdfs文件系统中**修改文件名称**（也可以用来**移动**文件到目录）\nhdfs dfs -mv /xcall.sh /call.sh\nhdfs dfs -mv /call.sh /shell\n\n# 在hdfs中拷贝文件到目录\nhdfs dfs -cp /xrsync.sh /shell\n\n# 列出本地文件的内容（默认是hdfs文件系统）\nhdfs dfs -ls file:///home/bruce/\n\n# linux find命令\nfind . -name 'edit*'\n\n# HDFS find命令\nhadoop fs -find / -name part-r-00000 # 在HDFS根目录中，查找part-r-00000文件\n```\n\n###  hdfs与getconf结合使用\n\n``````shell\n# 获取NameNode的节点名称（可能有多个）\nhdfs getconf -namenodes\n\n# 获取hdfs最小块信息\nhdfs getconf -confKey dfs.namenode.fs-limits.min-block-size\n\n# 查找hdfs的NameNode的RPC地址\nhdfs getconf -nnRpcAddresses\n``````\n\n##### hdfs与dfsadmin结合使用\n\n```shell\n# 帮助信息\nhdfs dfsadmin -help safemode\n\n# 查看当前的模式\nhdfs dfsadmin -safemode get\n\n# 进入安全模式\nhdfs dfsadmin -safemode enter\n```\n\n##### hdfs与fsck结合使用\n\n``````shell\n# fsck指令**显示HDFS块信息**\nhdfs fsck /02-041-0029.mp4 -files -blocks -locations # 查看文件02-041-0029.mp4的块信息\n``````\n\n##### 其他命令\n\n``````shell\n#m检查压缩库本地安装情况\nhadoop checknative\n\n# 格式化名称节点（**慎用**，一般只在初次搭建集群，使用一次；格式化成功后，不要再使用）\nhadoop namenode -format\n\n# 执行自定义jar包\nhadoop jar kfly-example-1.0-SNAPSHOT.jar org.kfly.WordCount /world.txt /out\n``````\n\n#### HDFS编程\n\n\n- 1.向hdfs中,上传一个文本文件\n```java\n   // 获取文件输入流\n  InputStream  inputStreamSourceFile = new BufferedInputStream(new FileInputStream(source));\n  // HDFS 读写配置文件\n  Configuration configuration = new Configuration();\n  // 通过url 返回文件系统实例\n  FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);\n  //调用Filesystem的create方法返回的是FSDataOutputStream对象\n  //该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n  // 获取文件系用的输出流\n  OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n  // 将文件输入流，写入输入流\n  IOUtils.copyBytes(inputStreamSourceFile,outputStreamTarget,4069,true);\n  System.out.println(\"上传成功\");\n```\n\n- 2.读取hdfs上的文件\n\n```java\n// HDFS 读写文件配置\nConfiguration configuration = new Configuration();\n// HDFS文件系统\nFileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n// 文件输入流，用于读取文件\nInputStream inputStream = fileSystem.open(new Path(source));\nBufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream));\nresult = readBufferReader(bufferedReader).toString();\n```\n\n\n\n- 3.列出某一个文件夹下的所有文件\n\n```java\n// HDFS文件系统\nFileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n// 传参 -> recursive：true 继续深入遍历\nRemoteIterator<LocatedFileStatus> iterator = fileSystem.listFiles(new Path(source),true);\n```\n\n- 4.列出多级目录名称和目录下的文件名称()\n\n  ```java\n  \t\t/**\n       * 递归目录和文件\n       * @param stringBuffer  文件目录名称集合\n       * @param fileSystem  hdfs 文件系统\n       * @param source path 路径\n       * @throws IOException\n       */\n  private static void list(FileSystem fileSystem, Path source) {\n    FileStatus[] iterator = fileSystem.listStatus(source);\n    for (FileStatus status:iterator) {\n      stringBuffer.append(status.getPath() + \"\\n\");\n      if(status.isDirectory()){\n        list(stringBuffer,fileSystem,status.getPath());\n      }\n    }\n  }\n  ```\n  \n  \n\n#### HDFS架构\n\n![](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E5%9F%BA%E7%A1%80/img/1558073557041.png)\n\n- 大多数分布式框架都是主从架构\n- HDFS也是主从架构Master|Slave或称为管理节点|工作节点\n\n##### NameNode\n\n文件系统**\n\n- file system文件系统：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读取、修改、转储、删除文件等\n- 读文件 =>>找到文件 =>> 在哪 + 叫啥？\n- 元数据\n  - 关于文件或目录的描述信息，如文件所在路径、文件名称、文件类型等等，这些信息称为文件的元数据metadata\n- 命名空间\n  - 文件系统中，为了便于管理存储介质上的，给每个目录、目录中的文件、子目录都起了名字，这样形成的层级结构，称之为命名空间\n  - 同一个目录中，不能有同名的文件或目录\n  - 这样通过目录+文件名称的方式能够唯一的定位一个文件\n\nHDFS-NameNode**\n\n- HDFS本质上也是文件系统filesystem，所以它也有元数据metadata；\n- 元数据metadata保存在NameNode**内存**中\n- NameNode作用\n  - HDFS的主节点，负责管理文件系统的命名空间，将HDFS的元数据存储在NameNode节点的内存中\n  - 负责响应客户端对文件的读写请求\n- HDFS元数据\n  - 文件目录树、所有的文件（目录）名称、文件属性（生成时间、副本、权限）、每个文件的块列表、每个block块所在的datanode列表\n\n  - 每个文件、目录、block占用大概**150Byte字节的元数据**；所以HDFS适合存储大文件，不适合存储小文件\n\n  - HDFS元数据信息以两种形式保存：①编辑日志**edits log**②命名空间镜像文件**fsimage**\n    - edits log：HDFS编辑日志文件 ，保存客户端对HDFS的所有更改记录，如增、删、重命名文件（目录），这些操作会修改HDFS目录树；\u0010NameNode会在编辑日志edit日志中记录下来；\n    - fsimage：HDFS元数据镜像文件 ，即将namenode内存中的数据落入磁盘生成的文件；保存了文件系统目录树信息以及文件、块、datanode的映射关系，如下图\n    \n    ```shell\n    # 说明：\n    # ①为hdfs-site.xml中属性dfs.namenode.edits.dir的值决定；用于namenode保存edits.log文件\n    # ②为hdfs-site.xml中属性dfs.namenode.name.dir的值决定；用于namenode保存fsimage文件\n    ```\n\n##### DataNode\n\n- DataNode数据节点的作用\n  - 存储block以及block元数据到datanode本地磁盘；此处的元数据包括数据块的长度、块数据的校验和、时间戳\n\n##### SeconddaryNameNode   \n\n- 为什么引入SecondaryNameNode\n\n  - 为什么元数据存储在NameNode在内存中？\n\n  - 这样做有什么问题？如何解决？\n\n  - HDFS编辑日志文件 editlog：在NameNode节点中的编辑日志editlog中，记录下来客户端对HDFS的所有更改的记录，如增、删、重命名文件（目录）；\n\n  - 作用：一旦系统出故障，可以从editlog进行恢复；\n\n  - 但editlog日志大小会随着时间变在越来越大，导致系统重启根据日志恢复的时候会越来越长；\n\n  - 为了避免这种情况，引入**检查点机制checkpoint**，命名空间镜像fsimage就是HDFS元数据的持久性检查点，即将内存中的元数据落磁盘生成的文件；\n\n  - 此时，namenode如果重启，可以将磁盘中的fsimage文件读入内容，将元数据恢复到某一个检查点，然后再执行检查点之后记录的编辑日志，最后完全恢复元数据。\n\n  - 但是依然，随着时间的推移，editlog记录的日志会变多，那么当namenode重启，恢复元数据过程中，会花越来越长的时间执行editlog中的每一个日志；而在namenode元数据恢复期间，HDFS不可用。\n\n  - 为了解决此问题，引入secondarynamenode辅助namenode，用来合并fsimage及editlog\n\n<img src=\"http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E5%9F%BA%E7%A1%80/img/Image201906211525.png\" style=\"zoom:67%;\" />\n\n- SecondaryNameNode定期做checkpoint检查点操作\n\n  - 创建检查点checkpoint的两大条件：\n    - SecondaryNameNode每隔1小时创建一个检查点\n    - 另外，Secondary NameNode每1分钟检查一次，从上一检查点开始，edits日志文件中是否已包括100万个事务，如果是，也会创建检查点\n  - Secondary NameNode首先请求原NameNode进行edits的滚动，这样新的编辑操作就能够进入新的文件中\n  - Secondary NameNode通过HTTP GET方式读取原NameNode中的fsimage及edits\n  - Secondary NameNode读取fsimage到内存中，然后执行edits中的每个操作，并创建一个新的统一的fsimage文件\n  - Secondary NameNode通过HTTP PUT方式将新的fsimage发送到原NameNode\n  - 原NameNode用新的fsimage替换旧的fsimage，同时系统会更新fsimage文件到记录检查点的时间。 \n  - 这个过程结束后，NameNode就有了最新的fsimage文件和更小的edits文件\n\n- SecondaryNameNode一般部署在另外一台节点上\n\n  - 因为它需要占用大量的CPU时间\n  - 并需要与namenode一样多的内存，来执行合并操作\n\n- 如何查看edits日志文件\n\n  ```shell\n  hdfs oev -i edits_0000000000000000256-0000000000000000363 -o /home/hadoop/edit1.xml\n  ```\n\n- 如何查看fsimage文件\n\n  ```shell\n  hdfs oiv -p XML -i fsimage_0000000000000092691 -o fsimage.xml  \n  ```\n\n- checkpoint相关属性\n\n  ```properties\n  # 3600秒(即1小时) 每隔1小时创建一个检查点\n  #The number of seconds between two periodic checkpoints\ndfs.namenode.checkpoint.period = 3600\n  \n  # edits日志文件中是否已包括100万个事务，如果是，也会创建检查点\n  # The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless of whether 'dfs.namenode.checkpoint.period' has expired.\n  dfs.namenode.checkpoint.txns = 1000000 \n  \n  # 60(1分钟)  SecondaryNameNode每1分钟检查一次\n  #  The SecondaryNameNode and CheckpointNode will poll the NameNode every 'dfs.namenode.checkpoint.check.period' seconds to query the number of uncheckpointed transactions.\n  dfs.namenode.checkpoint.check.period = 60\n  ```\n  \n\n##### 心跳机制\n\n![](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E5%9F%BA%E7%A1%80/img/Image201906211518.png)\n\n**工作原理：**\n\n1. NameNode启动的时候，会开一个ipc server在那里\n2. DataNode启动后向NameNode注册，每隔**3秒钟**向NameNode发送一个“**心跳heartbeat**”\n3. 心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一DataNode，或删除某个数据块\n4. 如果超过**10分钟**NameNode没有收到某个DataNode 的心跳，则认为该DataNode节点不可用\n5. DataNode周期性（**6小时**）的向NameNode上报当前DataNode上的块状态报告BlockReport；块状态报告包含了一个该 Datanode上所有数据块的列表\n\n**心跳的作用：**\n\n1. 通过周期心跳，NameNode可以向DataNode返回指令\n\n2. 可以判断DataNode是否在线\n\n3. 通过BlockReport，NameNode能够知道各DataNode的存储情况，如磁盘利用率、块列表；跟**负载均衡**有关\n\n4. **hadoop集群刚开始启动时，99.9%的block没有达到最小副本数(dfs.namenode.replication.min默认值为1)，集群处于安全模式，涉及BlockReport；**\n\n**心跳相关配置**\n\n- [hdfs-default.xml](<https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml>)\n- 心跳间隔\n\n| 属性                   | 值   | 解释                                               |\n| ---------------------- | ---- | -------------------------------------------------- |\n| dfs.heartbeat.interval | 3    | Determines datanode heartbeat interval in seconds. |\n\n- **block report**\n\n| More Actions属性             | 值               | 解释                                                 |\n| ---------------------------- | ---------------- | ---------------------------------------------------- |\n| dfs.blockreport.intervalMsec | 21600000 (6小时) | Determines block reporting interval in milliseconds. |\n\n- 查看hdfs-default.xml默认配置文件\n\n![](http://kflys.gitee.io/upic/2020/03/26/uPic/hadoop%E4%B9%8B-hdfs%E5%9F%BA%E7%A1%80/img/Image201907311730.png)\n\n##### 负载均衡\n\n- 什么原因会有可能造成不均衡？\n  - 机器与机器之间磁盘利用率不平衡是HDFS集群非常容易出现的情况\n  - 尤其是在DataNode节点出现故障或在现有的集群上增添新的DataNode的时候\n\n- 为什么需要均衡？\n  - 提升集群存储资源利用率\n  - 从存储与计算两方面提高集群性能\n\n- 如何手动负载均衡？\n\n```shell\n$HADOOP_HOME/sbin/start-balancer.sh -t 5%\t# 磁盘利用率最高的节点若比最少的节点，大于5%，触发均衡\n```\n","tags":["hadoop"]},{"title":"大数据环境搭建","url":"/2017/11/26/it/hadoop/大数据环境搭建/","content":"\n# 大数据环境搭建(MAC)\n\n## 1. Linux 环境配置\n\n### 1.1 Vmware Funsion Linux网络配置\n\n#### 1.1.1 查看本机网络\n\n```shell\n# 1. 查看vmnet网络配置，本机mac上\ncat /Library/Preferences/VMware\\ Fusion/vmnet8/nat.conf \n# 2. 可以看到如下部分信息，记住下面信息，用来配置linux\n[host]\n# NAT gateway address\nip = 192.168.83.2\nnetmask = 255.255.255.0\n```\n\n#### 1.1.2 配置linux网络\n\n1. setting -> Network Adapter -> Share with my msc  如下图\n\n2. 点击上图左下角 Advanced options，点击Generate（生成MAC Address）\n\n3. 启动虚拟机\n\n```shell\n# 1. 修改配置文件\nsudo vi /etc/sysconf/network-scripts/ifcg-ens192\n\n# 2. 修改下面内容\n# 改为静态\nBOOTPROTO=static\nONBOOT=yes\n# 与上面网段保持一致\nIPPADDR=192.168.83.100\n# 与上面一致\nNETMASK=255.255.255.0\n# 与上面ip一致\nGATEWAY=192.168.83.2\nDNS1=8.8.8.8\n\n#3. 修改之后，重启network\nservice network restart\n```\n\n4. 关闭防火墙，selinux\n\n```shell\n# 关闭\nsystemctl stop firewalld\n# 永久关闭\nsystemctl disable firewalld\n\n# 关闭selinux，修改下面文件内容\nvi /etc/selinux/config #进入selinux设置文件\nSELINUX=disabled\n```\n\n### 1.2. Linux 设置免密登陆\n\n#### 1.2.1 设置用户 & 权限\n\n```shell\n# 1. 创建hadoop用户\nuseradd hadoop #添加hadoop用户\npasswd hadoop #给hadoop用户添加密码\nhadoop #密码设为hadoop\n# 2. 设置用户权限 \nvisudo #进入用户权限配置文件\n## Allow root to run any commands anywhere\nroot    ALL=(ALL)       ALL\nhadoop  ALL=(ALL)\t    ALL # 给hadoop用户添加所有权限\n# 3. 切换到hadoop用户\nsu - hadoop # 加上- 表示切换同时拥有权限\n```\n\n#### 1.2.2 免密登陆\n\n1. 修改/etc/hosts 文件\n\n```shell\n   sudo vi /etc/hosts\n   \n   # 添加如下内容\n   192.168.83.100 node01\n   192.168.83.110 node02\n   192.168.83.120 node03\n```\n\n2. 配置免密登陆\n\n```shell\n# 1. hadoop用户下执行下列命令，必须！\n# 下面操作node01，node02，node03 都执行，回车next。\nssh-keygen -t rsa #生成公钥\n\n# 2. 三台机器的公钥全部拷贝到node01\nssh-copy-id node01 \n\n# 3. 第一台机器执行，“:PWD”的意思是：拷贝目标文件位置和node01的位置一致。\ncd /home/hadoop/.ssh/\nscp authorized_keys node02:$PWD #将node01的授权文件拷贝到node02\nscp authorized_keys node03:$PWD #将node01的授权文件拷贝到node03\n\n#4. 验证免密登录\n#在node01执行\nssh node02 #在node01登录node02，不需要密码就ok\nssh node03 #在node01登录node02，不需要密码就ok\n#回到node01\nlogout\t\t\n```\n\n### 1.3 时间同步\n\n#### 1.3.1 同步阿里云\n\n```shell\n# 安装ntpdate\nsudo yum -y install ntpdate\ncrontab -e \n*/1 * * * * /usr/sbin/ntpdate time1.aliyun.com\n\n# 如果时间不通可以执行\nsudo ntpdate -u asia.pool.ntp.org\n```\n\n#### 1.3.2 同步node01时间\n\n- 以下命令在root用户操作\n\n```shell\n# 1. 安装ntp软件（所有）\n# (注意：ntpd作为node01服务端，node02、node03 ntpdate作为客户端软件不同)\nsudo yum  -y  install ntpd\n\n# 2. 设置时区为中国上海（所有）\ntimedatectl set-timezone Asia/Shanghai\n\n# 3. node01启动ntp服务，作为服务端供其他节点同步时间\nsystemctl start ntpd\n\n# 4. node01设置开机启动\nsystemctl enable ntpd\n\n# 5.修改配置 node01上\nsudo vi /etc/ntp.conf\n# 注释掉以下四行，添加最后两行。对应自己的vmnt8内容\n#server 0.centos.pool.ntp.org iburst\n#server 1.centos.pool.ntp.org iburst\n#server 2.centos.pool.ntp.org iburst\n#server 3.centos.pool.ntp.org iburst\n\nrestrict 192.168.83.2 mask 255.255.255.0 nomodify notrap\nserver 127.127.1.0\n\n# 5.1 node02 node03修改配置\nsudo vi /etc/sysconfig/ntpdate\n# 修改为 yes\nSYNC_HWCLOCK=yes\n\ncrontab -e\n*/1 * * * * /usr/sbin/ntpdate 192.168.83.100\n\n# 6. node02、node03重启服务\nsystemctl restart ntpdate\n\n```\n\n## 2.Hadoop环境搭建\n\n### 2.1 配置hadoop-env.sh\n\n```shell\n# hadoop 用户下\nsu - hadoop\n\nvi /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/hadoop-env.sh \nexport JAVA_HOME=/kfly/install/jdk1.8.0_141 #修改为此变量\n```\n\n### 2.2 配置core-site.xml\n\n```shell\n#在hadoop用户下打开配置文件：\nvi /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/core-site.xml\n```\n\n```xml\n<configuration>\n\t<property>\n\t\t<name>fs.defaultFS</name>\n\t\t<value>hdfs://node01:8020</value>\n\t</property>\n\t<property>\n\t\t<name>hadoop.tmp.dir</name>\n\t\t<value>/kfly/install/hadoop-2.6.0/hadoopDatas/tempDatas</value>\n\t</property>\n\t<!--  缓冲区大小，实际工作中根据服务器性能动态调整 -->\n\t<property>\n\t\t<name>io.file.buffer.size</name>\n\t\t<value>4096</value>\n\t</property>\n  <property>\n    <name>fs.trash.interval</name>\n    <value>10080</value>\n    <description>检查点被删除后的分钟数。 如果为零，垃圾桶功能将被禁用。 \n      该选项可以在服务器和客户端上配置。 如果垃圾箱被禁用服务器端，则检查客户端配置。 \n      如果在服务器端启用垃圾箱，则会使用服务器上配置的值，并忽略客户端配置值。\n    </description>\n  </property>\n\n  <property>\n       <name>fs.trash.checkpoint.interval</name>\n       <value>0</value>\n       <description>垃圾检查点之间的分钟数。 应该小于或等于fs.trash.interval。 \n       如果为零，则将该值设置为fs.trash.interval的值。 每次检查指针运行时，\n       它都会从当前创建一个新的检查点，并删除比fs.trash.interval更早创建的检查点。\n    </description>\n  </property>\n</configuration>\n```\n\n### 2.3 配置hdfs-site.xml\n\n```shell\n#在hadoop用户下打开配置文件：\nvi /kkb/install/hadoop-2.6.0/etc/hadoop/hdfs-site.xml\n```\n\n```xml\n<configuration>\n\t<!-- NameNode存储元数据信息的路径，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割   --> \n\t<!--   集群动态上下线 \n\t<property>\n\t\t<name>dfs.hosts</name>\n\t\t<value>/kfly/install/hadoop-2.6.0/etc/hadoop/accept_host</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.hosts.exclude</name>\n\t\t<value>/kfly/install/hadoop-2.6.0/etc/hadoop/deny_host</value>\n\t</property>\n\t -->\n\t <property>\n\t\t\t<name>dfs.namenode.secondary.http-address</name>\n\t\t\t<value>node01:50090</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.http-address</name>\n\t\t<value>node01:50070</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.name.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/namenodeDatas</value>\n\t</property>\n\t<!--  定义dataNode数据存储的节点位置，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割  -->\n\t<property>\n\t\t<name>dfs.datanode.data.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/datanodeDatas</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.edits.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/dfs/nn/edits</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.checkpoint.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/dfs/snn/name</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.checkpoint.edits.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/dfs/nn/snn/edits</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.replication</name>\n\t\t<value>3</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.permissions</name>\n\t\t<value>false</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.blocksize</name>\n\t\t<value>134217728</value>\n\t</property>\n</configuration>\n```\n\n### 2.4 配置mapred-site.xml\n\n```shell\n#在hadoop用户下操作,进入指定文件夹：\ncd /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/\n#由于原来没有mapred-site.xml配置文件，需要根据模板复制一份：\ncp  mapred-site.xml.template mapred-site.xml\nvi /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/mapred-site.xml\n```\n\n```xml\n<!--指定运行mapreduce的环境是yarn -->\n<configuration>\n   <property>\n\t\t<name>mapreduce.framework.name</name>\n\t\t<value>yarn</value>\n\t</property>\n\t<property>\n\t\t<name>mapreduce.job.ubertask.enable</name>\n\t\t<value>true</value>\n\t</property>\n\t<property>\n\t\t<name>mapreduce.jobhistory.address</name>\n\t\t<value>node01:10020</value>\n\t</property>\n\t<property>\n\t\t<name>mapreduce.jobhistory.webapp.address</name>\n\t\t<value>node01:19888</value>\n\t</property>\n</configuration>\n```\n\n### 2.5 配置yarn-site.xml\n\n```shell\n#在hadoop用户下操作\nvi /kfly/install/hadoop-2.6.0/etc/hadoop/yarn-site.xml\n```\n\n```xml\n<configuration>\n\t<property>\n\t\t<name>yarn.resourcemanager.hostname</name>\n\t\t<value>node01</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.nodemanager.aux-services</name>\n\t\t<value>mapreduce_shuffle</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.log-aggregation-enable</name>\n\t\t<value>true</value>\n\t</property>\n\t<property>\n\t\t <name>yarn.log.server.url</name>\n\t\t <value>http://node01:19888/jobhistory/logs</value>\n\t</property>\n\t<!--多长时间聚合删除一次日志 此处-->\n\t<property>\n        <name>yarn.log-aggregation.retain-seconds</name>\n        <value>2592000</value><!--30 day-->\n\t</property>\n\t<!--时间在几秒钟内保留用户日志。只适用于如果日志聚合是禁用的-->\n\t<property>\n        <name>yarn.nodemanager.log.retain-seconds</name>\n        <value>604800</value><!--7 day-->\n\t</property>\n\t<!--指定文件压缩类型用于压缩汇总日志-->\n\t<property>\n        <name>yarn.nodemanager.log-aggregation.compression-type</name>\n        <value>gz</value>\n\t</property>\n\t<!-- nodemanager本地文件存储目录-->\n\t<property>\n        <name>yarn.nodemanager.local-dirs</name>\n        <value>/kkb/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/yarn/local</value>\n\t</property>\n\t<!-- resourceManager  保存最大的任务完成个数 -->\n\t<property>\n        <name>yarn.resourcemanager.max-completed-applications</name>\n        <value>1000</value>\n\t</property>\n</configuration>\n```\n\n### 2.6 编辑slaves\n\n此文件用于配置集群有多少个数据节点,我们把node2，node3作为数据节点,node1作为集群管理节点\n\n```shell\n#在hadoop用户下操作\nvim /kfly/install/hadoop-2.6.0/etc/hadoop/slaves\n\nnode01 #添加\nnode02 #添加\nnode03 #添加\n```\n\n### 2.7 创建文件存放目录\n\n```shell\n#在hadoop用户下操作\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/tempDatas\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits\n```\n\n### 2.8 拷贝到其他节点\n\n```shell\n# 分发到各个节点下,$PWD 相同目录\nscp -r hadoop-2.6.0 node02:$PWD\n\n#分发配置文件\nscp /etc/profile node02:$PWD\n```\n\n### 2.9 格式化节点\n\n```shell\n#下面命令只在node01上执行\nhdfs namenode -format #格式化\n\n# 启动\nstart-all.sh\n```\n\n### 2.10 访问\n\n- hadoop webui http://node01:50070/\n\n- hadoop application http://node01:8088\n\n  ```shell\n  # 启动 jobhistory\n  mr-jobhistory-daemon.sh start historyserver\n  ```\n\n- hadoop job http://node01:19888\n\n### 2.11. Hadoop Ha高可用\n\n​\t\t[hadoop ha 高可用](https://kfly.top/2019/10/28/zookeeper/zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E6%A1%86%E6%9E%B6%EF%BC%88%E4%BA%8C%EF%BC%89hadoop%20ha%E9%AB%98%E5%8F%AF%E7%94%A8%E5%AE%89%E8%A3%85/)\n\n## 3. hive环境搭建\n\n### 3.1 mysql安装\n\n```shell\n# 安装wget\nsudo yum install wget\n\n# 1. 换源\n# 1.1 备份系统源\nsudo mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup\n# 1.2 下载阿里云CENTOS7镜像文件\nwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n# 1.3 清理缓存、生成新的缓存\nsudo yum clean all\nsudo yum makecache\n# 1.4 更新源\nsudo yum update -y\n\n# 2. 使用yum安装MySQL,下载并安装MySQL官方的 Yum Repository\nsudo wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm\nsudo yum -y install mysql57-community-release-el7-10.noarch.rpm\nsudo yum -y install mysql-community-server\n\n# 3. 启动mysql，查看运行状态\nsystemctl start  mysqld.service\nsystemctl status mysqld.service\n\n# 4. 找出默认密码如下图所示\nsudo grep \"password\" /var/log/mysqld.log\n\n# 5. 登陆、修改密码\nmysql -uroot -p     # 回车后会提示输入密码\nALTER USER 'root'@'localhost' IDENTIFIED BY 'new password';\n # 注意： 如果失败，修改密码策略,先修改一个复杂的密码，在修改策略，修改密码\nALTER USER 'root'@'localhost' IDENTIFIED BY 'z?guwrBhH7p>';\nset global validate_password_policy=0;\nset global validate_password_policy=1;\n\n# 6. 设置mysql可以外部连接\ngrant all on *.* to root@'%' identified by '数据库密码';\n```\n\n### 3.2 hive安装\n\n#### 3.2.1 下载hive的安装包\n\n- http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.14.2.tar.gz\n\n#### 3.2.2 hive-env.sh\n\n```shell\nvim hive-env.sh\n\n#配置HADOOP_HOME路径\nexport HADOOP_HOME=/kfly/install/hadoop-2.6.0\n#配置HIVE_CONF_DIR路径\nexport HIVE_CONF_DIR=/kfly/install/hive-1.1.0-cdh5.14.2/conf\n```\n\n#### 3.2.3 hive-site.xml\n\n```xml\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n        <property>\n                <name>javax.jdo.option.ConnectionURL</name>\n                <value>jdbc:mysql://node02:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=latin1&amp;useSSL=false</value>\n        </property>\n\n        <property>\n                <name>javax.jdo.option.ConnectionDriverName</name>\n                <value>com.mysql.jdbc.Driver</value>\n        </property>\n        <property>\n                <name>javax.jdo.option.ConnectionUserName</name>\n                <value>root</value>\n        </property>\n        <property>\n                <name>javax.jdo.option.ConnectionPassword</name>\n                <value>123456</value>\n        </property>\n        <property>\n                <name>hive.cli.print.current.db</name>\n                <value>true</value>\n        </property>\n        <property>\n                <name>hive.cli.print.header</name>\n            <value>true</value>\n        </property>\n    <property>\n                <name>hive.server2.thrift.bind.host</name>\n                <value>node02</value>\n        </property>\n</configuration>\n```\n\n#### 3.2.4 日志路径\n\n```shell\nvim hive-log4j.properties\n\n#更改以下内容，设置我们的日志文件存放的路径\nhive.log.dir=/kkb/install/hive-1.1.0-cdh5.14.2/logs/\n```\n\n#### 3.4.5  lib包\n\n```shell\n ps: ==需要将mysql的驱动包上传到hive的lib目录下==\n  * 例如 mysql-connector-java-5.1.38.jar\n```\n\n### 3.3 hive使用Spark on Yarn作为计算引擎\n\n​\t\t[查看](http://lxw1234.com/archives/2016/05/673.htm)\n\n## 4. zookeeper环境搭建\n\n### 4.1 下载软件\n\n​\t\t\t[点击下载 zookeeper-3.4.5-cdh5.14.2.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.14.2.tar.gz)\n\n### 4.2 修改配置文件\n\n```shell\n# 1. copy配置文件\ncd /kfly/install/zookeeper-3.4.5-cdh5.14.2/conf\ncp zoo_sample.cfg zoo.cfg\n# 2. 创建存放数据目录\nmkdir -p /kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas\n# 编辑\nvim  zoo.cfg\n# 3. 文件内容如下\ndataDir=/kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas\nautopurge.snapRetainCount=3\nautopurge.purgeInterval=1\nserver.1=node01:2888:3888\nserver.2=node02:2888:3888\nserver.3=node03:2888:3888\n\n# 4 分发到各个节点\n# 5. 写入myid文件，myid分别对应，node01:1，node02:2,node03.3、一次累加\necho 1 >  /kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid\n\n# 6. 启动服务、查看状态\nbin/zkServer.sh start\nbin/zkServer.sh status\n```\n\n## 5. HBase环境搭建\n\n### 5.1 下载软件\n\n​\t\t[点击下载hbase-1.2.0-cdh5.14.2.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.14.2.tar.gz)\n\n### 5.2  hbase-env.sh\n\n```shell\nexport JAVA_HOME=/kfly/install/jdk1.8.0_141\n# 使用外部的zookeeper集群\nexport HBASE_MANAGES_ZK=false\n```\n\n### 5.3 hbase-site.xml\n\n```xml\n<configuration>\n\t<property>\n\t\t<name>hbase.rootdir</name>\n\t\t<value>hdfs://node01:8020/HBase</value>  \n\t</property>\n\t<property>\n\t\t<name>hbase.cluster.distributed</name>\n\t\t<value>true</value>\n\t</property>\n\t<!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 -->\n\t<property>\n\t\t<name>hbase.master.port</name>\n\t\t<value>16000</value>\n\t</property>\n\t<property>\n\t\t<name>hbase.zookeeper.quorum</name>\n\t\t<value>node01,node02,node03</value>\n\t</property>\n\t<property>\n\t\t<name>hbase.zookeeper.property.clientPort</name>\n\t\t<value>2181</value>\n\t</property>\n\t<property>\n\t\t<name>hbase.zookeeper.property.dataDir</name>\n\t\t<value>/kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas</value>\n\t</property>\n\t<property>\n\t\t<name>zookeeper.znode.parent</name>\n\t\t<value>/HBase</value>\n\t</property>\n</configuration>\n```\n\n### 5.4  regionservers\n\n```shell\n# 配置文件 目录 conf下\n vim regionservers\n \n# 内容如下\nnode01\nnode02\nnode03\n```\n\n### 5.5  back-masters\n\n- 创建back-masters配置文件，里边包含备份HMaster节点的主机名，每个机器独占一行，实现HMaster的高可用\n\n```shell\n[hadoop@node01 conf]$ vim backup-masters\n```\n\n- 将node02作为备份的HMaster节点，问价内容如下\n\n```properties\nnode02\n```\n\n### 5.6  创建软连接\n\n- **<font color='red'>注意：三台机器</font>**均做如下操作\n\n- 因为HBase集群需要读取hadoop的core-site.xml、hdfs-site.xml的配置文件信息，所以我们==三台机器==都要执行以下命令，在相应的目录创建这两个配置文件的软连接\n\n```shell\nln -s /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/core-site.xml  /kfly/install/hbase-1.2.0-cdh5.14.2/conf/core-site.xml\n\nln -s /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/hdfs-site.xml  /kfly/install/hbase-1.2.0-cdh5.14.2/conf/hdfs-site.xml\n\n```\n\n### 5.7 添加HBase环境变量\n\n- **<font color='red'>注意：三台机器</font>**均执行以下命令，添加环境变量\n\n```shell\nsudo vim /etc/profile\n# 添加如下\nexport HBASE_HOME=/kkb/install/hbase-1.2.0-cdh5.14.2\nexport PATH=$PATH:$HBASE_HOME/bin\n\n# 立即生效\nsource /etc/profile\n```\n\n### 5.8  HBase的启动与停止\n\n- <font color='red'>需要提前启动HDFS及ZooKeeper集群</font>\n\n- 第一台机器node01（HBase主节点）执行以下命令，启动HBase集群\n\n```shell\n[hadoop@node01 ~]$ start-hbase.sh\n\n#HMaster节点上启动HMaster命令\nhbase-daemon.sh start master\n\n#启动HRegionServer命令\nhbase-daemon.sh start regionserver\n```\n\n- 浏览器页面访问\n\n  http://node01:60010\n\n## 6. Flume环境搭建\n\n### 1. 下载软件\n\n​\t\t\t[点击下载 flume-ng-1.6.0-cdh5.14.2.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.2.tar.gz)\n\n### 2. flume-env.sh\n\n```properties\nexport JAVA_HOME=/kfly/install/jdk1.8.0_141\n```\n\n## 7. Sqoop环境搭建\n\n### 7.1. 下载软件\n\n​\t[点击下载 sqoop-1.4.6-cdh5.14.2.tar.gz ](http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.14.2.tar.gz)\n\n### 7.2. sqoop-env.sh\n\n```shell\n#Set path to where bin/hadoop is available\nexport HADOOP_COMMON_HOME=/kfly/install/hadoop-2.6.0\n\n#Set path to where hadoop-*-core.jar is available\nexport HADOOP_MAPRED_HOME=/kfly/install/hadoop-2.6.0\n\n#set the path to where bin/hbase is available\nexport HBASE_HOME=/kfly/install/hbase-1.2.0-cdh5.14.2\n\n#Set the path to where bin/hive is available\nexport HIVE_HOME=/kfly/install/hive-1.1.0-cdh5.14.2\n```\n\n## 8. zakaban环境搭建\n\n### 8.1 下载软件\n\n​\t[点击下载 ](https://azkaban.github.io/downloads.html)\n\n- azkaban-web-server-2.5.0.tar.gz\n- azkaban-executor-server-2.5.0.tar.gz\n- azkaban-sql-script-2.5.0.tar.gz\n\n### 8.2 azkaban web服务器安装\n\n#### 8.2.1 配置SSL安全访问协议\n\n~~~shell\n# 1. 创建安装目录，解压、解压文件重命名\nmkdir /kfly/install/azkaban\ntar –zxvf azkaban-web-server-2.5.0.tar.gz -C /kfly/install/azkaban\nmv /kkb/install/azkaban/azkaban-web-2.5.0 /kkb/install/azkaban/server\n\n# 2. 在server目下执行下边的命令\nkeytool -keystore keystore -alias jetty -genkey -keyalg RSA\n          # Keytool:   是java数据证书的管理工具，使用户能够管理自己的公/私钥对及相关证书。\n          # -keystore：指定密钥库的名称及位置(产生的各类信息将不在.keystore文件中)\n          # -alias：   对我们生成的.keystore 进行指认别名；如果没有默认是mykey\n          # -genkey：  在用户主目录中创建一个默认文件\".keystore\" \n          # -keyalg：  指定密钥的算法 RSA/DSA 默认是DSA\n\n          # 运行此命令后,会提示输入当前生成 keystore的密码及相应信息,输入的密码请劳记\n         -------------------------------------------------------------------\n            输入keystore密码： \n            再次输入新密码:\n            您的名字与姓氏是什么？\n              [Unknown]： \n            您的组织单位名称是什么？\n              [Unknown]： \n            您的组织名称是什么？\n              [Unknown]： \n            您所在的城市或区域名称是什么？\n              [Unknown]： \n            您所在的州或省份名称是什么？\n              [Unknown]： \n            该单位的两字母国家代码是什么\n              [Unknown]：  CN\n            CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？\n              [否]：  y\n\n              输入<jetty>的主密码\n                      （如果和 keystore 密码相同，按回车）： \n              再次输入新密码:\n          #完成上述工作后,将在当前目录生成 keystore 证书文件,\n# 3. 将keystore 考贝到 azkaban webserver 服务器根目录中.\ncp keystore /kfly/install/azkaban/server\n\n# 4. 配置时区、先生成时区配置文件Asia/Shanghai，用交互式命令 tzselect 即可.\ntzselect \n  \t# 选5 --->选9---->选1----->选1\n# 4.1、拷贝该时区文件，覆盖系统本地时区配置\ncp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  \n\n# 5.修改配置文件\n\n# 5.2 \n~~~\n\n#### 8.2.2 修改配置文件\n\n- 1. azkaban.properties\n\n~~~shell\nvim /kfly/install/azkaban/server/conf/azkaban.properties\n\n#内容说明如下:\n#Azkaban Personalization Settings\nazkaban.name=Test                   #服务器UI名称,用于服务器上方显示的名字\nazkaban.label=My Local Azkaban      #描述\nazkaban.color=#FF3601               #UI颜色\nazkaban.default.servlet.path=/index    \nweb.resource.dir=web/                 #默认根web目录\ndefault.timezone.id=Asia/Shanghai     #默认时区,已改为亚洲/上海 默认为美国\n \n#Azkaban UserManager class\nuser.manager.class=azkaban.user.XmlUserManager   #用户权限管理默认类\nuser.manager.xml.file=conf/azkaban-users.xml     #用户配置,具体配置参加下文\n \n#Loader for projects\nexecutor.global.properties=conf/global.properties    #global配置文件所在位置\nazkaban.project.dir=projects                                             \n \ndatabase.type=mysql               #数据库类型\nmysql.port=3306                   #端口号\nmysql.host=node03                 #数据库连接IP\nmysql.database=azkaban            #数据库实例名\nmysql.user=root                   #数据库用户名\nmysql.password=123456             #数据库密码\n \n# Velocity dev mode\nvelocity.dev.mode=false          #Jetty服务器属性.\njetty.maxThreads=25              #最大线程数\njetty.ssl.port=8443              #Jetty SSL端口\njetty.port=8081                  #Jetty端口\njetty.keystore=keystore          #SSL文件名\njetty.password=123456            #SSL文件密码\njetty.keypassword=123456         #Jetty主密码 与 keystore文件相同\njetty.truststore=keystore        #SSL文件名\njetty.trustpassword=123456       #SSL文件密码\n \n# 执行服务器属性\nexecutor.port=12321               #执行服务器端口\n \n# 邮件设置\nmail.sender=xxxxxxxx@163.com        #发送邮箱\nmail.host=smtp.163.com              #发送邮箱smtp地址\nmail.user=xxxxxxxx                  #发送邮件时显示的名称\nmail.password=**********            #邮箱密码\njob.failure.email=xxxxxxxx@163.com  #任务失败时发送邮件的地址\njob.success.email=xxxxxxxx@163.com  #任务成功时发送邮件的地址\nlockdown.create.projects=false       \ncache.directory=cache                #缓存目录\n~~~\n\n- 2. azkaban-users.xml\n\n```shell\nvim /kfly/install/azkaban/server/conf/azkaban-users.xml\n```\n\n~~~xml\n<azkaban-users>\n<user username=\"azkaban\" password=\"azkaban\" roles=\"admin\"groups=\"azkaban\"/>\n<user username=\"metrics\" password=\"metrics\" roles=\"metrics\"/>\n <!--新增admin用户--> \n<user username=\"admin\" password=\"admin\" roles=\"admin,metrics\" />\n<role name=\"admin\" permissions=\"ADMIN\" />\n<role name=\"metrics\" permissions=\"METRICS\"/>\n</azkaban-users>\n~~~\n\n### 8.3 azkaban 执行服器安装\n\n```shell\n# 1. 解压azkaban-executor-server-2.5.0.tar.gz,重命名文件\ntar -zxvf azkaban-executor-server-2.5.0.tar.gz -C /kfly/install/azkaban\nmv /kfly/install/azkaban/azkaban-executor-2.5.0 /kfly/install/azkaban/executor\n\n# 2. 修改配置文件\nvim /kfly/install/azkaban/executor/conf/azkaban.properties\n      # 内容如下----------\n      #Azkaban   #时区\n      default.timezone.id=Asia/Shanghai          \n\n      #数据库设置----->需要修改的信息\n      mysql.host=node3          #数据库IP地址\n      mysql.database=azkaban    #数据库实例名\n      mysql.user=root           #数据库用户名\n      mysql.password=123456     #数据库密码\n```\n\n### 8.4 azkaban脚本导入\n\n~~~shell\n# 1. 解压azkaban-sql-script-2.5.0.tar.gz\ntar -zxvf azkaban-sql-script-2.5.0.tar.gz -C /kfly/install/azkaban\n\n# 2. 把解压后的脚本导入到mysql中、进入到mysql\nmysql> create database azkaban;\nmysql> use azkaban;\nmysql> source /kfly/install/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql;\n~~~\n\n### 8.5 Azkaban启动\n\n* 在azkaban web server服务器目录下执行启动命令\n\n~~~shell\n# 1. 启动web server服务、在azkaban web server服务器目录下执行启动命令\nbin/azkaban-web-start.sh\n# 2. 启动executor执行服务、在azkaban executor服务器目录下执行启动命令\nbin/azkaban-executor-start.sh\n\n\n~~~\n\n* 启动完成后,在浏览器(建议使用谷歌浏览器)中输入==https://服务器IP地址:8443== ,即可访问azkaban服务了.在登录中输入刚才新的户用名及密码,点击 login.\n* 输入“IP地址:8443”无法访问 web 页面, 且后台报错，原因是浏览器安全证书限制\n* 解决办法：使用“https://ip:8443”访问, 发现已经可以访问了, 后台会报证数问题的错误, 忽略即可, 不影响使用, 选择高级 ------> 继续访问该网站\n\n~~~\n（1）、projects：azkaban最重要的一部分，创建一个工程，将所有的工作流放在工程中执行\n（2）、scheduling：定时调度任务用的\n（3）、executing:  显示当前运行的任务\n（4）、History : 显示历史运行任务\n\n一个project由3个按钮：\n（1）、Flows：一个工作流，由多个job组成\n（2）、Permissions:权限管理\n（3）、Project Logs：工程日志信息\n~~~\n\n## 9. Spark环境安装\n\n### 9.1 下载软件\n\n​\t[点击下载](https://archive.apache.org/dist/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz)\n\n### 9.2 修改配置文件\n\n#### 9.2.1 spark-env.sh \n\n```sh\n#配置java的环境变量\nexport JAVA_HOME=/kkb/install/jdk1.8.0_141\n#配置zk相关信息\nexport SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER  -Dspark.deploy.zookeeper.url=node01:2181,node02:2181,node03:2181  -Dspark.deploy.zookeeper.dir=/spark\"\n```\n\n- 注意\n\n```properties\n## 如果想运行spark-shell --master local[N] 读取HDFS上文件，则加上如下配置文件\nexport HADOOP_CONF_DIR=/kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop\n# 这就是读取hdfs下，hdfs://node01:8020/words.txt 文件\nsc.textFile(\"/words.txt\")\n```\n\n  \n\n#### 9.2.2  slaves\n\n```shell\n#指定spark集群的worker节点\nnode02\nnode03\n```\n\n#### 9.2.3 分发到各个节点\n\n```sh\n# 1. 分发到各个节点\nscp -r /kfly/install/spark node02:/kkb/install\n# 2. 修改source /etc/profile环境\nexport SPARK_HOME=/kfly/install/spark\nexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n```\n\n### 9.3 SparkSql整合Hive\n\n- 步骤\n  - 1、需要把hive安装目录下的配置文件hive-site.xml拷贝到每一个spark安装目录下对应的conf文件夹中\n  - 2、需要一个连接mysql驱动的jar包拷贝到spark安装目录下对应的jars文件夹中\n  - 3、可以使用spark-sql脚本 后期执行sql相关的任务\n\n```shell\nspark-sql \\\n--master spark://node01:7077 \\\n--executor-memory 1g \\\n--total-executor-cores 4 \\\n--conf spasrk.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse \n```\n\n```sh\n#!/bin/sh\n#定义sparksql提交脚本的头信息\nSUBMITINFO=\"spark-sql --master spark://node01:7077 --executor-memory 1g --total-executor-cores 4 --conf spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse\" \n#定义一个sql语句\nSQL=\"select * from kfly.psrson;\" \n#执行sql语句   类似于 hive -e sql语句\necho \"$SUBMITINFO\" \necho \"$SQL\"\n$SUBMITINFO -e \"$SQL\"\n```\n\n\n\n### 9.4 Spark on Yarn\n\n- [官网资料](http://spark.apache.org/docs/2.3.3/running-on-yarn.html)\n\n```shell\n## 注意这里不需要安装spark集群、只需要解压spark安装包到任意一台服务器\n#修改文件 spark-env.sh\n\n#指定java的环境变量\nexport JAVA_HOME=/kkb/install/jdk1.8.0_141\n#指定hadoop的配置文件目录\nexport HADOOP_CONF_DIR=/kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop\n```\n\n```shell\nspark-submit --class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode cluster \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 1 \\\n/kfly/install/spark-2.3.3-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.3.jar \\\n10\n```\n\n- 如果运行出现错误，可能是虚拟内存不足，可以添加参数\n\n  - vim yarn-site.xml\n\n  ```xml\n  <!--容器是否会执行物理内存限制默认为True-->\n  <property>\n      <name>yarn.nodemanager.pmem-check-enabled</name>\n      <value>false</value>\n  </property>\n  \n  <!--容器是否会执行虚拟内存限制    默认为True-->\n  <property>\n      <name>yarn.nodemanager.vmem-check-enabled</name>\n      <value>false</value>\n  </property>\n  ```\n\n## 10 kafka环境搭建\n\n### 10.1 下载软件\n\n*[点击下载 https://archive.apache.org/dist/kafka/1.0.1/kafka_2.11-1.0.1.tgz](https://archive.apache.org/dist/kafka/1.0.1/kafka_2.11-1.0.1.tgz)*\n\n### 10.2 修改配置文件\n\n- vi server.properties\n\n```properties\n#指定kafka对应的broker id ，唯一(eg：node01 0,node02 1,node03 2)\nbroker.id=0\n#指定数据存放的目录\nlog.dirs=/kfly/install/kafka/kafka-logs\n#指定zk地址\nzookeeper.connect=node01:2181,node02:2181,node03:2181\n#指定是否可以删除topic ,默认是false 表示不可以删除\ndelete.topic.enable=true\n#指定broker主机名,node01 node02 node03\nhost.name=node01\n```\n\n- 修改kafka环境变量\n\n```properties\nexport KAFKA_HOME=/kfly/install/kafka\nexport PATH=$PATH:$KAFKA_HOME/bin\n```\n\n- 分发到其他节点，然后运行\n\n```sh\n#!/bin/sh\ncase $1 in \n\"start\"){\nfor host in node01 node02 node03 \ndo\n  ssh $host \"source /etc/profile; nohup /kfly/install/kafka/bin/kafka-server-start.sh /kfly/install/kafka/config/server.properties > /dev/null 2>&1 &\"   \n  echo \"$host kafka is running...\"  \ndone\n};;\n\n\"stop\"){\nfor host in node01 node02 node03 \ndo\n  ssh $host \"source /etc/profile; nohup /kfly/install/kafka/bin/kafka-server-stop.sh >/dev/null  2>&1 &\"   \n  echo \"$host kafka is stopping...\"  \ndone\n};;\nesac\n```\n\n### 10.3 kafka监控工具的安装\n\n#### 10.3.1. Kafka Manager\n\n```\nkafkaManager它是由雅虎开源的可以监控整个kafka集群相关信息的一个工具。\n（1）可以管理几个不同的集群\n（2）监控集群的状态(topics, brokers, 副本分布, 分区分布)\n（3）创建topic、修改topic相关配置\n```\n\n- [点击下载 https://github.com/yahoo/kafka-manager/releases](https://github.com/yahoo/kafka-manager/releases)\n\n- vim application.conf\n\n  ```shell\n  #修改kafka-manager.zkhosts的值，指定kafka集群地址\n  kafka-manager.zkhosts=\"node01:2181,node02:2181,node03:2181\"\n  ```\n\n- 4、启动kafka-manager\n\n  - 启动zk集群，kafka集群，再使用root用户启动kafka-manager服务。\n  - bin/kafka-manager 默认的端口是9000，可通过 -Dhttp.port，指定端口\n  - -Dconfig.file=conf/application.conf指定配置文件\n\n  ```\n  nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 &\n  ```\n\n- 5.2. KafkaOffsetMonitor\n\n```\n该监控是基于一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全\n\n(1)消费者组列表\n(2)查看topic的历史消费信息.\n(3)每个topic的所有parition列表(topic,pid,offset,logSize,lag,owner)\n(4)对consumer消费情况进行监控,并能列出每个consumer offset,滞后数据。\n```\n\n- 1、下载安装包 [下载](https://github.com/quantifind/KafkaOffsetMonitor/tags)\n\n  ```\n  KafkaOffsetMonitor-assembly-0.2.0.jar\n  ```\n\n- 2、在服务器上新建一个目录kafka_moitor，把jar包上传到该目录中\n\n- 3、在kafka_moitor目录下新建一个脚本\n\n  - vim start_kafka_web.sh\n\n  ```shell\n  #!/bin/sh\n  java -cp KafkaOffsetMonitor-assembly-0.2.0.jar com.quantifind.kafka.offsetapp.OffsetGetterWeb --zk node01:2181,node02:2181,node03:2181 --port 8089 --refresh 10.seconds --retain 1.days\n  ```\n\n- 4、启动脚本\n\n  ```shell\n  nohup sh start_kafka_web.sh &\n  ```\n\n\n#### 10.3.2 Kafka Eagle\n\n- 1、下载Kafka Eagle安装包\n\n  - http://download.smartloli.org/\n    - kafka-eagle-bin-1.2.3.tar.gz\n\n- 2、解压 \n\n  - tar -zxvf kafka-eagle-bin-1.2.3.tar.gz -C /kkb/install\n  - 解压之后进入到kafka-eagle-bin-1.2.3目录中\n    - 得到kafka-eagle-web-1.2.3-bin.tar.gz\n    - 然后解压  tar -zxvf kafka-eagle-web-1.2.3-bin.tar.gz\n    - 重命名  mv kafka-eagle-web-1.2.3  kafka-eagle-web\n\n- 3、修改配置文件\n\n  - 进入到conf目录\n\n    - 修改system-config.properties\n\n```properties\n# 填上你的kafka集群信息\nkafka.eagle.zk.cluster.alias=cluster1\ncluster1.zk.list=node01:2181,node02:2181,node03:2181\n\n# kafka eagle页面访问端口\nkafka.eagle.webui.port=8048\n\n# kafka sasl authenticate\nkafka.eagle.sasl.enable=false\nkafka.eagle.sasl.protocol=SASL_PLAINTEXT\nkafka.eagle.sasl.mechanism=PLAIN\nkafka.eagle.sasl.client=/kfly/install/kafka-eagle-bin-1.2.3/kafka-eagle-web/conf/kafka_client_jaas.conf\n\n#  添加刚刚导入的ke数据库配置，我这里使用的是mysql\nkafka.eagle.driver=com.mysql.jdbc.Driver\nkafka.eagle.url=jdbc:mysql://node02:3306/ke?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull\nkafka.eagle.username=root\nkafka.eagle.password=123456\n```\n\n- 4、配置环境变量\n- vi /etc/profile\n\n```properties\n    export KE_HOME=/kfly/install/kafka-eagle-web\n    export PATH=$PATH:$KE_HOME/bin\n```\n\n- 5、启动kafka-eagle /访问\n\n```shell\n  sh bin/ke.sh start\n  # 访问 `http://node01:8048/ke`\n  # 用户名：admin password：123456\n```","tags":["环境搭建"]},{"title":"hive企业综合案例实战","url":"/2010/02/10/it/hive/hive企业综合案例实战/","content":"\n# hive的综合案例实战\n\n## 1、需求描述\n\n统计youtube影音视频网站的常规指标，各种TopN指标：\n\n--统计视频观看数Top10\n\n--统计视频类别热度Top10\n\n--统计视频观看数Top20所属类别\n\n--统计视频观看数Top50所关联视频的所属类别Rank\n\n--统计每个类别中的视频热度Top10\n\n--统计每个类别中视频流量Top10\n\n--统计上传视频最多的用户Top10以及他们上传的视频\n\n--统计每个类别视频观看数Top10\n\n## 2、项目表字段\n\n### 1、数据结构\n\n1．视频表\n\n| 字段          | 备注       | 详细描述               |\n| ------------- | ---------- | ---------------------- |\n| video id      | 视频唯一id | 11位字符串             |\n| uploader      | 视频上传者 | 上传视频的用户名String |\n| age           | 视频年龄   | 视频在平台上的整数天   |\n| category      | 视频类别   | 上传视频指定的视频分类 |\n| length        | 视频长度   | 整形数字标识的视频长度 |\n| views         | 观看次数   | 视频被浏览的次数       |\n| rate          | 视频评分   | 满分5分                |\n| ratings       | 流量       | 视频的流量，整型数字   |\n| conments      | 评论数     | 一个视频的整数评论数   |\n| related   ids | 相关视频id | 相关视频的id，最多20个 |\n\n\n\n2．用户表\n\n| 字段     | 备注         | 字段类型 |\n| -------- | ------------ | -------- |\n| uploader | 上传者用户名 | string   |\n| videos   | 上传视频数   | int      |\n| friends  | 朋友数量     | int      |\n\n## 3、ETL原始数据清洗\n\n通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用“\\t”进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用“&”分割，同时去掉两边空格，多个相关视频id也使用“&”进行分割。\n\n三件事情\n\n1. 长度不够9的删掉\n\n2. 视频类别删掉空格\n\n3. 该相关视频的分割符\n\n创建maven工程，并导入jar包\n\n1、代码开发：ETLUtil\n\n```\npublic class VideoUtil {\n    /**\n     * 对我们的数据进行清洗的工作，\n     * 数据切割，如果长度小于9 直接丢掉\n     * 视频类别中间空格 去掉\n     * 关联视频，使用 &  进行分割\n     * @param line\n     * @return\n     * FM1KUDE3C3k  renetto\t736\tNews & Politics\t1063\t9062\t4.57\t525\t488\tLnMvSxl0o0A&IKMtzNuKQso&Bq8ubu7WHkY&Su0VTfwia1w&0SNRfquDfZs&C72NVoPsRGw\n     */\n    public  static String washDatas(String line){\n        if(null == line || \"\".equals(line)) {\n            return null;\n        }\n        //判断数据的长度，如果小于9，直接丢掉\n        String[] split = line.split(\"\\t\");\n        if(split.length <9){\n            return null;\n        }\n        //将视频类别空格进行去掉\n        split[3] =  split[3].replace(\" \",\"\");\n        StringBuilder builder = new StringBuilder();\n        for(int i =0;i<split.length;i++){\n            if(i <9){\n                //这里面是前面八个字段\n                builder.append(split[i]).append(\"\\t\");\n            }else if(i >=9  && i < split.length -1){\n                builder.append(split[i]).append(\"&\");\n            }else if( i == split.length -1){\n                builder.append(split[i]);\n            }\n        }\n        return  builder.toString();\n    }\n}\n```\n\n\n\n2、代码开发：ETLMapper\n\n```java\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport java.io.IOException;\n\npublic class VideoMapper extends Mapper<LongWritable,Text,Text,NullWritable> {\n    private Text  key2 ;\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n        key2 = new Text();\n    }\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        String s = VideoUtils.washDatas(value.toString());\n        if(null != s ){\n            key2.set(s);\n            context.write(key2,NullWritable.get());\n        }\n    }\n}\n```\n\n\n\n3、代码开发：ETLRunner\n\n```\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\npublic class VideoMain extends Configured implements Tool {\n    @Override\n    public int run(String[] args) throws Exception {\n        Job job = Job.getInstance(super.getConf(), \"washDatas\");\n        job.setJarByClass(VideoMain.class);\n        job.setInputFormatClass(TextInputFormat.class);\n        TextInputFormat.addInputPath(job,new Path(args[0]));\n\n        job.setMapperClass(VideoMapper.class);\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(NullWritable.class);\n        job.setOutputFormatClass(TextOutputFormat.class);\n        TextOutputFormat.setOutputPath(job,new Path(args[1]));\n        //注意，我们这里没有自定义reducer，会使用默认的一个reducer类\n        job.setNumReduceTasks(7);\n        boolean b = job.waitForCompletion(true);\n        return b?0:1;\n    }\n    public static void main(String[] args) throws Exception {\n        int run = ToolRunner.run(new Configuration(), new VideoMain(), args);\n        System.exit(run);\n    }\n}\n```\n\n## 4、项目建表并加载数据\n\n### 1、创建表\n\n创建表：youtubevideo_ori，youtubevideo_user_ori，\n\n创建表：youtubevideo_orc，youtubevideo_user_orc\n\nyoutubevideo_ori：\n\n开启分桶表功能\n\n```\nset hive.enforce.bucketing=true;\nset mapreduce.job.reduces=-1;\n\ncreate database youtube;\nuse youtube;\ncreate table youtubevideo_ori(\n    videoId string, \n    uploader string, \n    age int, \n    category array<string>, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedId array<string>)\nrow format delimited \nfields terminated by \"\\t\"\ncollection items terminated by \"&\"\nstored as textfile;\n```\n\nyoutubevideo_user_ori：\n\n```\ncreate table youtubevideo_user_ori(\n    uploader string,\n    videos int,\n    friends int)\nclustered by (uploader) into 24 buckets\nrow format delimited \nfields terminated by \"\\t\" \nstored as textfile;\n\n```\n\n然后把原始数据插入到orc表中\n\nyoutubevideo_orc：\n\n```\ncreate table youtubevideo_orc(\n    videoId string, \n    uploader string, \n    age int, \n    category array<string>, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedId array<string>)\nclustered by (uploader) into 8 buckets \nrow format delimited fields terminated by \"\\t\" \ncollection items terminated by \"&\" \nstored as orc;\n```\n\nyoutubevideo_user_orc：\n\n```\ncreate table youtubevideo_user_orc(\n    uploader string,\n    videos int,\n    friends int)\nclustered by (uploader) into 24 buckets \nrow format delimited \nfields terminated by \"\\t\" \nstored as orc;\n\n```\n\n### 2、导入ETL之后的数据\n\nyoutubevideo_ori：\n\n```\nload data inpath \"/youtubevideo/output/video/2008/0222\" into table youtubevideo_ori;\n```\n\nyoutubevideo_user_ori：\n\n```\nload data inpath \"/youtubevideo/user/2008/0903\" into table youtubevideo_user_ori;\n```\n\n### 3、向ORC表插入数据\n\nyoutubevideo_orc：\n\n```\ninsert overwrite table youtubevideo_orc select * from youtubevideo_ori;\n```\n\nyoutubevideo_user_orc：\n\n```\ninsert into table youtubevideo_user_orc select * from youtubevideo_user_ori;\n```\n\n## 5、业务分析\n\n### 1、统计视频观看数Top10\n\n思路：使用order by按照views字段做一个全局排序即可，同时我们设置只显示前10条。\n\n最终代码：\n\n```sql\nselect \n    videoId, \n    uploader, \n    age, \n    category, \n    length, \n    views, \n    rate, \n    ratings, \n    comments \nfrom \n    youtubevideo_orc \norder by \n    views \ndesc limit \n    10;\n\n```\n\n\n\n### 2、统计视频类别热度Top10\n\n思路：\n\n1) 即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。\n\n2) 我们需要按照类别group by聚合，然后count组内的videoId个数即可。\n\n3) 因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。\n\n4) 最后按照热度排序，显示前10条。\n\n最终代码：\n\n```sql\nselect \n    category_name as category, \n    count(t1.videoId) as hot \nfrom (\n    select \n        videoId,\n        category_name \n    from \n        youtubevideo_orc lateral view explode(category) t_catetory as category_name) t1 \ngroup by \n    t1.category_name \norder by \n    hot \ndesc limit \n    10;\n\n```\n\n### 3、统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数\n\n思路：\n\n1) 先找到观看数最高的20个视频所属条目的所有信息，降序排列\n\n2) 把这20条信息中的category分裂出来(列转行)\n\n3) 最后查询视频分类名称和该分类下有多少个Top20的视频\n\n最终代码：\n\n```sql\nselect \n    category_name as category, \n    count(t2.videoId) as hot_with_views \nfrom (\n    select \n        videoId, \n        category_name \n    from (\n        select \n            * \n        from \n            youtubevideo_orc \n        order by \n            views \n        desc limit \n            20) t1 lateral view explode(category) t_catetory as category_name) t2 \ngroup by \n    category_name \norder by \n    hot_with_views \ndesc;\n```\n\n### 4、 统计视频观看数Top50所关联视频的所属类别Rank\n\n思路：\n\n1)       查询出观看数最多的前50个视频的所有信息(当然包含了每个视频对应的关联视频)，记为临时表t1\n\nt1：观看数前50的视频\n\n```sql\nselect \n    * \nfrom \n    youtubevideo_orc \norder by \n    views \ndesc limit \n    50;\n```\n\n2)       将找到的50条视频信息的相关视频relatedId列转行，记为临时表t2\n\nt2：将相关视频的id进行列转行操作\n\n```\nselect \n    explode(relatedId) as videoId \nfrom \n\tt1;\n\n\n```\n\n3)       将相关视频的id和youtubevideo_orc表进行inner join操作\n\nt5：得到两列数据，一列是category，一列是之前查询出来的相关视频id\n\n```\n(select \n    distinct(t2.videoId), \n    t3.category \nfrom \n    t2\ninner join \n    youtubevideo_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name;\n\n\n```\n\n4) 按照视频类别进行分组，统计每组视频个数，然后排行\n\n最终代码：\n\n```\nselect \n    category_name as category, \n    count(t5.videoId) as hot \nfrom (\n    select \n        videoId, \n        category_name \n    from (\n        select \n            distinct(t2.videoId), \n            t3.category \n        from (\n            select \n                explode(relatedId) as videoId \n            from (\n                select \n                    * \n                from \n                    youtubevideo_orc \n                order by \n                    views \n                desc limit \n                    50) t1) t2 \n        inner join \n            youtubevideo_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name) t5\ngroup by \n    category_name \norder by \n    hot \ndesc;\n\n\n```\n\n\n\n### 5、统计每个类别中的视频热度Top10，以Music为例\n\n思路：\n\n1) 要想统计Music类别中的视频热度Top10，需要先找到Music类别，那么就需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。\n\n2) 向category展开的表中插入数据。\n\n3) 统计对应类别（Music）中的视频热度。\n\n最终代码：\n\n创建表类别表：\n\n```\ncreate table youtubevideo_category(\n    videoId string, \n    uploader string, \n    age int, \n    categoryId string, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int, \n    relatedId array<string>)\nrow format delimited \nfields terminated by \"\\t\" \ncollection items terminated by \"&\" \nstored as orc;\n\n\n```\n\n向类别表中插入数据：\n\n```\ninsert into table youtubevideo_category  \n    select \n        videoId,\n        uploader,\n        age,\n        categoryId,\n        length,\n        views,\n        rate,\n        ratings,\n        comments,\n        relatedId \n    from \n        youtubevideo_orc lateral view explode(category) catetory as categoryId;\n\n```\n\n统计Music类别的Top10（也可以统计其他）\n\n```\nselect \n    videoId, \n    views\nfrom \n    youtubevideo_category \nwhere \n    categoryId = \"Music\" \norder by \n    views \ndesc limit\n    10;\n\n```\n\n\n\n### 6、 统计每个类别中视频流量Top10，以Music为例\n\n思路：\n\n1) 创建视频类别展开表（categoryId列转行后的表）\n\n2) 按照ratings排序即可\n\n最终代码：\n\n```\nselect videoid,views,ratings \nfrom youtubevideo_category \nwhere categoryid = \"Music\" order by ratings desc limit 10;\n\n```\n\n### 7、 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频\n\n思路：\n\n1) 先找到上传视频最多的10个用户的用户信息\n\n```sql\nselect \n    * \nfrom \n    youtubevideo_user_orc \norder by \n    videos \ndesc limit \n    10;\n```\n\n2) 通过uploader字段与youtubevideo_orc表进行join，得到的信息按照views观看次数进行排序即可。\n\n最终代码：\n\n```\nselect \n    t2.videoId, \n    t2.views,\n    t2.ratings,\n    t1.videos,\n    t1.friends \nfrom (\n    select \n        * \n    from \n        youtubevideo_user_orc \n    order by \n        videos desc \n    limit \n        10) t1 \njoin \n    youtubevideo_orc t2\non \n    t1.uploader = t2.uploader \norder by \n    views desc \nlimit \n    20;\n```\n\n### 8、统计每个类别视频观看数Top10\n\n思路：\n\n1) 先得到categoryId展开的表数据\n\n2) 子查询按照categoryId进行分区，然后分区内排序，并生成递增数字，该递增数字这一列起名为rank列\n\n3) 通过子查询产生的临时表，查询rank值小于等于10的数据行即可。\n\n最终代码：\n\n```\nselect \n    t1.* \nfrom (\n    select \n        videoId,\n        categoryId,\n        views,\n        row_number() over(partition by categoryId order by views desc) rank from youtubevideo_category) t1 \nwhere \n    rank <= 10;\n\n```\n\n\n\n","tags":["项目练习","hadoop","hive"]},{"title":"技术栈","url":"/1994/11/27/it/技术栈/","content":"\n# 技术栈\n\n## 数据采集\n\nFlume LogState Canal\n\n## 数据存储\n\nMySQL PostgreSQL MongoDB HDFS HBase\n\n## 缓冲层\n\nACTIVEMQ KAFKA REDIS\n\n## 计算层\n\nflink、SparkSQL、SparkStreaming、hive、MapReduce\n\n## 资源调度层\n\nyarn\n\n## 数据检索层\n\nElasticsearch、impalla、phoenix、kylin\n\n## 分布式调度工具\n\nZookeeper\n\n## 辅助工具\n\nsqoop、azkaban、hue、"}]