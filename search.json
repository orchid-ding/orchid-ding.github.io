[{"title":"MapReduce编程（二）","url":"/2019/10/20/MapReduce编程（二）/","content":"\n# MapReduce编程模型（二）\n\n### 1. 自定义分区\n\n#### 1.1 分区原理\n\n- 根据之前讲的shuffle，我们知道在map任务中，从环形缓冲区溢出写磁盘时，会先对kv对数据进行分区操作\n\n- 分区操作是由MR中的分区器负责的\n\n- MapReduce有自带的默认分区器\n\n  - **HashPartitioner**\n  - 关键方法getPartition返回当前键值对的**分区索引**(partition index)\n\n  ```java\n  public class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n  \n    public void configure(JobConf job) {}\n  \n    /** Use {@link Object#hashCode()} to partition. */\n    public int getPartition(K2 key, V2 value, int numReduceTasks) {\n      return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n  }\n  ```\n\n- 环形缓冲区溢出写磁盘前，将每个kv对，作为getPartition()的参数传入；\n\n- 先对键值对中的key求hash值（int类型），与MAX_VALUE按位与；再模上reduce task个数，假设reduce task个数设置为4（可在程序中使用job.setNumReduceTasks(4)指定reduce task个数为4）\n\n  - 那么map任务溢出文件有**4个分区**，分区index分别是0、1、2、3\n  - getPartition()结果有四种：0、1、2、3\n  - 根据计算结果，决定当前kv对，落入哪个分区，如结果是0，则当前kv对落入溢出文件的0分区中\n  - 最终被相应的reduce task通过http获得\n\n![](assets/Image201906280826.png)\n\n![](assets/Image201906272145.png)\n\n- 若是MR默认分区器，不满足需求；可根据业务逻辑，设计自定义分区器，比如实现图上的功能\n\n#### 1.2 默认分区\n\n> 程序执行略\n>\n> 代码详见工程com.kaikeba.hadoop.partitioner包\n\n- MR读取三个文件part1.txt、part2.txt、part3.txt；三个文件放到HDFS目录：/customParttitioner中\n\n  ![](assets/Image201909061640.png)\n  \n- part1.txt内容如下：\n\n  ```\n  Dear Bear River\n  Dear Car\n  ```\n  \n- part2.txt内容如下：\n\n  ```\n  Car Car River\n  Dear Bear\n  ```\n\n- part3.txt内容如下：\n\n  ```\n  Dear Car Bear\n  Car Car\n  ```\n\n- 默认HashPartitioner分区时，查看结果（看代码）\n\n![](assets/Image201906272204.png)\n\n- 运行参数：\n\n```shell\n/customParttitioner /cp01\n```\n\n- 打jar包运行，结果如下：\n\n![](assets/Image201906272210.png)\n\n> 只有part-r-00001、part-r-00003有数据；另外两个没有数据\n>\n> HashPartitioner将Bear分到index=1的分区；将Car|Dear|River分到index=3分区\n\n#### 1.3 自定义分区\n\n**1.3.1** 需求\n\n- 自定义分区，使得文件中，分别以Dear、Bear、River、Car为键的键值对，分别落到index是0、1、2、3的分区中\n\n**1.3.2** 逻辑分析\n\n- 若要实现以上的分区策略，需要自定义分区类\n  - 此类实现Partitioner接口\n  - 在getPartition()中实现分区逻辑\n- main方法中\n  - **设定reduce个数**为4\n  - 设置自定义的分区类，调用job.setPartitionerClass方法\n\n**1.3.3** MR代码\n\n> 完整代码见代码工程\n\n- 自定义分区类如下\n\n```java\npackage com.kaikeba.hadoop.partitioner;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\nimport java.util.HashMap;\n\npublic class CustomPartitioner extends Partitioner<Text, IntWritable> {\n    public static HashMap<String, Integer> dict = new HashMap<String, Integer>();\n\n    //定义每个键对应的分区index，使用map数据结构完成\n    static{\n        dict.put(\"Dear\", 0);\n        dict.put(\"Bear\", 1);\n        dict.put(\"River\", 2);\n        dict.put(\"Car\", 3);\n    }\n\n    public int getPartition(Text text, IntWritable intWritable, int i) {\n        //\n        int partitionIndex = dict.get(text.toString());\n        return partitionIndex;\n    }\n}\n```\n\n\n\n![](assets/Image201906272213.png)\n\n- 运行结果\n\n![](assets/Image201906272217.png)\n\n> 结果满足需求\n\n#### 1.4 总结\n\n- 如果默认分区器不满足业务需求，可以自定义分区器\n  - 自定义分区器的类继承Partitioner类\n  - 覆写getPartition()，在方法中，定义自己的分区策略\n  - 在main()方法中调用job.setPartitionerClass()\n  - main()中设置reduce任务数\n\n\n\n### 2. 自定义Combiner\n\n#### 2.1 需求\n\n- 普通的MR是reduce通过http，取得map任务的分区结果；具体的聚合出结果是在reduce端进行的；\n\n- 以单词计数为例：\n  - 下图中的第一个map任务(map1)，本地磁盘中的结果有5个键值对：(Dear, 1)、(Bear, 1)、(River, 1)、(Dear, 1)、(Car, 1)\n  - 其中，map1中的两个相同的键值对(Dear, 1)、(Dear, 1)，会被第一个reduce任务(reduce1)通过网络拉取到reduce1端\n  - 那么假设map1中(Dear, 1)有1亿个呢？按原思路，map1端需要存储1亿个(Dear, 1)，再将1亿个(Dear, 1)通过网络被reduce1获得，然后再在reduce1端汇总\n  - 这样做map端本地磁盘IO、数据从map端到reduce端传输的网络IO比较大\n  - 那么想，能不能在reduce1从map1拉取1亿个(Dear, 1)之前，在map端就提前先做下reduce汇总，得到结果(Dear, 100000000)，然后再将这个结果（一个键值对）传输到reduce1呢？\n  - 答案是可以的\n  - 我们称之为combine操作\n  \n- map端combine本地聚合（**本质是reduce**）\n\n  ![](assets/Image201906280906.png)\n\n#### 2.2 逻辑分析\n\n- **<font color='red'>注意：</font>**\n\n  - **不论运行多少次Combine操作，都不能影响最终的结果**\n\n  - **并非**所有的mr都适合combine操作，比如求平均值 \n\n    **参考：《并非所有MR都适合combine.txt》**\n\n- 原理图\n\n  > 看原图\n\n![](assets/Image201909091014.png)\n\n- 当每个map任务的环形缓冲区添满80%，开始溢写磁盘文件\n\n- 此过程会分区、每个分区内按键排序、再combine操作（若设置了combine的话）、若设置map输出压缩的话则再压缩\n\n  - 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作；\n  - 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销）\n\n- combine本质上也是reduce；因为自定义的combine类继承自Reducer父类\n\n- map: (K1, V1) -> list(K2, V2)\n\n- combiner: (K2, list(V2)) -> (K2, V2)\n\n- reduce: (K2, list(V2)) -> (K3, V3)\n\n  - reduce函数与combine函数通常是一样的\n  - K3与K2类型相同；\n  - V3与V2类型相同\n  - 即reduce的输入的kv类型分别与输出的kv类型相同\n  \n  \n\n#### 2.3 MR代码\n\n> 对原词频统计代码做修改；\n>\n> 详细代码见代码工程\n\n- WordCountMap、WordCountReduce代码保持不变\n- 唯一需要做的修改是在WordCountMain中，增加job.**setCombinerClass**(WordCountReduce.class);\n- 修改如下：\n\n![](assets/Image201906272006.png)\n\n#### 2.4 小结\n\n- 使用combine时，首先考虑当前MR是否适合combine\n- 总原则是不论使不使用combine不能影响最终的结果\n- 在MR时，发生数据倾斜，且可以使用combine时，可以使用combine缓解数据倾斜\n\n\n\n### 3. MR压缩\n\n#### 3.1 需求\n\n- 作用：在MR中，为了减少磁盘IO及网络IO，可考虑在map端、reduce端设置压缩功能\n- 给“MapReduce编程：用户搜索次数”代码，增加压缩功能\n\n#### 3.2 逻辑分析\n\n- 那么如何设置压缩功能呢？只需在main方法中，给Configuration对象增加如下设置即可\n\n\n```java\n//开启map输出进行压缩的功能\nconfiguration.set(\"mapreduce.map.output.compress\", \"true\");\n//设置map输出的压缩算法是：BZip2Codec，它是hadoop默认支持的压缩算法，且支持切分\nconfiguration.set(\"mapreduce.map.output.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n//开启job输出压缩功能\nconfiguration.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n//指定job输出使用的压缩算法\nconfiguration.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n```\n\n#### 3.3 MR代码\n\n- 给“MapReduce编程：用户搜索次数”代码，增加压缩功能，代码如下\n\n  > 如何打jar包，已演示过，此处不再赘述\n\n```java\npackage com.kaikeba.hadoop.mrcompress;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n * 本MR示例，用于统计每个用户搜索并查看URL链接的次数\n */\npublic class UserSearchCount {\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        //判断以下，输入参数是否是两个，分别表示输入路径、输出路径\n        if (args.length != 2 || args == null) {\n            System.out.println(\"please input Path!\");\n            System.exit(0);\n        }\n\n        Configuration configuration = new Configuration();\n        //configuration.set(\"mapreduce.job.jar\",\"/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n        //开启map输出进行压缩的功能\n        configuration.set(\"mapreduce.map.output.compress\", \"true\");\n        //设置map输出的压缩算法是：BZip2Codec，它是hadoop默认支持的压缩算法，且支持切分\n        configuration.set(\"mapreduce.map.output.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n        //开启job输出压缩功能\n        configuration.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n        //指定job输出使用的压缩算法\n        configuration.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n\n        //调用getInstance方法，生成job实例\n        Job job = Job.getInstance(configuration, UserSearchCount.class.getSimpleName());\n\n        //设置jar包，参数是包含main方法的类\n        job.setJarByClass(UserSearchCount.class);\n\n        //通过job设置输入/输出格式\n        //MR的默认输入格式是TextInputFormat，所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n        //设置输入/输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n//        FileOutputFormat.setCompressOutput(job, true);\n//        FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);\n\n        //设置处理Map阶段的自定义的类\n        job.setMapperClass(SearchCountMapper.class);\n        //设置map combine类，减少网路传出量\n        //job.setCombinerClass(WordCountReduce.class);\n        //设置处理Reduce阶段的自定义的类\n        job.setReducerClass(SearchCountReducer.class);\n\n        //如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n//        job.setMapOutputKeyClass(Text.class);\n//        job.setMapOutputValueClass(IntWritable.class);\n\n        //设置reduce task最终输出key/value的类型\n        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // 提交作业\n        job.waitForCompletion(true);\n    }\n\n    public static class SearchCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n        //定义共用的对象，减少GC压力\n        Text userIdKOut = new Text();\n        IntWritable vOut = new IntWritable(1);\n\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            //获得当前行的数据\n            //样例数据：20111230111645  169796ae819ae8b32668662bb99b6c2d        塘承高速公路规划线路图  1       1       http://auto.ifeng.com/roll/20111212/729164.shtml\n            String line = value.toString();\n\n            //切分，获得各字段组成的数组\n            String[] fields = line.split(\"\\t\");\n\n            //因为要统计每个user搜索并查看URL的次数，所以将userid放到输出key的位置\n            //注意：MR编程中，根据业务需求设计key是很重要的能力\n            String userid = fields[1];\n\n            //设置输出的key的值\n            userIdKOut.set(userid);\n            //输出结果\n            context.write(userIdKOut, vOut);\n        }\n    }\n\n    public static class SearchCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n        //定义共用的对象，减少GC压力\n        IntWritable totalNumVOut = new IntWritable();\n\n        @Override\n        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n            int sum = 0;\n\n            for(IntWritable value: values) {\n                sum += value.get();\n            }\n\n            //设置当前user搜索并查看总次数\n            totalNumVOut.set(sum);\n            context.write(key, totalNumVOut);\n        }\n    }\n}\n```\n\n- 生成jar包，并运行jar包\n\n```shell\n[hadoop@node01 target]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.mrcompress.UserSearchCount /sogou.2w.utf8 /compressed\n```\n\n- 查看结果\n\n  > 可增加数据量，查看使用压缩算法前后的系统各计数器的数据量变化\n\n```shell\n[hadoop@node01 target]$ hadoop fs -ls -h /compressed\n```\n\n![](assets/Image201908241707.png)\n\n#### 3.4 总结\n\n- MR过程中使用压缩可减少数据量，进而减少磁盘IO、网络IO数据量\n- 可设置map端输出的压缩\n- 可设置job最终结果的压缩\n- 通过相应的配置项即可实现\n\n\n\n### 4. 自定义InputFormat\n\n#### 4.1 MapReduce执行过程\n\n![](assets/Image201905211621.png)\n\n- 上图也描述了mapreduce的一个完整的过程；我们主要看map任务是如何从hdfs读取分片数据的部分\n\n  - 涉及3个关键的类\n\n  - ①InputFormat输入格式类\n    \n    ②InputSplit输入分片类：getSplits()\n    \n    - InputFormat输入格式类将输入文件分成一个个分片InputSplit\n    - 每个Map任务对应一个split分片\n    \n    ③RecordReader记录读取器类：createRecordReader()\n    \n    - RecordReader（记录读取器）读取分片数据，一行记录生成一个键值对\n    - 传入map任务的map()方法，调用map()\n    \n    ![](assets/Image201910161117.png)\n    \n    \n\n- 所以，如果需要根据自己的业务情况，自定义输入的话，需要自定义两个类：\n  - InputFormat类\n  - RecordReader类\n\n- 详细流程：\n\n  - 客户端调用InputFormat的**getSplits()**方法，获得输入文件的分片信息\n\n    ![](assets/Image201909111008.png)\n\n  - 针对每个MR job会生成一个相应的app master，负责map\\reduce任务的调度及监控执行情况\n\n  - 将分片信息传递给MR job的app master\n\n  - app master根据分片信息，尽量将map任务尽量调度在split分片数据所在节点（**移动计算不移动数据**）\n\n    ![](assets/Image201909111013.png)\n\n  - 有几个分片，就生成几个map任务\n  \n  - 每个map任务将split分片传递给createRecordReader()方法，生成此分片对应的RecordReader\n  \n  - RecordReader用来读取分片的数据，生成记录的键值对\n  \n    - nextKeyValue()判断是否有下一个键值对，如果有，返回true；否则，返回false\n    - 如果返回true，调用getCurrentKey()获得当前的键\n    - 调用getCurrentValue()获得当前的值\n  \n  - map任务运行过程\n  \n    ![](assets/Image201909111022.png)\n  \n    - map任务运行时，会调用run()\n  \n    - 首先运行一次setup()方法；只在map任务启动时，运行一次；一些初始化的工作可以在setup方法中完成；如要连接数据库之类的操作\n  \n    - while循环，调用context.nextKeyValue()；会委托给RecordRecord的nextKeyValue()，判断是否有下一个键值对\n  \n    - 如果有下一个键值对，调用context.getCurrentKey()、context.getCurrentValue()获得当前的键、值的值（也是调用RecordReader的同名方法）\n  \n      ![](assets/Image201909111045.png)\n  \n    - 作为参数传入map(key, value, context)，调用一次map()\n  \n    - 当读取分片尾，context.nextKeyValue()返回false；退出循环\n  \n    - 调用cleanup()方法，只在map任务结束之前，调用一次；所以，一些回收资源的工作可在此方法中实现，如关闭数据库连接\n\n#### 4.2 需求\n\n- 无论hdfs还是mapreduce，处理小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案\n\n#### 4.3 逻辑分析\n\n- 小文件的优化无非以下几种方式：\n  - 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS(SequenceFile方案)\n  - 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并；可使用**自定义InputFormat**实现\n  - 在mapreduce处理时，可采用**CombineFileInputFormat**提高效率\n- 本例使用第二种方案，自定义输入格式\n\n#### 4.4 MR代码\n\n- 自定义InputFormat\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.JobContext;\n  import org.apache.hadoop.mapreduce.RecordReader;\n  import org.apache.hadoop.mapreduce.TaskAttemptContext;\n  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n  \n  import java.io.IOException;\n  \n  /**\n   * 自定义InputFormat类；\n   * 泛型：\n   *  键：因为不需要使用键，所以设置为NullWritable\n   *  值：值用于保存小文件的内容，此处使用BytesWritable\n   */\n  public class WholeFileInputFormat extends FileInputFormat<NullWritable, BytesWritable> {\n  \n      /**\n       *\n       * 返回false，表示输入文件不可切割\n       * @param context\n       * @param file\n       * @return\n       */\n      @Override\n      protected boolean isSplitable(JobContext context, Path file) {\n          return false;\n      }\n  \n      /**\n       * 生成读取分片split的RecordReader\n       * @param split\n       * @param context\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public RecordReader<NullWritable, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException,InterruptedException {\n          //使用自定义的RecordReader类\n          WholeFileRecordReader reader = new WholeFileRecordReader();\n          //初始化RecordReader\n          reader.initialize(split, context);\n          return reader;\n      }\n  }\n  ```\n\n- 自定义RecordReader\n\n  实现6个相关方法\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.conf.Configuration;\n  import org.apache.hadoop.fs.FSDataInputStream;\n  import org.apache.hadoop.fs.FileSystem;\n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.IOUtils;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.RecordReader;\n  import org.apache.hadoop.mapreduce.TaskAttemptContext;\n  import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n  \n  import java.io.IOException;\n  \n  /**\n   *\n   * RecordReader的核心工作逻辑：\n   * 通过nextKeyValue()方法去读取数据构造将返回的key   value\n   * 通过getCurrentKey 和 getCurrentValue来返回上面构造好的key和value\n   *\n   * @author\n   */\n  public class WholeFileRecordReader extends RecordReader<NullWritable, BytesWritable> {\n  \n      //要读取的分片\n      private FileSplit fileSplit;\n      private Configuration conf;\n  \n      //读取的value数据\n      private BytesWritable value = new BytesWritable();\n      /**\n       *\n       * 标识变量，分片是否已被读取过；因为小文件设置成了不可切分，所以一个小文件只有一个分片；\n       * 而这一个分片的数据，只读取一次，一次读完所有数据\n       * 所以设置此标识\n       */\n      private boolean processed = false;\n  \n      /**\n       * 初始化\n       * @param split\n       * @param context\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public void initialize(InputSplit split, TaskAttemptContext context)\n              throws IOException, InterruptedException {\n          this.fileSplit = (FileSplit) split;\n          this.conf = context.getConfiguration();\n      }\n  \n      /**\n       * 判断是否有下一个键值对。若有，则读取分片中的所有的数据\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public boolean nextKeyValue() throws IOException, InterruptedException {\n          if (!processed) {\n              byte[] contents = new byte[(int) fileSplit.getLength()];\n              Path file = fileSplit.getPath();\n              FileSystem fs = file.getFileSystem(conf);\n              FSDataInputStream in = null;\n              try {\n                  in = fs.open(file);\n                  IOUtils.readFully(in, contents, 0, contents.length);\n                  value.set(contents, 0, contents.length);\n              } finally {\n                  IOUtils.closeStream(in);\n              }\n              processed = true;\n              return true;\n          }\n          return false;\n      }\n  \n      /**\n       * 获得当前的key\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public NullWritable getCurrentKey() throws IOException,\n              InterruptedException {\n          return NullWritable.get();\n      }\n  \n      /**\n       * 获得当前的value\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public BytesWritable getCurrentValue() throws IOException,\n              InterruptedException {\n          return value;\n      }\n  \n      /**\n       * 获得分片读取的百分比；因为如果读取分片数据的话，会一次性的读取完；所以进度要么是1，要么是0\n       * @return\n       * @throws IOException\n       */\n      @Override\n      public float getProgress() throws IOException {\n          //因为一个文件作为一个整体处理，所以，如果processed为true，表示已经处理过了，进度为1；否则为0\n          return processed ? 1.0f : 0.0f;\n      }\n  \n      @Override\n      public void close() throws IOException {\n      }\n  }\n  ```\n\n- main方法\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.conf.Configuration;\n  import org.apache.hadoop.conf.Configured;\n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.io.Text;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.Job;\n  import org.apache.hadoop.mapreduce.Mapper;\n  import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n  import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\n  import org.apache.hadoop.util.Tool;\n  import org.apache.hadoop.util.ToolRunner;\n  \n  import java.io.IOException;\n  \n  /**\n   * 让主类继承Configured类，实现Tool接口\n   * 实现run()方法\n   * 将以前main()方法中的逻辑，放到run()中\n   * 在main()中，调用ToolRunner.run()方法，第一个参数是当前对象；第二个参数是输入、输出\n   */\n  public class SmallFiles2SequenceFile extends Configured implements Tool {\n  \n      /**\n       * 自定义Mapper类\n       * mapper类的输入键值对类型，与自定义InputFormat的输入键值对保持一致\n       * mapper类的输出的键值对类型，分别是文件名、文件内容\n       */\n      static class SequenceFileMapper extends\n              Mapper<NullWritable, BytesWritable, Text, BytesWritable> {\n  \n          private Text filenameKey;\n  \n          /**\n           * 取得文件名\n           * @param context\n           * @throws IOException\n           * @throws InterruptedException\n           */\n          @Override\n          protected void setup(Context context) throws IOException,\n                  InterruptedException {\n              InputSplit split = context.getInputSplit();\n              //获得当前文件路径\n              Path path = ((FileSplit) split).getPath();\n              filenameKey = new Text(path.toString());\n          }\n  \n          @Override\n          protected void map(NullWritable key, BytesWritable value,\n                             Context context) throws IOException, InterruptedException {\n              context.write(filenameKey, value);\n          }\n      }\n  \n      public int run(String[] args) throws Exception {\n          Configuration conf = new Configuration();\n          Job job = Job.getInstance(conf,\"combine small files to sequencefile\");\n          job.setJarByClass(SmallFiles2SequenceFile.class);\n  \n          //设置自定义输入格式\n          job.setInputFormatClass(WholeFileInputFormat.class);\n  \n          WholeFileInputFormat.addInputPath(job,new Path(args[0]));\n          //设置输出格式SequenceFileOutputFormat及输出路径\n          job.setOutputFormatClass(SequenceFileOutputFormat.class);\n          SequenceFileOutputFormat.setOutputPath(job,new Path(args[1]));\n  \n          job.setOutputKeyClass(Text.class);\n          job.setOutputValueClass(BytesWritable.class);\n          job.setMapperClass(SequenceFileMapper.class);\n          return job.waitForCompletion(true) ? 0 : 1;\n      }\n  \n      public static void main(String[] args) throws Exception {\n          int exitCode = ToolRunner.run(new SmallFiles2SequenceFile(),\n                  args);\n          System.exit(exitCode);\n  \n      }\n  }\n  ```\n\n#### 4.5 总结\n\n- 若要自定义InputFormat的话\n  - 需要自定义InputFormat类，并覆写getRecordReader()方法\n  - 自定义RecordReader类，实现方法\n    - initialize()\n    - nextKeyValue()\n    - getCurrentKey()\n    - getCurrentValue()\n    - getProgress()\n    - close()\n\n\n\n## 5、拓展点、未来计划、行业趋势\n\n1. MR中还有一些自带的输入格式，扩展阅读：《Hadoop权威指南 第4版》8.2 输入格式\n\n   ![](assets/Image201909091251.png)\n\n   \n","tags":["hadoop","MapReduce"]},{"title":"MapRedecer编程（一）","url":"/2019/10/15/MapRedecer编程（一）/","content":"\n# MapReduce编程模型\n\n\n## 一、知识要点\n\n### 1. MapReduce编程模型\n\n- Hadoop架构图\n\n  Hadoop由HDFS分布式存储、**MapReduce分布式计算**、Yarn资源调度三部分组成\n\n![](assets/Image201906191834-1562922704761.png)\n\n- MapReduce是采用一种**分而治之**的思想设计出来的分布式计算框架\n- MapReduce由两个阶段组成：\n  - Map阶段（切分成一个个小的任务）\n  - Reduce阶段（汇总小任务的结果）\n- 那什么是分而治之呢？\n  - 比如一复杂、计算量大、耗时长的的任务，暂且称为“大任务”；\n  - 此时使用单台服务器无法计算或较短时间内计算出结果时，可将此大任务切分成一个个小的任务，小任务分别在不同的服务器上**并行**的执行\n  - 最终再汇总每个小任务的结果\n\n![](assets/Image201906251747.png)\n\n#### 1.1 Map阶段\n\n- map阶段有一个关键的map()函数；\n- 此函数的输入是**键值对**\n- 输出是一系列**键值对**，输出写入**本地磁盘**。\n\n#### 1.2 Reduce阶段\n\n- reduce阶段有一个关键的函数reduce()函数\n\n- 此函数的输入也是键值对（即map的输出（kv对））\n\n- 输出也是一系列键值对，结果最终写入HDFS\n\n#### 1.3 Map&Reduce\n\n![](assets/Image201906251807.png)\n\n\n\n### 2. MapReduce编程示例\n\n- 以**MapReduce的词频统计**为例：统计一批英文文章当中，每个单词出现的总次数\n\n#### 2.1 MapReduce原理图\n\n![](assets/Image201906271715.png)\n\n- Map阶段\n  - 假设MR的输入文件“**Gone With The Wind**”有三个block；block1、block2、block3 \n  - MR编程时，每个block对应一个分片split\n  - 每一个split对应一个map任务（map task）\n  - 如图共3个map任务（map1、map2、map3）；这3个任务的逻辑一样，所以以第一个map任务（map1）为例分析 \n  - map1读取block1的数据；一次读取block1的一行数据；\n    - 产生键值对(key/value)，作为map()的参数传入，调用map()；\n    - 假设当前所读行是第一行\n    - 将当前所读行的行首相对于当前block开始处的字节偏移量作为key（0）\n    - 当前行的内容作为value（Dear Bear River）\n  - map()内\n    - (按需求，写业务代码)，将value当前行内容按空格切分，得到三个单词Dear | Bear | River\n    - 将每个单词变成键值对，输出出去(Dear, 1) | (Bear, 1) | (River, 1)；最终结果写入map任务所在节点的本地磁盘中（内里还有细节，讲到shuffle时，再细细展开）\n    - block的第一行的数据被处理完后，接着处理第二行；逻辑同上\n    - 当map任务将当前block中所有的数据全部处理完后，此map任务即运行结束\n  - 其它的每一个map任务都是如上逻辑，不再赘述\n- Reduce阶段\n  - reduce任务（reduce task）的个数由自己写的程序编程指定，main()内的job.setNumReduceTasks(4)指定reduce任务是4个（reduce1、reduce2、reduce3、reduce4）\n  - 每一个reduce任务的逻辑一下，所以以第一个reduce任务（reduce1）为例分析\n  - map1任务完成后，reduce1通过网络，连接到map1，将map1输出结果中属于reduce1的分区的数据，通过网络获取到reduce1端（拷贝阶段）\n  - 同样也如此连接到map2、map3获取结果\n  - 最终reduce1端获得4个(Dear, 1)键值对；由于key键相同，它们分到同一组；\n  - 4个(Dear, 1)键值对，转换成[Dear, Iterable(1, 1, 1, )]，作为两个参数传入reduce()\n  - 在reduce()内部，计算Dear的总数为4，并将(Dear, 4)作为键值对输出\n  - 每个reduce任务最终输出文件（内里还有细节，讲到shuffle时，再细细展开），文件写入到HDFS\n\n#### 2.2 MR中key的作用\n\n- <font color='red'>**MapReduce编程中，key有特殊的作用**</font>\n\n  - **①数据中，若要针对某个值进行分组、聚合时，需将此值作为MR中的reduce的输入的key**\n\n  - **如当前的词频统计例子，按单词进行分组，每组中对出现次数做聚合（计算总和）；所以需要将每个单词作为reduce输入的key，MapReduce框架自动按照单词分组，进而求出每组即每个单词的总次数**\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201910141101.png)\n\n  - **②另外，key还具有可排序的特性，因为MR中的key类需要实现WritableComparable接口；而此接口又继承Comparable接口（可查看源码）**\n\n  - **MR编程时，要充分利用以上两点；结合实际业务需求，设置合适的key**\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201908221717.png)\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201908221718.png)\n\n\n\n#### 2.4 MR参考代码\n\n**2.4.1 Mapper代码**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * 类Mapper<LongWritable, Text, Text, IntWritable>的四个泛型分别表示\n * map方法的输入的键的类型kin、值的类型vin；输出的键的类型kout、输出的值的类型vout\n * kin指的是当前所读行行首相对于split分片开头的字节偏移量,所以是long类型，对应序列化类型LongWritable\n * vin指的是当前所读行，类型是String，对应序列化类型Text\n * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text\n * vout根据需求，输出值指的是单词的个数，1，类型是int，对应序列化类型是IntWritable\n *\n */\npublic class WordCountMap extends Mapper<LongWritable, Text, Text, IntWritable> {\n\n    /**\n     * 处理分片split中的每一行的数据；针对每行数据，会调用一次map方法\n     * 在一次map方法调用时，从一行数据中，获得一个个单词word，再将每个单词word变成键值对形式(word, 1)输出出去\n     * 输出的值最终写到本地磁盘中\n     * @param key 当前所读行行首相对于split分片开头的字节偏移量\n     * @param value  当前所读行\n     * @param context\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public void map(LongWritable key, Text value, Context context)\n            throws IOException, InterruptedException {\n        //当前行的示例数据(单词间空格分割)：Dear Bear River\n        //取得当前行的数据\n        String line = value.toString();\n        //按照\\t进行分割，得到当前行所有单词\n        String[] words = line.split(\"\\t\");\n\n        for (String word : words) {\n            //将每个单词word变成键值对形式(word, 1)输出出去\n            //同样，输出前，要将kout, vout包装成对应的可序列化类型，如String对应Text，int对应IntWritable\n            context.write(new Text(word), new IntWritable(1));\n        }\n    }\n}\n\n```\n\n**2.4.2 Reducer代码**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\n/**\n *\n * Reducer<Text, IntWritable, Text, IntWritable>的四个泛型分别表示\n * reduce方法的输入的键的类型kin、输入值的类型vin；输出的键的类型kout、输出的值的类型vout\n * 注意：因为map的输出作为reduce的输入，所以此处的kin、vin类型分别与map的输出的键类型、值类型相同\n * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text\n * vout根据需求，输出值指的是每个单词的总个数，类型是int，对应序列化类型是IntWritable\n *\n */\npublic class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {\n    /**\n     *\n     * key相同的一组kv对，会调用一次reduce方法\n     * 如reduce task汇聚了众多的键值对，有key是hello的键值对，也有key是spark的键值对，如下\n     * (hello, 1)\n     * (hello, 1)\n     * (hello, 1)\n     * (hello, 1)\n     * ...\n     * (spark, 1)\n     * (spark, 1)\n     * (spark, 1)\n     *\n     * 其中，key是hello的键值对被分成一组；merge成[hello, Iterable(1,1,1,1)]，调用一次reduce方法\n     * 同样，key是spark的键值对被分成一组；merge成[spark, Iterable(1,1,1)]，再调用一次reduce方法\n     *\n     * @param key 当前组的key\n     * @param values 当前组中，所有value组成的可迭代集和\n     * @param context reduce上下文环境对象\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public void reduce(Text key, Iterable<IntWritable> values,\n                          Context context) throws IOException, InterruptedException {\n        //定义变量，用于累计当前单词出现的次数\n        int sum = 0;\n\n        for (IntWritable count : values) {\n            //从count中获得值，累加到sum中\n            sum += count.get();\n        }\n\n        //将单词、单词次数，分别作为键值对，输出\n        context.write(key, new IntWritable(sum));// 输出最终结果\n    };\n}\n```\n\n**2.4.3 Main程序入口**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport java.io.IOException;\n\n/**\n *\n * MapReduce程序入口\n * 注意：\n *  导包时，不要导错了；\n *  另外，map\\reduce相关的类，使用mapreduce包下的，是新API，如org.apache.hadoop.mapreduce.Job;；\n */\npublic class WordCountMain {\n    //若在IDEA中本地执行MR程序，需要将mapred-site.xml中的mapreduce.framework.name值修改成local\n    //参数 c:/test/README.txt c:/test/wc\n    public static void main(String[] args) throws IOException,\n            ClassNotFoundException, InterruptedException {\n\n        //判断一下，输入参数是否是两个，分别表示输入路径、输出路径\n       if (args.length != 2 || args == null) {\n            System.out.println(\"please input Path!\");\n            System.exit(0);\n        }\n\n        Configuration configuration = new Configuration();\n        //configuration.set(\"mapreduce.framework.name\",\"local\");\n\n\n        //告诉程序，要运行的jar包在哪\n        //configuration.set(\"mapreduce.job.jar\",\"/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n\n        //调用getInstance方法，生成job实例\n        Job job = Job.getInstance(configuration, WordCountMain.class.getSimpleName());\n\n        //设置job的jar包，如果参数指定的类包含在一个jar包中，则此jar包作为job的jar包； 参数class跟主类在一个工程即可；一般设置成主类\n//        job.setJarByClass(WordCountMain.class);\n        job.setJarByClass(WordCountMain.class);\n\n        //通过job设置输入/输出格式\n        //MR的默认输入格式是TextInputFormat，输出格式是TextOutputFormat；所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n        //设置输入/输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        //设置处理Map阶段的自定义的类\n        job.setMapperClass(WordCountMap.class);\n        //设置map combine类，减少网路传出量\n        job.setCombinerClass(WordCountReduce.class);\n        //设置处理Reduce阶段的自定义的类\n        job.setReducerClass(WordCountReduce.class);\n        //注意：如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n//        job.setMapOutputKeyClass(Text.class);\n//        job.setMapOutputValueClass(IntWritable.class);\n\n        //设置reduce task最终输出key/value的类型\n        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // 提交作业\n        job.waitForCompletion(true);\n\n    }\n}\n```\n\n> 程序运行有两种方式，分别是windows本地运行、集群运行，依次演示\n\n\n#### 2.5 集群运行\n\n- 用maven插件打jar包；①点击Maven，②双击package打包\n\n```shell\n[hadoop@node01 ~]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.wordcount.WordCountMain /README.txt /wordcount01\n```\n\n> 说明：\n>\n> com.kaikeba.hadoop-1.0-SNAPSHOT.jar是jar包名\n>\n> com.kaikeba.hadoop.wordcount.WordCountMain是包含main方法的类的全限定名\n>\n> /NOTICE.txt和/wordcount是main方法的两个参数，表示输入路径、输出路径\n\n![](assets/hadoop jar.gif)\n\n- 确认结果\n\n```shell\n[hadoop@node01 ~]$ hadoop fs -ls /wordcount01\n```\n\n![](assets/Image201908221620.png)\n\n#### 2.6 总结\n\n- MR分为两个阶段：map阶段、reduce阶段\n- MR输入的文件有几个block，就会生成几个map任务\n- MR的reduce任务的个数，由程序中编程指定：job.setNumReduceTasks(4)\n- map任务\n  - map任务中map()一次读取block的一行数据，以kv对的形式输入map()\n  - map()的输出作为reduce()的输入\n- reduce任务\n  - reduce任务通过网络将各执行完成的map任务输出结果中，属于自己的数据取过来\n  - key相同的键值对作为一组，调用一次reduce()\n  - reduce任务生成一个结果文件\n  - 文件写入HDFS\n\n\n\n### 3. WEB UI查看结果\n\n#### 3.1 Yarn\n\n> node01是resourcemanager所在节点主机名，根据自己的实际情况修改主机名\n\n浏览器访问url地址：http://node01:8088\n\n![](assets/Image201908221638.png)\n\n#### 3.2 HDFS结果\n\n浏览器输入URL：http://node01:50070\n\n①点击下拉框；②浏览文件系统；③输入根目录，查看hdfs根路径中的内容\n\n![](assets/Image201908221639.png)\n\n\n\n### 4. Shuffle\n\n- shuffle主要指的是map端的输出作为reduce端输入的过程\n\n#### 4.1 shuffle简图\n\n![](assets/Image201905231409.png)\n\n#### 4.2 shuffle细节图\n\n![](assets/Image201906280906.png)\n\n- 分区用到了分区器，默认分区器是HashPartitioner\n\n  源码：\n\n  ```java\n  public class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n  \n    public void configure(JobConf job) {}\n  \n    /** Use {@link Object#hashCode()} to partition. */\n    public int getPartition(K2 key, V2 value,\n                            int numReduceTasks) {\n      return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n  \n  }\n  ```\n\n#### 4.3 map端\n\n  - 每个map任务都有一个对应的环形内存缓冲区；输出是kv对，先写入到环形缓冲区（默认大小100M），当内容占据80%缓冲区空间后，由一个后台线程将缓冲区中的数据溢出写到一个磁盘文件\n  - 在溢出写的过程中，map任务可以继续向环形缓冲区写入数据；但是若写入速度大于溢出写的速度，最终造成100m占满后，map任务会暂停向环形缓冲区中写数据的过程；只执行溢出写的过程；直到环形缓冲区的数据全部溢出写到磁盘，才恢复向缓冲区写入\n  - 后台线程溢写磁盘过程，有以下几个步骤：\n    - 先对每个溢写的kv对做分区；分区的个数由MR程序的reduce任务数决定；默认使用HashPartitioner计算当前kv对属于哪个分区；计算公式：(key.hashCode() & Integer.MAX_VALUE) % numReduceTasks\n    - 每个分区中，根据kv对的key做内存中排序；\n    - 若设置了map端本地聚合combiner，则对每个分区中，排好序的数据做combine操作；\n    - 若设置了对map输出压缩的功能，会对溢写数据压缩\n  - 随着不断的向环形缓冲区中写入数据，会多次触发溢写（每当环形缓冲区写满100m），本地磁盘最终会生成多个溢出文件\n  - 合并溢写文件：在map task完成之前，所有溢出文件会被合并成一个大的溢出文件；且是已分区、已排序的输出文件\n  - 小细节：\n    - 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作；\n    - 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销）\n\n#### 4.4 reduce端\n\n- reduce task会在每个map task运行完成后，通过HTTP获得map task输出中，属于自己的分区数据（许多kv对）\n\n- 如果map输出数据比较小，先保存在reduce的jvm内存中，否则直接写入reduce磁盘\n\n- 一旦内存缓冲区达到阈值（默认0.66）或map输出数的阈值（默认1000），则触发**归并merge**，结果写到本地磁盘\n\n- 若MR编程指定了combine，在归并过程中会执行combine操作\n\n- 随着溢出写的文件的增多，后台线程会将它们合并大的、排好序的文件\n\n- reduce task将所有map task复制完后，将合并磁盘上所有的溢出文件\n\n- 默认一次合并10个\n\n- 最后一批合并，部分数据来自内存，部分来自磁盘上的文件\n\n- 进入“归并、排序、分组阶段”\n\n- 每组数据调用一次reduce方法\n\n- 参考文件《**reduce端merge 排序 分组.txt**》\n\n\n\n#### 4.5 总结\n\n- map端\n  - map()输出结果先写入环形缓冲区\n  - 缓冲区100M；写满80M后，开始溢出写磁盘文件\n  - 此过程中，会进行分区、排序、combine（可选）、压缩（可选）\n  - map任务完成前，会将多个小的溢出文件，合并成一个大的溢出文件（已分区、排序）\n- reduce端\n  - 拷贝阶段：reduce任务通过http将map任务属于自己的分区数据拉取过来\n  - 开始merge及溢出写磁盘文件\n  - 所有map任务的分区全部拷贝过来后，进行阶段合并、排序、分组阶段\n  - 每组数据调用一次reduce()\n  - 结果写入HDFS\n","tags":["-hadoop -mapreduce"]},{"title":"HDFS文件系统（二）","url":"/2019/10/14/HDFS文件系统/","content":"\n#  HDFS分布式文件系统\n\n### 1. HDFS读写流程\n\n#### 1.1 数据写流程\n\n![1557999856839](img/1557999856839.png)\n\n![HDFS写入文件流程](img/HDFS写入文件流程.png)\n\n**1.1 详细流程**\n\n- 创建文件：\t\n\n  - HDFS客户端向HDFS写数据，先调用DistributedFileSystem.create()方法，在HDFS创建新的空文件\n  - RPC（ClientProtocol.create()）远程过程调用NameNode（NameNodeRpcServer）的create()，首先在HDFS目录树指定路径添加新文件\n  - 然后将创建新文件的操作记录在editslog中\n  - NameNode.create方法执行完后，DistributedFileSystem.create()返回FSDataOutputStream，它本质是封装了一个DFSOutputStream对象\n\n- 建立数据流管道：\n\n  - 客户端调用DFSOutputStream.write()写数据\n  - DFSOutputStream调用ClientProtocol.addBlock()，首先向NameNode申请一个空的数据块\n  - addBlock()返回LocatedBlock对象，对象包含当前数据块的所有datanode的位置信息\n  - 根据位置信息，建立数据流管道\n\n- 向数据流管道pipeline中写当前块的数据：\n\n  - 客户端向流管道中写数据，先将数据写入一个检验块chunk中，大小512Byte，写满后，计算chunk的检验和checksum值（4Byte）\n  - 然后将chunk数据本身加上checksum，形成一个带checksum值的chunk（516Byte）\n  - 保存到一个更大一些的结构**packet数据包**中，packet为64kB大小\n- packet写满后，先被写入一个**dataQueue**队列中\n  - packet被从队列中取出，向pipeline中写入，先写入datanode1，再从datanoe1传到datanode2，再从datanode2传到datanode3中\n- 一个packet数据取完后，后被放入到**ackQueue**中等待pipeline关于该packet的ack的反馈\n  - 每个packet都会有ack确认包，逆pipeline（dn3 -> dn2 -> dn1）传回输出流\n- 若packet的ack是SUCCESS成功的，则从ackQueue中，将packet删除；否则，将packet从ackQueue中取出，重新放入dataQueue，重新发送\n  - 如果当前块写完后，文件还有其它块要写，那么再调用addBlock方法（**流程同上**）\n- 文件最后一个block块数据写完后，会再发送一个空的packet，表示当前block写完了，然后关闭pipeline\n  - 所有块写完，close()关闭流\n- ClientProtocol.complete()通知namenode当前文件所有块写完了\n\n**6.1.2 容错**\n\n- 在写的过程中，pipeline中的datanode出现故障（如网络不通），输出流如何恢复\n  - 输出流中ackQueue缓存的所有packet会被重新加入dataQueue\n  - 输出流调用ClientProtocol.updateBlockForPipeline()，为block申请一个新的时间戳，namenode会记录新时间戳\n  - 确保故障datanode即使恢复，但由于其上的block时间戳与namenode记录的新的时间戳不一致，故障datanode上的block进而被删除\n  - 故障的datanode从pipeline中删除\n  - 输出流调用ClientProtocol.getAdditionalDatanode()通知namenode分配新的datanode到数据流pipeline中，并使用新的时间戳建立pipeline\n  - 新添加到pipeline中的datanode，目前还没有存储这个新的block，HDFS客户端通过DataTransferProtocol通知pipeline中的一个datanode复制这个block到新的datanode中\n  - pipeline重建后，输出流调用ClientProtocol.updatePipeline()，更新namenode中的元数据\n  - 故障恢复完毕，完成后续的写入流程\n\n#### 1.2 数据读流程\n\n**1.2.1 基本流程**\n\n![HDFS文件读取流程](img/HDFS文件读取流程.png)\n\n- 1、client端读取HDFS文件，client调用文件系统对象DistributedFileSystem的open方法\n- 2、返回FSDataInputStream对象（对DFSInputStream的包装）\n- 3、构造DFSInputStream对象时，调用namenode的getBlockLocations方法，获得file的开始若干block（如blk1, blk2, blk3, blk4）的存储datanode（以下简称dn）列表；针对每个block的dn列表，会根据网络拓扑做排序，离client近的排在前；\n- 4、调用DFSInputStream的read方法，先读取blk1的数据，与client最近的datanode建立连接，读取数据\n- 5、读取完后，关闭与dn建立的流\n- 6、读取下一个block，如blk2的数据（重复步骤4、5、6）\n- 7、这一批block读取完后，再读取下一批block的数据（重复3、4、5、6、7）\n- 8、完成文件数据读取后，调用FSDataInputStream的close方法\n\n**1.2.2 容错**\n\n- 情况一：读取block过程中，client与datanode通信中断\n\n  - client与存储此block的第二个datandoe建立连接，读取数据\n  - 记录此有问题的datanode，不会再从它上读取数据\n\n- 情况二：client读取block，发现block数据有问题\n  -  client读取block数据时，同时会读取到block的校验和，若client针对读取过来的block数据，计算检验和，其值与读取过来的校验和不一样，说明block数据损坏\n  -  client从存储此block副本的其它datanode上读取block数据（也会计算校验和）\n  -  同时，client会告知namenode此情况；\n\n\n\n\n\n### 2. Hadoop HA高可用\n\n#### 2.1 HDFS高可用原理\n\n![](img/Image201905211519.png)\n\n- 对于HDFS ，NN存储元数据在内存中，并负责管理文件系统的命名空间和客户端对HDFS的读写请求。但是，如果只存在一个NN，一旦发生“单点故障”，会使整个系统失效。\n- 虽然有个SNN，但是它并不是NN的热备份\n- 因为SNN无法提供“热备份”功能，在NN故障时，无法立即切换到SNN对外提供服务，即HDFS处于停服状态。\n- HDFS2.x采用了HA（High Availability高可用）架构。\n  - 在HA集群中，可设置两个NN，一个处于“活跃（Active）”状态，另一个处于“待命（Standby）”状态。\n  - 由zookeeper确保一主一备（讲zookeeper时具体展开）\n  - 处于Active状态的NN负责响应所有客户端的请求，处于Standby状态的NN作为热备份节点，保证与active的NN的元数据同步\n  - Active节点发生故障时，zookeeper集群会发现此情况，通知Standby节点立即切换到活跃状态对外提供服务\n  - 确保集群一直处于可用状态\n- 如何热备份元数据：\n  - Standby NN是Active NN的“热备份”，因此Active NN的状态信息必须实时同步到StandbyNN。\n  - 可借助一个共享存储系统来实现状态同步，如NFS(NetworkFile System)、QJM(Quorum Journal Manager)或者Zookeeper。\n  - Active NN将更新数据写入到共享存储系统，Standby NN一直监听该系统，一旦发现有新的数据写入，就立即从公共存储系统中读取这些数据并加载到Standby NN自己内存中，从而保证元数据与Active NN状态一致。\n- 块报告：\n  - NN保存了数据块到实际存储位置的映射信息，为了实现故障时的快速切换，必须保证StandbyNN中也包含最新的块映射信息\n  - 因此需要给所有DN配置Active和Standby两个NN的地址，把块的位置和心跳信息同时发送到两个NN上。\n\n### 3. Hadoop联邦\n\n#### 3.1 为什么需要联邦\n\n- 虽然HDFS HA解决了“单点故障”问题，但HDFS在扩展性、整体性能和隔离性方面仍有问题\n  - 系统扩展性方面，元数据存储在NN内存中，受限于内存上限（每个文件、目录、block占用约150字节）\n  - 整体性能方面，吞吐量受单个NN的影响\n  - 隔离性方面，一个程序可能会影响其他程序的运行，如果一个程序消耗过多资源会导致其他程序无法顺利运行\n  - HDFS HA本质上还是单名称节点\n\n#### 3.2 联邦\n\n![](img/Image201909041239.png)\n\n\n- HDFS联邦可以解决以上三个问题\n  - HDFS联邦中，设计了多个命名空间；每个命名空间有一个NN或一主一备两个NN，使得HDFS的命名服务能够水平扩展\n  - 这些NN分别进行各自命名空间namespace和块的管理，相互独立，不需要彼此协调\n  - 每个DN要向集群中所有的NN注册，并周期性的向所有NN发送心跳信息和块信息，报告自己的状态\n  - HDFS联邦每个相互独立的NN对应一个独立的命名空间\n  - 每一个命名空间管理属于自己的一组块，这些属于同一命名空间的块对应一个“块池”的概念。\n  - 每个DN会为所有块池提供块的存储，块池中的各个块实际上是存储在不同DN中的\n\n#### 3.3 扩展\n\n[联邦-官网](<https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/Federation.html>)\n\n\n\n\n\n### 4. 文件压缩\n\n#### 4.1 压缩算法\n\n- 文件压缩好处：\n\n  - 减少数据所占用的磁盘空间\n  - 加快数据在磁盘、网络上的IO\n\n- 常用压缩格式\n\n  | 压缩格式 | UNIX工具 | 算      法 | 文件扩展名 | 可分割 |\n  | -------- | -------- | ---------- | ---------- | ------ |\n  | DEFLATE  | 无       | DEFLATE    | .deflate   | No     |\n  | gzip     | gzip     | DEFLATE    | .gz        | No     |\n  | zip      | zip      | DEFLATE    | .zip       | YES    |\n  | bzip     | bzip2    | bzip2      | .bz2       | YES    |\n  | LZO      | lzop     | LZO        | .lzo       | No     |\n  | Snappy   | 无       | Snappy     | .snappy    | No     |\n\n- Hadoop的压缩实现类；均实现CompressionCodec接口\n\n  | 压缩格式 | 对应的编码/解码器                          |\n  | -------- | ------------------------------------------ |\n  | DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |\n  | gzip     | org.apache.hadoop.io.compress.GzipCodec    |\n  | bzip2    | org.apache.hadoop.io.compress.BZip2Codec   |\n  | LZO      | com.hadoop.compression.lzo.LzopCodec       |\n  | Snappy   | org.apache.hadoop.io.compress.SnappyCodec  |\n\n- 查看集群是否支持本地压缩（所有节点都要确认）\n\n  ```\n  [hadoop@node01 ~]$ hadoop checknative\n  ```\n\n  ![](img/Image201910111114.png)\n\n#### 4.2 编程实践\n\n- 编程：上传压缩过的文件到HDFS\n\n\n```java\n\n    /**\n     * 上传压缩文件到服务器\n     *  传递参数\n     *  args[0] 本地文件路径\n     *  args[1] hdoop文件系统 路径\n     */\n    public static void uploadFileZipToFileSystem(String source,String targetUrl){\n        System.out.println(\"文件地址：\" + source);\n        System.out.println(\"目标服务器：\" + targetUrl);\n        InputStream inputStreamSourceFile = null;\n\n        try {\n            // 获取文件输入流\n            inputStreamSourceFile = new BufferedInputStream(new FileInputStream(source));\n            // HDFS 读写配置文件\n            Configuration configuration = new Configuration();\n            // 压缩类型\n            BZip2Codec codec = new BZip2Codec();\n            codec.setConf(configuration);\n            // 通过url 返回文件系统实例\n            FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);\n            //调用Filesystem的create方法返回的是FSDataOutputStream对象\n            //该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n            // 获取文件系用的输出流\n            OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n            // 对输出流进行压缩\n            CompressionOutputStream compressionOut = codec.createOutputStream(outputStreamTarget);\n            // 将文件输入流，写入输入流\n            IOUtils.copyBytes(inputStreamSourceFile,compressionOut,4069,true);\n            System.out.println(\"上传成功\");\n        } catch (FileNotFoundException e) {\n            System.err.println(e.getMessage());\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n```\n\n- 扩展阅读\n  - 《Hadoop权威指南》 5.2章节 压缩\n  - [HDFS文件压缩](<https://blog.csdn.net/qq_38262266/article/details/79171524>)\n\n\n\n\n\n### 5. 小文件治理\n\n#### 5.1 有没有问题\n\n- NameNode存储着文件系统的元数据，每个文件、目录、块大概有150字节的元数据；\n- 因此文件数量的限制也由NN内存大小决定，如果小文件过多则会造成NN的压力过大\n- 且HDFS能存储的数据总量也会变小\n\n#### 5.2 HAR文件方案（10分钟）\n\n- 本质启动mr程序，所以需要启动yarn\n\n![1558004541101](img/1558004541101.png)\n\n用法：\n\n```sh\narchive -archiveName <NAME>.har -p <parent path> [-r <replication factor>]<src>* <dest>\n```\n\n![](img/Image201909041408.png)\n\n![alt](img/Image201906210960.png)\n\n```shell\n# 创建archive文件；/testhar有两个子目录th1、th2；两个子目录中有若干文件\nhadoop archive -archiveName test.har -p /testhar -r 3 th1 th2 /outhar # 原文件还存在，需手动删除\n\n# 查看archive文件\nhdfs dfs -ls -R har:///outhar/test.har\n\n# 解压archive文件\n# 方式一\nhdfs dfs -cp har:///outhar/test.har/th1 hdfs:/unarchivef # 顺序\nhadoop fs -ls /unarchivef\t\n# 方式二\nhadoop distcp har:///outhar/test.har/th1 hdfs:/unarchivef2 # 并行，启动MR\n```\n\n#### 5.3 Sequence Files方案（*）\n\n- SequenceFile文件，主要由一条条record记录组成；每个record是键值对形式的\n- SequenceFile文件可以作为小文件的存储容器；\n  - 每条record保存一个小文件的内容\n  - 小文件名作为当前record的键；\n  - 小文件的内容作为当前record的值；\n  - 如10000个100KB的小文件，可以编写程序将这些文件放到一个SequenceFile文件。\n- 一个SequenceFile是**可分割**的，所以MapReduce可将文件切分成块，每一块独立操作。\n- 具体结构（如下图）：\n  - 一个SequenceFile首先有一个4字节的header（文件版本号）\n  - 接着是若干record记录\n  - 记录间会随机的插入一些同步点sync marker，用于方便定位到记录边界\n- 不像HAR，SequenceFile**支持压缩**。记录的结构取决于是否启动压缩\n  - 支持两类压缩：\n    - 不压缩NONE，如下图\n    - 压缩RECORD，如下图\n    - 压缩BLOCK，①一次性压缩多条记录；②每一个新块Block开始处都需要插入同步点；如下图\n  - 在大多数情况下，以block（注意：指的是SequenceFile中的block）为单位进行压缩是最好的选择\n  - 因为一个block包含多条记录，利用record间的相似性进行压缩，压缩效率更高\n  - 把已有的数据转存为SequenceFile比较慢。比起先写小文件，再将小文件写入SequenceFile，一个更好的选择是直接将数据写入一个SequenceFile文件，省去小文件作为中间媒介.\n\n![](img/Image201907101934.png)\n\n\n\n![](img/Image201907101935.png)\n\n- 向SequenceFile写入数据\n\n```java\npackage com.kaikeba.hadoop.sequencefile;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.SequenceFile;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.BZip2Codec;\n\nimport java.io.IOException;\nimport java.net.URI;\n\npublic class SequenceFileWriteNewVersion {\n\n    //模拟数据源\n    private static final String[] DATA = {\n            \"The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.\",\n            \"It is designed to scale up from single servers to thousands of machines, each offering local computation and storage.\",\n            \"Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer\",\n            \"o delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.\",\n            \"Hadoop Common: The common utilities that support the other Hadoop modules.\"\n    };\n\n    public static void main(String[] args) throws IOException {\n        //输出路径：要生成的SequenceFile文件名\n        String uri = \"hdfs://node01:9000/writeSequenceFile\";\n\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(URI.create(uri), conf);\n        //向HDFS上的此SequenceFile文件写数据\n        Path path = new Path(uri);\n\n        //因为SequenceFile每个record是键值对的\n        //指定key类型\n        IntWritable key = new IntWritable();\n        //指定value类型\n        Text value = new Text();\n//\n//            FileContext fileContext = FileContext.getFileContext(URI.create(uri));\n//            Class<?> codecClass = Class.forName(\"org.apache.hadoop.io.compress.SnappyCodec\");\n//            CompressionCodec SnappyCodec = (CompressionCodec)ReflectionUtils.newInstance(codecClass, conf);\n//            SequenceFile.Metadata metadata = new SequenceFile.Metadata();\n//            //writer = SequenceFile.createWriter(fs, conf, path, key.getClass(), value.getClass());\n//            writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(path), SequenceFile.Writer.keyClass(IntWritable.class),\n//                                        SequenceFile.Writer.valueClass(Text.class));\n\n        //创建向SequenceFile文件写入数据时的一些选项\n        //要写入的SequenceFile的路径\n        SequenceFile.Writer.Option pathOption       = SequenceFile.Writer.file(path);\n        //record的key类型选项\n        SequenceFile.Writer.Option keyOption        = SequenceFile.Writer.keyClass(IntWritable.class);\n        //record的value类型选项\n        SequenceFile.Writer.Option valueOption      = SequenceFile.Writer.valueClass(Text.class);\n        //SequenceFile压缩方式：NONE | RECORD | BLOCK三选一\n        //方案一：RECORD、不指定压缩算法\n        SequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD);\n        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);\n\n\n        //方案二：BLOCK、不指定压缩算法\n//        SequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.BLOCK);\n//        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);\n\n\n\n        //方案三：使用BLOCK、压缩算法BZip2Codec；压缩耗时间\n        //再加压缩算法\n//        BZip2Codec codec = new BZip2Codec();\n//        codec.setConf(conf);\n//        SequenceFile.Writer.Option compressAlgorithm = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD, codec);\n//        //创建写数据的Writer实例\n//        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressAlgorithm);\n\n\n\n        for (int i = 0; i < 100000; i++) {\n            //分别设置key、value值\n            key.set(100 - i);\n            value.set(DATA[i % DATA.length]);\n            System.out.printf(\"[%s]\\t%s\\t%s\\n\", writer.getLength(), key, value);\n            //在SequenceFile末尾追加内容\n            writer.append(key, value);\n        }\n        //关闭流\n        IOUtils.closeStream(writer);\n    }\n}\n```\n\n- 命令查看SequenceFile内容\n\n```shell\n hadoop fs -text /writeSequenceFile\n```\n\n- 读取SequenceFile文件\n\n```java\npackage com.kaikeba.hadoop.sequencefile;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.SequenceFile;\nimport org.apache.hadoop.io.Writable;\nimport org.apache.hadoop.util.ReflectionUtils;\n\nimport java.io.IOException;\n\npublic class SequenceFileReadNewVersion {\n\n    public static void main(String[] args) throws IOException {\n        //要读的SequenceFile\n        String uri = \"hdfs://node01:9000/writeSequenceFile\";\n        Configuration conf = new Configuration();\n        Path path = new Path(uri);\n\n        //Reader对象\n        SequenceFile.Reader reader = null;\n        try {\n            //读取SequenceFile的Reader的路径选项\n            SequenceFile.Reader.Option pathOption = SequenceFile.Reader.file(path);\n\n            //实例化Reader对象\n            reader = new SequenceFile.Reader(conf, pathOption);\n\n            //根据反射，求出key类型\n            Writable key = (Writable)\n                    ReflectionUtils.newInstance(reader.getKeyClass(), conf);\n            //根据反射，求出value类型\n            Writable value = (Writable)\n                    ReflectionUtils.newInstance(reader.getValueClass(), conf);\n\n            long position = reader.getPosition();\n            System.out.println(position);\n\n            while (reader.next(key, value)) {\n                String syncSeen = reader.syncSeen() ? \"*\" : \"\";\n                System.out.printf(\"[%s%s]\\t%s\\t%s\\n\", position, syncSeen, key, value);\n                position = reader.getPosition(); // beginning of next record\n            }\n        } finally {\n            IOUtils.closeStream(reader);\n        }\n    }\n}\n```\n\n\n\n###  6. 文件快照\n\n####  6.1 什么是快照\n\n- 快照比较常见的应用场景是数据备份，以防一些用户错误或灾难恢复\n- 快照snapshots是HDFS文件系统的，只读的、某时间点的拷贝\n- 可以针对某个目录，或者整个文件系统做快照\n- 创建快照时，block块并不会被拷贝。快照文件中只是记录了block列表和文件大小，**不会做任何数据拷贝**\n\n####  6.2 快照操作\n\n- 允许快照\n\n  允许一个快照目录被创建。如果这个操作成功完成，这个目录就变成snapshottable\n\n  用法：hdfs dfsadmin -allowSnapshot <snapshotDir>\n\n  ```shell\n  hdfs dfsadmin -allowSnapshot /wordcount\n  ```\n\n- 禁用快照\n\n  用法：hdfs dfsadmin -disallowSnapshot <snapshotDir>\n\n  ```shell\n  hdfs dfsadmin -disallowSnapshot /wordcount\n  ```\n\n- 创建快照\n\n  用法：hdfs dfs -createSnapshot <snapshotDir> [<snapshotName>]\n\n  ```shell\n  #注意：先将/wordcount目录变成允许快照的\n  hdfs dfs -createSnapshot /wordcount wcSnapshot\n  ```\n\n- 查看快照\n\n  ```shell\n  hdfs dfs -ls /wordcount/.snapshot\n  \n  ```\n\n  ![](img/Image201909041346.png)\n\n- 重命名快照\n\n  这个操作需要拥有snapshottabl目录所有者权限\n\n  用法：hdfs dfs -renameSnapshot <snapshotDir> <oldName> <newName>\n\n  ```shell\n  hdfs dfs -renameSnapshot /wordcount wcSnapshot newWCSnapshot\n  \n  ```\n\n- 用快照恢复误删除数据\n\n  HFDS的/wordcount目录，文件列表如下\n\n  ![](img/Image201909041356.png)\n\n  误删除/wordcount/edit.xml文件\n\n  ```shell\n  hadoop fs -rm /wordcount/edit.xml\n  \n  ```\n\n  ![](img/Image201909041400.png)\n\n  恢复数据\n\n  ```shell\n  hadoop fs -cp /wordcount/.snapshot/newWCSnapshot/edit.xml /wordcount\n  \n  ```\n\n- 删除快照\n\n  这个操作需要拥有snapshottabl目录所有者权限\n\n  用法：hdfs dfs -deleteSnapshot <snapshotDir> <snapshotName>\n\n  ```shell\n  hdfs dfs -deleteSnapshot /wordcount newWCSnapshot\n  \n  ```\n\n\n\n\n\n\n##  7、拓展点、未来计划、行业趋势\n\n1. HDFS存储地位\n\n2. **block块为什么设置的比较大**\n\n- [磁盘基础知识](<https://www.cnblogs.com/jswang/p/9071847.html>)\t\n\n  - 盘片platter、磁头head、磁道track、扇区sector、柱面cylinder\n  - 为了最小化寻址开销；从磁盘传输数据的时间明显大于定位这个块开始位置所需的时间\n\n- 问：块的大小是不是设置的越大越好呢？\n\n  1、 不是，寻址的时间大概是 100ms，设计一般设置为寻址时间占用十分之一，也就是一秒。 硬盘的传输速录大概是100m/s 一秒大概为100M，最接近100的大小为128M。 \n\n![](img/Image201906211143.png)\n","tags":["hadoop","hafs read / write","文件快照"]},{"title":"大数据概论-HDFS理论基础-","url":"/2019/10/10/Java大数据基础概论/","content":"## 大数据概论\n\n> 概念： 大数据（big data）是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产\n\n| 数据单位 | B    | KB   | MB   | GB   | PE   | PB   | EB   | ZB   | YB   |\n| -------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| 基数     |      | 2    | 2    | 2    | 2    | 2    | 2    | 10   | 10   |\n| 次方     | 0    | 10   | 20   | 30   | 40   | 50   | 60   | 21   | 24   |\n\n### 一、大数据特性\n\n1. 数据量大（Volume） \n2. 类型繁多（Variety） \n3. 价值密度低（Value） \n4. 速度快时效高（Velocity）\n\n### 二、大数据的挑战\n\n1. 存储： 每天几TB、GB的数据增量，并且还在持续的增长中。\n2. 分析： 如何从巨大的数据中挖掘出隐藏的商业价值。\n3. 管理： 如何快速构建并且保证系统的安全、简单可用。\n\n### 三、传统的大数据项目流程\n\n```flow\nst=>start: 开始\ndataCollect=>operation: 数据收集 ： Flume、Kafaka、Scribe\ndataStore=>operation: 数据存储 ： HDFS、HBase、Cassadra\ndataCaculate=>operation: 数据计算 : Mapreduce、Strom、Impala、Spark、Spark Streaming...\n数据计算三大类：\n1、离线处理平台： Spark、Spark Core\n2、交互式处理平台： Spark SQL、Hive 、Impala\n3、流处理平台 ： Strom、Spring Stoeaming 、 Flink\ndataAnalyse=>operation: 分析与挖掘 ： Mahour、R语言、Hive、Pig\ndataEtl=>operation: ETL ： sqoop、DataX\ndataView=>operation: 可视化 ： Echarts.js 、 E3.js、 数据报表系统\ndataActual=>operation: 项目实战\ne=>end: 结束\n\nst->dataCollect->dataStore->dataCaculate->dataAnalyse->dataEtl->dataView->dataActual->e\n\n```\n\n## 分布式文件系统\n\n### 一、Hadoop简介\n\n1. Hadoop架构\n\n   ![Image201906191834](img/Image201906191834.png)\n\n2. Hadoop历史\n\n   ![Image201906202055](img/Image201906202055.png)\n\n### 二、HDFS\n\n- HDFS是Hadoop中的一个存储子模块\n- HDFS (全称Hadoop Distributed File System)，即hadoop的分布式文件系统\n- File System**文件系统**：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读出、修改、转储、删除文件等\n- 当数据集大小超出一台计算机的存储能力时，就有必要将它拆分成若干部分，然后分散到不同的计算机中存储。管理网络中跨多台计算机存储的文件系统称之为**分布式文件系统**（distributed filesystem）\n\n#### 2.1 HDFS特点\n\n**2.1.1 优点：**\n\n- 适合存储大文件，能用来存储管理PB级的数据；不适合存储小文件\n- 处理非结构化数据\n- 流式的访问数据，一次写入、多次读写\n- 运行于廉价的商用机器集群上，成本低\n- 高容错：故障时能继续运行且不让用户察觉到明显的中断\n- 可扩展\n\n![](/Users/dingchuangshi/Documents/Java大数据课件/第八章HDFS课件/20191009-HDFS-第一次/assets/Image201907081216.png)\n\n**2.1.2 局限性**\n\n- 不适合处理低延迟数据访问\n  - DFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的\n  - 对于低延时的访问需求，HBase是更好的选择\n- 无法高效存储大量的小文件\n  - 小文件会给Hadoop的扩展性和性能带来严重问题\n  - 利用SequenceFile、MapFile等方式归档小文件\n- 不支持多用户写入及任意修改文件\n  - 文件有一个写入者，只能执行追加操作\n  - 不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改\n\n#### 2.2 HDFS常用命令\n\n> HDFS两种命令风格，两种命令效果等同\n>\n> hadoop fs / hdfs dfs\n\n![image-20191010155353956](/Users/dingchuangshi/Library/Application Support/typora-user-images/image-20191010155353956.png)\n\n\n\n1. 如何查看hdfs或hadoop子命令的**帮助信息**，如ls子命令\n\n   ```shell\n   hdfs dfs -help ls\n   hadoop fs -help ls\t#两个命令等价\n   ```\n\n2. **查看**hdfs文件系统中已经存在的文件。对比linux命令ls\n\n   ```shell\n   hdfs dfs -ls /\n   hadoop fs -ls /\n   ```\n\n3. 在hdfs文件系统中创建文件\n\n   ```shell\n   hdfs dfs -touchz /edits.txt\n   ```\n\n4. 向HDFS文件中追加内容\n\n    ```shell\n    hadoop fs -appendToFile edit1.xml /edits.txt #将本地磁盘当前目录的edit1.xml内容追加到HDFS根目录 的edits.txt文件\n    ```\n\n5. 查看HDFS文件内容\n\n    ```shell\n    hdfs dfs -cat /edits.txt\n    ```\n\n6. **从本地路径上传文件至HDFS**\n\n    ````` shell\n    #用法：hdfs dfs -put /本地路径 /hdfs路径\n    hdfs dfs -put hadoop-2.7.3.tar.gz /\n    hdfs dfs -copyFromLocal hadoop-2.7.3.tar.gz /  #根put作用一样\n    hdfs dfs -moveFromLocal hadoop-2.7.3.tar.gz /  #根put作用一样，只不过，源文件被拷贝成功后，会被删除\n    `````\n\n7. **在hdfs文件系统中下载文件**\n\n     ```shell\n     hdfs dfs -get /hdfs路径 /本地路径\n     hdfs dfs -copyToLocal /hdfs路径 /本地路径  #根get作用一样\n     ```\n\n8. 在hdfs文件系统中**创建目录**\n\n     ```shell\n     hdfs dfs -mkdir /shell\n     ```\n\n9. 在hdfs文件系统中**删除**文件\n\n     ```shell\n     hdfs dfs -rm /edits.txt\n     hdfs dfs -rm -r /shell\n     ```\n\n10. 在hdfs文件系统中**修改文件名称**（也可以用来**移动**文件到目录）\n\n     ```shell\n     hdfs dfs -mv /xcall.sh /call.sh\n     hdfs dfs -mv /call.sh /shell\n     ```\n\n11. 在hdfs中拷贝文件到目录\n\n      ```shell\n      hdfs dfs -cp /xrsync.sh /shell\n      ```\n\n12. 递归删除目录\n\n      ```shell\n      hdfs dfs -rmr /shell\n      ```\n\n13. 列出本地文件的内容（默认是hdfs文件系统）\n\n      ```shell\n      hdfs dfs -ls file:///home/bruce/\n      ```\n\n14. 查找文件\n\n      ```shell\n      # linux find命令\n      find . -name 'edit*'\n      \n      # HDFS find命令\n      hadoop fs -find / -name part-r-00000 # 在HDFS根目录中，查找part-r-00000文件\n      ```\n\n\n> 还有许多其他命令，大家可以自己探索一下   \n\n##### 2.2.1 命令行小结\n\n- 输入hadoop fs 或hdfs dfs，回车，查看所有的HDFS命令\n\n- 许多命令与linux命令有很大的相似性，学会举一反三\n- 有用的help，如查看ls命令的使用说明：hadoop fs -help ls\n\n##### 2.2.2 hdfs与getconf结合使用\n\n1. 获取NameNode的节点名称（可能有多个）\n\n      ``````shell\n      hdfs getconf -namenodes\n      ``````\n\n2. 获取hdfs最小块信息\n\n      ``````shell\n      hdfs getconf -confKey dfs.namenode.fs-limits.min-block-size\n      ``````\n\n3. 查找hdfs的NameNode的RPC地址\n\n\t``````shell\n\thdfs getconf -nnRpcAddresses\n\t``````\n\t\n\t\n\n##### 2.2.3 hdfs与dfsadmin结合使用\n\n1. 同样要学会借助帮助信息\n\n      ```shell\n      hdfs dfsadmin -help safemode\n      ```\n\n2. 查看hdfs dfsadmin的帮助信息\n\n      ``````shell\n      hdfs dfsadmin\n      ``````\n\n3. 查看当前的模式\n\n      ``````shell\n      hdfs dfsadmin -safemode get\n      ``````\n\n4. 进入安全模式\n\n  ``````shell\n  hdfs dfsadmin -safemode enter\n  ``````\n\n  \n\n##### 2.2.4 hdfs与fsck结合使用\n\n1. fsck指令**显示HDFS块信息**\n\n\t``````shell\n\thdfs fsck /02-041-0029.mp4 -files -blocks -locations # 查看文件02-041-0029.mp4的块信息\n\t``````\n\t\n\t\n\n##### 2.2.5 其他命令\n\n1. 检查压缩库本地安装情况\n\n      ``````shell\n      hadoop checknative\n      ``````\n\n2. 格式化名称节点（**慎用**，一般只在初次搭建集群，使用一次；格式化成功后，不要再使用）\n\n      ``````shell\n      hadoop namenode -format\n      ``````\n\n3. 执行自定义jar包\n\n   ``````shell\n   hadoop jar YinzhengjieMapReduce-1.0-SNAPSHOT.jar com.kaikeba.hadoop.WordCount /world.txt /out\n   ``````\n\n#### 2.3 HDFS编程\n\n\n- 1.向hdfs中,上传一个文本文件\n\n  ```java\n   /**\n       * 上传文件到服务器\n       *  传递参数\n       *  args[0] 本地文件路径\n       *  args[1] hdoop文件系统 路径\n       */\n      public static void uploadFileToFileSystem(String source,String targetUrl){\n          System.out.println(\"文件地址：\" + source);\n          System.out.println(\"目标服务器：\" + targetUrl);\n          InputStream inputStreamSourceFile = null;\n          try {\n              // 获取文件输入流\n              inputStreamSourceFile = new BufferedInputStream(new FileInputStream(source));\n              // HDFS 读写配置文件\n              Configuration configuration = new Configuration();\n              // 通过url 返回文件系统实例\n              FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);\n              //调用Filesystem的create方法返回的是FSDataOutputStream对象\n              //该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n              // 获取文件系用的输出流\n              OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n              // 将文件输入流，写入输入流\n              IOUtils.copyBytes(inputStreamSourceFile,outputStreamTarget,4069,true);\n              System.out.println(\"上传成功\");\n          } catch (FileNotFoundException e) {\n              System.err.println(e.getMessage());\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n      }\n  \n  ```\n\n- 2.读取hdfs上的文件\n\n```java\n\n    /**\n     * 从文件系统中读取文件\n     * @param source 需要读取的文件\n     * @return 读取文件内容\n     */\n    public static String readFileFromFileSystem(String source){\n        String result = null;\n        try {\n            // HDFS 读写文件配置\n            Configuration configuration = new Configuration();\n            // HDFS文件系统\n            FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n            // 文件输入流，用于读取文件\n            InputStream inputStream = fileSystem.open(new Path(source));\n            BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream));\n            result = readBufferReader(bufferedReader).toString();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return result;\n    }\n    \n    \n    /**\n     * 获取内容\n     * @param br\n     * @return\n     */\n    private static StringBuffer readBufferReader(BufferedReader br) throws IOException {\n        StringBuffer stringBuffer = new StringBuffer();\n        String temp = \"\";\n        while ((temp = br.readLine()) != null){\n            stringBuffer.append(temp);\n        }\n        return stringBuffer;\n    }\n```\n\n\n\n- 3.列出某一个文件夹下的所有文件\n\n```java\n\n    /**\n     * 列出当前目录下所有字文件名称\n     * @param source\n     * @return\n     */\n    public static String listAllFileChildren(String source){\n        StringBuffer stringBuffer = new StringBuffer();\n        try {\n            // HDFS 读写文件配置\n            Configuration configuration = new Configuration();\n            // HDFS文件系统\n            FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n            // recursive 继续深入遍历\n            RemoteIterator<LocatedFileStatus> iterator = fileSystem.listFiles(new Path(source),true);\n            while (iterator.hasNext()){\n                LocatedFileStatus fileStatus = iterator.next();\n                stringBuffer.append(fileStatus.getPath() + \"\\n\");\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return stringBuffer.toString();\n    }\n\n```\n\n\n\n- 4.列出多级目录名称和目录下的文件名称\n\n  ```java\n  \n      /**\n       * 递归列出当前目录下所有目录和文件名称\n       * @param source\n       * @return\n       */\n      public static String listAllChildren(String source){\n          StringBuffer stringBuffer = new StringBuffer();\n          try {\n              // HDFS 读写文件配置\n              Configuration configuration = new Configuration();\n              // HDFS文件系统\n              FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n              list(stringBuffer,fileSystem,new Path(source));\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n          return stringBuffer.toString();\n      }\n  \n      /**\n       * 递归目录和文件\n       * @param stringBuffer  文件目录名称集合\n       * @param fileSystem  hdfs 文件系统\n       * @param source path 路径\n       * @throws IOException\n       */\n      private static void list(StringBuffer stringBuffer,FileSystem fileSystem, Path source) throws IOException {\n          FileStatus[] iterator = fileSystem.listStatus(source);\n          for (FileStatus status:iterator) {\n              stringBuffer.append(status.getPath() + \"\\n\");\n              if(status.isDirectory()){\n                  list(stringBuffer,fileSystem,status.getPath());\n              }\n  \n          }\n      }\n  \n  \n  ```\n\n  \n\n#### 2.4  HDFS架构\n\n![](img/1558073557041.png)\n\n- 大多数分布式框架都是主从架构\n- HDFS也是主从架构Master|Slave或称为管理节点|工作节点\n\n##### 1、NameNode\n\n**1.1 文件系统**\n\n- file system文件系统：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读取、修改、转储、删除文件等\n- 读文件 =>>找到文件 =>> 在哪 + 叫啥？\n- 元数据\n  - 关于文件或目录的描述信息，如文件所在路径、文件名称、文件类型等等，这些信息称为文件的元数据metadata\n- 命名空间\n  - 文件系统中，为了便于管理存储介质上的，给每个目录、目录中的文件、子目录都起了名字，这样形成的层级结构，称之为命名空间\n  - 同一个目录中，不能有同名的文件或目录\n  - 这样通过目录+文件名称的方式能够唯一的定位一个文件\n\n![](img/Image201906211418.png)\n\n**5.1.2 HDFS-NameNode**\n\n- HDFS本质上也是文件系统filesystem，所以它也有元数据metadata；\n- 元数据metadata保存在NameNode**内存**中\n- NameNode作用\n  - HDFS的主节点，负责管理文件系统的命名空间，将HDFS的元数据存储在NameNode节点的内存中\n  - 负责响应客户端对文件的读写请求\n- HDFS元数据\n  - 文件目录树、所有的文件（目录）名称、文件属性（生成时间、副本、权限）、每个文件的块列表、每个block块所在的datanode列表\n\n![](img/Image201909031504.png)\n\n  - 每个文件、目录、block占用大概**150Byte字节的元数据**；所以HDFS适合存储大文件，不适合存储小文件\n\n  - HDFS元数据信息以两种形式保存：①编辑日志**edits log**②命名空间镜像文件**fsimage**\n    - edits log：HDFS编辑日志文件 ，保存客户端对HDFS的所有更改记录，如增、删、重命名文件（目录），这些操作会修改HDFS目录树；NameNode会在编辑日志edit日志中记录下来；\n    - fsimage：HDFS元数据镜像文件 ，即将namenode内存中的数据落入磁盘生成的文件；保存了文件系统目录树信息以及文件、块、datanode的映射关系，如下图\n\n\n![](img/Image201910091133.png)\n\n> 说明：\n>\n> ①为hdfs-site.xml中属性dfs.namenode.edits.dir的值决定；用于namenode保存edits.log文件\n>\n> ②为hdfs-site.xml中属性dfs.namenode.name.dir的值决定；用于namenode保存fsimage文件\n\n##### 2、DataNode\n\n- DataNode数据节点的作用\n  - 存储block以及block元数据到datanode本地磁盘；此处的元数据包括数据块的长度、块数据的校验和、时间戳\n\n##### 3 SeconddaryNameNode   \n\n- 为什么引入SecondaryNameNode\n\n  - 为什么元数据存储在NameNode在内存中？\n\n  - 这样做有什么问题？如何解决？\n\n  - HDFS编辑日志文件 editlog：在NameNode节点中的编辑日志editlog中，记录下来客户端对HDFS的所有更改的记录，如增、删、重命名文件（目录）；\n\n  - 作用：一旦系统出故障，可以从editlog进行恢复；\n\n  - 但editlog日志大小会随着时间变在越来越大，导致系统重启根据日志恢复的时候会越来越长；\n\n  - 为了避免这种情况，引入**检查点机制checkpoint**，命名空间镜像fsimage就是HDFS元数据的持久性检查点，即将内存中的元数据落磁盘生成的文件；\n\n  - 此时，namenode如果重启，可以将磁盘中的fsimage文件读入内容，将元数据恢复到某一个检查点，然后再执行检查点之后记录的编辑日志，最后完全恢复元数据。\n\n  - 但是依然，随着时间的推移，editlog记录的日志会变多，那么当namenode重启，恢复元数据过程中，会花越来越长的时间执行editlog中的每一个日志；而在namenode元数据恢复期间，HDFS不可用。\n\n  - 为了解决此问题，引入secondarynamenode辅助namenode，用来合并fsimage及editlog\n\n\n\n![](img/Image201906211525.png)\n\n- SecondaryNameNode定期做checkpoint检查点操作\n\n  - 创建检查点checkpoint的两大条件：\n    - SecondaryNameNode每隔1小时创建一个检查点\n    - 另外，Secondary NameNode每1分钟检查一次，从上一检查点开始，edits日志文件中是否已包括100万个事务，如果是，也会创建检查点\n  - Secondary NameNode首先请求原NameNode进行edits的滚动，这样新的编辑操作就能够进入新的文件中\n  - Secondary NameNode通过HTTP GET方式读取原NameNode中的fsimage及edits\n  - Secondary NameNode读取fsimage到内存中，然后执行edits中的每个操作，并创建一个新的统一的fsimage文件\n  - Secondary NameNode通过HTTP PUT方式将新的fsimage发送到原NameNode\n  - 原NameNode用新的fsimage替换旧的fsimage，同时系统会更新fsimage文件到记录检查点的时间。 \n  - 这个过程结束后，NameNode就有了最新的fsimage文件和更小的edits文件\n\n- SecondaryNameNode一般部署在另外一台节点上\n\n  - 因为它需要占用大量的CPU时间\n  - 并需要与namenode一样多的内存，来执行合并操作\n\n- 如何查看edits日志文件\n\n  ```shell\n  hdfs oev -i edits_0000000000000000256-0000000000000000363 -o /home/hadoop/edit1.xml\n  ```\n\n- 如何查看fsimage文件\n\n  ```shell\n  hdfs oiv -p XML -i fsimage_0000000000000092691 -o fsimage.xml  \n  ```\n\n- checkpoint相关属性\n\n  | 属性                                 | 值              | 解释                                                         |\n  | ------------------------------------ | --------------- | ------------------------------------------------------------ |\n  | dfs.namenode.checkpoint.period       | 3600秒(即1小时) | The number of seconds between two periodic checkpoints.      |\n  | dfs.namenode.checkpoint.txns         | 1000000         | The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless of whether 'dfs.namenode.checkpoint.period' has expired. |\n  | dfs.namenode.checkpoint.check.period | 60(1分钟)       | The SecondaryNameNode and CheckpointNode will poll the NameNode every 'dfs.namenode.checkpoint.check.period' seconds to query the number of uncheckpointed transactions. |\n\n  \n\n##### 4 心跳机制\n\n![](img/Image201906211518.png)\n\n**工作原理：**\n\n1. NameNode启动的时候，会开一个ipc server在那里\n2. DataNode启动后向NameNode注册，每隔**3秒钟**向NameNode发送一个“**心跳heartbeat**”\n3. 心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一DataNode，或删除某个数据块\n4. 如果超过**10分钟**NameNode没有收到某个DataNode 的心跳，则认为该DataNode节点不可用\n5. DataNode周期性（**6小时**）的向NameNode上报当前DataNode上的块状态报告BlockReport；块状态报告包含了一个该 Datanode上所有数据块的列表\n\n**心跳的作用：**\n\n1. 通过周期心跳，NameNode可以向DataNode返回指令\n\n2. 可以判断DataNode是否在线\n\n3. 通过BlockReport，NameNode能够知道各DataNode的存储情况，如磁盘利用率、块列表；跟**负载均衡**有关\n\n4. hadoop集群刚开始启动时，99.9%的block没有达到最小副本数(dfs.namenode.replication.min默认值为1)，集群处于**安全模式**，涉及BlockReport；\n\n**心跳相关配置**\n\n- [hdfs-default.xml](<https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml>)\n- 心跳间隔\n\n| 属性                   | 值   | 解释                                               |\n| ---------------------- | ---- | -------------------------------------------------- |\n| dfs.heartbeat.interval | 3    | Determines datanode heartbeat interval in seconds. |\n\n- **block report**\n\n| More Actions属性             | 值               | 解释                                                 |\n| ---------------------------- | ---------------- | ---------------------------------------------------- |\n| dfs.blockreport.intervalMsec | 21600000 (6小时) | Determines block reporting interval in milliseconds. |\n\n- 查看hdfs-default.xml默认配置文件\n\n![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第八章HDFS课件/20191009-HDFS-第一次/assets/Image201907311730.png)\n\n##### 5 负载均衡\n\n- 什么原因会有可能造成不均衡？\n  - 机器与机器之间磁盘利用率不平衡是HDFS集群非常容易出现的情况\n  - 尤其是在DataNode节点出现故障或在现有的集群上增添新的DataNode的时候\n\n- 为什么需要均衡？\n  - 提升集群存储资源利用率\n  - 从存储与计算两方面提高集群性能\n\n- 如何手动负载均衡？\n\n```shell\n$HADOOP_HOME/sbin/start-balancer.sh -t 5%\t# 磁盘利用率最高的节点若比最少的节点，大于5%，触发均衡\n```\n\n##### 6 小结\n\n- NameNode负责存储元数据，存在内存中\n- DataNode负责存储block块及块的元数据\n- SecondaryNameNode主要负责对HDFS元数据做checkpoint操作\n- 集群的心跳机制，让集群中各节点形成一个整体；主节点知道从节点的死活\n- 节点的上下线，导致存储的不均衡，可以手动触发负载均衡\n","tags":["hadoop"]}]