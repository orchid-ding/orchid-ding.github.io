[{"title":"azkaban工作流调度器","url":"/2019/11/26/flume-sqoop-zakaban/azkaban工作流调度器/","content":"\n## Azkaban工作流调度器\n\n### 1. 为什么需要工作流调度系统\n\n* 一个完整的数据分析系统通常都是由大量任务单元组成；\n  * shell脚本程序、java程序、mapreduce程序、hive脚本等\n* 各任务单元之间存在时间先后及前后依赖关系\n\n* 为了==很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行==\n\n\n\n### 2. Azkaban是什么\n\n* Azkaban是由Linkedin开源的一个==批量工作流任务调度器==。用于在一个工作流内以一个特定的顺序运行一组工作和流程。\n* Azkaban定义了一种==KV文件(properties)格式==来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。\n  * <https://azkaban.github.io/>\n\n![azkaban.github](assets/azkaban.github.png)\n\n* 功能特点\n  * 提供功能清晰、简单易用的web UI界面\n  * 提供job配置文件快速建立任务和任务之间的关系\n  * 提供模块化的可插拔机制，原生支持command、java、hive、hadoop\n  * 基于java开发，代码结构清晰，易于二次开发\n\n\n\n### 3. Azkaban基本架构\n\n![azkaban](assets/azkaban.png)\n\n\n\n* Azkaban由三部分构成\n\n  * 1、==Azkaban Web Server==\n    * 提供了Web UI，是azkaban的主要管理者，包括 project 的管理，认证，调度，对工作流执行过程的监控等。\n\n\n\n  * 2、==Azkaban Executor Server==\n    * 负责具体的工作流和任务的调度提交\n\n\n\n  * 3、==Mysql==\n    * 用于保存项目、日志或者执行计划之类的信息\n\n\n\n### 4. Azkaban架构的三种运行模式\n\n* 1、solo server mode(单机模式）\n\n~~~\nH2\nweb server 和 executor server运行在一个进程里\n最简单的模式，数据库内置的H2数据库，管理服务器和执行服务器都在一个进程中运行，任务量不大项目可以采用此模式。\n\n~~~\n\n\n\n* 2、two server mode\n\n~~~\nweb server 和 executor server运行在不同的进程\n数据库为mysql，管理服务器和执行服务器在不同进程，这种模式下，管理服务器和执行服务器互不影响。\n\n~~~\n\n\n\n* 3、multiple executor mode\n\n~~~\nweb server 和 executor server运行在不同的进程，executor server有多个\n该模式下，执行服务器和管理服务器在不同主机上，且执行服务器可以有多个。\n\n~~~\n\n\n\n### 5. Azkaban安装部署\n\n[点击查看](https://kfly.top/2019/11/26/hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n\n### 6. Azkaban实战\n\n#### 6.1 command类型单一job\n\n* 1、==创建job描述文件  以.job后缀结尾==\n  * 创建 command.job 文件\n\n~~~shell\n#command.job\ntype=command\ncommand=echo 'hello azkaban......'\n~~~\n\n* 2、将job资源文件打包成zip文件\n* 例如\n  * command.zip\n\n* 3、通过azkaban的web管理平台创建project并上传job压缩包\n\n![create_project](assets/create_project.png)\n\n![upload_zip](assets/upload_zip.png)\n\n* 3、点击运行\n\n![execute-flow](assets/execute-flow.png)\n\n![execute](assets/execute.png)\n\n\n\n* 4、运行完成\n\n![success](assets/success.png)\n\n#### 6.2 command类型多job工作流\n\n* 1、创建有依赖关系的多个job描述\n\n  * 第一个job：start1.job\n\n  ~~~shell\n  #start1.job\n  type=command\n  command= echo 'start1...start1...'\n  ~~~\n\n  * 第二个job：start2.job    它依赖start1.job\n\n  ~~~shell\n  #start2.job\n  type=command\n  dependencies=start1\n  command= echo 'start2...start2...'\n  ~~~\n\n* 2、将job资源文件打包成zip文件\n\n  * start12.zip\n\n  ![start12job](assets/start12job.png)\n\n* 3、创建工程，上传zip包，最后启动工作流\n\n![execute-start12](assets/execute-start12.png)\n\n* ==补充==\n  * 如果一个job有多个依赖的job，可以使用逗号隔开\n\n~~~shell\n例如： \n#start1.job\ntype=command\ncommand= echo \"start1 job\"\n\n#start2.job\ntype=command\ncommand= echo \"start2 job\"\n\n#stop.job\ntype=command\ndenpendencies=start1,start2\ncommand=echo \"stop job\"\n\n注意：有多个依赖的job，用逗号隔开\n~~~\n\n\n\n#### 6.3 HDFS操作任务\n\n* 1、创建job描述文件\n\n  * vim fs.job\n\n  ~~~shell\n  #fs.job\n  type=command\n  command=echo \"start execute\"\n  command.1=/kfly/install/hadoop-2.6.0-cdh5.14.2/bin/hdfs dfs -mkdir /azkaban\n  command.2=/kfly/install/hadoop-2.6.0-cdh5.14.2/bin/hdfs dfs -put /home/hadoop/source.txt /azkaban\n  ~~~\n\n* 2、将job资源文件打包成zip文件\n\n  * fs.zip\n\n* 3、创建工程，上传zip包，最后启动工作流\n\n![fs](assets/fs.png)\n\n![fs-hdfs](assets/fs-hdfs.png)\n\n#### 6.4 MAPREDUCE任务\n\n* MR任务依然可以使用command的job类型来执行\n\n* 1、创建job描述文件，及mr程序jar包\n\n  * 示例中直接使用hadoop自带的example jar\n  * hdfs dfs -mkdir -p  /wordcount/in\n\n  ~~~shell\n  #mr.job\n  type=command\n  command=/kfly/install/hadoop-2.6.0/bin/hadoop  jar hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar wordcount /wordcount/in /wordcount/out\n  ~~~\n\n* 2、将job资源文件打包成zip文件\n\n  - mr.zip\n\n* 3、创建工程，上传zip包，最后启动工作流\n\n![mr](assets/mr.png)\n\n#### 6.5 HIVE脚本任务\n\n* 1、 创建job描述文件和hive脚本\n\n  * Hive脚本： test.sql\n\n  ~~~sql\n  use default;\n  create table if not exists test_azkaban(id int,name string,address string) row format delimited fields terminated by ',';\n  load data local inpath '/home/hadoop/azkaban/test.txt' into table test_azkaban;\n  create table if not exists countaddress as select address,count(*) as num from test_azkaban group by address ;\n  \n  insert overwrite local directory '/home/hadoop/azkaban/out' select * from countaddress; \n  ~~~\n\n  * 准备数据\n\n    * vim /home/hadoop/azkaban/test.txt\n\n    ~~~properties\n    1,zhangsan,shanghai\n    2,lisi,beijing\n    3,xiaoming,shanghai\n    4,xiaozhang,shanghai\n    5,xiaogang,beijing\n    ~~~\n\n* 2、创建job描述文件\n\n  * hive.job\n\n  ~~~shell\n  # hive.job\n  type=command\n  command=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive -f 'test.sql'\n  ~~~\n\n\n\n* 3、将job资源文件打包成zip文件\n  \n  * hive.zip\n  \n### 7. 任务定时调度\n\n* 在启动工作流的时候，可以点击==Schedule==，实现定时调度一个工作流\n\n![schedule1](assets/schedule1.png)\n\n\n\n![schedule2](assets/schedule2.png)\n\n\n\n### 8. webUI 传递参数\n\n* 可以通过webUI动态给job传递参数\n\n* 1、创建一个job的描述文件\n\n  * parameter.job\n\n    ~~~shell\n    #parameter.job\n    type=command\n    parameter=${param}\n    command= echo ${parameter}\n    ~~~\n\n  * 其中\n\n    * ==${param}== 表示解析页面传递的参数param的值，通过声明一个变量parameter去接受\n    * ==${parameter}==表示获取该parameter变量的值\n\n* 2、将job资源文件打包成zip文件\n\n  * parameter.zip\n\n* 3、创建工程，上传zip包，最后启动工作流，并且设置参数\n\n![1572232468448](assets/1572232468448.png)\n\n* 4、运行完成后的结果\n\n  ![1572232525618](assets/1572232525618.png)\n","tags":["-azkaban"]},{"title":"sqoop数据迁移工具","url":"/2019/11/26/flume-sqoop-zakaban/sqoop数据迁移工具/","content":"\n### 1. Sqoop是什么\n\n* Sqoop是apache旗下的一款 ”==Hadoop和关系数据库之间传输数据==”的工具\n  * ==导入数据== import\n    * 将MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统\n  * ==导出数据== export\n    * 从Hadoop的文件系统中导出数据到关系数据库\n\n![sqoop](assets/sqoop.png)\n\n\n\n### 2. Sqoop的工作机制\n\n* 将导入和导出的命令翻译成mapreduce程序实现\n  * 在翻译出的mapreduce中主要是对inputformat和outputformat进行定制\n\n\n\n### 3. Sqoop基本架构\n\n* sqoop在发展中的过程中演进出来了两种不同的架构.[架构演变史](<https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop#comment-1561314193000>)\n\n* ==sqoop1的架构图==\n\n![sqoop1](assets/sqoop1.jpg)\n\n~~~\n版本号为1.4.x0\n~~~\n\n\n\n* ==sqoop2的架构图==\n\n\n\n![sqoop2](assets/sqoop2.jpg)\n\n~~~\n版本号为1.99x为sqoop2 \n在架构上：sqoop2引入了sqoop server，对connector实现了集中的管理 \n访问方式：REST API、 JAVA API、 WEB UI以及CLI控制台方式进行访问 \n~~~\n\n![sqoop1 VS sqoop2](assets/sqoop1 VS sqoop2.png)\n\n### 4. Sqoop安装部署\n\n​\t\t[点击查看](https://kfly.top/2019/11/26/hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n\n\n### 5. Sqooq数据的导入\n\n* 导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据） \n\n#### 5.1 列举出所有的数据库 \n\n* 命令行查看帮助文档\n\n~~~shell\nsqoop list-databases --help\n~~~\n\n* 列出node03上mysql数据库中所有的数据库名称\n\n~~~shell\nsqoop list-databases --connect jdbc:mysql://node03:3306/ --username root --password 123456\n~~~\n\n* 查看某一个数据库下面的所有数据表\n\n~~~shell\nsqoop list-tables --connect jdbc:mysql://node03:3306/hive --username root --password 123456\n~~~\n\n\n\n#### 5.2 导入数据库表数据到HDFS\n\n* 在MySQL数据库服务器中创建一个数据库userdb, 然后在创建一张表 emp，添加点测试数据到表中\n\n* 从MySQL数据库服务器中的userdb数据库下的emp表导入HDFS上\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node02:3306/userdb \\\n--username root   \\\n--password 123456 \\\n--table emp \\\n--m 1\n\n\n\n#参数解释\n--connect   指定mysql链接地址\n--username  连接mysql的用户名\n--password  连接mysql的密码\n--table     指定要导入的mysql表名称\n--m:        表示这个MR程序需要多少个MapTask去运行，默认为4\n默认路径是/user/hadoop下\n~~~\n\n* 提交之后，会运行一个MR程序，最后查看HDFS上的目录看是否有数据生成\n\n![table2hdfs](assets/table2hdfs.png)\n\n\n\n#### 5.3 导入数据库表数据到HDFS指定目录\n\n* 在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。\n* 使用参数 ==--target-dir==来指定导出目的地，\n* 使用参数==--delete-target-dir==来判断导出目录是否存在，如果存在就删掉\n\n~~~shell\nsqoop import  --connect jdbc:mysql://node03:3306/userdb --username root --password 123456  --table emp  --target-dir /sqoop/emp  --delete-target-dir --m 1\n~~~\n\n* 提交查看HDFS上的目录看是否有数据生成\n\n![table2hdfsDir](assets/table2hdfsDir.png)\n\n\n\n#### 5.4 导入数据库表数据到HDFS指定目录并且指定数据字段的分隔符\n\n* 这里使用参数 \n  * ==--fields-terminated-by 分隔符==\n\n~~~shell\nsqoop import  \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--delete-target-dir \\\n--table emp  \\\n--target-dir /sqoop/emp1 \\\n--fields-terminated-by '#' \\\n--m 1\n~~~\n\n#### 5.5 导入关系表到Hive中\n\n* (1) 将我们mysql表当中的数据直接导入到hive表中的话，需要将hive的一个叫做==hive-exec-1.1.0-cdh5.14.2.jar==包拷贝到sqoop的lib目录下\n\n~~~shell\ncp /kkb/install/hive-1.1.0-cdh5.14.2/lib/hive-exec-1.1.0-cdh5.14.2.jar /kkb/install/sqoop-1.4.6-cdh5.14.2/lib/\n~~~\n\n* (2) 准备hive数据库与表\n\n  * 在hive中创建一个数据库和表\n\n  ~~~sql\n  create database sqooptohive;\n  \n  create external table sqooptohive.emp_hive(id int,name string,deg string,salary double ,dept string) row format delimited fields terminated by '\\001';\n  ~~~\n\n* (3) 把mysql表数据导入到hive表中\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--fields-terminated-by '\\001' \\\n--hive-import \\\n--hive-table sqooptohive.emp_hive \\\n--hive-overwrite \\\n--delete-target-dir \\\n--m 1\n\n##参数解释\n--hive-table      指定要导入到hive表名\n--hive-import     导入数据到hive表中\n--hive-overwrite  覆盖hive表中已存有的数据\n\n~~~\n\n分为两步\n\n* (4) 执行完成了查看hive中表的数据\n  * **select * from sqooptohive.emp_hive;**\n\n![emp_hive](assets/emp_hive.png)\n\n\n\n#### 5.6 导入数据库表数据到hive中(并自动创建hive表)\n\n* 可以通过命令来将我们的mysql的表直接导入到hive表当中去，==不需要事先创建hive表==\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--hive-database sqooptohive \\\n--hive-table emp1 \\\n--hive-import \\\n--m 1 \n\n\ndrop table emp1\n~~~\n\n*  执行完成了查看hive中表的数据\n   *  **select * from sqooptohive.emp1;**\n\n![em1](assets/em1.png)\n\n\n\n#### 5.7 导入表数据子集\n\n* 我们可以导入表的使用Sqoop导入工具，\"where\"子句的一个子集。它执行在各自的数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。\n* 按照条件进行查找，通过==**--where**==参数来查找表emp当中==**dept**==字段的值为 **==TP==** 的所有数据导入到hdfs上面去\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--target-dir /sqoop/emp_where \\\n--delete-target-dir \\\n--where \"dept = 'TP'\" \\\n--m 1 \n~~~\n\n* 提交查看HDFS上的目录看是否有数据生成\n\n  ![](assets/emp_where.png)\n\n\n\n#### 5.8 sql语句查找导入hdfs\n\n* 我们还可以通过 -–query参数来指定我们的sql语句，通过sql语句来过滤我们的数据进行导入\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--target-dir /sqoop/emp_sql \\\n--delete-target-dir \\\n--query 'select * from emp where salary >30000 and $CONDITIONS' \\\n--m 1\n~~~\n\n* 提交查看HDFS上的目录看是否有数据生成\n\n![emp_conditions](assets/emp_conditions.png)\n\n* ==补充：==\n\n  ~~~shell\n  $CONTITONS是linux系统的变量，如果你想通过并行的方式导入结果，每个map task需要执行sql查询后脚语句的副本，结果会根据sqoop推测的边界条件分区。query必须包含$CONDITIONS。这样每个sqoop程序都会被替换为一个独立的条件。同时你必须指定 --split-by '字段'，后期是按照字段进行数据划分，最后可以达到多个MapTask并行运行。\n  \n  \n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456 \\\n  --target-dir /sqoop/emp_sql_2 \\\n  --delete-target-dir \\\n  --query 'select * from emp where salary >30000 and $CONDITIONS' \\\n  --split-by 'id' \\\n  --m 2\n  \n  \n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456 \\\n  --target-dir /sqoop/emp_sql_2 \\\n  --delete-target-dir \\\n  --query 'select * from emp where id >1 and $CONDITIONS' \\\n  --split-by 'salary' \\\n  --m 2\n  \n  \n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456 \\\n  --target-dir /sqoop/emp_sql_2 \\\n  --delete-target-dir \\\n  --query 'select * from emp where id >1 and $CONDITIONS' \\\n  --split-by 'id' \\\n  --m 7\n  \n  \n  \n  \n  --split-by '字段'： 后期按照字段进行数据划分实现并行运行多个MapTask。\n  ~~~\n\n\n#### 5.9 增量导入\n\n* 在实际工作当中，数据的导入很多时候都是==只需要导入增量数据即可==，并不需要将表中的数据全部导入到hive或者hdfs当中去，肯定会出现重复的数据的状况，所以我们一般都是选用一些字段进行增量的导入，为了支持增量的导入，sqoop也给我们考虑到了这种情况并且支持增量的导入数据\n\n* 增量导入是仅导入新添加的表中的行的技术。\n\n* 它需要添加 ==‘incremental’, ‘check-column’, 和 ‘last-value’==选项来执行增量导入。\n\n  ~~~\n  --incremental <mode>\n  --check-column <column name>\n  --last value <last check column value>\n  ~~~\n\n* ==第一种增量导入实现==\n\n  * ==基于递增列的增量数据导入（Append方式）==\n  * 导入emp表当中id大于1202的所有数据\n    * 注意：==这里不能加上 --delete-target-dir  参数，添加就报错==\n\n  ~~~shell\n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456 \\\n  --table emp \\\n  --incremental append \\\n  --check-column id \\\n  --last-value 1202  \\\n  --target-dir /sqoop/increment \\\n  --m 1\n  \n  \n  ##参数解释\n  --incremental   这里使用基于递增列的增量数据导入\n  --check-column  递增列字段\n  --last-value    指定上一次导入中检查列指定字段最大值\n  --target-dir    数据导入的目录\n  ~~~\n\n  * 提交查看HDFS上的目录看是否有数据生成\n\n![sqoop_increment1](assets/sqoop_increment1.png)\n\n\n\n* ==第二种增量导入实现==\n\n  * ==基于时间列的增量数据导入（LastModified方式）==\n\n    * 此方式要求原有表中有time字段，它能指定一个时间戳\n      * user表结构和数据\n\n    ![table_user](assets/table_user.png)\n\n  ~~~shell\n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456  \\\n  --table user \\\n  --incremental lastmodified  \\\n  --check-column createTime  \\\n  --last-value '2019-10-01 10:30:00'  \\\n  --target-dir /sqoop/increment2 \\\n  --m 1\n  \n  ##参数解释\n  --incremental   这里使用基于时间列的增量导入\n  --check-column  时间字段\n  --last-value    指定上一次导入中检查列指定字段最大值\n  --target-dir    数据导入的目录\n  \t\t\t\t如果该目录存在(可能已经有数据)\n  \t\t\t\t再使用的时候需要添加 --merge-key or --append\n  \t\t--merge-key 指定合并key（对于有修改的）\n  \t\t--append    直接追加修改的数据\n  ~~~\n\n  * 提交查看HDFS上的目录看是否有数据生成\n\n![sqoop_increment1](assets/sqoop_increment2.png)\n\n\n\n#### 5.10 mysql表的数据导入到hbase中\n\n* 实现把一张mysql表数据导入到hbase中\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--table emp \\\n--hbase-table  mysqluser \\\n--column-family  f1 \\\n--hbase-create-table \\\n--hbase-row-key id  \\\n--m 1 \n\n\n#参数说明\n--hbase-table  \t\t\t指定hbase表名\n--column-family \t\t指定表的列族\n--hbase-create-table \t表不存在就创建\n--hbase-row-key \t\t指定hbase表的id\n--m  \t\t\t\t\t指定使用的MapTask个数\nlist\nscan 'mysqluser'\ndisable 'mysqluser'\ndrop 'mysqluser'\n\n\n# mysql导入hbase 不同的列族\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--columns id,salary,dept \\\n--table emp \\\n--hbase-table  mysqluser2 \\\n--column-family  f1 \\\n--hbase-create-table \\\n--hbase-row-key id  \\\n--m 1 \n\n\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--columns id,name,deg \\\n--table emp \\\n--hbase-table  mysqluser2 \\\n--column-family  f2 \\\n--hbase-create-table \\\n--hbase-row-key id  \\\n--m 1 \n\n\n~~~\n\n\n\n### 6. Sqoop数据的导出\n\n* 将数据从HDFS把文件导出到RDBMS数据库\n  * 导出前，目标表必须存在于目标数据库中。\n    * 默认操作是从将文件中的数据使用INSERT语句插入到表中\n    * 更新模式下，是生成UPDATE语句更新表数据\n\n\n\n#### 6.1 hdfs文件导出到mysql表中\n\n* 1、数据是在HDFS当中的如下目录/user/hive/warehouse/hive_source，数据内容如下\n\n~~~\n1 zhangsan 20 hubei\n2 lisi 30 hunan\n3 wangwu 40 beijing\n4 xiaoming 50 shanghai\n~~~\n\n* 2、创建一张mysql表\n  * 注意mysql中的这个表一定要先创建！ 不然报错！\n\n~~~sql\nCREATE TABLE  userdb.fromhdfs (\n   id INT DEFAULT NULL,\n   name VARCHAR(100) DEFAULT NULL,\n   age int DEFAULT NULL,\n   address VARCHAR(100) DEFAULT NULL\n) ENGINE=INNODB DEFAULT CHARSET=utf8;\n~~~\n\n* 3、执行导出命令\n\n~~~shell\nsqoop export \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table fromhdfs \\\n--export-dir /user/hive/warehouse/hive_source \\\n--input-fields-terminated-by \" \" \n\n\n##参数解释\n--table \t\t\t\t\t  指定导出的mysql表名\n--export-dir \t\t\t\t  指定hdfs数据文件目录\n--input-fields-terminated-by  指定文件数据字段的分隔符\n~~~\n\n* 4、验证mysql表数据\n\n![fromhdfs](assets/fromhdfs.png)\n\n### 7. Sqoop job\n\n* 将事先定义好的数据导入导出任务按照指定流程运行\n\n* 语法\n\n~~~\nsqoop job (generic-args) (job-args)\n   [-- [subtool-name] (subtool-args)]\n~~~\n\n#### 7.1 创建job\n\n* ==--create==\n  * 创建一个名为myjob,实现从mysql表数据导入到hdfs上的作业\n    * 注意\n      * 在创建job时，==命令\"-- import\" 中间有个空格==\n\n~~~shell\nsqoop job --help\n\n##创建一个sqoop作业\nsqoop job \\\n--create myjob1 \\\n-- import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--target-dir /sqoop/myjob \\\n--delete-target-dir \\\n--m 1\n\n##创建一个sqoop增量导入的作业\nsqoop  job  \\\n--create incrementJob1 \\\n-- import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--table user \\\n--target-dir /sqoop/incrementJob \\\n--incremental append  \\\n--check-column createTime  \\\n--last-value '2019-11-19 16:40:21'  \\\n--m 1\n\n\n# incremental.last.value\n~~~\n\n#### 7.2 验证 job\n\n* ==--list==\n\n* 验证作业是否创建成功\n\n  * 执行如下命令\n\n  ~~~shell\n  sqoop job --list\n  \n  \n  最后显示：\n  Available jobs:\n    myjob\n  ~~~\n\n\n\n#### 7.3 查看job\n\n* ==--show==\n* 查看作业的详细信息\n  * 执行如下命令\n\n~~~shell\nsqoop job --show myjob1\n~~~\n\n\n\n#### 7.4 执行job\n\n* ==--exec==\n\n  * 用于执行保存的作业\n\n  ~~~shell\n  sqoop job --exec myjob1\n  ~~~\n\n  * 解决sqoop需要输入密码的问题\n    * 修改配置文件\n      * vi /kkb/install/sqoop-1.4.6-cdh5.14.2/conf/sqoop-site.xml\n\n  ~~~xml\n  <property>\n      <name>sqoop.metastore.client.record.password</name>\n      <value>true</value>\n      <description>If true, allow saved passwords in the metastore.\n      </description>\n  </property>\n  ~~~\n\n\n\n#### 7.5 删除job\n\n- ==--delete==\n  - 用于删除保存作业\n\n```shell\nsqoop job --delete myjob\n```\n\n","tags":["sqoop"]},{"title":"大数据环境搭建","url":"/2019/11/26/hadoop/大数据环境搭建/","content":"\n# 大数据环境搭建(MAC)\n\n## 1. Linux 环境配置\n\n### 1.1 Vmware Funsion Linux网络配置\n\n#### 1.1.1 查看本机网络\n\n```shell\n# 1. 查看vmnet网络配置，本机mac上\ncat /Library/Preferences/VMware\\ Fusion/vmnet8/nat.conf \n# 2. 可以看到如下部分信息，记住下面信息，用来配置linux\n[host]\n# NAT gateway address\nip = 192.168.83.2\nnetmask = 255.255.255.0\n```\n\n#### 1.1.2 配置linux网络\n\n1. setting -> Network Adapter -> Share with my msc  如下图\n\n<img src=\"assets/image-20191127172534764.png\" alt=\"image-20191127172534764\" style=\"zoom:50%;\" />\n\n2. 点击上图左下角 Advanced options，点击Generate（生成MAC Address）\n\n   <img src=\"assets/image-20191127172700292.png\" alt=\"image-20191127172700292\" style=\"zoom:50%;\" />\n\n3. 启动虚拟机\n\n```shell\n# 1. 修改配置文件\nsudo vi /etc/sysconf/network-scripts/ifcg-ens192\n\n# 2. 修改下面内容\n# 改为静态\nBOOTPROTO=static\nONBOOT=yes\n# 与上面网段保持一致\nIPPADDR=192.168.83.100\n# 与上面一致\nNETMASK=255.255.255.0\n# 与上面ip一致\nGATEWAY=192.168.83.2\nDNS1=8.8.8.8\n\n#3. 修改之后，重启network\nservice network restart\n```\n\n4. 关闭防火墙，selinux\n\n```shell\n# 关闭\nsystemctl stop firewalld\n# 永久关闭\nsystemctl disable firewalld\n\n# 关闭selinux，修改下面文件内容\nvi /etc/selinux/config #进入selinux设置文件\nSELINUX=disabled\n```\n\n### 1.2. Linux 设置免密登陆\n\n#### 1.2.1 设置用户 & 权限\n\n```shell\n# 1. 创建hadoop用户\nuseradd hadoop #添加hadoop用户\npasswd hadoop #给hadoop用户添加密码\nhadoop #密码设为hadoop\n# 2. 设置用户权限 \nvisudo #进入用户权限配置文件\n## Allow root to run any commands anywhere\nroot    ALL=(ALL)       ALL\nhadoop  ALL=(ALL)\t    ALL # 给hadoop用户添加所有权限\n# 3. 切换到hadoop用户\nsu - hadoop # 加上- 表示切换同时拥有权限\n```\n\n#### 1.2.2 免密登陆\n\n1. 修改/etc/hosts 文件\n\n```shell\n   sudo vi /etc/hosts\n   \n   # 添加如下内容\n   192.168.83.100 node01\n   192.168.83.110 node02\n   192.168.83.120 node03\n```\n\n2. 配置免密登陆\n\n```shell\n# 1. hadoop用户下执行下列命令，必须！\n# 下面操作node01，node02，node03 都执行，回车next。\nssh-keygen -t rsa #生成公钥\n\n# 2. 三台机器的公钥全部拷贝到node01\nssh-copy-id node01 \n\n# 3. 第一台机器执行，“:PWD”的意思是：拷贝目标文件位置和node01的位置一致。\ncd /home/hadoop/.ssh/\nscp authorized_keys node02:$PWD #将node01的授权文件拷贝到node02\nscp authorized_keys node03:$PWD #将node01的授权文件拷贝到node03\n\n#4. 验证免密登录\n#在node01执行\nssh node02 #在node01登录node02，不需要密码就ok\nssh node03 #在node01登录node02，不需要密码就ok\n#回到node01\nlogout\t\t\n```\n\n### 1.3 时间同步\n\n#### 1.3.1 同步阿里云\n\n```shell\n# 安装ntpdate\nsudo yum -y install ntpdate\ncrontab -e \n*/1 * * * * /usr/sbin/ntpdate time1.aliyun.com\n\n# 如果时间不通可以执行\nsudo ntpdate -u asia.pool.ntp.org\n```\n\n#### 1.3.2 同步node01时间\n\n```shell\n# 1. 安装ntp软件（所有）\nsudo yum  -y  install ntp\n\n# 2. 设置时区为中国上海（所有）\ntimedatectl set-timezone Asia/Shanghai\n\n# 3. node01启动ntp服务，作为服务端供其他节点同步时间\nsystemctl start ntpd\n\n# 4. node01设置开机启动\nsystemctl enable ntpd\n\n# 5.修改配置 node01上\nsudo vi /etc/ntp.conf\n# 注释掉以下四行，添加最后两行。对应自己的vmnt8内容\n#server 0.centos.pool.ntp.org iburst\n#server 1.centos.pool.ntp.org iburst\n#server 2.centos.pool.ntp.org iburst\n#server 3.centos.pool.ntp.org iburst\n\nrestrict 192.168.83.2 mask 255.255.255.0 nomodify notrap\nserver 127.127.1.0\n\n# 5.1 node02 node03修改配置\nsudo vi /etc/sysconfig/ntpdate\n# 修改为 yes\nSYNC_HWCLOCK=yes\n\n# 6. 重启服务\nsystemctl restart ntpd\n\n```\n\n## 2.Hadoop环境搭建\n\n#### 2.1 配置hadoop-env.sh\n\n```shell\n# hadoop 用户下\nsu - hadoop\n\nvi /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/hadoop-env.sh \nexport JAVA_HOME=/kfly/install/jdk1.8.0_141 #修改为此变量\n```\n\n#### 2.2 配置core-site.xml\n\n```shell\n#在hadoop用户下打开配置文件：\nvi /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/core-site.xml\n```\n\n```xml\n<configuration>\n\t<property>\n\t\t<name>fs.defaultFS</name>\n\t\t<value>hdfs://node01:8020</value>\n\t</property>\n\t<property>\n\t\t<name>hadoop.tmp.dir</name>\n\t\t<value>/kfly/install/hadoop-2.6.0/hadoopDatas/tempDatas</value>\n\t</property>\n\t<!--  缓冲区大小，实际工作中根据服务器性能动态调整 -->\n\t<property>\n\t\t<name>io.file.buffer.size</name>\n\t\t<value>4096</value>\n\t</property>\n  <property>\n    <name>fs.trash.interval</name>\n    <value>10080</value>\n    <description>检查点被删除后的分钟数。 如果为零，垃圾桶功能将被禁用。 \n      该选项可以在服务器和客户端上配置。 如果垃圾箱被禁用服务器端，则检查客户端配置。 \n      如果在服务器端启用垃圾箱，则会使用服务器上配置的值，并忽略客户端配置值。\n    </description>\n  </property>\n\n  <property>\n       <name>fs.trash.checkpoint.interval</name>\n       <value>0</value>\n       <description>垃圾检查点之间的分钟数。 应该小于或等于fs.trash.interval。 \n       如果为零，则将该值设置为fs.trash.interval的值。 每次检查指针运行时，\n       它都会从当前创建一个新的检查点，并删除比fs.trash.interval更早创建的检查点。\n    </description>\n  </property>\n</configuration>\n```\n\n#### 2.3 配置hdfs-site.xml\n\n```shell\n#在hadoop用户下打开配置文件：\nvi /kkb/install/hadoop-2.6.0/etc/hadoop/hdfs-site.xml\n```\n\n```xml\n<configuration>\n\t<!-- NameNode存储元数据信息的路径，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割   --> \n\t<!--   集群动态上下线 \n\t<property>\n\t\t<name>dfs.hosts</name>\n\t\t<value>/kfly/install/hadoop-2.6.0/etc/hadoop/accept_host</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.hosts.exclude</name>\n\t\t<value>/kfly/install/hadoop-2.6.0/etc/hadoop/deny_host</value>\n\t</property>\n\t -->\n\t <property>\n\t\t\t<name>dfs.namenode.secondary.http-address</name>\n\t\t\t<value>node01:50090</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.http-address</name>\n\t\t<value>node01:50070</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.name.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/namenodeDatas</value>\n\t</property>\n\t<!--  定义dataNode数据存储的节点位置，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割  -->\n\t<property>\n\t\t<name>dfs.datanode.data.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/datanodeDatas</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.edits.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/dfs/nn/edits</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.checkpoint.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/dfs/snn/name</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.checkpoint.edits.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/dfs/nn/snn/edits</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.replication</name>\n\t\t<value>3</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.permissions</name>\n\t\t<value>false</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.blocksize</name>\n\t\t<value>134217728</value>\n\t</property>\n</configuration>\n```\n\n#### 2.4 配置mapred-site.xml\n\n```shell\n#在hadoop用户下操作,进入指定文件夹：\ncd /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/\n#由于原来没有mapred-site.xml配置文件，需要根据模板复制一份：\ncp  mapred-site.xml.template mapred-site.xml\nvi /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/mapred-site.xml\n```\n\n```xml\n<!--指定运行mapreduce的环境是yarn -->\n<configuration>\n   <property>\n\t\t<name>mapreduce.framework.name</name>\n\t\t<value>yarn</value>\n\t</property>\n\t<property>\n\t\t<name>mapreduce.job.ubertask.enable</name>\n\t\t<value>true</value>\n\t</property>\n\t<property>\n\t\t<name>mapreduce.jobhistory.address</name>\n\t\t<value>node01:10020</value>\n\t</property>\n\t<property>\n\t\t<name>mapreduce.jobhistory.webapp.address</name>\n\t\t<value>node01:19888</value>\n\t</property>\n</configuration>\n```\n\n#### 2.5 配置yarn-site.xml\n\n```shell\n#在hadoop用户下操作\nvi /kfly/install/hadoop-2.6.0/etc/hadoop/yarn-site.xml\n```\n\n```xml\n<configuration>\n\t<property>\n\t\t<name>yarn.resourcemanager.hostname</name>\n\t\t<value>node01</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.nodemanager.aux-services</name>\n\t\t<value>mapreduce_shuffle</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.log-aggregation-enable</name>\n\t\t<value>true</value>\n\t</property>\n\t<property>\n\t\t <name>yarn.log.server.url</name>\n\t\t <value>http://node01:19888/jobhistory/logs</value>\n\t</property>\n\t<!--多长时间聚合删除一次日志 此处-->\n\t<property>\n        <name>yarn.log-aggregation.retain-seconds</name>\n        <value>2592000</value><!--30 day-->\n\t</property>\n\t<!--时间在几秒钟内保留用户日志。只适用于如果日志聚合是禁用的-->\n\t<property>\n        <name>yarn.nodemanager.log.retain-seconds</name>\n        <value>604800</value><!--7 day-->\n\t</property>\n\t<!--指定文件压缩类型用于压缩汇总日志-->\n\t<property>\n        <name>yarn.nodemanager.log-aggregation.compression-type</name>\n        <value>gz</value>\n\t</property>\n\t<!-- nodemanager本地文件存储目录-->\n\t<property>\n        <name>yarn.nodemanager.local-dirs</name>\n        <value>/kkb/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/yarn/local</value>\n\t</property>\n\t<!-- resourceManager  保存最大的任务完成个数 -->\n\t<property>\n        <name>yarn.resourcemanager.max-completed-applications</name>\n        <value>1000</value>\n\t</property>\n</configuration>\n```\n\n#### 2.6 编辑slaves\n\n此文件用于配置集群有多少个数据节点,我们把node2，node3作为数据节点,node1作为集群管理节点\n\n```shell\n#在hadoop用户下操作\nvim /kfly/install/hadoop-2.6.0/etc/hadoop/slaves\n\nnode01 #添加\nnode02 #添加\nnode03 #添加\n```\n\n#### 2.7 创建文件存放目录\n\n```shell\n#在hadoop用户下操作\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/tempDatas\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits\n```\n\n#### 2.8 拷贝到其他节点\n\n```shell\n# 分发到各个节点下,$PWD 相同目录\nscp -r hadoop-2.6.0 node02:$PWD\n\n#分发配置文件\nscp /etc/profile node02:$PWD\n```\n\n#### 2.9 格式化节点\n\n```shell\n#下面命令只在node01上执行\nhdfs namenode -format #格式化\n\n# 启动\nstart-all.sh\n```\n\n#### 1.10 访问\n\n- hadoop webui http://node01:50070/\n\n- hadoop application http://node01:8088\n\n  ```shell\n  # 启动 jobhistory\n  mr-jobhistory-daemon.sh start historyserver\n  ```\n\n- hadoop job http://node01:19888\n\n#### 2.10. Hadoop Ha高可用\n\n​\t\t[hadoop ha 高可用](https://kfly.top/2019/10/28/zookeeper/zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E6%A1%86%E6%9E%B6%EF%BC%88%E4%BA%8C%EF%BC%89hadoop%20ha%E9%AB%98%E5%8F%AF%E7%94%A8%E5%AE%89%E8%A3%85/)\n\n### 3. hive环境搭建\n\n#### 3.1 mysql安装\n\n```shell\n# 安装wget\nsudo yum install wget\n\n# 1. 换源\n# 1.1 备份系统源\nsudo mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup\n# 1.2 下载阿里云CENTOS7镜像文件\nwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n# 1.3 清理缓存、生成新的缓存\nsudo yum clean all\nsudo yum makecache\n# 1.4 更新源\nsudo yum update -y\n\n# 2. 使用yum安装MySQL,下载并安装MySQL官方的 Yum Repository\nsudo wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm\nsudo yum -y install mysql57-community-release-el7-10.noarch.rpm\nsudo yum -y install mysql-community-server\n\n# 3. 启动mysql，查看运行状态\nsystemctl start  mysqld.service\nsystemctl status mysqld.service\n\n# 4. 找出默认密码如下图所示\nsudo grep \"password\" /var/log/mysqld.log\n\n# 5. 登陆、修改密码\nmysql -uroot -p     # 回车后会提示输入密码\nALTER USER 'root'@'localhost' IDENTIFIED BY 'new password';\n # 注意： 如果失败，修改密码策略,先修改一个复杂的密码，在修改策略，修改密码\nALTER USER 'root'@'localhost' IDENTIFIED BY 'z?guwrBhH7p>';\nset global validate_password_policy=0;\nset global validate_password_policy=1;\n\n# 6. 设置mysql可以外部连接\ngrant all on *.* to root@'%' identified by '数据库密码';\n```\n\n#### 3.2 hive安装\n\n##### 3.2.1 下载hive的安装包\n\n- http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.14.2.tar.gz\n\n##### 3.2.2 hive-env.sh\n\n```shell\nvim hive-env.sh\n\n#配置HADOOP_HOME路径\nexport HADOOP_HOME=/kfly/install/hadoop-2.6.0\n#配置HIVE_CONF_DIR路径\nexport HIVE_CONF_DIR=/kfly/install/hive-1.1.0-cdh5.14.2/conf\n```\n\n##### 3.2.3 hive-site.xml\n\n```xml\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n        <property>\n                <name>javax.jdo.option.ConnectionURL</name>\n                <value>jdbc:mysql://node02:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=latin1&amp;useSSL=false</value>\n        </property>\n\n        <property>\n                <name>javax.jdo.option.ConnectionDriverName</name>\n                <value>com.mysql.jdbc.Driver</value>\n        </property>\n        <property>\n                <name>javax.jdo.option.ConnectionUserName</name>\n                <value>root</value>\n        </property>\n        <property>\n                <name>javax.jdo.option.ConnectionPassword</name>\n                <value>123456</value>\n        </property>\n        <property>\n                <name>hive.cli.print.current.db</name>\n                <value>true</value>\n        </property>\n        <property>\n                <name>hive.cli.print.header</name>\n            <value>true</value>\n        </property>\n    <property>\n                <name>hive.server2.thrift.bind.host</name>\n                <value>node02</value>\n        </property>\n</configuration>\n```\n\n##### 3.2.4 日志路径\n\n```shell\nvim hive-log4j.properties\n\n#更改以下内容，设置我们的日志文件存放的路径\nhive.log.dir=/kkb/install/hive-1.1.0-cdh5.14.2/logs/\n```\n\n##### 3.4.5  lib包\n\n```shell\n ps: ==需要将mysql的驱动包上传到hive的lib目录下==\n  * 例如 mysql-connector-java-5.1.38.jar\n```\n\n### 4. zookeeper环境搭建\n\n##### 4.1 下载软件\n\n​\t\t\t[点击下载 zookeeper-3.4.5-cdh5.14.2.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.14.2.tar.gz)\n\n##### 4.2 修改配置文件\n\n```shell\n# 1. copy配置文件\ncd /kfly/install/zookeeper-3.4.5-cdh5.14.2/conf\ncp zoo_sample.cfg zoo.cfg\n# 2. 创建存放数据目录\nmkdir -p /kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas\n# 编辑\nvim  zoo.cfg\n# 3. 文件内容如下\ndataDir=/kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas\nautopurge.snapRetainCount=3\nautopurge.purgeInterval=1\nserver.1=node01:2888:3888\nserver.2=node02:2888:3888\nserver.3=node03:2888:3888\n\n# 4 分发到各个节点\n# 5. 写入myid文件，myid分别对应，node01:1，node02:2,node03.3、一次累加\necho 1 >  /kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid\n\n# 6. 启动服务、查看状态\nbin/zkServer.sh start\nbin/zkServer.sh status\n```\n\n### 5. HBase环境搭建\n\n##### 5.1 下载软件\n\n​\t\t[点击下载hbase-1.2.0-cdh5.14.2.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.14.2.tar.gz)\n\n##### 5.2  hbase-env.sh\n\n```shell\nexport JAVA_HOME=/kfly/install/jdk1.8.0_141\n# 使用外部的zookeeper集群\nexport HBASE_MANAGES_ZK=false\n```\n\n##### 5.3 hbase-site.xml\n\n```xml\n<configuration>\n\t<property>\n\t\t<name>hbase.rootdir</name>\n\t\t<value>hdfs://node01:8020/HBase</value>  \n\t</property>\n\t<property>\n\t\t<name>hbase.cluster.distributed</name>\n\t\t<value>true</value>\n\t</property>\n\t<!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 -->\n\t<property>\n\t\t<name>hbase.master.port</name>\n\t\t<value>16000</value>\n\t</property>\n\t<property>\n\t\t<name>hbase.zookeeper.quorum</name>\n\t\t<value>node01,node02,node03</value>\n\t</property>\n\t<property>\n\t\t<name>hbase.zookeeper.property.clientPort</name>\n\t\t<value>2181</value>\n\t</property>\n\t<property>\n\t\t<name>hbase.zookeeper.property.dataDir</name>\n\t\t<value>/kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas</value>\n\t</property>\n\t<property>\n\t\t<name>zookeeper.znode.parent</name>\n\t\t<value>/HBase</value>\n\t</property>\n</configuration>\n```\n\n##### 5.4  regionservers\n\n```shell\n# 配置文件 目录 conf下\n vim regionservers\n \n# 内容如下\nnode01\nnode02\nnode03\n```\n\n##### 5.5  back-masters\n\n- 创建back-masters配置文件，里边包含备份HMaster节点的主机名，每个机器独占一行，实现HMaster的高可用\n\n```shell\n[hadoop@node01 conf]$ vim backup-masters\n```\n\n- 将node02作为备份的HMaster节点，问价内容如下\n\n```properties\nnode02\n```\n\n##### 5.6  创建软连接\n\n- **<font color='red'>注意：三台机器</font>**均做如下操作\n\n- 因为HBase集群需要读取hadoop的core-site.xml、hdfs-site.xml的配置文件信息，所以我们==三台机器==都要执行以下命令，在相应的目录创建这两个配置文件的软连接\n\n```shell\nln -s /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/core-site.xml  /kfly/install/hbase-1.2.0-cdh5.14.2/conf/core-site.xml\n\nln -s /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/hdfs-site.xml  /kfly/install/hbase-1.2.0-cdh5.14.2/conf/hdfs-site.xml\n\n```\n\n##### 5.7 添加HBase环境变量\n\n- **<font color='red'>注意：三台机器</font>**均执行以下命令，添加环境变量\n\n```shell\nsudo vim /etc/profile\n# 添加如下\nexport HBASE_HOME=/kkb/install/hbase-1.2.0-cdh5.14.2\nexport PATH=$PATH:$HBASE_HOME/bin\n\n# 立即生效\nsource /etc/profile\n```\n\n##### 5.8  HBase的启动与停止\n\n- <font color='red'>需要提前启动HDFS及ZooKeeper集群</font>\n\n- 第一台机器node01（HBase主节点）执行以下命令，启动HBase集群\n\n```shell\n[hadoop@node01 ~]$ start-hbase.sh\n\n#HMaster节点上启动HMaster命令\nhbase-daemon.sh start master\n\n#启动HRegionServer命令\nhbase-daemon.sh start regionserver\n```\n\n- 浏览器页面访问\n\n  http://node01:60010\n\n### 6. Flume环境搭建\n\n##### 1. 下载软件\n\n​\t\t\t[点击下载 flume-ng-1.6.0-cdh5.14.2.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.2.tar.gz)\n\n##### 2. flume-env.sh\n\n```properties\nexport JAVA_HOME=/kfly/install/jdk1.8.0_141\n```\n\n### 7. Sqoop环境搭建\n\n##### 7.1. 下载软件\n\n[点击下载 sqoop-1.4.6-cdh5.14.2.tar.gz ](http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.14.2.tar.gz)\n\n##### 7.2. sqoop-env.sh\n\n```shell\n#Set path to where bin/hadoop is available\nexport HADOOP_COMMON_HOME=/kfly/install/hadoop-2.6.0\n\n#Set path to where hadoop-*-core.jar is available\nexport HADOOP_MAPRED_HOME=/kfly/install/hadoop-2.6.0\n\n#set the path to where bin/hbase is available\nexport HBASE_HOME=/kfly/install/hbase-1.2.0-cdh5.14.2\n\n#Set the path to where bin/hive is available\nexport HIVE_HOME=/kfly/install/hive-1.1.0-cdh5.14.2\n```\n\n\n\n### 8. zakaban环境搭建\n\n##### 8.1 下载软件\n\n​\t[点击下载 ](https://azkaban.github.io/downloads.html)\n\n- azkaban-web-server-2.5.0.tar.gz\n- azkaban-executor-server-2.5.0.tar.gz\n- azkaban-sql-script-2.5.0.tar.gz\n\n##### 8.2 azkaban web服务器安装\n\n###### 8.2.1 配置SSL安全访问协议\n\n~~~shell\n# 1. 创建安装目录，解压、解压文件重命名\nmkdir /kfly/install/azkaban\ntar –zxvf azkaban-web-server-2.5.0.tar.gz -C /kfly/install/azkaban\nmv /kkb/install/azkaban/azkaban-web-2.5.0 /kkb/install/azkaban/server\n\n# 2. 在server目下执行下边的命令\nkeytool -keystore keystore -alias jetty -genkey -keyalg RSA\n          # Keytool:   是java数据证书的管理工具，使用户能够管理自己的公/私钥对及相关证书。\n          # -keystore：指定密钥库的名称及位置(产生的各类信息将不在.keystore文件中)\n          # -alias：   对我们生成的.keystore 进行指认别名；如果没有默认是mykey\n          # -genkey：  在用户主目录中创建一个默认文件\".keystore\" \n          # -keyalg：  指定密钥的算法 RSA/DSA 默认是DSA\n\n          # 运行此命令后,会提示输入当前生成 keystore的密码及相应信息,输入的密码请劳记\n         -------------------------------------------------------------------\n            输入keystore密码： \n            再次输入新密码:\n            您的名字与姓氏是什么？\n              [Unknown]： \n            您的组织单位名称是什么？\n              [Unknown]： \n            您的组织名称是什么？\n              [Unknown]： \n            您所在的城市或区域名称是什么？\n              [Unknown]： \n            您所在的州或省份名称是什么？\n              [Unknown]： \n            该单位的两字母国家代码是什么\n              [Unknown]：  CN\n            CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？\n              [否]：  y\n\n              输入<jetty>的主密码\n                      （如果和 keystore 密码相同，按回车）： \n              再次输入新密码:\n          #完成上述工作后,将在当前目录生成 keystore 证书文件,\n# 3. 将keystore 考贝到 azkaban webserver 服务器根目录中.\ncp keystore /kfly/install/azkaban/server\n\n# 4. 配置时区、先生成时区配置文件Asia/Shanghai，用交互式命令 tzselect 即可.\ntzselect \n  \t# 选5 --->选9---->选1----->选1\n# 4.1、拷贝该时区文件，覆盖系统本地时区配置\ncp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  \n\n# 5.修改配置文件\n\n# 5.2 \n~~~\n\n###### 8.2.2 修改配置文件\n\n- 1. azkaban.properties\n\n~~~shell\nvim /kfly/install/azkaban/server/conf/azkaban.properties\n\n#内容说明如下:\n#Azkaban Personalization Settings\nazkaban.name=Test                   #服务器UI名称,用于服务器上方显示的名字\nazkaban.label=My Local Azkaban      #描述\nazkaban.color=#FF3601               #UI颜色\nazkaban.default.servlet.path=/index    \nweb.resource.dir=web/                 #默认根web目录\ndefault.timezone.id=Asia/Shanghai     #默认时区,已改为亚洲/上海 默认为美国\n \n#Azkaban UserManager class\nuser.manager.class=azkaban.user.XmlUserManager   #用户权限管理默认类\nuser.manager.xml.file=conf/azkaban-users.xml     #用户配置,具体配置参加下文\n \n#Loader for projects\nexecutor.global.properties=conf/global.properties    #global配置文件所在位置\nazkaban.project.dir=projects                                             \n \ndatabase.type=mysql               #数据库类型\nmysql.port=3306                   #端口号\nmysql.host=node03                 #数据库连接IP\nmysql.database=azkaban            #数据库实例名\nmysql.user=root                   #数据库用户名\nmysql.password=123456             #数据库密码\n \n# Velocity dev mode\nvelocity.dev.mode=false          #Jetty服务器属性.\njetty.maxThreads=25              #最大线程数\njetty.ssl.port=8443              #Jetty SSL端口\njetty.port=8081                  #Jetty端口\njetty.keystore=keystore          #SSL文件名\njetty.password=123456            #SSL文件密码\njetty.keypassword=123456         #Jetty主密码 与 keystore文件相同\njetty.truststore=keystore        #SSL文件名\njetty.trustpassword=123456       #SSL文件密码\n \n# 执行服务器属性\nexecutor.port=12321               #执行服务器端口\n \n# 邮件设置\nmail.sender=xxxxxxxx@163.com        #发送邮箱\nmail.host=smtp.163.com              #发送邮箱smtp地址\nmail.user=xxxxxxxx                  #发送邮件时显示的名称\nmail.password=**********            #邮箱密码\njob.failure.email=xxxxxxxx@163.com  #任务失败时发送邮件的地址\njob.success.email=xxxxxxxx@163.com  #任务成功时发送邮件的地址\nlockdown.create.projects=false       \ncache.directory=cache                #缓存目录\n~~~\n\n- 2. azkaban-users.xml\n\n```shell\nvim /kfly/install/azkaban/server/conf/azkaban-users.xml\n```\n\n~~~xml\n<azkaban-users>\n<user username=\"azkaban\" password=\"azkaban\" roles=\"admin\"groups=\"azkaban\"/>\n<user username=\"metrics\" password=\"metrics\" roles=\"metrics\"/>\n <!--新增admin用户--> \n<user username=\"admin\" password=\"admin\" roles=\"admin,metrics\" />\n<role name=\"admin\" permissions=\"ADMIN\" />\n<role name=\"metrics\" permissions=\"METRICS\"/>\n</azkaban-users>\n~~~\n\n##### 8.3 azkaban 执行服器安装\n\n```shell\n# 1. 解压azkaban-executor-server-2.5.0.tar.gz,重命名文件\ntar -zxvf azkaban-executor-server-2.5.0.tar.gz -C /kfly/install/azkaban\nmv /kfly/install/azkaban/azkaban-executor-2.5.0 /kfly/install/azkaban/executor\n\n# 2. 修改配置文件\nvim /kfly/install/azkaban/executor/conf/azkaban.properties\n      # 内容如下----------\n      #Azkaban   #时区\n      default.timezone.id=Asia/Shanghai          \n\n      #数据库设置----->需要修改的信息\n      mysql.host=node3          #数据库IP地址\n      mysql.database=azkaban    #数据库实例名\n      mysql.user=root           #数据库用户名\n      mysql.password=123456     #数据库密码\n```\n\n##### 8.4 azkaban脚本导入\n\n~~~shell\n# 1. 解压azkaban-sql-script-2.5.0.tar.gz\ntar -zxvf azkaban-sql-script-2.5.0.tar.gz -C /kfly/install/azkaban\n\n# 2. 把解压后的脚本导入到mysql中、进入到mysql\nmysql> create database azkaban;\nmysql> use azkaban;\nmysql> source /kfly/install/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql;\n~~~\n\n##### 8.5 Azkaban启动\n\n* 在azkaban web server服务器目录下执行启动命令\n\n~~~shell\n# 1. 启动web server服务、在azkaban web server服务器目录下执行启动命令\nbin/azkaban-web-start.sh\n# 2. 启动executor执行服务、在azkaban executor服务器目录下执行启动命令\nbin/azkaban-executor-start.sh\n\n\n~~~\n\n* 启动完成后,在浏览器(建议使用谷歌浏览器)中输入==https://服务器IP地址:8443== ,即可访问azkaban服务了.在登录中输入刚才新的户用名及密码,点击 login.\n* 输入“IP地址:8443”无法访问 web 页面, 且后台报错，原因是浏览器安全证书限制\n* 解决办法：使用“https://ip:8443”访问, 发现已经可以访问了, 后台会报证数问题的错误, 忽略即可, 不影响使用, 选择高级 ------> 继续访问该网站\n\n<img src=\"assets/azkaban-web.png\" alt=\"azkaban-web\" style=\"zoom: 33%;\" />\n\n\n\n<img src=\"assets/azkaban界面.png\" alt=\"azkaban界面\" style=\"zoom:33%;\" />\n\n~~~\n（1）、projects：azkaban最重要的一部分，创建一个工程，将所有的工作流放在工程中执行\n（2）、scheduling：定时调度任务用的\n（3）、executing:  显示当前运行的任务\n（4）、History : 显示历史运行任务\n\n一个project由3个按钮：\n（1）、Flows：一个工作流，由多个job组成\n（2）、Permissions:权限管理\n（3）、Project Logs：工程日志信息\n~~~\n\n","tags":["环境搭建"]},{"title":"大数据之批流处理的未来之路Flink","url":"/2019/11/22/flink/大数据之批流处理的未来之路Flink/","content":"\n# 大数据批流处理的未来之路\n\n## 1 Flink介绍\n\n```\n早起柏林工业大学联合发起的一个关于数据库的研究项目，叫做：stratorsphere。直到2014年4月份捐献给apache基金会，称为apache基金会的孵化项目， 在孵化期间项目stratosphere改名为Flink 随后到2014年12月，该项目成为了apache基金会的顶级项目。\n目前新的flink版本已经到了1.9\n```\n\n- Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态的计算。 \n- 官网地址:http://flink.apache.org \n\n![img](https://flink.apache.org/img/flink-home-graphic.png)\n\n- 上图分为三部分\n\n  1. 首先要有数据，负责接收数据\n\n  2. 中间就是进行计算的部分，具体对数据处理的地方\n\n  3. 最终数据输出的地方，把结果存储在某地方\n\n### 1.1 核心功能\n\n- 同时支持高吞吐、低延迟、高性能\n  - Sparck Core： 支持高吞吐、高性能： 相对延迟较高\n  - Stream ：低延迟、高性能框架\n- 支持事件时间（Event Time）概念\n- 支持有状态的计算\n- 支持高度灵活的窗口操作（流式计算）\n- 基于轻量级的分布式快照（Snapshot）容错\n- 基于JVM实现独立的内存管理\n- SavePoints保存点\n\n### 1.2 应用场景\n\n- 实时智能推荐系统\n  - 今日头条广告： 与淘宝共享检索数据，实时推送广告\n- 复杂事件处理\n- 实时欺诈监测\n- 实时数仓ETL\n- 流数据分析\n- 实时报表分析\n\n## 2 Flink的基础架构\n### 2.1 基础组建栈\n\n![image-20191122163707117](assets/image-20191122163707117.png)\n\n### 2.2 基础架构图\n\n![image-20191122163747496](assets/image-20191122163747496.png)\n\n## 3 Flient API开发\n\n### 3.1 批处理开发\n\n==使用DataSet Api开发批处理程序==\n\n- 创建maven工程依赖\n\n```xml\n  <dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-scala_2.12</artifactId>\n    <version>1.8.1</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-clients_2.12</artifactId>\n    <version>1.8.1</version>\n  </dependency>\n```\n\n  - Scala代码\n\n```scala\n    package flink\n    \n    import org.apache.flink.api.scala._\n    \n    /**\n     * 通过Scala语言开发Flink 批处理程序\n     */\n    object ScalaWorkCount {\n    \n      def main(args: Array[String]): Unit = {\n    \n        // 获取Flink批处理执行环境\n        var env:ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment;\n    \n        // 初始化原数据\n        var data:DataSet[String] = env.fromCollection(List(\"hadoop mapreduce\",\"hadoop spark\",\"spark core\"))\n    \n        // 数据处理，切分每一行数据获取所有单词\n        var words : DataSet[String] = data.flatMap(_.split(\" \"))\n    \n        // 把每个单词记为1，封装成元组\n        var wordAndOne:DataSet[(String,Int)] = words.map((_,1))\n    \n        // 按照单词进行分组\n        var groupByWord:GroupedDataSet[(String,Int)] = wordAndOne.groupBy(0)\n    \n        // 对相同的单词进行分组\n        var aggregateDataSet: AggregateDataSet[(String,Int)] = groupByWord.sum(1)\n    \n        // 打印\n        aggregateDataSet.print()\n      }\n    }\n    \n```\n\n  - Java代码\n\n```java\n    package flink;\n    \n    import org.apache.flink.api.common.functions.FlatMapFunction;\n    import org.apache.flink.api.java.DataSet;\n    import org.apache.flink.api.java.ExecutionEnvironment;\n    import org.apache.flink.api.java.operators.DataSource;\n    import org.apache.flink.api.java.tuple.Tuple2;\n    \n    /**\n     * @author dingchuangshi\n     */\n    public class ScalaWorkCountJava {\n        public static void main(String[] args) throws Exception {\n    \n            // 获取Flink批处理执行环境\n            ExecutionEnvironment env = ExecutionEnvironment.createCollectionsEnvironment();\n    \n            // 初始化原数据\n            DataSource<String> data = env.fromElements(\"hadoop mapreduce\", \"hadoop spark\", \"spark core\");\n    \n            // 数据处理\n            DataSet<Tuple2<String, Integer>> words = data\n                    .flatMap((FlatMapFunction<String, Tuple2<String,Integer>>)(s,out)->{\n                        for (String word: s.split(\" \")) {\n                            out.collect(new Tuple2<String,Integer>(word,1));\n                        }\n                    })\n                    .groupBy(0)\n                    .sum(1);\n    \n            // sink\n            words.print();\n        }\n    }\n    \n```\n\n### 3.2 流式处理开发\n\n```xml\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-scala_2.12</artifactId>\n  <version>1.8.1</version>\n</dependency>\n```\n\n- Scala代码\n\n```scala\n  package flink\n\n\nimport org.apache.flink.streaming.api.scala._\nimport org.apache.flink.streaming.api.windowing.time.Time\nimport org.apache.flink.streaming.api.windowing.windows.TimeWindow\n\n/**\n  * 基于scala语言开发Flink的流式处理程序\n */\nobject ScalaStreamWordCount {\n\n  def main(args: Array[String]): Unit = {\n\n       //1. 获取flink的流式处理环境\n      val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n\n      //2. 构建source数据源\n      val socketTextStream: DataStream[String] = env.socketTextStream(\"192.168.18.238\",9999)\n\n      //3. 数据处理\n      //3.1 切分每一行，获取所有的单词\n      val words: DataStream[String] = socketTextStream.flatMap(_.split(\" \"))\n\n      //3.2 把每个单词计为1 封装成元组(单词，1)\n      val wordAndOne: DataStream[(String, Int)] = words.map((_,1))\n\n      //3.3 按照单词进行分组\n      val groupByWord: KeyedStream[(String, Int), String] = wordAndOne.keyBy(_._1)\n\n      //3.4 设置时间窗口\n      val timeWindow: WindowedStream[(String, Int), String, TimeWindow] = groupByWord.timeWindow(Time.seconds(5))\n\n      //3.5 相同单词出现的1累加\n      val result: DataStream[(String, Int)] = timeWindow.reduce((v1,v2)=> (v1._1,v1._2+v2._2))\n\n      //4. 构建Sink\n      result.print()\n\n      //5. 启动流式应用程序\n      env.execute(\"ScalaStreamWordCount\")\n  }\n}\n```\n\n  \n\n- 模拟一个socket服务器\n\n```shell\nnc -lk 9999\n```\n\n\n\n\n## 4 Flink的Table & SQL API\n\n![image-20191122170247587](assets/image-20191122170247587.png)\n\n- Table Api是一种关系型Api，类SQL的API。 用户可以像操作表一样的操作数据，非常直观和方便。\n\n- Table & SQL API的出现可以解决流处理和批处理统一的API层\n\n  - 批处理上的查询会随着输入数据的结束而结束并生成有限的结果集。\n  - 流处理上的查询会一直运行并生成结果流\n  - Table & SQL API 做到了批与流上的查询具有相同的语法，因此不用修改代码就能同时实现批和流。\n\n  ```xml\n  <dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-table-planner_2.11</artifactId> \n    <version>1.8.1</version>\n  </dependency>\n  ```\n  \n  \n  \n### 4.1 批处理开发\n\n  ```scala\n  package flink\n  \n  import org.apache.flink.api.scala._\n  import org.apache.flink.core.fs.FileSystem.WriteMode\n  import org.apache.flink.table.api.{Table, Types}\n  import org.apache.flink.table.api.scala.BatchTableEnvironment\n  import org.apache.flink.table.sinks.CsvTableSink\n  import org.apache.flink.table.sources.CsvTableSource\n  \n  /**\n   * 基于table & sql api开发 Flink的批处理程序\n   */\n  object ScalaTableSQLBatchWordCount {\n  \n    def main(args: Array[String]): Unit = {\n  \n        //1. 获取flink的table批处理环境\n        val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment\n  \n        val tEnv: BatchTableEnvironment = BatchTableEnvironment.create(env)\n  \n        //2.构建Source数据源\n        val tableSource: CsvTableSource = CsvTableSource.builder()\n          .path(\"./data/person.txt\")\n          .fieldDelimiter(\" \")\n          .field(\"id\", Types.INT)\n          .field(\"name\", Types.STRING)\n          .field(\"age\", Types.INT)\n          .ignoreParseErrors().lineDelimiter(\"\\r\\n\")\n          .build()\n  \n        //将tableSource注册成表 tEnv.registerTableSource(\"person\",tableSource)\n        //3. 查询\n        //3.1 查询年龄大于30岁的人\n        val result1: Table = tEnv.scan(\"person\").filter(\"age > 30\")\n  \n        //3.2 统计不同的年龄用户数\n        val result2: Table = tEnv.sqlQuery(\"select age,count(*) from person group by age \")\n  \n        //4. 构建sink //打印表的元数据schema信息\n        result1.printSchema()\n  \n        //保存结果数据到文件中 val tableSink1 = new\n        var tableSink1 = new CsvTableSink(\"./out/result1.txt\", \"\\t\", 1, WriteMode.OVERWRITE)\n        result1.writeToSink(tableSink1)\n  \n        val tableSink2 = new CsvTableSink(\"./out/result2.txt\", \"\\t\", 1, WriteMode.OVERWRITE)\n        result2.writeToSink(tableSink2)\n  \n        //开启计算\n        env.execute()\n    }\n  }\n  ```\n\n  \n\n### 4.2 流处理开发\n\n```scala\npackage flink\n\nimport org.apache.flink.core.fs.FileSystem.WriteMode\nimport org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}\nimport org.apache.flink.table.api.Table\nimport org.apache.flink.table.api.scala.StreamTableEnvironment\nimport org.apache.flink.table.sinks.CsvTableSink\nimport org.apache.flink.api.scala._\n\n/**\n * 基于table & sql api开发 Flink的流式处理程序\n */\nobject ScalaTableSQLStreamWordCount {\n\n  case class User(id:Int,name:String,age:Int)\n\n  def main(args: Array[String]): Unit = {\n\n    //1. 获取flink的table流式处理的环境\n    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n\n    val streamSQLEnv: StreamTableEnvironment = StreamTableEnvironment.create(env)\n    //2. 构建Source数据源\n    /**\n      * 101,zhangsan,18\n      * 102,lisi,20\n      * 103,wangwu,25\n      * 104,zhaoliu,15\n      */\n    val socketDataStream: DataStream[String] = env.socketTextStream(\"node01\",9999)\n\n    val userDataStream: DataStream[User] = socketDataStream.map(x=>x.split(\",\")).map(y=>User(y(0).toInt,y(1),y(2).toInt ))\n\n    //3. 将流注册成一张表\n    streamSQLEnv.registerDataStream(\"userTable\",userDataStream)\n\n    //4. 使用table && sql api来查询数据\n\n     // 使用table 的api查询年龄大于20岁的人\n    val result1: Table = streamSQLEnv.scan(\"userTable\").filter(\"age >20\")\n\n    //使用sql 的api查询\n    val result2: Table = streamSQLEnv.sqlQuery(\" select * from userTable \")\n\n    //5. 构建Sink\n    val tableSink1 = new CsvTableSink(\"./out/tableSink1.txt\",\"\\t\",1,WriteMode.OVERWRITE)\n    result1.writeToSink(tableSink1)\n\n    val tableSink2 = new CsvTableSink(\"./out/tableSink2.txt\",\"\\t\",1,WriteMode.OVERWRITE)\n    result2.writeToSink(tableSink2) //开启执行流式计算\n\n    env.execute() }\n}\n```\n\n### 4.3 批流一体处理逻辑\n\n#### 4.3.1 初始化环境\n\n![image-20191122192954640](assets/image-20191122192954640.png)\n\n#### 4.3.2 获取一个table\n\n![image-20191122193054844](assets/image-20191122193054844.png)\n\n#### 4.2.3 输出一个table\n\n![image-20191122193124451](assets/image-20191122193124451.png)\n\n#### 4.2.4 Table API\n\n![image-20191122193206331](assets/image-20191122193206331.png)","tags":["hadoop","flink"]},{"title":"Flume日志采集框架","url":"/2019/11/21/flume-sqoop-zakaban/Flume日志采集框架/","content":"\n# Flume日志采集框架\n\n### 1. Flume是什么 \n\n![image-20191126223347760](assets/image-20191126223347760.png)\n\n~~~\n\t在一个完整的离线大数据处理系统中，除了hdfs+mapreduce+hive组成分析系统的核心之外，还需要数据采集、结果数据导出、任务调度等不可或缺的辅助系统，而这些辅助工具在hadoop生态体系中都有便捷的开源框架。\n~~~\n\n* Flume是Cloudera提供的一个高可用的，高可靠的，分布式的==海量日志采集、聚合和传输的系统==\n* Flume支持在日志系统中定制各类数据发送方，用于收集数据；\n* Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。\n\n重构后的版本统称为 Flume NG（next generation）；改动的另一原因是将 Flume 纳入 apache 旗下，cloudera Flume 改名为 Apache Flume。\n\n　　备注：Flume参考资料\n\n　　　　官方网站： http://flume.apache.org/\n　　　　用户文档： http://flume.apache.org/FlumeUserGuide.html\n　　　　开发文档： http://flume.apache.org/FlumeDeveloperGuide.html\n\n### 2. Flume的架构\n\n* Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。\n* Flume分布式系统中==最核心的角色是agent==，flume采集系统就是由一个个agent所连接起来形成。\n\n#### 2.1 核心概念\n\n- Client：\n  \n  - Client生产数据，运行在一个独立的线程。\n  \n- Event：\n  \n  -  一个数据单元，消息头和消息体组成。（Events可以是日志记录、 avro 对象等。）\n  \n- Flow：\n  \n  -  Event从源点到达目的点的迁移的抽象。\n  \n- Agent：\n  \n  - 一个独立的Flume进程，包含组件Source、 Channel、 Sink。（Agent使用JVM 运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。）\n  \n- Source：\n  \n     -  数据收集组件。（source从Client收集数据，传递给Channel）\n     \n- Channel：\n\n     -  中转Event的一个临时存储，保存由Source组件传递过来的Event。（Channel连接 sources 和 sinks ，这个有点像一个队列。）\n\n- Sink： \n\n     - 从Channel中读取并移除Event， 将Event传递到FlowPipeline中的下一个Agent（如果有的话）（Sink从Channel收集数据，运行在一个独立线程。） \n\n#### 2.2 Agent结构\n\n　Flume 运行的核心是 Agent。Flume以agent为最小的独立运行单位。一个agent就是一个JVM。它是一个完整的数据收集工具，含有三个核心组件，分别是\n\n　　 source、 channel、 sink。通过这些组件， Event 可以从一个地方流向另一个地方，如下图所示。\n\n　　![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108130603872-780242084.png)\n\n#### 2.3 \t Source\n\n- 　\tSource是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入Channel中。 Flume提供了很多内置的Source， 支持 Avro， log4j， syslog 和 http post(body为json格式)。可以让应用程序同已有的Source直接打交道，如AvroSource，SyslogTcpSource。 如果内置的Source无法满足需要， Flume还支持自定义Source。\n  　　![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108130818481-670496307.png)\n\n- 　　source类型：\n\n![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108130931325-512757774.png)\n\n#### 2.4、Channel\n\n- 　　Channel是连接Source和Sink的组件，大家可以将它看做一个数据的缓冲区（数据队列），它可以将事件暂存到内存中也可以持久化到本地磁盘上， 直到Sink处理完该事件。介绍两个较为常用的Channel， MemoryChannel和FileChannel。\n\n- 　　Channel类型：\n\n　　![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108131248653-2068131817.png)\n\n#### 2.4、Sink\n\n- 　　Sink从Channel中取出事件，然后将数据发到别处，可以向文件系统、数据库、 hadoop存数据， 也可以是其他agent的Source。在日志数据较少时，可以将数据存储在文件系统中，并且设定一定的时间间隔保存数据。\n\n![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108131438466-1752568401.png)\n\n- 　　Sink类型：\n\n　　![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108131516809-1930573599.png)\n\n### 3. Flume采集系统结构图\n\n#### 3.1 简单结构\n\n* 单个agent采集数据\n\n![](assets/flume.png)\n\n\n\n#### 3.2 复杂结构\n\n* 2个agent串联\n\n![UserGuide_image03](assets/UserGuide_image03.png)\n\n\n\n* 多个agent串联\n\n![](assets/UserGuide_image02.png)\n\n\n\n* 多个channel\n\n![](assets/UserGuide_image04.png)\n\n\n\n### 4. Flume安装部署\n\n==Flume安装很简单，解压好基本上就可以使用==\n\n* 1、下载安装包\n\n  - <http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.2.tar.gz>\n  - flume-ng-1.6.0-cdh5.14.2.tar.gz\n\n* 2、规划安装目录\n\n  - /kkb/install\n\n* 3、上传安装包到服务器\n\n* 4、解压安装包到指定的规划目录\n\n  - tar -zxvf flume-ng-1.6.0-cdh5.14.2.tar.gz -C /kkb/install\n\n* 5、重命名解压目录\n\n  - mv apache-flume-1.6.0-cdh5.14.2-bin  flume-1.6.0-cdh5.14.2\n\n* 6、修改配置\n\n  * 进入到flume安装目录下的conf文件夹中\n\n    * 先重命名文件\n\n      * mv flume-env.sh.template flume-env.sh\n\n    * 修改文件，添加java环境变量\n\n      * vim flume-env.sh\n\n      ~~~shell\n      export JAVA_HOME=/kkb/install/jdk1.8.0_141\n      ~~~\n\n\n\n### 5. Flume实战\n\n#### 5.1 采集文件到控制台\n\n- 1、需求描述\n\n  ```\n  监控一个文件如果有新增的内容就把数据采集之后打印控制台，通常用于测试/调试目的\n  ```\n\n- 2、==flume配置文件开发==\n\n  * 在flume的安装目录下创建一个文件夹myconf， 后期存放flume开发的配置文件\n    * mkdir /kfly/install/flume-1.6.0-cdh5.14.2/myconf\n\n  - vim tail-memory-logger.conf\n\n  ```properties\n  # Name the components on this agent\n  #定义一个agent，分别指定source、channel、sink别名\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  #配置source\n  #指定source的类型为exec，通过Unix命令来传输结果数据\n  a1.sources.r1.type = exec\n  #监控一个文件，有新的数据产生就不断采集走\n  a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n  #指定source的数据流入的channel中\n  a1.sources.r1.channels = c1\n  \n  #配置channel\n  #指定channel的类型为memory\n  a1.channels.c1.type = memory\n  #指定channel的最多可以存放数据的容量\n  a1.channels.c1.capacity = 1000\n  #指定在一个事务中source写数据到channel或者sink从channel取数据最大条数\n  a1.channels.c1.transactionCapacity = 100\n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  #类型是日志格式，结果会打印在控制台\n  a1.sinks.k1.type = logger\n  ```\n\n- **3、启动agent**\n\n  * 进入到node01上的/kkb/install/flume-1.6.0-cdh5.14.2目录下执行\n\n  ```shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/tail-memory-logger.conf -Dflume.root.logger=info,console\n  \n  \n  其中：\n  -n表示指定该agent名称\n  -c表示配置文件所在的目录\n  -f表示配置文件的路径名称\n  -D表示指定key=value键值对---这里指定的是启动的日志输出级别\n  ```\n\n\n\n#### 5.2 采集文件到HDFS\n\n- 1、需求描述\n\n  ```\n  监控一个文件如果有新增的内容就把数据采集到HDFS上\n  ```\n\n- 2、结构示意图\n\n![file-Flume-HDFS](assets/file-Flume-HDFS.png)\n\n- ==3、flume配置文件开发==\n\n  - vim file2Hdfs.conf\n\n  ```properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  #配置source\n  a1.sources.r1.type = exec\n  a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n  a1.sources.r1.channels = c1\n  \n  #配置channel\n  a1.channels.c1.type = file\n  #设置检查点目录--该目录是记录下event在数据目录下的位置\n  a1.channels.c1.checkpointDir=/kfly/data/flume_checkpoint\n  #数据存储所在的目录\n  a1.channels.c1.dataDirs=/kfly/data/flume_data\n  \n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  #指定sink类型为hdfs\n  a1.sinks.k1.type = hdfs\n  #指定数据收集到hdfs目录\n  a1.sinks.k1.hdfs.path = hdfs://node01:8020/tailFile/%Y-%m-%d/%H%M\n  #指定生成文件名的前缀\n  a1.sinks.k1.hdfs.filePrefix = events-\n  \n  #是否启用时间上的”舍弃”   -->控制目录 \n  a1.sinks.k1.hdfs.round = true\n  #时间上进行“舍弃”的值\n  # 如 12:10 -- 12:19 => 12:10\n  # 如 12:20 -- 12:29 => 12:20\n  a1.sinks.k1.hdfs.roundValue = 10\n  #时间上进行“舍弃”的单位\n  a1.sinks.k1.hdfs.roundUnit = minute\n  \n  # 控制文件个数\n  #60s或者50字节或者10条数据，谁先满足，就开始滚动生成新文件\n  a1.sinks.k1.hdfs.rollInterval = 60\n  a1.sinks.k1.hdfs.rollSize = 50\n  a1.sinks.k1.hdfs.rollCount = 10\n  \n  #每个批次写入的数据量\n  a1.sinks.k1.hdfs.batchSize = 100\n  \n  #开始本地时间戳--开启后就可以使用%Y-%m-%d去解析时间\n  a1.sinks.k1.hdfs.useLocalTimeStamp = true\n  \n  #生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\n  a1.sinks.k1.hdfs.fileType = DataStream\n  ```\n\n- **4、启动agent**\n\n  - 进入到node01上的/kkb/install/flume-1.6.0-cdh5.14.2目录下执行\n\n    ```shell\n    bin/flume-ng agent -n a1 -c myconf -f myconf/file2Hdfs.conf -Dflume.root.logger=info,console\n    ```\n\n\n\n#### 5.3 采集目录到HDFS \n\n* 1、需求描述\n\n  ~~~\n  一个目录中不断有新的文件产生，需要把目录中的文件不断地进行数据收集保存到HDFS上\n  \n  ~~~\n\n* 2、结构示意图\n\n  ![Flume-HDFS](/Users/dingchuangshi/Documents/hexo-kfly-blog/source/_posts/flume/assets/Dir-Flume-HDFS.png)\n\n* ==3、flume配置文件开发==\n\n  * 在myconf目录中创建配置文件添加内容\n    * vim  dir2Hdfs.conf\n\n  ~~~properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  # 配置source\n  ##注意：不能往监控目中重复丢同名文件\n  a1.sources.r1.type = spooldir\n  a1.sources.r1.spoolDir = /kfly/install/flumeData/files\n  # 是否将文件的绝对路径添加到header\n  a1.sources.r1.fileHeader = true\n  a1.sources.r1.channels = c1\n  \n  \n  #配置channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  \n  \n  #配置sink\n  a1.sinks.k1.type = hdfs\n  a1.sinks.k1.channel = c1\n  a1.sinks.k1.hdfs.path = hdfs://node01:8020/spooldir/%Y-%m-%d/%H%M\n  a1.sinks.k1.hdfs.filePrefix = events-\n  a1.sinks.k1.hdfs.round = true\n  a1.sinks.k1.hdfs.roundValue = 10\n  a1.sinks.k1.hdfs.roundUnit = minute\n  a1.sinks.k1.hdfs.rollInterval = 60\n  a1.sinks.k1.hdfs.rollSize = 50\n  a1.sinks.k1.hdfs.rollCount = 10\n  a1.sinks.k1.hdfs.batchSize = 100\n  a1.sinks.k1.hdfs.useLocalTimeStamp = true\n  #生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\n  a1.sinks.k1.hdfs.fileType = DataStream\n  \n  \n  ~~~\n\n\n* **4、启动agent**\n\n\n  * 进入到node01上的/kkb/install/flume-1.6.0-cdh5.14.2目录下执行\n\n  ```shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/dir2Hdfs.conf -Dflume.root.logger=info,console\n\n  ```\n\n\n\n\n#### 5.4 两个agent级联\n\n* 1、需求描述\n\n~~~\n\t第一个agent负责监控某个目录中新增的文件进行数据收集，通过网络发送到第二个agent当中去，第二个agent负责接收第一个agent发送的数据，并将数据保存到hdfs上面去。\n\n~~~\n\n* 2、结构示意图\n\n![](assets/2个agent级联.png)\n\n* 3、在node01和node02上分别都安装flume\n\n* 4、创建node01上的flume配置文件\n\n  * vim dir2avro.conf\n\n  ~~~properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  # 配置source\n  ##注意：不能往监控目中重复丢同名文件\n  a1.sources.r1.type = spooldir\n  a1.sources.r1.spoolDir = /kfly/install/flumeData/files\n  a1.sources.r1.fileHeader = true\n  a1.sources.r1.channels = c1\n  \n  \n  #配置channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  #AvroSink是用来通过网络来传输数据的,可以将event发送到RPC服务器(比如AvroSource)\n  a1.sinks.k1.type = avro\n  \n  #node02 注意修改为自己的hostname\n  a1.sinks.k1.hostname = node02\n  a1.sinks.k1.port = 5211\n  \n  ~~~\n\n* 5、创建node02上的flume配置文件\n\n  * vim avro2Hdfs.conf\n\n  ~~~properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  #配置source\n  #通过AvroSource接受AvroSink的网络数据\n  a1.sources.r1.type = avro\n  a1.sources.r1.channels = c1\n  #AvroSource服务的ip地址\n  a1.sources.r1.bind = node02\n  #AvroSource服务的端口\n  a1.sources.r1.port = 5211\n  \n  #配置channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  a1.sinks.k1.type = hdfs\n  a1.sinks.k1.hdfs.path = hdfs://node01:8020/avro-hdfs/%Y-%m-%d/%H-%M\n  a1.sinks.k1.hdfs.filePrefix = events-\n  a1.sinks.k1.hdfs.round = true\n  a1.sinks.k1.hdfs.roundValue = 10\n  a1.sinks.k1.hdfs.roundUnit = minute\n  a1.sinks.k1.hdfs.rollInterval = 60\n  a1.sinks.k1.hdfs.rollSize = 50\n  a1.sinks.k1.hdfs.rollCount = 10\n  a1.sinks.k1.hdfs.batchSize = 100\n  a1.sinks.k1.hdfs.useLocalTimeStamp = true\n  #生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\n  a1.sinks.k1.hdfs.fileType = DataStream\n  \n  ~~~\n\n* **6、启动agent**\n\n  * 先启动node02上的flume。然后在启动node01上的flume\n\n    * 在node02上的flume安装目录下执行\n\n    ~~~shell\n    bin/flume-ng agent -n a1 -c myconf -f myconf/avro2Hdfs.conf -Dflume.root.logger=info,console\n    \n    ~~~\n\n    * 在node01上的flume安装目录下执行\n\n    ~~~shell\n    bin/flume-ng agent -n a1 -c myconf -f myconf/dir2avro.conf -Dflume.root.logger=info,console\n    \n    ~~~\n\n    * 最后在node01上的/kfly/install/flumeData/files目录下创建一些数据文件，最后去HDFS上查看数据。\n\n\n\n### 6. 高可用配置案例\n\n#### 6.1 failover故障转移\n\n![flume-failover](assets/flume-failover.png)\n\n* 1、节点分配\n\n|    名称    | 服务器主机名 |     ip地址      |    角色    |\n| :--------: | :----------: | :-------------: | :--------: |\n|   Agent1   |    node01    | 192.168.200.200 | WebServer  |\n| Collector1 |    node02    | 192.168.200.210 | AgentMstr1 |\n| Collector2 |    node03    | 192.168.200.220 | AgentMstr2 |\n\n~~~\nAgent1数据分别流入到Collector1和Collector2，Flume NG本身提供了Failover机制，可以自动切换和恢复。\n\n~~~\n\n* 2、开发配置文件\n\n  * node01、node02、node03分别都要安装flume\n\n  * ==创建node01上的flume配置文件==\n\n    * vim flume-client-failover.conf\n\n    ~~~properties\n    #agent name\n    a1.channels = c1\n    a1.sources = r1\n    #定义了2个sink\n    a1.sinks = k1 k2\n    \n    #set gruop\n    #设置一个sink组，一个sink组下可以包含很多个sink\n    a1.sinkgroups = g1\n    \n    #set sink group\n    #指定g1这个sink组下有k1  k2 这2个sink\n    a1.sinkgroups.g1.sinks = k1 k2\n    \n    #set source\n    a1.sources.r1.channels = c1\n    a1.sources.r1.type = exec\n    a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n    \n    #set channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    \n    # set sink1    指定sink1的数据会传输给node02\n    a1.sinks.k1.channel = c1\n    a1.sinks.k1.type = avro\n    a1.sinks.k1.hostname = node02\n    a1.sinks.k1.port = 52020\n    \n    # set sink2    指定sink2的数据会传输给node03\n    a1.sinks.k2.channel = c1\n    a1.sinks.k2.type = avro\n    a1.sinks.k2.hostname = node03\n    a1.sinks.k2.port = 52020\n    \n    #set failover\n    #指定sink组高可用的策略---failover故障转移\n    a1.sinkgroups.g1.processor.type = failover\n    #指定k1这个sink的优先级\n    a1.sinkgroups.g1.processor.priority.k1 = 10\n    #指定k2这个sink的优先级\n    a1.sinkgroups.g1.processor.priority.k2 = 5\n    #指定故障转移的最大时间，如果超时会出现异常\n    a1.sinkgroups.g1.processor.maxpenalty = 10000\n    \n    ~~~\n\n    ~~~properties\n    说明：\n    #这里首先要申明一个sinkgroups,然后再设置2个sink ,k1与k2,其中2个优先级是10和5。\n    #而processor的maxpenalty被设置为10秒，默认是30秒.表示故障转移的最大时间\n    \n    ~~~\n\n    \n\n  * ==创建node02和node03上的flume配置文件==\n  \n  * node02和node03上配置信息相同\n    * vim flume-server-failover.conf\n  \n    ~~~properties\n    #set Agent name\n    a1.sources = r1\n    a1.channels = c1\n    a1.sinks = k1\n    \n    #set channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    # set source\n    a1.sources.r1.type = avro\n    a1.sources.r1.bind = 0.0.0.0\n    a1.sources.r1.port = 52020\n    a1.sources.r1.channels = c1\n    \n    #配置拦截器\n    #指定2个拦截器  i1  i2 \n    a1.sources.r1.interceptors = i1 i2\n    #i1的类型为时间戳拦截器  可以解析%Y-%m-%d 时间\n    a1.sources.r1.interceptors.i1.type = timestamp\n    #i2的类型为主机拦截器，可以获取当前event中携带的主机名\n    a1.sources.r1.interceptors.i2.type = host\n    #指定主机名变量\n    a1.sources.r1.interceptors.i2.hostHeader=hostname\n    \n    #set sink to hdfs\n    a1.sinks.k1.channel = c1\n    a1.sinks.k1.type=hdfs\n    a1.sinks.k1.hdfs.path=hdfs://node01:8020/failover/logs/%{hostname}\n    a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d\n    a1.sinks.k1.hdfs.round = true\n    a1.sinks.k1.hdfs.roundValue = 10\n    a1.sinks.k1.hdfs.roundUnit = minute\n    a1.sinks.k1.hdfs.rollInterval = 60\n    a1.sinks.k1.hdfs.rollSize = 50\n    a1.sinks.k1.hdfs.rollCount = 10\n    a1.sinks.k1.hdfs.batchSize = 100\n    a1.sinks.k1.hdfs.fileType = DataStream\n    \n    ~~~\n\n\n* 3、启动flume配置\n\n  * 先分别在node02和node03上启动flume\n    * 分别进入到flume的安装目录下执行命令\n\n  ~~~shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/flume-server-failover.conf -Dflume.root.logger=info,console\n  \n  ~~~\n\n\n\n  * 然后在node01上启动flume\n    * 进入到flume的安装目录下执行命令\n\n  ~~~shell\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-client-failover.conf -Dflume.root.logger=info,console\n\n  ~~~\n\n\n\n  * 最后在hdfs目录上观察数据\n\n  ~~~shell\nhdfs://node01:8020/failover/logs\n\n  ~~~\n\n\n\n\n#### 6.2 load balance负载均衡\n\n* 实现多个flume采集数据的时候避免单个flume的负载比较高，实现多个flume采集器负载均衡。\n\n* 1、节点分配\n\n  * 与failover故障转移的节点分配\n\n* 2、开发配置文件\n\n  * ==创建node01上的flume配置文件==\n\n    * vim  flume-client-loadbalance.conf\n\n    ~~~properties\n    #agent name\n    a1.channels = c1\n    a1.sources = r1\n    a1.sinks = k1 k2\n    \n    #set gruop\n    a1.sinkgroups = g1\n    \n    #set sink group\n    a1.sinkgroups.g1.sinks = k1 k2\n    \n    #set source\n    a1.sources.r1.channels = c1\n    a1.sources.r1.type = exec\n    a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n    \n    #set channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    \n    # set sink1\n    a1.sinks.k1.channel = c1\n    a1.sinks.k1.type = avro\n    a1.sinks.k1.hostname = node02\n    a1.sinks.k1.port = 52020\n    \n    # set sink2\n    a1.sinks.k2.channel = c1\n    a1.sinks.k2.type = avro\n    a1.sinks.k2.hostname = node03\n    a1.sinks.k2.port = 52020\n    \n    #set load-balance\n    #指定sink组高可用的策略---load_balance负载均衡\n    a1.sinkgroups.g1.processor.type =load_balance\n    # 默认是round_ robin，还可以选择random\n    a1.sinkgroups.g1.processor.selector = round_robin\n    #如果backoff被开启，则sink processor会屏蔽故障的sink\n    a1.sinkgroups.g1.processor.backoff = true\n    \n    ~~~\n\n  * ==创建node02和node03上的flume配置文件==\n\n    * vim  flume-server-loadbalance.conf\n\n\n  ~~~properties\n  #set Agent name\n  a1.sources = r1\n  a1.channels = c1\n  a1.sinks = k1\n  \n  #set channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  \n  # set source\n  a1.sources.r1.type = avro\n  a1.sources.r1.bind = 0.0.0.0\n  a1.sources.r1.port = 52020\n  a1.sources.r1.channels = c1\n  \n  #配置拦截器\n  a1.sources.r1.interceptors = i1 i2\n  a1.sources.r1.interceptors.i1.type = timestamp\n  a1.sources.r1.interceptors.i2.type = host\n  a1.sources.r1.interceptors.i2.hostHeader=hostname\n  #hostname不使用ip显示，直接就是该服务器对应的主机名\n  a1.sources.r1.interceptors.i2.useIP=false\n  \n  #set sink to hdfs\n  a1.sinks.k1.channel = c1\n  a1.sinks.k1.type=hdfs\n  a1.sinks.k1.hdfs.path=hdfs://node01:8020/loadbalance/logs/%{hostname}\n  a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d\n  a1.sinks.k1.hdfs.round = true\n  a1.sinks.k1.hdfs.roundValue = 10\n  a1.sinks.k1.hdfs.roundUnit = minute\n  a1.sinks.k1.hdfs.rollInterval = 60\n  a1.sinks.k1.hdfs.rollSize = 50\n  a1.sinks.k1.hdfs.rollCount = 10\n  a1.sinks.k1.hdfs.batchSize = 100\n  a1.sinks.k1.hdfs.fileType = DataStream\n\n  ~~~\n\n\n* 3、启动flume配置\n\n  - 先分别在node02和node03上启动flume\n    - 分别进入到flume的安装目录下执行命令\n\n  ```shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/flume-server-loadbalance.conf -Dflume.root.logger=info,console\n  \n  ```\n\n\n  * 然后在node01上启动flume\n\n    * 分别进入到flume的安装目录下执行命令\n\n  ~~~shell\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-client-loadbalance.conf -Dflume.root.logger=info,console\n\n  ~~~\n\n\n  - 最后在hdfs上目录观察数据\n\n  ~~~shell\nhdfs://node01:8020/loadbalance/logs\n\n  ~~~\n\n\n### 7. flume企业案例\n\n#### 7.1 flume案例之静态拦截器使用\n\n* 1、案例场景\n\n~~~\nA、B两台日志服务机器实时生产日志主要类型为access.log、nginx.log、web.log \n现在需要把A、B 机器中的access.log、nginx.log、web.log 采集汇总到C机器上然后统一收集到hdfs中。\n但是在hdfs中要求的目录为：\n/source/logs/access/20180101/**\n/source/logs/nginx/20180101/**\n/source/logs/web/20180101/**\n\n~~~\n\n* 2、场景分析\n\n![flume采集不同的日志数据](assets/flume采集不同的日志数据.png)\n\n* 3、数据流程处理分析\n\n![](assets/flume采集不同的日志数据流程分析.png)\n\n* 4、开发配置文件\n\n  * ==在node01与node02服务器开发flume的配置文件==\n\n    * vim exec_source_avro_sink.conf\n\n    ~~~properties\n    # Name the components on this agent\n    #定义三个source\n    a1.sources = r1 r2 r3\n    a1.sinks = k1\n    a1.channels = c1\n    \n    # Describe/configure the source\n    a1.sources.r1.type = exec\n    a1.sources.r1.command = tail -F /kfly/install/flumeData/access.log\n    #指定source r1 使用拦截器i1\n    a1.sources.r1.interceptors = i1\n    #拦截器类型static静态\n    a1.sources.r1.interceptors.i1.type = static\n    ## static拦截器的功能就是往采集到的数据的header中插入自己定义的key-value对\n    # 自己进行设置,我们这里的key和value相当于键值对,k=type v=access\n    a1.sources.r1.interceptors.i1.key = type\n    a1.sources.r1.interceptors.i1.value = access\n    \n    a1.sources.r2.type = exec\n    a1.sources.r2.command = tail -F /kfly/install/flumeData/nginx.log\n    #指定source r2 使用拦截器i2\n    a1.sources.r2.interceptors = i2\n    #拦截器类型static静态\n    a1.sources.r2.interceptors.i2.type = static\n    # 自己进行设置\n    a1.sources.r2.interceptors.i2.key = type\n    a1.sources.r2.interceptors.i2.value = nginx\n    \n    a1.sources.r3.type = exec\n    a1.sources.r3.command = tail -F /kfly/install/flumeData/web.log\n    #指定source r3 使用拦截器i3\n    a1.sources.r3.interceptors = i3\n    #拦截器类型static静态\n    a1.sources.r3.interceptors.i3.type = static\n    # 自己进行设置\n    a1.sources.r3.interceptors.i3.key = type\n    a1.sources.r3.interceptors.i3.value = web\n    \n    # Use a channel which buffers events in memory\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 20000\n    a1.channels.c1.transactionCapacity = 10000\n    \n    # Describe the sink\n    a1.sinks.k1.type = avro\n    a1.sinks.k1.hostname = node03\n    a1.sinks.k1.port = 41414\n    a1.sinks.k1.channel = c1\n    \n    # Bind the source and sink to the channel\n    a1.sources.r1.channels = c1\n    a1.sources.r2.channels = c1\n    a1.sources.r3.channels = c1\n    \n    \n    ~~~\n\n  * ==在node03服务器上开发flume配置文件==\n\n    * vim avro_source_hdfs_sink.conf\n\n    ~~~properties\n    a1.sources = r1\n    a1.sinks = k1\n    a1.channels = c1\n    #定义source\n    a1.sources.r1.type = avro\n    a1.sources.r1.bind = node03\n    a1.sources.r1.port =41414\n    \n    #定义channels\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 20000\n    a1.channels.c1.transactionCapacity = 1000\n    \n    #定义sink\n    a1.sinks.k1.type = hdfs\n    # 此处的%{type} 这里是取我们在node01和node02定义的type的值,也就是value\n    a1.sinks.k1.hdfs.path=hdfs://node01:8020/source/logs/%{type}/%Y%m%d\n    a1.sinks.k1.hdfs.filePrefix =events-\n    a1.sinks.k1.hdfs.fileType = DataStream\n    a1.sinks.k1.hdfs.writeFormat = Text\n    #时间类型\n    a1.sinks.k1.hdfs.useLocalTimeStamp = true\n    #生成的文件不按条数生成\n    a1.sinks.k1.hdfs.rollCount = 0\n    #生成的文件按时间生成\n    a1.sinks.k1.hdfs.rollInterval = 30\n    #生成的文件按大小生成\n    a1.sinks.k1.hdfs.rollSize  = 10485760\n    #批量写入hdfs的个数\n    a1.sinks.k1.hdfs.batchSize = 1000\n    #flume操作hdfs的线程数（包括新建，写入等）\n    a1.sinks.k1.hdfs.threadsPoolSize=10\n    #操作hdfs超时时间\n    a1.sinks.k1.hdfs.callTimeout=30000\n    \n    #组装source、channel、sink\n    a1.sources.r1.channels = c1\n    a1.sinks.k1.channel = c1\n    \n    ~~~\n\n* 5、启动flume配置\n\n  * 先在node03上启动flume\n\n  ```shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/avro_source_hdfs_sink.conf -Dflume.root.logger=info,console\n  \n  ```\n\n\n#### 7.2 flume案例之自定义拦截器\n\n* 1、案例场景\n\n~~~\n在数据采集之后，通过flume的拦截器，实现不需要的数据过滤掉，并将指定的第一个字段进行加密，加密之后再往hdfs上面保存\n\n~~~\n\n* 2、数据文件 user.txt\n\n~~~\n13901007610,male,30,sing,beijing\n18600000035,male,40,dance,shanghai\n13366666659,male,20,Swimming,wuhan\n13801179888,female,18,dance,tianjin\n18511111114,male,35,sing,beijing\n13718428888,female,40,Foodie,shanghai\n13901057088,male,50,Basketball,taiwan\n13671057777,male,60,Bodybuilding,xianggang\n\n~~~\n\n![](assets/flume-custom-interceptor.png)\n\n* 3、创建maven工程添加依赖\n\n~~~xml\n<dependency>\n        <groupId>org.apache.flume</groupId>\n        <artifactId>flume-ng-core</artifactId>\n        <version>1.6.0-cdh5.14.2</version>\n    </dependency>\n~~~\n\n\n\n* 4、代码开发\n\n~~~java\npackage bigdata.flume;\n\nimport com.google.common.base.Charsets;\nimport org.apache.flume.Context;\nimport org.apache.flume.Event;\nimport org.apache.flume.interceptor.Interceptor;\nimport java.math.BigInteger;\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.ArrayList;\nimport java.util.List;\n\n/**\n * @author dingchuangshi\n */\npublic class CustomInterceptor implements Interceptor {\n\n    /**\n     * encrypted_field_index.\n     * 指定需要加密的字段下标\n     */\n    private final String encrypted_field_index;\n\n\n    /**\n     * The out_index.\n     * 指定不需要对应列的下标\n     */\n    private final String out_index;\n\n\n    /**\n     * 提供构建方法，后期可以接受配置文件中的参数\n     * @param encrypted_field_index\n     * @param out_index\n     */\n    public CustomInterceptor(String encrypted_field_index, String out_index) {\n        this.encrypted_field_index = encrypted_field_index;\n        this.out_index = out_index;\n    }\n\n    /**\n     * 定义拦截器规则\n     * @param event\n     * @return\n     */\n    @Override\n    public Event intercept(Event event) {\n        if(event == null){\n            return null;\n        }\n       try{\n           String line = new String(event.getBody(), Charsets.UTF_8);\n           String newLine = \"\";\n           String[] splits = line.split(\",\");\n           for (int i = 0; i < splits.length; i++) {\n               // 加密索引\n               int encryptedField = Integer.parseInt(encrypted_field_index);\n               // 忽略索引\n               int outIndex = Integer.parseInt(out_index);\n\n               if(i == encryptedField){\n                   // 加密\n                   newLine += md5(splits[encryptedField]) + \",\";\n               }else if(i != outIndex){\n                   // 忽略取消数据\n                   newLine += splits[i] + \",\";\n               }\n           }\n           // 去掉最后一个'，'符号\n           newLine = newLine.substring(0,newLine.length() - 1);\n           event.setBody(newLine.getBytes(Charsets.UTF_8));\n       }catch (Exception e){\n           e.printStackTrace();\n       }\n        return event;\n    }\n\n    @Override\n    public List<Event> intercept(List<Event> events) {\n        List<Event> out = new ArrayList<Event>();\n        for (Event event : events) {\n            Event outEvent = intercept(event);\n            if (outEvent != null) {\n                out.add(outEvent);\n            }\n        }\n        return out;\n    }\n\n    @Override\n    public void initialize() {\n\n    }\n\n    @Override\n    public void close() {\n\n    }\n\n    /**\n     * md5加密\n     * @return\n     */\n    public String md5(String plainText){\n        byte[] secretBytes = null;\n        try {\n            MessageDigest instance = MessageDigest.getInstance(\"md5\");\n            instance.update(plainText.getBytes());\n            secretBytes = instance.digest();\n        } catch (NoSuchAlgorithmException e) {\n            System.out.println(\"没有md5这个算法\");\n            e.printStackTrace();\n        }\n        String md5Code = new BigInteger(1,secretBytes).toString(16);\n        for (int i = 0; i < 32 - md5Code.length(); i++) {\n            md5Code =\"0\" + md5Code;\n        }\n        return md5Code;\n    }\n\n\n    public static class MyBuilder  implements CustomInterceptor.Builder {\n        /**\n         * encrypted_field_index.\n         * 指定需要加密的字段下标\n         */\n        private String encrypted_field_index;\n\n        /**\n         * The out_index.\n         * 指定不需要对应列的下标\n         */\n        private String out_index;\n\n        @Override\n        public CustomInterceptor build() {\n            return new CustomInterceptor(encrypted_field_index, out_index);\n        }\n\n\n        @Override\n        public void configure(Context context) {\n            this.encrypted_field_index = context.getString(\"encrypted_field_index\", \"\");\n            this.out_index = context.getString(\"out_index\", \"\");\n        }\n    }\n}\n\n~~~\n\n* 5、打成jar包后放到flume安装目录下的lib中\n\n\n\n* 6、创建配置文件 flume-interceptor-hdfs.conf\n\n~~~properties\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n#配置source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /kfly/install/flumeData/user.txt\na1.sources.r1.channels = c1\na1.sources.r1.interceptors =i1\na1.sources.r1.interceptors.i1.type =bigdata.flume.CustomInterceptor$MyBuilder\na1.sources.r1.interceptors.i1.encrypted_field_index=0\na1.sources.r1.interceptors.i1.out_index=3\n\n#配置channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n\n\n#配置sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.channel = c1\na1.sinks.k1.hdfs.path = hdfs://node01:8020/interceptor/files/%Y-%m-%d/%H%M\na1.sinks.k1.hdfs.filePrefix = events-\n# 时间舍弃\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = minute\n# 限定生成文件时机\na1.sinks.k1.hdfs.rollInterval = 5\na1.sinks.k1.hdfs.rollSize = 50\na1.sinks.k1.hdfs.rollCount = 10\na1.sinks.k1.hdfs.batchSize = 100\na1.sinks.k1.hdfs.useLocalTimeStamp = true\n#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\na1.sinks.k1.hdfs.fileType = DataStream\n\n~~~\n\n* 7、进入到flume安装目录下启动flume\n\n~~~shell\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-interceptor-hdfs.conf -Dflume.root.logger=info,console\n\n~~~\n\n\n\n### 8. flume自定义Source\n\n#### 8.1 场景描述\n\n~~~\n\t官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。如：实时监控MySQL，从MySQL中获取数据传输到HDFS或者其他存储框架，所以此时需要我们自己实现MySQLSource。\n\n官方也提供了自定义source的接口：\n官网说明：https://flume.apache.org/FlumeDeveloperGuide.html#source\n\n~~~\n\n\n\n#### 8.2 自定义MysqlSource步骤\n\n* 1、根据官方说明自定义mysqlsource需要继承AbstractSource类并实现Configurable和PollableSource接口。\n\n\n\n* 2、实现对应的方法\n  * configure(Context context)\n    * 初始化context\n  * process()\n    * 从mysql表中获取数据，然后把数据封装成event对象写入到channel，该方法被一直调用\n  * stop()\n    * 关闭相关资源\n\n\n\n* 3、开发流程\n\n  * 3.1 创建mysql数据库以及mysql数据库表\n\n  ~~~sql\n  --创建一个数据库\n  CREATE DATABASE IF NOT EXISTS mysqlsource DEFAULT CHARACTER SET utf8 ;\n  \n  --创建一个表，用户保存拉取目标表位置的信息\n  CREATE TABLE mysqlsource.flume_meta (\n    source_tab varchar(255) NOT NULL,\n    currentIndex varchar(255) NOT NULL,\n    PRIMARY KEY (source_tab)\n  ) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n  \n  --插入数据\n  insert  into mysqlsource.flume_meta(source_tab,currentIndex) values ('student','4');\n  \n  \n  --创建要拉取数据的表\n  CREATE TABLE mysqlsource.student(\n    id int(11) NOT NULL AUTO_INCREMENT,\n    name varchar(255) NOT NULL,\n    PRIMARY KEY (id)\n  ) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;\n  \n  --向student表中添加测试数据\n  insert  into mysqlsource.student(id,name) values (1,'zhangsan'),(2,'lisi'),(3,'wangwu'),(4,'zhaoliu');\n  \n  ~~~\n\n  * 3.2 代码开发实现\n\n    *  构建maven工程，添加依赖\n\n    ~~~xml\n        <dependencies>\n            <dependency>\n                <groupId>mysql</groupId>\n                <artifactId>mysql-connector-java</artifactId>\n                <version>5.1.38</version>\n            </dependency>\n            <dependency>\n                <groupId>org.apache.commons</groupId>\n                <artifactId>commons-lang3</artifactId>\n                <version>3.6</version>\n            </dependency>\n        </dependencies>\n    \n    \n    ~~~\n\n    * 在resources资源文件夹下添加jdbc.properties\n\n      * ==jdbc.properties==\n\n      ~~~~properties\n      dbDriver=com.mysql.jdbc.Driver\n      dbUrl=jdbc:mysql://node03:3306/mysqlsource?useUnicode=true&characterEncoding=utf-8\n      dbUser=root\n      dbPassword=123456\n      \n      ~~~~\n\n    * ==核心代码==\n\n      ~~~java\n      /**\n       * 自定义source\n       * @author dingchuangshi\n       */\n      public class CustomSource extends AbstractSource implements Configurable, PollableSource {\n      \n          private static Logger logger = LoggerFactory.getLogger(CustomSource.class);\n      \t\t// 自定义的查询类\n          private QueryMysql sqlSourceHelp;\n      \n          @Override\n          public Status process() throws EventDeliveryException {\n              try {\n                  //查询数据表\n                  List<List<Object>> result = sqlSourceHelp.executeQuery();\n                  //存放event的集合\n                  List<Event> events = new ArrayList<>();\n                  //存放event头集合\n                  Map<String, String> header = new HashMap<>();\n                  //如果有返回数据，则将数据封装为event\n                  if (!result.isEmpty()) {\n                      List<String> allRows = sqlSourceHelp.getAllRows(result);\n                      Event event = null;\n                      for (String row : allRows) {\n                          event = new SimpleEvent();\n                          event.setBody(row.getBytes());\n                          event.setHeaders(header);\n                          events.add(event);\n                      }\n                      //将event写入channel\n                      this.getChannelProcessor().processEventBatch(events);\n                      //更新数据表中的offset信息\n                      sqlSourceHelp.updateOffset2DB(result.size());\n                  }\n                  //等待时长\n                  Thread.sleep(sqlSourceHelp.getRunQueryDelay());\n                  return Status.READY;\n              } catch (InterruptedException e) {\n                  logger.error(\"Error procesing row\", e);\n                  return Status.BACKOFF;\n              }\n          }\n      }\n      \n      ~~~\n\n\n\n* 4、测试\n\n  * 4.1 ==程序打成jar包，上传jar包到flume的lib目录下==\n\n  * 4.2 ==配置文件准备==\n\n    * vim mysqlsource.conf\n\n    ~~~properties\n    # Name the components on this agent\n    a1.sources = r1\n    a1.sinks = k1\n    a1.channels = c1\n    \n    # Describe/configure the source\n    a1.sources.r1.type = bigdata.flume.source.CustomSource\n    # 老师的是node01,同学们改成自己的节点 一定要注意\n    a1.sources.r1.connection.url = jdbc:mysql://node03:3306/mysqlsource\n    a1.sources.r1.connection.user = root\n    a1.sources.r1.connection.password = 123456\n    a1.sources.r1.table = student\n    a1.sources.r1.columns.to.select = *\n    a1.sources.r1.start.from=0\n    a1.sources.r1.run.query.delay=3000\n    \n    # Describe the channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    # Describe the sink\n    a1.sinks.k1.type = logger\n    \n    \n    # Bind the source and sink to the channel\n    a1.sources.r1.channels = c1\n    a1.sinks.k1.channel = c1\n    \n    ~~~\n\n* 4.3 ==启动flume配置==\n\n  ~~~shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/mysqlsource.conf -Dflume.root.logger=info,console\n  \n  ~~~\n\n  \n  * 4.4 最后向表添加数据，观察控制台信息\n\n\n\n### 9. flume自定义Sink\n\n#### 9.1 场景描述\n\n```\n\t官方提供的sink类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些sink。如：需要把接受到的数据按照规则进行过滤之后写入到某张mysql表中，所以此时需要我们自己实现MySQLSink。\n\n官方也提供了自定义sink的接口：\n官网说明：https://flume.apache.org/FlumeDeveloperGuide.html#sink\n```\n\n\n\n#### 9.2 自定义MysqlSink步骤\n\n- 1、根据官方说明自定义MysqlSink需要继承AbstractSink类并实现Configurable\n\n- 2、实现对应的方法\n  - configure(Context context)\n\n    - 初始化context\n\n  - start()\n\n    - 启动准备操作\n\n  - process()\n\n    - 从channel获取数据，然后解析之后，保存在mysql表中\n\n  - stop()\n\n    - 关闭相关资源\n\n\n- 3、开发流程\n\n  - 3.1 ==创建mysql数据库以及mysql数据库表==\n\n  ~~~SQL\n  --创建一个数据库\n  CREATE DATABASE IF NOT EXISTS mysqlsource DEFAULT CHARACTER SET utf8 ;\n  \n  --创建一个表，用户保存拉取目标表位置的信息\n  CREATE TABLE mysqlsource.flume2mysql (\n    id int(11) NOT NULL AUTO_INCREMENT,\n    create_time varchar(64) NOT NULL,\n    content varchar(255) NOT NULL,\n    PRIMARY KEY (id)\n  ) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n  ~~~\n\n\n\n  - 3.2  代码开发实现\n\n    - ==定义MysqlSink类==\n\n\n```java\npackage com.kaikeba.sink;\n\nimport org.apache.flume.conf.Configurable;\nimport org.apache.flume.*;\nimport org.apache.flume.sink.AbstractSink;\n\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.PreparedStatement;\nimport java.sql.SQLException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * 自定义MysqlSink\n */\npublic class MysqlSink extends AbstractSink implements Configurable {\n    private String mysqlurl = \"\";\n    private String username = \"\";\n    private String password = \"\";\n    private String tableName = \"\";\n\n    Connection con = null;\n\n    @Override\n    public Status process(){\n        Status status = null;\n        // Start transaction\n        Channel ch = getChannel();\n        Transaction txn = ch.getTransaction();\n        txn.begin();\n        try\n        {\n            Event event = ch.take();\n\n            if (event != null)\n            {\n                    //获取body中的数据\n                    String body = new String(event.getBody(), \"UTF-8\");\n\n                    //如果日志中有以下关键字的不需要保存，过滤掉\n                if(body.contains(\"delete\") || body.contains(\"drop\") || body.contains(\"alert\")){\n                    status = Status.BACKOFF;\n                }else {\n\n                    //存入Mysql\n                    SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n                    String createtime = df.format(new Date());\n\n                    PreparedStatement stmt = con.prepareStatement(\"insert into \" + tableName + \" (createtime, content) values (?, ?)\");\n                    stmt.setString(1, createtime);\n                    stmt.setString(2, body);\n                    stmt.execute();\n                    stmt.close();\n                    status = Status.READY;\n                }\n           }else {\n                    status = Status.BACKOFF;\n                }\n\n            txn.commit();\n        } catch (Throwable t){\n            txn.rollback();\n            t.getCause().printStackTrace();\n            status = Status.BACKOFF;\n        } finally{\n            txn.close();\n        }\n\n        return status;\n    }\n    /**\n     * 获取配置文件中指定的参数\n     * @param context\n     */\n    @Override\n    public void configure(Context context) {\n        mysqlurl = context.getString(\"mysqlurl\");\n        username = context.getString(\"username\");\n        password = context.getString(\"password\");\n        tableName = context.getString(\"tablename\");\n    }    \n    \n    @Override\n    public synchronized void start() {\n        try{\n              //初始化数据库连接\n            con = DriverManager.getConnection(mysqlurl, username, password);\n            super.start();\n            System.out.println(\"finish start\");\n        }catch (Exception ex){\n            ex.printStackTrace();\n        }\n    }\n    \n    @Override\n    public synchronized void stop(){\n        try{\n            con.close();\n        }catch(SQLException e) {\n            e.printStackTrace();\n        }\n        super.stop();\n    }\n\n}\n\n```\n\n\n\n\n* 4、测试\n\n  * 4.1 ==程序打成jar包，上传jar包到flume的lib目录下==\n\n  * 4.2 ==配置文件准备==\n\n    * vim mysqlsink.conf\n\n~~~properties\n    a1.sources = r1\n    a1.sinks = k1\n    a1.channels = c1\n    \n    #配置source\n    a1.sources.r1.type = exec\n    a1.sources.r1.command = tail -F /kfly/install/flumeData/data.log\n    a1.sources.r1.channels = c1\n    \n    #配置channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    #配置sink\n    a1.sinks.k1.channel = c1\n    a1.sinks.k1.type = bigdata.flume.sink.CustomSink\n    a1.sinks.k1.mysqlurl=jdbc:mysql://node03:3306/mysqlsource?useSSL=false\n    a1.sinks.k1.username=root\n    a1.sinks.k1.password=123456\n    a1.sinks.k1.tablename=flume2mysql\n~~~\n\n  * 4.3 ==启动flume配置==\n\n  ~~~shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/mysqlsink.conf -Dflume.root.logger=info,console\n  ~~~\n\n  * 4.4 最后向文件中添加数据，观察mysql表中的数据\n\n### 10. Flume实际使用注意事项\n\n#### 1、注意启动脚本命名的书写\n\n```\nagent 的名称别写错了，后台执行加上 nohup ... &\n```\n\n#### 2、channel参数\n\n```\ncapacity：默认该通道中最大的可以存储的event数量\ntrasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量\n注意：capacity > trasactionCapacity\n\n```\n\n#### 3、日志采集到HDFS配置说明1（sink端）\n\n```shell\n#定义sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.path=hdfs://node01:8020/source/logs/%{type}/%Y%m%d\na1.sinks.k1.hdfs.filePrefix =events\na1.sinks.k1.hdfs.fileType = DataStream\na1.sinks.k1.hdfs.writeFormat = Text\n#时间类型\na1.sinks.k1.hdfs.useLocalTimeStamp = true\n#生成的文件不按条数生成\na1.sinks.k1.hdfs.rollCount = 0\n#生成的文件按时间生成\na1.sinks.k1.hdfs.rollInterval = 0\n#生成的文件按大小生成\na1.sinks.k1.hdfs.rollSize  = 10485760\n#批量写入hdfs的个数\na1.sinks.k1.hdfs.batchSize = 10000\n#flume操作hdfs的线程数（包括新建，写入等）\na1.sinks.k1.hdfs.threadsPoolSize=10\n#操作hdfs超时时间\na1.sinks.k1.hdfs.callTimeout=30000\n\n\n```\n\n#### 4、日志采集到HDFS配置说明2（sink端）\n\n| hdfs.round          | false  | Should the timestamp be rounded down (if true, affects all time based escape sequences except %t) |\n| ------------------- | ------ | ------------------------------------------------------------ |\n| **hdfs.roundValue** | 1      | Rounded down to the highest multiple of this (in the unit configured usinghdfs.roundUnit), less than current time. |\n| **hdfs.roundUnit**  | second | The unit of the round down value - second, minute or hour.   |\n\nØ round： 默认值：false 是否启用时间上的”舍弃”，这里的”舍弃”，类似于”四舍五入”\n\nØ roundValue：默认值：1  时间上进行“舍弃”的值；\n\nØ roundUnit： 默认值：seconds时间上进行”舍弃”的单位，包含：second,minute,hour\n\n```properties\n# 案例一：\na1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d/%H:%M/%S\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = minute\n# 当时间为2015-10-16 17:38:59时候，hdfs.path依然会被解析为：\n# /flume/events/2015-10-16/17:30/00\n# /flume/events/2015-10-16/17:40/00\n# /flume/events/2015-10-16/17:50/00\n# 因为设置的是舍弃10分钟内的时间，因此，该目录每10分钟新生成一个。\n\n# 案例二：\na1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d/%H:%M/%S\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = second\n# 现象：10秒为时间梯度生成对应的目录，目录下面包括很多小文件！！！\n# 格式如下：\n# /flume/events/2016-07-28/18:45/10\n# /flume/events/2016-07-28/18:45/20\n# /flume/events/2016-07-28/18:45/30\n# /flume/events/2016-07-28/18:45/40\n# /flume/events/2016-07-28/18:45/50\n# /flume/events/2016-07-28/18:46/10\n# /flume/events/2016-07-28/18:46/20\n# /flume/events/2016-07-28/18:46/30\n# /flume/events/2016-07-28/18:46/40\n# /flume/events/2016-07-28/18:46/50\n\n```\n\n#### 5、实现数据的断点续传\n\n- 当一个flume挂掉之后重启的时候还是可以接着上一次的数据继续收集\n  - flume在1.7版本之前使用的监控一个文件（source exec）、监控一个目录（source spooldir）都无法直接实现\n- flume在1.7版本之后已经集成了该功能\n  - 其本质就是记录下每一次消费的位置，把消费信息的位置保存到文件中，后续程序挂掉了再重启的时候，可以接着上一次消费的数据位置继续拉取。\n- 配置文件\n  - ==vim taildir.conf==\n    - source 类型---->taildir\n\n```properties\na1.sources = s1\na1.channels = ch1\na1.sinks = hdfs-sink1\n\n#channel\na1.channels.ch1.type = memory\na1.channels.ch1.capacity=10000\na1.channels.ch1.transactionCapacity=500\n\n#source\na1.sources.s1.channels = ch1\n#监控一个目录下的多个文件新增的内容\na1.sources.s1.type = taildir\n#通过 json 格式存下每个文件消费的偏移量，避免从头消费\na1.sources.s1.positionFile = /k/install/flumeData/index/taildir_position.json\na1.sources.s1.filegroups = f1 f2 f3 \na1.sources.s1.filegroups.f1 = /home/hadoop/taillogs/access.log\na1.sources.s1.filegroups.f2 = /home/hadoop/taillogs/nginx.log\na1.sources.s1.filegroups.f3 = /home/hadoop/taillogs/web.log\na1.sources.s1.headers.f1.headerKey = access\na1.sources.s1.headers.f2.headerKey = nginx\na1.sources.s1.headers.f3.headerKey = web\na1.sources.s1.fileHeader  = true\n\n##sink\na1.sinks.hdfs-sink1.channel = ch1\na1.sinks.hdfs-sink1.type = hdfs\na1.sinks.hdfs-sink1.hdfs.path =hdfs://node01:8020/demo/data/%{headerKey}\na1.sinks.hdfs-sink1.hdfs.filePrefix = event_data\na1.sinks.hdfs-sink1.hdfs.fileSuffix = .log\na1.sinks.hdfs-sink1.hdfs.rollSize = 1048576\na1.sinks.hdfs-sink1.hdfs.rollInterval =20\na1.sinks.hdfs-sink1.hdfs.rollCount = 10\na1.sinks.hdfs-sink1.hdfs.batchSize = 1500\na1.sinks.hdfs-sink1.hdfs.round = true\na1.sinks.hdfs-sink1.hdfs.roundUnit = minute\na1.sinks.hdfs-sink1.hdfs.threadsPoolSize = 25\na1.sinks.hdfs-sink1.hdfs.fileType =DataStream\na1.sinks.hdfs-sink1.hdfs.writeFormat = Text\na1.sinks.hdfs-sink1.hdfs.callTimeout = 60000\n\n```\n\n\n\n~~~json\n# 运行后生成的 taildir_position.json文件信息如下：\n[\n{\"inode\":102626782,\"pos\":123,\"file\":\"/home/hadoop/taillogs/access.log\"},{\"inode\":102626785,\"pos\":123,\"file\":\"/home/hadoop/taillogs/web.log\"},{\"inode\":102626786,\"pos\":123,\"file\":\"/home/hadoop/taillogs/nginx.log\"}\n]\n\n#这里inode就是标记文件的，文件名称改变，这个iNode不会变，pos记录偏移量，file就是绝对路径\n\n~~~\n\n\n\n#### 6、flume的header参数配置讲解\n\n- ==vim test-header.conf==\t\n\n```properties\n#配置信息test-header.conf\na1.channels=c1\na1.sources=r1\na1.sinks=k1\n\n#source\na1.sources.r1.channels=c1\na1.sources.r1.type= spooldir\na1.sources.r1.spoolDir= /home/hadoop/test\na1.sources.r1.batchSize= 100\na1.sources.r1.inputCharset= UTF-8\n#是否添加一个key存储目录下文件的绝对路径\na1.sources.r1.fileHeader= true\n#指定存储目录下文件的绝对路径的key\na1.sources.r1.fileHeaderKey= mm\n#是否添加一个key存储目录下的文件名称\na1.sources.r1.basenameHeader= true\n#指定存储目录下文件的名称的key\na1.sources.r1.basenameHeaderKey= nn\n\n#channel\na1.channels.c1.type= memory\na1.channels.c1.capacity=10000\na1.channels.c1.transactionCapacity=500\n\n\n#sink\na1.sinks.k1.type=logger\na1.sinks.k1.channel=c1\n\n```\n\n- 准备数据文件，添加内容\n\n```\n/home/hadoop/test/abc.txt\n/home/hadoop/test/def.txt\n```\n\n- 启动flume配置\n\n```\nbin/flume-ng agent -n a1 -c myconf -f myconf/test-header.conf -Dflume.root.logger=info,console\n```\n\n- 查看控制台\n\n```\nEvent: { headers:{mm=/home/hadoop/test/abc.txt, nn=abc.txt} body: 68 65 6C 6C 6F 20 73 70 61 72 6B                hello spark }\n19/08/30 19:23:15 INFO sink.LoggerSink: Event: { headers:{mm=/home/hadoop/test/abc.txt, nn=abc.txt} body: 68 65 6C 6C 6F 20 68 61 64 6F 6F 70             hello hadoop }\n```\n\n####  7 tail / tail -f /tail -F区别\n\n```\ntail -f\n\n等同于--follow=descriptor，根据文件描述符进行追踪，当文件改名或被删除，追踪停止\n\ntail -F\n\n等同于--follow=name --retry，根据文件名进行追踪，并保持重试，即该文件被删除或改名后，如果再次创建相同的文件名，会继续追踪\n\ntailf\n\n等同于tail -f -n 10（貌似tail -f或-F默认也是打印最后10行，然后追踪文件），与tail -f不同的是，如果文件不增长，它不会去访问磁盘文件，所以tailf特别适合那些便携机上跟踪日志文件，因为它减少了磁盘访问，\n```","tags":["flume"]},{"title":"大数据开发之HBase（四）微博案例","url":"/2019/11/19/hbase/大数据开发之HBase（四）微博案例/","content":"\n#  1. hbase微博实战案例\n\n## 1.1 需求分析\n\n> 1. hbase属于OLAP数据库，表设计与OLTP数据库不同，支持百万列。列的动态变化。\n>\n> 2. 设计规范\n>\n>    | **项目**     | **说明**                                                     | **示例**                                                     |\n>    | ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n>    | **命名空间** | 采用英文单词、阿拉伯数字的组合形式，其中，单词必须大写，并且首字符必须为英文字符，不能是数字。不建议用连接符（下划线）拼接多个单词，简单语义的可采用单个单词，复杂语义的可采用多个单词的首字母拼接。长度尽量限制在4~8字符之间。命名空间一般可与项目名称、组织机构名称等保持一致。 | 根据项目名称构建命名空间：DLQX（电力气象首字母拼接形式），简短明了。不建议过长的命名空间名称，譬如不推荐采用以下形式：USER_INFO_MANAGE等。 |\n>    | **表名称**   | 采用英文单词、阿拉伯数字、连接符（_）的组合形式，其中，单词必须大写，并且首字符必须为英文字符，不能是数字，可用连接符拼接多个单词。长度尽量限制在8~16字符之间。尽量采用具有明确意义的英文单词，而不建议采用汉字的拼音字母或者拼音首字母组合。 | 符合规范的表名称：USER_INFO_MANAGE、WEATHER_DATA、T_ELECTRIC_GATHER等。 |\n>    | **列族名称** | 采用英文单词、阿拉伯数字的组合形式，其中，单词必须大写，并且首字符必须为英文字符，不能是数字。长度尽量限制在1~6字符之间，过长的列族名称将占用更多的存储空间。 | 符合规范的列族名称：D1、D2、DATA等。不推荐的列族名称：USER_INFO、D_1等。 |\n>    | **列名称**   | 采用英文单词、阿拉伯数字、连接符（_）的组合形式，其中，单词必须大写，并且首字符必须为英文字符，不能是数字，可用连接符拼接多个单词。长度尽量限制在1~16字符之间。尽量采用具有明确意义的英文单词，而不建议采用汉字的拼音字母或者拼音首字母组合。 | 符合规范的列名称：USER_ID、DATA_1、REMARK等。不推荐的列名称：UserID、1_DA |\n\n\n\n1) 微博内容的浏览，数据库表设计\n\n- rowKey: uid + timestamp\n\n![image-20191119172809987](assets/image-20191119172809987.png)\n\n2) 用户社交体现：关注用户，取关用户\n\n- rowKey： uid\n\n- ColumnFamily： 关注（attends）、粉丝（fans）两个列族\n\n- 列族下列名称和cell值直接使用uid，当新增关注或者粉丝时直接动态增加列即可\n\n![image-20191119172951520](assets/image-20191119172951520.png)\n\n3) 拉取关注的人的微博内容\n\n- 发送微博直接推送给粉丝，需要表微博收件箱\n- rowKey 和列都用uid，动态增加列接收发送的微博\n- cell值使用微博id，设置微博版本ß\n\n![image-20191119174811329](assets/image-20191119174811329.png)\n\n## 1.2 代码实现\n\n### 1.2.1 准备工作\n\n- 第一步：创建maven工程并导入jar包\n\n  直接使用在版本确界当中创建的工程以及导入的jar包即可\n\n- 第二步：拷贝三个配置文件到maven工程的下\n\n  将node01服务器的三个配置文件，分别是\n\n  core-site.xml、hdfs-site.xml、hbase-site.xml三个配置文件，拷贝到maven工程的resources资源目录下\n\n### 1.2.2 代码设计总览：\n\n1) 创建命名空间以及表名的定义\n\n2) 创建微博内容表\n\n3) 创建用户关系表\n\n4) 创建用户微博内容接收邮件表\n\n5) 发布微博内容\n\n6) 添加关注用户\n\n7) 移除（取关）用户\n\n8) 获取关注的人的微博内容\n\n### 1.2.3 创建命名空间以及表名的定义\n\n**代码实现：**\n\n```java\n//微博内容表的表名\nprivate static final byte[] TABLE_CONTENT = Bytes.toBytes(\"weibo:content\");\n//用户关系表的表名\nprivate static final byte[] TABLE_RELATIONS = Bytes.toBytes(\"weibo:relations\");\n//微博收件箱表的表名\nprivate static final byte[] TABLE_RECEIVE_CONTENT_EMAIL = Bytes.toBytes(\"weibo:receive_content_email\");\n/**\n * 初始化命名空间\n */\npublic void initNameSpace() throws IOException {\n    Connection connection = getConnection();\n    Admin admin = connection.getAdmin();\n    NamespaceDescriptor namespaceDescriptor = NamespaceDescriptor.create(\"weibo2\").addConfiguration(\"creator\",\"jim\").build();\n    admin.createNamespace(namespaceDescriptor);\n    admin.close();\n    connection.close();\n}\n\npublic Connection getConnection() throws IOException {\n    Configuration configuration = HBaseConfiguration.create();\n    configuration.set(\"hbase.zookeeper.quorum\",\"node01:2181,node02:2181,node03:2181\");\n    Connection connection = ConnectionFactory.createConnection();\n    return connection;\n}\n\n```\n\n### 1.2.4 创建微博内容表\n\n**表结构：**\n\n| **方法名**   | creatTableeContent |\n| ------------ | ------------------ |\n| Table Name   | weibo:content      |\n| RowKey       | 用户ID_时间戳      |\n| ColumnFamily | info               |\n| ColumnLabel  | 标题,内容,图片     |\n| Version      | 1个版本            |\n\n**代码实现：**\n\n```java\n/**\n * 创建微博内容存储表\n *\n * 方法名 creatTableeContent\n Table Name    weibo:content\n RowKey    用户ID_时间戳\n ColumnFamily  info\n ColumnLabel   标题,内容,图片\n Version   1个版本\n\n *\n */\npublic void creatTableeContent() throws IOException {\n    //获取连接\n    Connection connection = getConnection();\n    //获取管理员对象\n    Admin admin = connection.getAdmin();\n    //得到HTableDescriptor对象\n    HTableDescriptor hTableDescriptor = new HTableDescriptor(TableName.valueOf(\"weibo:content\"));\n    //添加列族info\n    HColumnDescriptor info = new HColumnDescriptor(\"info\");\n    //设置版本确界\n    info.setMaxVersions(1);\n    info.setMinVersions(1);\n    info.setBlockCacheEnabled(true);\n    //设置数据压缩\n    //   info.setCompressionType(Compression.Algorithm.SNAPPY);\n    info.setBlocksize(2048*1024);\n    hTableDescriptor.addFamily(info);\n    //创建表\n    admin.createTable(hTableDescriptor);\n    admin.close();\n    connection.close();\n}\n\n```\n\n### 1.2.5 创建用户关系表\n\n**表结构：**\n\n| **方法名**   | createTableRelations   |\n| ------------ | ---------------------- |\n| Table Name   | weibo:relations        |\n| RowKey       | 用户ID                 |\n| ColumnFamily | attends、fans          |\n| ColumnLabel  | 关注用户ID，粉丝用户ID |\n| ColumnValue  | 用户ID                 |\n| Version      | 1个版本                |\n\n**代码实现：**\n\n```java\n/**\n * 创建用户关系表\n * 方法名 createTableRelations\n Table Name    weibo:relations\n RowKey    用户ID\n ColumnFamily  attends、fans\n ColumnLabel   关注用户ID，粉丝用户ID\n ColumnValue   用户ID\n Version   1个版本\n\n */\npublic void createTableRelations() throws IOException {\n    //获取连接\n    Connection connection = getConnection();\n    //获取管理员对象\n    Admin admin = connection.getAdmin();\n    //得到表定义\n    HTableDescriptor hTableDescriptor = new HTableDescriptor(TableName.valueOf(TABLE_RELATIONS));\n    HColumnDescriptor attends = new HColumnDescriptor(\"attends\");\n    HColumnDescriptor fans = new HColumnDescriptor(\"fans\");\n\n    attends.setBlocksize(2048*1024);\n    attends.setBlockCacheEnabled(true);\n    attends.setMinVersions(1);\n    attends.setMaxVersions(1);\n\n    fans.setBlocksize(2048*1024);\n    fans.setBlockCacheEnabled(true);\n    fans.setMinVersions(1);\n    fans.setMaxVersions(1);\n\n    hTableDescriptor.addFamily(attends);\n    hTableDescriptor.addFamily(fans);\n    admin.createTable(hTableDescriptor);\n    admin.close();\n    connection.close();\n}\n\n```\n\n### 1.2.6 创建微博收件箱表\n\n**表结构：**\n\n| **方法名**   | createTableReceiveContentEmails |\n| ------------ | ------------------------------- |\n| Table Name   | weibo:receive_content_email     |\n| RowKey       | 用户ID                          |\n| ColumnFamily | info                            |\n| ColumnLabel  | 用户ID                          |\n| ColumnValue  | 取微博内容的RowKey              |\n| Version      | 1000                            |\n\n**代码实现：**\n\n```java\n/**\n * 表结构：\n 方法名   createTableReceiveContentEmails\n Table Name    weibo:receive_content_email\n RowKey    用户ID\n ColumnFamily  info\n ColumnLabel   用户ID\n ColumnValue   取微博内容的RowKey\n Version   1000\n\n */\n\npublic void createTableReceiveContentEmails() throws IOException {\n    //获取连接\n    Connection connection = getConnection();\n    //得到管理员对象\n    Admin admin = connection.getAdmin();\n    //获取HTableDescriptor\n    HTableDescriptor hTableDescriptor = new HTableDescriptor(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));\n    //定义列族名称\n    HColumnDescriptor info = new HColumnDescriptor(\"info\");\n    info.setMaxVersions(1000);\n    info.setMinVersions(1000);\n    info.setBlockCacheEnabled(true);\n    info.setBlocksize(2048*1024);\n    hTableDescriptor.addFamily(info);\n    //创建表\n    admin.createTable(hTableDescriptor);\n    admin.close();\n    connection.close();\n}\n\n```\n\n### 1.2.7 发布微博内容\n\na、微博内容表中添加1条数据\n\nb、微博收件箱表对所有粉丝用户添加数据\n\n**代码实现：**\n\n```java\n/**\n * 发布微博内容\n * @param uid\n * @param content\n */\npublic void publishWeiBo(String uid ,String content) throws IOException {\n    Connection connection = getConnection();\n    Table table = connection.getTable(TableName.valueOf(TABLE_CONTENT));\n    String rowkey = uid + \"_\"+ System.currentTimeMillis();\n    Put put = new Put(rowkey.getBytes());\n    put.addColumn(\"info\".getBytes(),\"content\".getBytes(),System.currentTimeMillis(),content.getBytes());\n    table.put(put);\n    //微博用户关系表\n    Table table_relations = connection.getTable(TableName.valueOf(TABLE_RELATIONS));\n    Get get = new Get(uid.getBytes());\n    get.addFamily(\"fans\".getBytes());\n    Result result = table_relations.get(get);\n    Cell[] cells = result.rawCells();\n    if(cells.length <= 0){\n        return ;\n    }\n    List<byte[]>  allFans = new ArrayList<byte[]>();\n    //将所有的粉丝都获取到，然后将数据保存到粉丝表当中去\n    for (Cell cell : cells) {\n        byte[] bytes = CellUtil.cloneQualifier(cell);\n        allFans.add(bytes);\n    }\n    Table table_receive_content_email = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));\n    List<Put> putFansList = new ArrayList<>();\n    for (byte[] allFan : allFans) {\n        Put put1 = new Put(allFan);\n        put1.addColumn(\"info\".getBytes(),Bytes.toBytes(uid),System.currentTimeMillis(),rowkey.getBytes());\n        putFansList.add(put1);\n    }\n    table_receive_content_email.put(putFansList);\n}\n\n```\n\n### 1.2.8 添加关注用户\n\na、在微博用户关系表中，对当前主动操作的用户添加新关注的好友\n\nb、在微博用户关系表中，对被关注的用户添加新的粉丝\n\nc、微博收件箱表中添加所关注的用户发布的微博\n\n**代码实现：**\n\n```java\n/**\n * 添加关注用户，一次可能添加多个关注用户\n * A 关注一批用户 B,C ,D\n * 第一步：A是B,C,D的关注者   在weibo:relations 当中attend列族当中以A作为rowkey，B,C,D作为列名，B,C,D作为列值，保存起来\n * 第二步：B,C,D都会多一个粉丝A  在weibo:relations 当中fans列族当中分别以B,C,D作为rowkey，A作为列名，A作为列值，保存起来\n * 第三步：A需要获取B,C,D 的微博内容存放到 receive_content_email 表当中去，以A作为rowkey，B,C,D作为列名，获取B,C,D发布的微博rowkey，放到对应的列值里面去\n *\n *\n * @param uid\n * @param attends\n */\npublic void addAttends(String uid ,String ...attends) throws IOException {\n    Connection connection = getConnection();\n    Table relation_table = connection.getTable(TableName.valueOf(TABLE_RELATIONS));\n    //用户关注人,attend列族当中添加数据\n    Put put = new Put(uid.getBytes());\n    for (String attend : attends) {\n        put.addColumn(\"attends\".getBytes(),attend.getBytes(),attend.getBytes());\n    }\n    relation_table.put(put);\n    //粉丝fans添加粉丝，A 关注B，那么自然B就需要添加一个粉丝A\n    for (String attend : attends) {\n        Put put1 = new Put(attend.getBytes());\n        put1.addColumn(\"fans\".getBytes(),uid.getBytes(),uid.getBytes());\n        relation_table.put(put1);\n    }\n\n    //获取uid的所有关注人的收件箱，放到收件箱列表weibo:receive_content_email里面去\n    //A 关注B，那么A需要获取B所有的微博内容\n    Table table_content = connection.getTable(TableName.valueOf(TABLE_CONTENT));\n    Scan scan = new Scan();\n    List<byte[]> rowkeyBytes = new ArrayList<>();\n    for (String attend : attends) {\n        RowFilter rowFilter = new RowFilter(CompareFilter.CompareOp.EQUAL\n,new SubstringComparator(attend+\"_\"));\n        scan.setFilter(rowFilter);\n        ResultScanner scanner = table_content.getScanner(scan);\n        for (Result result : scanner) {\n            //获取到数据的rowkey\n            byte[] rowkey = result.getRow();\n            rowkeyBytes.add(rowkey);\n        }\n    }\n    if(rowkeyBytes.size() > 0){\n        Table table_receive_content = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));\n        List<Put> recPuts = new ArrayList<Put>();\n        for (byte[] rowkeyByte : rowkeyBytes) {\n            Put put1 = new Put(uid.getBytes());\n            String rowKeyStr = Bytes.toString(rowkeyByte);\n            String attendUid = rowKeyStr.substring(0, rowKeyStr.indexOf(\"_\"));\n            long timestamp = Long.parseLong(rowKeyStr.substring(rowKeyStr.indexOf(\"_\") + 1));\n\n            put1.addColumn(\"info\".getBytes(),attendUid.getBytes(), timestamp,rowkeyByte);\n            recPuts.add(put1);\n        }\n        table_receive_content.put(recPuts);\n    }\n}\n\n```\n\n### 1.2.9 移除（取关）用户\n\na、在微博用户关系表中，对当前主动操作的用户移除取关的好友(attends)\n\nb、在微博用户关系表中，对被取关的用户移除粉丝\n\nc、微博收件箱中删除取关的用户发布的微博\n\n**代码实现：**\n\n```java\n/**\n * 取消关注 A取消关注 B,C,D这三个用户\n * 其实逻辑与关注B,C,D相反即可\n * 第一步：在weibo:relation关系表当中，在attends列族当中删除B,C,D这三个列\n * 第二步：在weibo:relation关系表当中，在fans列族当中，以B,C,D为rowkey，查找fans列族当中A这个粉丝，给删除掉\n * 第三步：A取消关注B,C,D,在收件箱中，删除取关的人的微博的rowkey\n */\npublic void attendCancel(String uid,String ...cancelAttends) throws IOException {\n    Connection connection = getConnection();\n    Table table_relations = connection.getTable(TableName.valueOf(TABLE_RELATIONS));\n\n    //移除A关注的B,C,D这三个用户\n    for (String cancelAttend : cancelAttends) {\n        Delete delete = new Delete(uid.getBytes());\n        delete.addColumn(\"attends\".getBytes(),cancelAttend.getBytes());\n        table_relations.delete(delete);\n    }\n\n    //B,C,D这三个用户移除粉丝A\n    for (String cancelAttend : cancelAttends) {\n        Delete delete = new Delete(cancelAttend.getBytes());\n        delete.addColumn(\"fans\".getBytes(),uid.getBytes());\n        table_relations.delete(delete);\n    }\n\n    //收件箱表当中  A移除掉B,C,D的信息\n    Table table_receive_connection = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));\n\n    for (String cancelAttend : cancelAttends) {\n        Delete delete = new Delete(uid.getBytes());\n        delete.addColumn(\"info\".getBytes(),cancelAttend.getBytes());\n        table_receive_connection.delete(delete);\n\n    }\n    table_receive_connection.close();\n    table_relations.close();\n    connection.close();\n}\n\n```\n\n### 1.2.10 获取关注的人的微博内容\n\na、从微博收件箱中获取所关注的用户的微博RowKey \n\nb、根据获取的RowKey，得到微博内容\n\n**代码实现：**\n\n```java\n/**\n * 某个用户获取收件箱表内容\n * 例如A用户刷新微博，拉取他所有关注人的微博内容\n * A 从 weibo:receive_content_email  表当中获取所有关注人的rowkey\n * 通过rowkey从weibo:content表当中获取微博内容\n */\npublic void getContent(String uid) throws IOException {\n    //从weibo:receive_content_email 表当中获取用户id为uid的人的所有的微博列表\n    Connection connection = getConnection();\n    //从 weibo:receive_content_email\n    Table table_receive_content_email = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));\n    //定义list集合里面存储我们的所有的Get对象，用于下一步的查询\n    List<Get>  rowkeysList = new ArrayList<Get>();\n    Get get = new Get(uid.getBytes());\n    //设置最大版本为5个\n    get.setMaxVersions(5);\n    Result result = table_receive_content_email.get(get);\n    Cell[] cells = result.rawCells();\n    for (Cell cell : cells) {\n        byte[] rowkeys = CellUtil.cloneValue(cell);\n        Get get1 = new Get(rowkeys);\n        rowkeysList.add(get1);\n    }\n    //从weibo:content表当中通过用户id进行查询微博内容\n    //table_content内容表\n    Table table_content = connection.getTable(TableName.valueOf(TABLE_CONTENT));\n    //所有查询出来的内容进行打印出来\n    Result[] results = table_content.get(rowkeysList);\n    for (Result result1 : results) {\n        byte[] value = result1.getValue(\"info\".getBytes(), \"content\".getBytes());\n        byte[] row = result1.getRow();\n        String rowkey = Bytes.toString(row);\n        String[] split = rowkey.split(\"_\");\n        Content content = new Content();\n        content.setUid(split[0]);\n        content.setTimeStamp(Long.parseLong(split[1]));\n        content.setContent(Bytes.toString(value));\n        System.out.println(content.toString());\n    }\n}\n```\n\n\n\n[点击获取代码示例](https://github.com/orchid-ding/hadoop-example/tree/master/src/main/java/hbase/weibo)","tags":["项目练习","hbase"]},{"title":"大数据开发之HBase（三）","url":"/2019/11/17/hbase/大数据开发之HBase（三）/","content":"\n\n\n# 大数据数据库之HBase\n\n## 1. HBase协处理器\n\n- http://hbase.apache.org/book.html#cp\n- 起源：\n  - Hbase 作为列族数据库最经常被人诟病的特性包括：无法轻易建立“二级索引”，难以执行求和、计数、排序等操作。比如，在旧版本的(<0.92)Hbase 中，统计数据表的总行数，需 要使用 Counter 方法，执行一次 MapReduce Job 才能得到。\n  - 虽然 HBase 在数据存储层中集成了 MapReduce，能够有效用于数据表的分布式计算。然而在很多情况下，做一些简单的相加或者聚合计算的时候， 如果直接将计算过程放置在 server端，能够减少通讯开销，从而获得很好的性能提升。\n  - 于是， HBase 在 0.92 之后引入了协处理器(coprocessors)，实现一些激动人心的新特性：能够轻易建立二次索引、复杂过滤器(谓词下推)以及访问控制等。\n\n### 1.1 两种协处理器\n\n- 协处理器有两种：observer和endpoint\n\n#### 1.1.1 observer\n\n- Observer 类似于传统数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。\n- Observer Coprocessor就是一些散布在 HBase Server 端代码中的 hook 钩子， 在固定的事件发生时被调用。\n  - 比如： put 操作之前有钩子函数 prePut，该函数在put操作执行前会被Region Server调用；在 put 操作之后则有 postPut 钩子函数\n\n- 以 HBase0.92 版本为例，它提供了三种观察者接口：\n  - RegionObserver：提供客户端的数据操纵事件钩子： Get、 Put、 Delete、 Scan 等。\n  - WALObserver：提供 WAL 相关操作钩子。\n  - MasterObserver：提供 DDL类型的操作钩子。如创建、删除、修改数据表等。\n  - 到 0.96 版本又新增一个 RegionServerObserver\n\n![](assets/Image201911151202.png)\n\n- 下图是以 RegionObserver 为例子讲解 Observer 这种协处理器的原理：\n\n![1122015-20170511100919222-711579099](assets/1122015-20170511100919222-711579099.png)\n\n#### 1.1.2 endpoint\n\n- Endpoint协处理器类似传统数据库中的存储过程，客户端可以调用这些 Endpoint 协处理器执行一段 Server 端代码，并将 Server 端代码的结果返回给客户端进一步处理\n- 最常见的用法就是进行聚集操作。\n  - 如果没有协处理器，当用户需要找出一张表中的最大数据，即max 聚合操作，就必须进行全表扫描，在客户端代码内遍历扫描结果，并执行求最大值的操作。\n  - 这样的方法无法利用底层集群的并发能力，而将所有计算都集中到 Client 端统一执 行，势必效率低下。\n  - 利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，HBase将利用底层cluster 的多个节点并发执行求最大值的操作。即在每个 Region范围内执行求最大值的代码，将每个 Region 的最大值在 Region Server 端计算出，仅仅将该 max 值返回给客户端。\n  - 在客户端进一步将多个 Region 的最大值进一步处理而找到其中的最大值。这样整体的执行效率就会提高很多\n\n#### 1.1.3 总结\n\n- Observer允许集群在正常的客户端操作过程中可以有不同的行为表现\n- Endpoint 允许扩展集群的能力，对客户端应用开放新的运算命令\n- observer 类似于 RDBMS 中的触发器，主要在服务端工作\n- endpoint 类似于 RDBMS 中的存储过程，主要在 client 端工作\n- observer 可以实现权限管理、优先级设置、监控、 ddl 控制、 二级索引等功能\n- endpoint 可以实现 min、 max、 avg、 sum、 distinct、 group by 等功能\n\n### 1.2 协处理器加载方式  \n\n- 协处理器的加载方式有两种\n  - 静态加载方式（ Static Load）；静态加载的协处理器称之为 System Coprocessor\n  - 动态加载方式 （ Dynamic Load）；动态加载的协处理器称 之为 Table Coprocessor\n\n#### 1.2.1 静态加载 \n\n- 通过修改 hbase-site.xml 这个文件来实现， 如启动全局 aggregation，能过操纵所有的表数据。只需要在hbase-site.xml里面添加以下配置即可\n- ==注意==：修改完配置之后需要**重启HBase集群**\n\n```xml\n<property>\n\t<name>hbase.coprocessor.user.region.classes</name>\n\t<value>org.apache.hadoop.hbase.coprocessor.AggregateImplementation</value>\n</property>\n```\n\n- 为所有table加载了一个 cp class，可以用” ,”分割加载多个 class，修改\n\n#### 1.2.2 动态加载\n\n- 启用表aggregation，只对特定的表生效。\n- 通过 HBase Shell 来实现。\n-  disable 指定表。\n\n```ruby\nhbase> disable 'mytable'\n```\n\n-  添加 aggregation\n\n```ruby\nhbase> alter 'mytable', METHOD => 'table_att','coprocessor'=>'|org.apache.Hadoop.hbase.coprocessor.AggregateImplementation||'\n```\n\n- 重启指定表 \n\n```ruby\nhbase> enable 'mytable'\n```\n\n- 协处理器卸载\n\n   ![xxx](assets/xxx.png)\n\n### 1.3 协处理器Observer实战\n\n<img src=\"assets/xdfsdfsdf.png\" alt=\"xdfsdfsdf\" style=\"zoom:80%;\" />\n\n- 通过协处理器Observer实现向hbase当中一张表插入数据时，通过协处理器，将数据复制一份保存到另外一张表当中去；但是只取第一张表当中的部分列数据，保存到第二张表当中去\n\n#### 1.3.1 创建第一张表proc1\n\n- 打开hbase shell\n\n```shell\ncd //install/hbase-1.2.0-cdh5.14.2/\nbin/hbase shell\n```\n\n- 在HBase当中创建一张表，表名user2，并只有一个列族info\n\n```ruby\nhbase(main):053:0> create 'proc1','info'\n```\n\n#### 1.3.2 创建第二张表proc2\n\n- 创建第二张表proc2，作为目标表\n- 将第一张表当中插入数据的部分列，使用协处理器，复制到proc2表当中来\n\n```ruby\nhbase(main):054:0> create 'proc2','info'\n```\n\n#### 1.3.3 开发HBase协处理器\n\n- 创建maven工程所用的repositories、dependencies、plugins跟之前的一样\n\n```xml\n<dependency>\n  <groupId>org.apache.hadoop</groupId>\n  <artifactId>hadoop-client</artifactId>\n  <version>2.6.0-mr1-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-client</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-server</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n```\n\n- 开发HBase的协处理器\n\n```\nimport org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;\n\npublic class MyProcessor extends BaseRegionObserver {\n    /**\n     *\n     * @param e\n     * @param put   插入到proc1表里面的数据，都是封装在put对象里面了\n     *              插入到proc1表里面的数据都在put对象里面，就可以解析put对象，获取数据，获取到了数据之后，插入到proc2表里面去\n     * @param edit\n     * @param durability\n     * @throws IOException\n     */\n    @Override\n    public void prePut(ObserverContext<RegionCoprocessorEnvironment> e, Put put, WALEdit edit, Durability durability) throws IOException {\n        //获取连接\n        Configuration configuration = HBaseConfiguration.create();\n        configuration.set(\"hbase.zookeeper.quorum\", \"node01:2181,node02:2181,node03:2181\");\n        Connection connection = ConnectionFactory.createConnection(configuration);\n\n        //涉及到多个版本问题\n        List<Cell> cells = put.get(\"info\".getBytes(), \"name\".getBytes());\n        Cell nameCell = cells.get(0);//获取最新的那个版本数据\n        //Cell nameCell = put.get(\"info\".getBytes(), \"name\".getBytes()).get(0);\n        Put put1 = new Put(put.getRow());\n        put1.add(nameCell);\n        Table reverseuser = connection.getTable(TableName.valueOf(\"proc2\"));\n        reverseuser.put(put1);\n        reverseuser.close();\n        connection.close();\n    }\n}\n```\n\n#### 1.3.4 将项目打成jar包，并上传到HDFS上面\n\n- 将我们的协处理器打成一个jar包，此处不需要用任何的打包插件即可\n- 然后将打好的jar包上传到linux的/kfly/install路径下\n- 再将jar包上传到HDFS\n\n```shell\ncd /kfly/install\n# 名称必须为processor.jar\nmv original-hbaseStudy-1.0-SNAPSHOT.jar  processor.jar\nhdfs dfs -mkdir -p /processor\nhdfs dfs -put processor.jar /processor\n```\n\n\n\n#### 1.3.5 将jar包挂载到proc1表\n\n```ruby\nhbase(main):056:0> describe 'proc1'\nhbase(main):055:0> alter 'observer:source',METHOD => 'table_att','Coprocessor'=>'hdfs://node01:8020/processor/processor.jar|top.kfly.hbasemr.MyProcessor|1001|'\n```\n\n- 再次查看'proc1'表；可以查看到我们的卸载器已经加载了\n\n```ruby\nhbase(main):043:0> describe 'proc1'\n```\n\n#### 1.3.6 向proc1表添加数据\n\n```\npackage com.kaikeba.hbase.cp.test;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.Connection;\nimport org.apache.hadoop.hbase.client.ConnectionFactory;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.client.Table;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.testng.annotations.Test;\n\npublic class TestObserver {\n    @Test\n    public void testPut() throws Exception{\n        //获取连接\n        Configuration configuration = HBaseConfiguration.create();\n        configuration.set(\"hbase.zookeeper.quorum\", \"node01,node02\");\n\n        Connection connection = ConnectionFactory.createConnection(configuration);\n\n        Table user5 = connection.getTable(TableName.valueOf(\"proc1\"));\n\n        Put put1 = new Put(Bytes.toBytes(\"hello_world\"));\n\n        put1.addColumn(Bytes.toBytes(\"info\"),\"name\".getBytes(),\"helloworld\".getBytes());\n        put1.addColumn(Bytes.toBytes(\"info\"),\"gender\".getBytes(),\"abc\".getBytes());\n        put1.addColumn(Bytes.toBytes(\"info\"),\"nationality\".getBytes(),\"test\".getBytes());\n        user5.put(put1);\n        byte[] row = put1.getRow();\n        System.out.println(Bytes.toString(row));\n        user5.close();\n    }\n}\n```\n\n- 注意：如果需要卸载我们的协处理器，那么进入hbase的shell命令行，执行以下命令即可\n\n```ruby\ndisable 'proc1'\nalter 'proc1',METHOD=>'table_att_unset',NAME=>'coprocessor$1'\nenable 'proc1'\n```\n\n\n\n## 2. HBase表的rowkey设计\n\n- rowkey设计三原则\n\n### 2.1 rowkey长度原则\n\n- rowkey是一个二进制码流，可以是任意字符串，最大长度64kb，实际应用中一般为10-100bytes，以byte[]形式保存，一般设计成定长。\n\n* 建议尽可能短；但是也不能太短，否则rowkey前缀重复的概率增大\n* 设计过长会降低memstore内存的利用率和HFile存储数据的效率。\n\n### 2.2 rowkey散列原则\n\n- 建议将rowkey的高位作为散列字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。\n- 如果没有散列字段，首字段直接是时间信息。所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。\t\n\n### 2.3 rowkey唯一原则\n\n- 必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的\n- 因此，设计rowkey的时候，要充分利用这个排序的特点，可以将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块\n- 下图为电信上网详单数据，保存在HBase的一个应用场景\n\n![2019-10-16_112336](assets/2019-10-16_112336.png)\n\n![2019-10-16_112157](assets/2019-10-16_112157.png)\n\n\n\n![2019-10-16_112529](assets/2019-10-16_112529.png)\n\n\n\n## 3. HBase表的热点\n\n### 3.2 什么是热点\n\n- 检索habse的记录首先要通过row key来定位数据行。\n- 当大量的client访问hbase集群的一个或少数几个节点，造成少数region server的读/写请求过多、负载过大，而其他region server负载却很小，就造成了“热点”现象。\n\n### 3.2 热点的解决方案\n\n#### 3.2.1 预分区\n\n- 预分区的目的让表的数据可以均衡的分散在集群中，而不是默认只有一个region分布在集群的一个节点上。\n\n#### 3.2.3 加盐             \n\n- 这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同\n\n#### 3.2.4 哈希\n\n- 哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。\n\n~~~\nrowkey=MD5(username).subString(0,10)+时间戳\t\n~~~\n\n#### 3.2.4 反转\n\n- 反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。\n- 这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。\n\n~~~\n电信公司：\n移动-----------> 136xxxx9301  ----->1039xxxx631\n\t\t\t\t136xxxx1234  \n\t\t\t\t136xxxx2341 \n电信\n联通\n\nuser表\nrowkey    name    age   sex    address\n\t\t  lisi1    21     m       beijing\n\t\t  lisi2    22     m       beijing\n\t\t  lisi3    25     m       beijing\n\t\t  lisi4    30     m       beijing\n\t\t  lisi5    40     f       shanghai\n\t\t  lisi6    50     f       tianjin\n\t          \n需求：后期想经常按照居住地和年龄进行查询？\t\nrowkey= address+age+随机数\n        beijing21+随机数\n        beijing22+随机数\n        beijing25+随机数\n        beijing30+随机数\n   \nrowkey= address+age+随机数\n~~~\n\n\n\n## 4. HBase的数据备份\n\n### 4.1 基于HBase提供的类对表进行备份\n\n* 使用HBase提供的类把HBase中某张表的数据导出到HDFS，之后再导出到测试hbase表中。\n\n* (1)  ==从hbase表导出到HDFS==\n\n  ~~~shell\n  [hadoop@node01 shells]$ hbase org.apache.hadoop.hbase.mapreduce.Export myuser /hbase_data/myuser_bak\n  \n  ~~~\n  \n* (2) ==文件导入hbase表==\n\n  hbase shell中创建备份目标表\n  \n  ```ruby\n  create 'myuser_bak','f1','f2'\n  ```\n  \n* 将HDFS上的数据导入到备份目标表中\n\n  ~~~shell\n  hbase org.apache.hadoop.hbase.mapreduce.Driver import myuser_bak /hbase_data/myuser_bak/*\n  ~~~\n\n* 补充说明\n\n  以上都是对数据进行了全量备份，后期也可以实现表的**增量数据备份**，增量备份跟全量备份操作差不多，只不过要在后面加上时间戳。\n  \n  例如：\n  HBase数据导出到HDFS\n  \n  ~~~\n  hbase org.apache.hadoop.hbase.mapreduce.Export test /hbase_data/test_bak_increment 开始时间戳  结束时间戳\n  ~~~\n\n\n### 4.2 基于snapshot快照对表进行备份\n\n* 通过snapshot快照的方式实现HBase数据的迁移和拷贝。这种方式比较常用，效率高，也是最为推荐的数据迁移方式。\n\n*  HBase的snapshot其实就是一组==metadata==信息的集合（文件列表），通过这些metadata信息的集合，就能将表的数据回滚到snapshot那个时刻的数据。\n\n  * 首先我们要了解一下所谓的HBase的LSM类型的系统结构，我们知道在HBase中，数据是先写入到Memstore中，当Memstore中的数据达到一定条件，就会flush到HDFS中，形成HFile，后面就不允许原地修改或者删除了。\n  * 如果要更新或者删除的话，只能追加写入新文件。既然数据写入以后就不会在发生原地修改或者删除，这就是snapshot做文章的地方。做snapshot的时候，只需要给快照表对应的所有文件创建好指针（元数据集合），恢复的时候只需要根据这些指针找到对应的文件进行恢复就Ok。这是原理的最简单的描述，下图是描述快照时候的简单流程：\t\n  \n  ![snapshot](assets/snapshot.png)\n\n\n\n### 4.3 **快照实战**\n\n* 1、创建表的snapshot\n\n~~~\nsnapshot 'tableName', 'snapshotName'\n~~~\n\n  * 2、查看snapshot\n\n  ~~~\nlist_snapshots  \n  ~~~\n\n​\t\t查找以test开头的snapshot\n\n```\nlist_snapshots 'test.*'\n```\n\n  * 3、恢复snapshot\n\n​\t\tps:这里需要对表进行disable操作，先把表置为不可用状态，然后在进行进行restore_snapshot的操作\n\n```\ndisable 'tableName'\nrestore_snapshot 'snapshotName'\nenable 'tableName'\n```\n\n  * 4、删除snapshot\n\n  ~~~\ndelete_snapshot 'snapshotName'\n  ~~~\n\n  * 5、迁移 snapshot\n\n  ~~~\n  hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \\\n  -snapshot snapshotName  \\\n  -copy-from hdfs://src-hbase-root-dir/hbase \\\n  -copy-to hdfs://dst-hbase-root-dir/hbase \\\n  -mappers 1 \\\n  -bandwidth 1024\n  \n  例如：\n  hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \\\n  -snapshot test  \\\n  -copy-from hdfs://node01:8020/hbase \\\n  -copy-to hdfs://node01:8020/hbase1 \\\n  -mappers 1 \\\n  -bandwidth 1024\n  ~~~\n\n​\t\t注意：这种方式用于将快照表迁移到另外一个集群的时候使用，使用MR进行数据的拷贝，速度很快，使用的时候记得设置好bandwidth参数，以免由于网络打满导致的线上业务故障。\n\n  * 6、将snapshot使用bulkload的方式导入\n\n  ~~~\n  hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles \\\n  hdfs://dst-hbase-root-dir/hbase/archive/datapath/tablename/filename \\\n  tablename\n  \n  例如：\n  创建一个新表\n  create 'newTest','f1','f2'\n  hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles hdfs://node1:9000/hbase1/archive/data/default/test/6325fabb429bf45c5dcbbe672225f1fb newTest\n  ~~~\n\n\n\n## 5. HBase二级索引\n\n![hbase寻址](assets/hbase寻址.png)\n\n- HBase表后期按照rowkey查询性能是最高的。rowkey就相当于hbase表的一级索引\n- 但是在实际的工作中，我们做的查询基本上都是按照一定的条件进行查找，无法事先知道满足这些条件的rowkey是什么，正常是可以通过hbase过滤器去实现。但是效率非常低，这是由于查询的过程中需要在底层进行大量的文件扫描。\n\n- HBase的二级索引\n- 为了HBase的数据查询更高效、适应更多的场景，诸如使用非rowkey字段检索也能做到秒级响应，或者支持各个字段进行模糊查询和多字段组合查询等， 因此需要在HBase上面构建二级索引， 以满足现实中更复杂多样的业务需求。\n  - hbase的二级索引其本质就是建立HBase表中列与行键之间的映射关系。\n\n\n\n![](assets/二级索引思想.png)\n\n- 构建hbase二级索引方案\n  - MapReduce方案 \n  - Hbase Coprocessor(协处理器)方案 \n  - Solr+hbase方案\n  - ES+hbase方案\n  - Phoenix+hbase方案\n    - [点击查看](https://kfly.top/2019/11/17/phoenix/Phoenix%E6%9E%84%E5%BB%BA%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95/) \n\n## 6. HBase的namespace\n\n### 6.1 namespace基本介绍\n\n- 在HBase中，namespace命名空间指对一组表的逻辑分组，类似RDBMS中的database，方便对表在业务上划分。\n- Apache HBase从0.98.0, 0.95.2两个版本号开始支持namespace级别的授权操作，HBase**全局管理员**能够创建、改动和回收namespace的授权。\n\n### 6.2 namespace的作用\n\n- 配额管理：限制一个namespace可以使用的资源，包括region和table\n- 命名空间安全管理：提供了另一个层面的多租户安全管理\n\n- Region服务器组：一个命名或一张表，可以被固定到一组RegionServers上，从而保证了数据隔离性\n\n### 6.3 namespace的基本操作\n\n```sql\n创建namespace\nhbase>create_namespace 'nametest'  \n\n查看namespace\nhbase>describe_namespace 'nametest'  \n\n列出所有namespace\nhbase>list_namespace  \n\n在namespace下创建表\nhbase>create 'nametest:testtable', 'fm1' \n\n查看namespace下的表\nhbase>list_namespace_tables 'nametest'  \n\n删除namespace\nhbase>drop_namespace 'nametest'  \n```\n\n \n\n## 7. HBase的数据版本的确界以及TTL\n\n### 7.1 数据的确界\n\n- 在HBase当中，我们可以为数据设置上界和下界，其实就是定义数据的历史版本保留多少个，通过自定义历史版本保存的数量，我们可以实现数据多个历史版本的数据查询\n\n- 版本的下界\n  - 默认的版本下界是0，即禁用。row版本使用的最小数目是与生存时间（TTL Time To Live）相结合的，并且我们根据实际需求可以有0或更多的版本，使用0，即只有1个版本的值写入cell。\n\n- 版本的上界\n  - 之前默认的版本上界是3，也就是一个row保留3个副本（基于时间戳的插入）。\n  - 该值不要设计的过大，一般的业务不会超过100。如果cell中存储的数据版本号超过了3个，再次插入数据时，最新的值会将最老的值覆盖。（现版本已默认为1）\n\n### 7.2 数据的TTL\n\n- 在实际工作当中经常会遇到有些数据过了一段时间我们可能就不需要了，那么这时候我们可以使用定时任务去定时的删除这些数据\n- 或者我们也可以使用Hbase的TTL（Time  To  Live）功能，让我们的数据定期的会进行清除\n\n- 使用代码来设置数据的确界以及设置数据的TTL如下\n\n####  7.2.1 创建maven工程\n\n- 创建maven工程，导入jar包坐标\n\n```xml\n<dependency>\n  <groupId>org.apache.hadoop</groupId>\n  <artifactId>hadoop-client</artifactId>\n  <version>2.6.0-mr1-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-client</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-server</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n```\n\n#### 7.2.2 代码开发\n\n```java\n /**\n     * 初始化连接\n     * @throws IOException\n     */\n    public void init() throws IOException {\n        Configuration conf = HBaseConfiguration.create();\n        conf.set(\"hbase.zookeeper.quorum\",\"node01:2181,node02:2181,node03:2181\");\n        conf.set(\"zookeeper.znode.parent\",\"/HBase\");\n        conf.set(\"fs.fefaultFS\",\"hadoop.hdfs://node01:8020\");\n        connection = ConnectionFactory.createConnection(conf);\n    }\n\n    /**\n     * 创建表\n     */\n    public void createTable() throws IOException {\n        Admin admin = connection.getAdmin();\n        if(!admin.tableExists(TableName.valueOf(TABLE_NAME))){\n            // table\n            HTableDescriptor table = new HTableDescriptor(TableName.valueOf(TABLE_NAME));\n            // columa family\n            HColumnDescriptor column = new HColumnDescriptor(\"f1\");\n            // version\n            column.setMaxVersions(5);\n            column.setMinVersions(3);\n            // ttl unit s\n            column.setTimeToLive(30);\n            table.addFamily(column);\n            admin.createTable(table);\n        }\n        admin.close();\n    }\n\n    /**\n     * insert data\n     * @throws IOException\n     */\n    public void insertData() throws IOException {\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        for (int i = 0; i < 6 ; i++) {\n            Put put = new Put((\"column\").getBytes());\n            put.addColumn(\"f1\".getBytes(),\"col1\".getBytes(), System.currentTimeMillis(),Bytes.toBytes(\"column\" + i));\n            table.put(put);\n        }\n    }\n\n    /**\n     * get raw cell\n     * @throws IOException\n     */\n    public void getRawCell() throws IOException {\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        Get get = new Get(\"column\".getBytes());\n        get.setMaxVersions();\n        Result result = table.get(get);\n        Cell[] cells = result.rawCells();\n        for (int i = 0; i < cells.length; i++) {\n            System.out.println(Bytes.toString(CellUtil.cloneValue(cells[i])));\n        }\n    }\n```\n\n","tags":["hbase","协处理器","rowkey设计"]},{"title":"Phoenix构建二级索引","url":"/2019/11/17/phoenix/Phoenix构建二级索引/","content":"\n\n\n## Phoenix构建二级索引\n\n### 1、为什么需要用二级索引？\n\n- 对于HBase而言，如果想精确地定位到某行记录，唯一的办法是通过rowkey来查询。如果不通过rowkey来查找数据，就必须逐行地比较每一列的值，即全表扫瞄。\n- 对于较大的表，全表扫描的代价是不可接受的。但是，很多情况下，需要从多个角度查询数据。例如，在定位某个人的时候，可以通过姓名、身份证号、学籍号等不同的角度来查询，要想把这么多角度的数据都放到rowkey中几乎不可能（业务的灵活性不允许，对rowkey长度的要求也不允许）。所以需要secondary index（二级索引）来完成这件事。secondary index的原理很简单，但是如果自己维护的话则会麻烦一些。现在，Phoenix已经提供了对HBase secondary index的支持。\n\n### 2、Phoenix Global Indexing And Local Indexing\n\n#### 2.1 Global Indexing\n\n- Global indexing，全局索引，适用于读多写少的业务场景。\n- 使用Global indexing在写数据的时候开销很大，因为所有对数据表的更新操作（DELETE, UPSERT VALUES and UPSERT SELECT），都会引起索引表的更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。\n- 在读数据的时候Phoenix会选择索引表来降低查询消耗的时间。在默认情况下如果想查询的字段不是索引字段的话索引表不会被使用，也就是说不会带来查询速度的提升。\n\n#### 2.2 Local Indexing\n\n- Local indexing，本地索引，适用于写操作频繁以及空间受限制的场景。\n- 与Global indexing一样，Phoenix会自动判定在进行查询的时候是否使用索引。使用Local indexing时，索引数据和数据表的数据存放在相同的服务器中，这样避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。使用Local indexing的时候即使查询的字段不是索引字段索引表也会被使用，这会带来查询速度的提升，这点跟Global indexing不同。对于Local Indexing，一个数据表的所有索引数据都存储在一个单一的独立的可共享的表中\n\n### 3 Immutable index And Mutable index\n\n#### 3.1 immutable index\n\n- immutable index，不可变索引，适用于数据只增加不更新并且按照时间先后顺序存储（time-series data）的场景，如保存日志数据或者事件数据等。\n- 不可变索引的存储方式是write one，append only。当在Phoenix使用create table语句时指定IMMUTABLE_ROWS = true表示该表上创建的索引将被设置为不可变索引。Phoenix默认情况下如果在create table时不指定IMMUTABLE_ROW = true时，表示该表为mutable。不可变索引分为Global immutable index和Local immutable index两种。\n\n#### 3.2 mutable index\n\n- mutable index，可变索引，适用于数据有增删改的场景。\n- Phoenix默认情况创建的索引都是可变索引，除非在create table的时候显式地指定IMMUTABLE_ROWS = true。可变索引同样分为Global immutable index和Local immutable index两种。\n\n### 4、配置HBase支持Phoenix二级索引\n\n#### 4.1 修改配置文件\n\n- 如果要启用phoenix的二级索引功能，需要修改配置文件hbase-site.xml\n\n- vim hbase-site.xml      \n\n  ~~~xml\n  <!-- 添加配置 -->\n  <property>\n  \t<name>hbase.regionserver.wal.codec</name>\n  \t<value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>\n  </property>\n  <property>\n     <name>hbase.region.server.rpc.scheduler.factory.class</name>\n     <value>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory</value>\n  </property>\n  <property>\n  \t\t<name>hbase.rpc.controllerfactory.class</name>\n  \t\t<value>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</value>\n  </property>\n  ~~~\n\n#### 4.2 重启hbase\n\n- 完成上述修改后重启hbase集群使配置生效。\n\n\n\n### 5、实战\n\n#### 5.1 在phoenix中创建表\n\n- 首先，在phoenix中创建一个user table\n\n- node02执行以下命令，进入phoenix客户端\n\n~~~~\ncd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/\nbin/sqlline.py node01:2181\t\n~~~~\n\n- 创建表\n\n```sql\ncreate  table user (\n\"session_id\" varchar(100) not null primary key, \n\"f\".\"cookie_id\" varchar(100), \n\"f\".\"visit_time\" varchar(100), \n\"f\".\"user_id\" varchar(100), \n\"f\".\"age\" varchar(100), \n\"f\".\"sex\" varchar(100), \n\"f\".\"visit_url\" varchar(100), \n\"f\".\"visit_os\" varchar(100), \n\"f\".\"browser_name\" varchar(100),\n\"f\".\"visit_ip\" varchar(100), \n\"f\".\"province\" varchar(100),\n\"f\".\"city\" varchar(100),\n\"f\".\"page_id\" varchar(100), \n\"f\".\"goods_id\" varchar(100),\n\"f\".\"shop_id\" varchar(100)) column_encoded_bytes=0;\n```\n\n#### 5.2 导入测试数据\n\n- 将课件当中的user50w.csv 这个文件上传到node02的/kkb/install/phoenixsql 这个路径下 \n  该CSV文件中有250万条记录\n  node02执行以下命令，导入50W的测试数据\n\n~~~shell\ncd /kkb/install/apache-phoenix-4.14.0-cdh5.14.2-bin/\nbin/psql.py -t USER node01:2181 /kkb/install/phoenixsql/user50w.csv\n~~~\n\n#### 5.3 Global Indexing的二级索引测试\n\n##### 5.3.1 正常查询一条数据所需的时间\n\n- 在为表USER创建secondary index之前，先看看查询一条数据所需的时间\n  在node02服务器，进入phoenix的客户端\n\n~~~shell\nbin/sqlline.py node01:2181\ncd /kkb/install/apache-phoenix-4.14.0-cdh5.14.2-bin\n~~~\n\n- 然后执行以下sql语句，查询数据，查看耗费时间\n\n  可以看到，对名为cookie_id的列进行按值查询需要10秒左右。\n\n```sql\nselect * from user where \"cookie_id\" = '99738fd1-2084-44e9';\n```\n\n- 我们可以通过explain来查看执行计划\n  EXPLAIN(语句的执行逻辑及计划):\n\n  由图看知先进行了全表扫描再通过过滤器来筛选出目标数据，显示这种查询方式效率是很低的。\n\n\n~~~\nexplain select * from user where \"cookie_id\" = '99738fd1-2084-44e9';\n~~~\n\n##### 5.3.2 给表USER创建基于Global Indexing的二级索引\n\n- 进入到phoenix的客户端，然后执行以下命令进行创建索引\n  在cookie_id列上面创建二级索引：\n\n  查看当前所有表会发现多一张USER_COOKIE_ID_INDEX索引表，查询该表数据。\n\n~~~\n0: jdbc:phoenix:node01:2181> create index USER_COOKIE_ID_INDEX on USER (\"f\".\"cookie_id\"); \n~~~\n\n![](assets/user_cookie_id_index.PNG)\n\n* 再次执行查询\"cookie_id\"='99738fd1-2084-44e9'的数据记录\n\n  ```\nselect \"cookie_id\" from user where \"cookie_id\" = '99738fd1-2084-44e9';\n  ```\n\n  ![](assets/3.PNG)\n\n  此时：查询速度由10秒左右减少到了毫秒级别。\n  \n  注意：select所带的字段必须包含在覆盖索引内\n  \n  EXPLAIN(语句的执行逻辑及计划):\n\n  可以看到使用到了创建的索引USER_COOKIE_ID_INDEX。\n  \n  ~~~\n  explain select \"cookie_id\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n  ~~~\n  \n  ![](assets/4.PNG)\t\n\n\n##### 5.3.3 以下查询不会用到索引表\n\n  ~~~~\nselect \"cookie_id\",\"age\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n  ~~~~\n\n- (虽然cookie_id是索引字段，但age不是索引字段，所以不会使用到索引)\n    也可以通过EXPLAIN查询语句的执行逻辑及计划\n\n```\nEXPLAIN\tselect \"cookie_id\",\"age\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n```\n\n- 同理要查询的字段不是索引字段，也不会使用到索引表。\n\n  ~~~\nselect \"sex\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n  ~~~\n\n\n\n#### 5.4  Local Indexing的二级索引测试\n\n##### 5.4.1 正常查询一条数据所需的时间\n\n- 在为表USER创建secondary index之前，先看看查询一条数据所需的时间\n\n~~~\nselect * from user where \"user_id\"='371e963d-c-487065';\n~~~\n\n可以看到，对名为user_id的列进行按值查询需要11秒左右。\n\n- EXPLAIN(语句的执行逻辑及计划):\n\n~~~\nexplain select * from user where \"user_id\"='371e963d-c-487065';\n~~~\n\n由图看知先进行了全表扫描再通过过滤器来筛选出目标数据，显示这种查询方式效率是很低的。\n\n##### 5.4.2 给表USER创建基于Local Indexing的二级索引\n\n- 在user_id列上面创建二级索引：\n\n~~~\ncreate local index USER_USER_ID_INDEX on USER (\"f\".\"user_id\");\n~~~\n\n查看当前所有表会发现多一张USER_USER_ID_INDEX索引表，查询该表数据。\n\n- 再次执行查询\"user_id\"='371e963d-c-487065'的数据记录\n\n  ```\nselect * from user where \"user_id\"='371e963d-c-487065';\n  可以看到，对名为user_id的列进行按值查询需要0.3秒左右。\n  ```\n\n- EXPLAIN(语句的执行逻辑及计划):\n\n```\n    explain select * from user where \"user_id\"='371e963d-c-487065';\n```\n\n查看执行计划，没有执行全表扫描，效率更高了\n\n此时：查询速度由11秒左右减少到了毫秒级别。\n\nEXPLAIN(语句的执行逻辑及计划):\n\n  ~~~\nexplain select \"user_id\",\"age\",\"sex\" from user where \"user_id\"='371e963d-c-487065';\n  ~~~\n\n可以看到使用到了创建的索引USER_USER_ID_INDEX.\n\n#### 5.5 如何确保query查询使用Index\n\n​\t要想让一个查询使用index，有三种方式实现。\n\n##### 5.5.1  创建 convered index\n\n~~~\n\t如果在某次查询中，查询项或者查询条件中包含除被索引列之外的列（主键MY_PK除外）。默认情况下，该查询会触发full table scan（全表扫描），但是使用covered index则可以避免全表扫描。 \n\t创建包含某个字段的覆盖索引,创建方式如下：\n\t create index USER_COOKIE_ID_AGE_INDEX on USER (\"f\".\"cookie_id\") include(\"f\".\"age\");\n\t \n\t 查看当前所有表会发现多一张USER_COOKIE_ID_AGE_INDEX索引表，查询该表数据。\n~~~\n\n\n\n~~~\n查询数据\nselect \"age\" from user where  \"cookie_id\"='99738fd1-2084-44e9';\nselect \"age\",\"sex\" from user where  \"cookie_id\"='99738fd1-2084-44e9';\n~~~\n\n![](assets/16.PNG)\n\n![](assets/17.PNG)\n\n##### 5.5.2 在查询中提示其使用index\n\n~~~\n在select和column_name之间加上/*+ Index(<表名> <index名>)*/，通过这种方式强制使用索引。\n例如：\nselect /*+ index(user,USER_COOKIE_ID_AGE_INDEX) */ \"age\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n\n如果sex是索引字段，那么就会直接从索引表中查询\n如果sex不是索引字段，那么将会进行全表扫描，所以当用户明确知道表中数据较少且符合检索条件时才适用，此时的性能才是最佳的。\n~~~\n\n![](assets/18.PNG)\n\n![](assets/19.PNG)\n\n##### 5.5.3 使用本地索引 (创建Local Indexing 索引)\n\n* 详细见上面\n\n\n\n#### 5.6 索引重建\n\n~~~\nPhoenix的索引重建是把索引表清空后重新装配数据。\nalter index USER_COOKIE_ID_INDEX on user rebuild;\n~~~\n\n\n\n#### 5.7 删除索引\n\n~~~\n删除某个表的某张索引：\n语法\tdrop index 索引名称 on 表名\n例如：  \ndrop index USER_COOKIE_ID_INDEX on user;\n\n如果表中的一个索引列被删除，则索引也将被自动删除，如果删除的是\n覆盖索引上的列，则此列将从覆盖索引中被自动删除。\n~~~\n\n### 6、索引性能调优\n\n​\t一般来说，索引已经很快了，不需要特别的优化。这里也提供了一些方法，让你在面对特定的环境和负载的时候可以进行一些调优。下面的这些需要在hbase-site.xml文件中设置，针对所有的服务器。\n\n~~~\n1. index.builder.threads.max \n创建索引时，使用的最大线程数。 \n默认值: 10。\n\n2. index.builder.threads.keepalivetime \n创建索引的创建线程池中线程的存活时间，单位：秒。 \n默认值: 60\n\n3. index.writer.threads.max \n写索引表数据的写线程池的最大线程数。 \n更新索引表可以用的最大线程数，也就是同时可以更新多少张索引表，数量最好和索引表的数量一致。 \n默认值: 10\n\n4. index.writer.threads.keepalivetime \n索引写线程池中，线程的存活时间，单位：秒。\n默认值：60\n \n\n5. hbase.htable.threads.max \n每一张索引表可用于写的线程数。 \n默认值: 2,147,483,647\n\n6. hbase.htable.threads.keepalivetime \n索引表线程池中线程的存活时间，单位：秒。 \n默认值: 60\n\n7. index.tablefactory.cache.size \n允许缓存的索引表的数量。 \n增加此值，可以在写索引表时不用每次都去重复的创建htable，这个值越大，内存消耗越多。 \n默认值: 10\n\n8. org.apache.phoenix.regionserver.index.handler.count \n处理全局索引写请求时，可以使用的线程数。 \n默认值: 30\n~~~\n\n","tags":["phoenix","二级索引"]},{"title":"Phoenix环境部署安装","url":"/2019/11/15/phoenix/Phoenix环境部署安装/","content":"\n# Phoenix安装部署\n\n- 需要先安装好HBase集群\n- phoenix只是一个工具，只需要在一台机器上安装就可以了，这里我们选择node02服务器来进行安装一台即可\n\n# 1、下载安装包\n\n- 从对应的地址下载：http://archive.apache.org/dist/phoenix/\n- 注意版本兼容问题；这里我们使用的是\n  - apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz\n\n# 2、上传解压\n\n- 将安装包上传到node02服务器的/kfly/soft路径下，然后进行解压\n\n```shell\ncd /kfly/soft/\ntar -zxf apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz -C /kfly/install/\n```\n\n# 3、修改配置\n\n## 3.1 拷贝jar包\n\n- 将phoenix目录下的==phoenix-4.8.2-HBase-1.2-server.jar==、==phoenix-core-4.8.2-HBase-1.2.jar==拷贝到==各个 HBase节点的lib目录==下。\n\n- node02执行以下命令\n\n  ```shell\n  cd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin\n  \n  scp phoenix-4.14.0-cdh5.14.2-server.jar phoenix-core-4.14.0-cdh5.14.2.jar node01:/kfly/install/hbase-1.2.0-cdh5.14.2/lib/ \n  \n  scp phoenix-4.14.0-cdh5.14.2-server.jar phoenix-core-4.14.0-cdh5.14.2.jar node02:/kfly/install/hbase-1.2.0-cdh5.14.2/lib/ \n  \n  scp phoenix-4.14.0-cdh5.14.2-server.jar phoenix-core-4.14.0-cdh5.14.2.jar node03:/kfly/install/hbase-1.2.0-cdh5.14.2/lib/ \n  ```\n\n## 3.2 拷贝配置文件\n\n- 将HBase的配置文件==hbase-site.xml==、 hadoop下的配置文件==core-site.xml== 、==hdfs-site.xml==放到phoenix/bin/下，替换phoenix原来的配置文件。\n\n  node02执行以下命令，进行拷贝配置文件\n\n  ```shell\n  cp /kfly/install/hadoop-2.6.0/etc/hadoop/core-site.xml  /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/\n  \n  scp /kfly/install/hadoop-2.6.0/etc/hadoop/hdfs-site.xml /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/\n  \n  scp /kfly/install/hbase-1.2.0-cdh5.14.2/conf/hbase-site.xml /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/\n  ```\n\n- 重启hbase集群，使Phoenix的jar包生效。\n\n  node01执行以下命令来重启hbase的集群\n\n  ```\n  cd /kfly/install/hbase-1.2.0-cdh5.14.2/\n  bin/stop-hbase.sh \n  bin/start-hbase.sh \n  ```\n\n\n\n# 4、验证是否成功\n\n- 在phoenix/bin下输入命令, 进入到命令行，接下来就可以操作了\n\n- node02执行以下命令，进入phoenix客户端\n\n  ~~~shell\n  cd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/\n  bin/sqlline.py node01:2181\n  ~~~\n\n# 5、Phoenix使用\n\n## 5.1 批处理方式\n\n* node02执行以下命令创建user_phoenix.sql文件\n\n~~~\nmkdir -p /kfly/doc/phoenixsql\ncd /kfly/install/phoenixsql/\nvi user_phoenix.sql\n~~~\n\n​\t内容如下\n\n```sql\ncreate table if not exists user_phoenix (state varchar(10) NOT NULL,  city varchar(20) NOT NULL,  population BIGINT  CONSTRAINT my_pk PRIMARY KEY (state, city));\n```\n\n* node02执行以下命令，创建user_phoenix.csv数据文件\n\n~~~\ncd /kfly/install/phoenixsql/\nvi user_phoenix.csv\n~~~\n\n​\t内容如下\n\n```\nNY,New York,8143197\nCA,Los Angeles,3844829\nIL,Chicago,2842518\nTX,Houston,2016582\nPA,Philadelphia,1463281\nAZ,Phoenix,1461575\nTX,San Antonio,1256509\nCA,San Diego,1255540\nTX,Dallas,1213825\nCA,San Jose,912332\n```\n\n* 创建user_phoenix_query.sql文件\n\n~~~\ncd /kfly/doc/phoenixsql\nvi user_phoenix_query.sql\n~~~\n\n​\t\t内容如下\n\n```sql\nselect state as \"userState\",count(city) as \"City Count\",sum(population) as \"Population Sum\" FROM user_phoenix GROUP BY state; \n```\n\n* 执行sql语句\n\n~~~shell\ncd /kfly/doc/phoenixsql\n\n/kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/psql.py  node01:2181 user_phoenix.sql user_phoenix.csv user_phoenix_query.sql\n~~~\n\n\n\n## 5.2 命令行方式\n\n* 执行命令\n\n~~~shell\ncd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/\nbin/sqlline.py node01:2181\n~~~\n\n* 退出命令行方式\n\n  phoenix的命令都需要一个==感叹号==\n\n~~~\n!quit\n~~~\n\n- 查看phoenix的帮助文档，显示所有命令；用!help\n\n  ```\n  0: jdbc:phoenix:node01:2181> !help\n  !all                Execute the specified SQL against all the current\n                    connections\n  !autocommit         Set autocommit mode on or off\n  !batch              Start or execute a batch of statements\n  !brief              Set verbose mode off\n  !call               Execute a callable statement\n  !close              Close the current connection to the database\n  !closeall           Close all current open connections\n  !columns            List all the columns for the specified table\n  !commit             Commit the current transaction (if autocommit is off)\n  !connect            Open a new connection to the database.\n  !dbinfo             Give metadata information about the database\n  !describe           Describe a table\n  !dropall            Drop all tables in the current database\n  !exportedkeys       List all the exported keys for the specified table\n  !go                 Select the current connection\n  !help               Print a summary of command usage\n  !history            Display the command history\n  !importedkeys       List all the imported keys for the specified table\n  !indexes            List all the indexes for the specified table\n  !isolation          Set the transaction isolation for this connection\n  !list               List the current connections\n  !manual             Display the SQLLine manual\n  !metadata           Obtain metadata information\n  !nativesql          Show the native SQL for the specified statement\n  !outputformat       Set the output format for displaying results\n                    (table,vertical,csv,tsv,xmlattrs,xmlelements)\n  !primarykeys        List all the primary keys for the specified table\n  !procedures         List all the procedures\n  !properties         Connect to the database specified in the properties file(s)\n  !quit               Exits the program\n  !reconnect          Reconnect to the database\n  !record             Record all output to the specified file\n  !rehash             Fetch table and column names for command completion\n  !rollback           Roll back the current transaction (if autocommit is off)\n  !run                Run a script from the specified file\n  !save               Save the current variabes and aliases\n  !scan               Scan for installed JDBC drivers\n  !script             Start saving a script to a file\n  !set                Set a sqlline variable\n  ```\n\nVariable        Value      Description\n=============== ========== ================================\nautoCommit      true/false Enable/disable automatic\n                           transaction commit\nautoSave        true/false Automatically save preferences\ncolor           true/false Control whether color is used\n                           for display\nfastConnect     true/false Skip building table/column list\n                           for tab-completion\nforce           true/false Continue running script even\n                           after errors\nheaderInterval  integer    The interval between which\n                           headers are displayed\nhistoryFile     path       File in which to save command\n                           history. Default is\n                           $HOME/.sqlline/history (UNIX,\n                           Linux, Mac OS),\n                           $HOME/sqlline/history (Windows)\nincremental     true/false Do not receive all rows from\n                           server before printing the first\n                           row. Uses fewer resources,\n                           especially for long-running\n                           queries, but column widths may\n                           be incorrect.\nisolation       LEVEL      Set transaction isolation level\nmaxColumnWidth  integer    The maximum width to use when\n                           displaying columns\nmaxHeight       integer    The maximum height of the\n                           terminal\nmaxWidth        integer    The maximum width of the\n                           terminal\nnumberFormat    pattern    Format numbers using\n                           DecimalFormat pattern\noutputFormat    table/vertical/csv/tsv Format mode for\n                           result display\npropertiesFile  path       File from which SqlLine reads\n                           properties on startup; default is\n                           $HOME/.sqlline/sqlline.properties\n                           (UNIX, Linux, Mac OS),\n                           $HOME/sqlline/sqlline.properties\n                           (Windows)\nrowLimit        integer    Maximum number of rows returned\n                           from a query; zero means no\n                           limit\nshowElapsedTime true/false Display execution time when\n                           verbose\nshowHeader      true/false Show column names in query\n                           results\nshowNestedErrs  true/false Display nested errors\nshowWarnings    true/false Display connection warnings\nsilent          true/false Be more silent\ntimeout         integer    Query timeout in seconds; less\n                           than zero means no timeout\ntrimScripts     true/false Remove trailing spaces from\n                           lines read from script files\nverbose         true/false Show verbose error messages and\n                           debug info\n!sql                Execute a SQL command\n!tables             List all the tables in the database\n!typeinfo           Display the type map for the current connection\n!verbose            Set verbose mode on\n\n  ```\n* 1、建立employee的映射表\n\n  进入hbase客户端，创建一个普通表employee，并且有两个列族 company 和family\n\n  ~~~\n  cd /kfly/install/hbase-1.2.0-cdh5.14.2/\n  bin/hbase shell\n  hbase(main):001:0> create 'employee','company','family'\n  ~~~\n\n* 2、添加数据\n\n  ~~~\n  put 'employee','row1','company:name','ted'\n  put 'employee','row1','company:position','worker'\n  put 'employee','row1','family:tel','13600912345'\n  put 'employee','row2','company:name','michael'\n  put 'employee','row2','company:position','manager'\n  put 'employee','row2','family:tel','1894225698'\n  ~~~\n\n* 3、建立hbase到phoenix的映射表\n\n  node02进入到phoenix的客户端，然后创建映射表\n\n  ~~~\n  cd kflyb/install/apache-phoenix-4.14.0-cdh5.14.2-bin\n  bin/sqlline.py node01:2181\n  ~~~\n\n  执行语句\n\n  ```sql\n  CREATE TABLE IF NOT EXISTS \"employee\" (\"no\" VARCHAR(10) NOT NULL PRIMARY KEY, \"company\".\"name\" VARCHAR(30),\"company\".\"position\" VARCHAR(20), \"family\".\"tel\" VARCHAR(20), \"family\".\"age\" INTEGER) column_encoded_bytes=0;\n\n  ```\n\n  > 说明\n  >\n  > 在建立映射表之前要说明的是，Phoenix是==大小写敏感==的，并且所有命令都是大写，如果你建的表名没有用双引号括起来，那么无论你输入的是大写还是小写，建立出来的表名都是大写的，如果你需要建立出同时包含大写和小写的表名和字段名，请把表名或者字段名用双引号括起来。 \n\n* 4、查询映射表数据\n\n~~~\n0: jdbc:phoenix:node1:2181> select * from \"employee\";\n+-------+----------+-----------+--------------+-------+\n|  no   |   name   | position  |     tel      |  age  |\n+-------+----------+-----------+--------------+-------+\n| row1  | ted      | worker    | 13600912345  | null  |\n| row2  | michael  | manager   | 1894225698   | null  |\n+-------+----------+-----------+--------------+-------+\n\n0: jdbc:phoenix:node01:2181> select * from \"employee\" where \"tel\" = '13600912345';\n+-------+-------+-----------+--------------+-------+\n|  no   | name  | position  |     tel      |  age  |\n+-------+-------+-----------+--------------+-------+\n| row1  | ted   | worker    | 13600912345  | null  |\n+-------+-------+-----------+--------------+-------+\n\n\n~~~\n\n## 5.3 GUI方式\n\n- 通过dbeaver来连接phoenix\n- [点击下载]( https://dbeaver.io/files/6.2.4/dbeaver-ce-6.2.4-macos.dmg)\n\n### 5.3.1 准备两个文件\n\n- 我们通过dbeaver来连接phoenix需要两个文件\n\n  - 第一个文件是phoenix-4.14.0-cdh5.14.2-client.jar\n  - 第二个文件是hbase-site.xml\n\n- 进入到phoenix的安装目录，获取第一个文件\n\n  node02执行以下命令，进入到以下路径，获取第一个文件\n\n  找到 phoenix-4.14.0-cdh5.14.2-client.jar  这个jar包，并将其下载下来备用\n\n```shell\ncd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin\n\n```\n\n- 然后进入到node02服务器的hbase的安装配置文件路径，获取hbase-site.xml这个文件\n\n  找到hbase-site.xml，将其下载下来备用\n\n```shell\ncd /kfly/install/hbase-1.2.0-cdh5.14.2/conf/\n\n```\n\n\n\n### 5.3.2 更新jar包\n\n- 将hbase-site.xml放到phoenix-4.14.0-cdh5.14.2-client.jar这个jar包里面去\n\n- 我们在第一步找到了hbase-site.xml和phoenix-4.14.0-cdh5.14.2-client.jar 这两个文件之后，我们需要使用解压缩工具，将phoenix-4.14.0-cdh5.14.2-client.jar 这个jar包解压开，然后将hbase-site.xml放入到phoenix-4.14.0-cdh5.14.2-client.jar 这个jar包里面去\n\n![1571282342354](/Users/dingchuangshi/Downloads/20191115-HBase第四次课/phoenix安装部署/assets/1571282342354.png)\n\n### 5.3.3 通过dbeaver连接phoenix\n\n![1571282405612](/Users/dingchuangshi/Downloads/20191115-HBase第四次课/phoenix安装部署/assets/1571282405612.png)\n\n![1571282452922](/Users/dingchuangshi/Downloads/20191115-HBase第四次课/phoenix安装部署/assets/1571282452922.png)\n\n![1571282684917](/Users/dingchuangshi/Downloads/20191115-HBase第四次课/phoenix安装部署/assets/1571282684917.png)\n\n- 注意：如果连接不上，可能不是操作配置的问题，有可能是因为dbeaver软件的问题，将dbeaver软件重启几次试试看\n\n### 5.4.4 创建数据库表，并实现sql进行操作\n\n- 直接在phoenix当中通过sql语句的方式来创建表并\n\n```sql\nCREATE TABLE IF NOT EXISTS US_POPULATION (\n      state CHAR(2) NOT NULL,\n      city VARCHAR NOT NULL,\n      population BIGINT\n      CONSTRAINT my_pk PRIMARY KEY (state, city));\n\nUPSERT INTO US_POPULATION (state, city, population) values ('NY','New York',8143197);\nUPSERT INTO US_POPULATION (state, city, population) values ('CA','Los Angeles',3844829);\n\nSELECT * FROM US_POPULATION WHERE population > 8000000;\n```\n\n## 5.4 JDBC调用方式\n\n* 创建maven工程并导入jar包\n\n~~~xml\n<dependencies>\n    <dependency>\n        <groupId>org.apache.phoenix</groupId>\n        <artifactId>phoenix-core</artifactId>\n        <version>4.14.0-cdh5.14.2</version>\n    </dependency>\n    <dependency>\n        <groupId>junit</groupId>\n        <artifactId>junit</artifactId>\n        <version>4.12</version>\n    </dependency>\n    <dependency>\n        <groupId>org.testng</groupId>\n        <artifactId>testng</artifactId>\n        <version>6.14.3</version>\n    </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <!-- 限制jdk版本插件 -->\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                    <encoding>UTF-8</encoding>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n~~~\n\n* 代码开发\n\n~~~java\nimport org.testng.annotations.BeforeTest;\nimport org.testng.annotations.Test;\nimport java.sql.*;\npublic class PhoenixSearch {\n    private Connection connection;\n    private Statement statement;\n    private ResultSet rs;\n    @BeforeTest\n    public void init() throws SQLException {\n        //定义phoenix的连接url地址\n        String url=\"jdbc:phoenix:node01:2181\";\n        connection = DriverManager.getConnection(url);\n        //构建Statement对象\n        statement = connection.createStatement();\n    }\n    @Test\n    public void queryTable() throws SQLException {\n        //定义查询的sql语句，注意大小写\n        String sql=\"select * from US_POPULATION\";\n        //执行sql语句\n        try {\n            rs=statement.executeQuery(sql);\n            while(rs.next()){\n                System.out.println(\"state:\"+rs.getString(\"state\"));\n                System.out.println(\"city:\"+rs.getString(\"city\"));\n                System.out.println(\"population:\"+rs.getInt(\"population\"));\n                System.out.println(\"-------------------------\");\n            }\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }finally {\n            if(connection!=null){\n                connection.close();\n            }\n        }\n    }\n}\n~~~\n\n","tags":["环境搭建","phoenix"]},{"title":"大数据问题集合","url":"/2019/11/15/exceptions/大数据问题集合/","content":"\n## 1 Maven无法下载cdh 中jar包的问题\n\n### 1.1 问题描述 \n\n> 1. **maven仓库中、pom文件加载如下配置，无法下载jar包,示例如下：**\n\n```xml\n <dependency>\n     <groupId>org.apache.hbase</groupId>\n     <artifactId>hbase-client</artifactId>\n     <version>1.2.0-cdh5.14.2</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.hbase</groupId>\n    <artifactId>hbase-server</artifactId>\n    <version>1.2.0-cdh5.14.2</version>\n  </dependency>\n```\n\n![image-20191115173654291](img/image-20191115173654291.png)\n\n###1.2 解决方法\n\n#### 1.2.1 maven 配置文件setting.xml\n\n```xml\n# 在该用aliyun仓库的同时，使用cloudera\n# !cloudera 启用cloudera仓库\n<mirror>\n    <id>nexus-aliyun</id>\n    <mirrorOf>*,!cloudera</mirrorOf>\n    <name>Nexus aliyun</name>                     \n    <url>\n      http://maven.aliyun.com/nexus/content/groups/public\n    </url>\n</mirror>\n```\n\n#### 1.2.2 项目pom.xml\n\n![](img/image-201911111111.png)\n\n#### 1.2.3 刷新maven项目即可\n\n![Screen Shot 2019-11-15 at 17.48.43](img/Screen Shot 2019-11-15 at 17.48.43.png)\n\n## 2运行mr程序报错\n\n### 2.1问题描述\n\n```shell\n# 异常信息\n18:13:40 INFO mapred.JobClient: Cleaning up the staging area file:/kfly/install/hadoop-2.6.0/hadoopDatas/tempDatas/mapred/staging/dingchuangshi606034797/.staging/job_local606034797_0001\n19/11/15 18:13:40 WARN security.UserGroupInformation: PriviledgedActionException as:dingchuangshi (auth:SIMPLE) cause:ExitCodeException exitCode=1: chmod: /kfly/install/hadoop-2.6.0/hadoopDatas/tempDatas/mapred/staging/dingchuangshi606034797/.staging/job_local606034797_0001: No such file or directory\n\nException in thread \"main\" ExitCodeException exitCode=1: chmod: /kfly/install/hadoop-2.6.0/hadoopDatas/tempDatas/mapred/staging/dingchuangshi606034797/.staging/job_local606034797_0001: No such file or directory\n\n# 错误所在行\n return job.waitForCompletion(true) ? 0 :1;\n```\n\n\n\n### 2.2 解决方法\n\n```java\n// 配置加上目录\nconf.set(\"zookeeper.znode.parent\",\"/HBase\");\n// 有时连接到本地hadoop，需要加上\nconf.set(\"fs.defaultFS\",\"hdfs://node01:8020\");\n```\n\n","tags":["问题集合"]},{"title":"大数据开发之HBase（二）","url":"/2019/11/13/hbase/大数据开发之HBase（二）/","content":"\n# 大数据数据库之hbase\n\n##  1. HBase的数据存储原理\n\n\n\n![hbase存储架构](assets/hbase%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84.png?lastModify=1573631775)\n\n![img](assets/hbase_data_storage-1565601156263.png?lastModify=1573631775)\n\n- 一个HRegionServer会负责管理很多个region\n- 一个**==region==**包含很多个==store==\n  - 一个**==列族==**就划分成一个**==store==**\n  - 如果一个表中只有1个列族，那么每一个region中只有一个store\n  - 如果一个表中有N个列族，那么每一个region中有N个store\n- ==一个store==里面只有==一个memstore==\n  - memstore是一块**内存区域**，写入的数据会先写入memstore进行缓冲，然后再把数据刷到磁盘\n- 一个store里面有很多个**==StoreFile==**, 最后数据是以很多个**==HFile==**这种数据结构的文件保存在HDFS上\n  - StoreFile是HFile的抽象对象，如果说到StoreFile就等于HFile\n  - ==每次memstore刷写数据到磁盘，就生成对应的一个新的HFile文件出来==\n\n![region](assets/region.png?lastModify=1573631775)\n\n\n\n## 2. HBase读数据流程\n\n![img](assets/hbase%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png?lastModify=1573631775)\n\n> 说明：HBase集群，只有一张meta表，此表只有一个region，该region数据保存在一个HRegionServer上\n\n- 1、客户端首先与zk进行连接；从zk找到meta表的region位置，即meta表的数据存储在某一HRegionServer上；客户端与此HRegionServer建立连接，然后读取meta表中的数据；meta表中存储了所有用户表的region信息，我们可以通过`scan  'hbase:meta'`来查看meta表信息\n- 2、根据要查询的namespace、表名和rowkey信息。找到写入数据对应的region信息\n- 3、找到这个region对应的regionServer，然后发送请求\n- 4、查找并定位到对应的region\n- 5、先从memstore查找数据，如果没有，再从BlockCache上读取\n  - HBase上Regionserver的内存分为两个部分\n    - 一部分作为Memstore，主要用来写；\n    - 另外一部分作为BlockCache，主要用于读数据；\n- 6、如果BlockCache中也没有找到，再到StoreFile上进行读取\n  - 从storeFile中读取到数据之后，不是直接把结果数据返回给客户端，而是把数据先写入到BlockCache中，目的是为了加快后续的查询；然后在返回结果给客户端。\n\n\n\n## 3. HBase写数据流程\n\n![img](assets/hbase%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png?lastModify=1573631775)\n\n- 1、客户端首先从zk找到meta表的region位置，然后读取meta表中的数据，meta表中存储了用户表的region信息\n- 2、根据namespace、表名和rowkey信息。找到写入数据对应的region信息\n- 3、找到这个region对应的regionServer，然后发送请求\n- 4、把数据分别写到HLog（write ahead log）和memstore各一份\n- 5、memstore达到阈值后把数据刷到磁盘，生成storeFile文件\n- 6、删除HLog中的历史数据\n\n```\n补充：\nHLog（write ahead log）：\n  也称为WAL意为Write ahead log，类似mysql中的binlog,用来做灾难恢复时用，HLog记录数据的所有变更,一旦数据修改，就可以从log中进行恢复。\n```\n\n\n\n## 4. HBase的flush、compact机制\n\n![](assets/hbase-split-compaction.png)\n\n### 4.1 Flush触发条件\n\n#### 4.1.1 memstore级别限制\n\n- 当Region中任意一个MemStore的大小达到了上限（hbase.hregion.memstore.flush.size，默认128MB），会触发Memstore刷新。\n\n```xml\n<property>\n\t<name>hbase.hregion.memstore.flush.size</name>\n\t<value>134217728</value>\n</property>\n```\n\n  #### 4.1.2 region级别限制\n\n- 当Region中所有Memstore的大小总和达到了上限（hbase.hregion.memstore.block.multiplier * hbase.hregion.memstore.flush.size，默认 2* 128M = 256M），会触发memstore刷新。\n\n```xml\n<property>\n\t<name>hbase.hregion.memstore.flush.size</name>\n\t<value>134217728</value>\n</property>\n<property>\n\t<name>hbase.hregion.memstore.block.multiplier</name>\n\t<value>2</value>\n</property>   \n```\n\n#### 4.1.3 Region Server级别限制\n\n- 当一个Region Server中所有Memstore的大小总和超过低水位阈值hbase.regionserver.global.memstore.size.lower.limit*hbase.regionserver.global.memstore.size（前者默认值0.95），RegionServer开始强制flush；\n- 先Flush Memstore最大的Region，再执行次大的，依次执行；\n- 如写入速度大于flush写出的速度，导致总MemStore大小超过高水位阈值hbase.regionserver.global.memstore.size（默认为JVM内存的40%），此时RegionServer会阻塞更新并强制执行flush，直到总MemStore大小低于低水位阈值\n\n```xml\n<property>\n\t<name>hbase.regionserver.global.memstore.size.lower.limit</name>\n\t<value>0.95</value>\n</property>\n<property>\n\t<name>hbase.regionserver.global.memstore.size</name>\n\t<value>0.4</value>\n</property>\n```\n\n#### 4.1.4 HLog数量上限\n\n- 当一个Region Server中HLog数量达到上限（可通过参数hbase.regionserver.maxlogs配置）时，系统会选取最早的一个 HLog对应的一个或多个Region进行flush\n\n#### 4.1.5 定期刷新Memstore\n\n- 默认周期为1小时，确保Memstore不会长时间没有持久化。为避免所有的MemStore在同一时间都进行flush导致的问题，定期的flush操作有20000左右的随机延时。\n\n#### 4.1.6 手动flush\n\n- 用户可以通过shell命令`flush ‘tablename’`或者`flush ‘region name’`分别对一个表或者一个Region进行flush。\n\n### 4.2 flush的流程\n\n- 为了减少flush过程对读写的影响，将整个flush过程分为三个阶段：\n  - prepare阶段：遍历当前Region中所有的Memstore，将Memstore中当前数据集CellSkipListSet做一个**快照snapshot**；然后再新建一个CellSkipListSet。后期写入的数据都会写入新的CellSkipListSet中。prepare阶段需要加一把updateLock对**写请求阻塞**，结束之后会释放该锁。因为此阶段没有任何费时操作，因此持锁时间很短。\n\n  - flush阶段：遍历所有Memstore，将prepare阶段生成的snapshot持久化为**临时文件**，临时文件会统一放到目录.tmp下。这个过程因为涉及到磁盘IO操作，因此相对比较耗时。\n  - commit阶段：遍历所有Memstore，将flush阶段生成的临时文件移到指定的ColumnFamily目录下，针对HFile生成对应的storefile和Reader，把storefile添加到HStore的storefiles列表中，最后再**清空**prepare阶段生成的snapshot。\n\n### 4.3  Compact合并机制\n\n- hbase为了==防止小文件过多==，以保证查询效率，hbase需要在必要的时候将这些小的store file合并成相对较大的store file，这个过程就称之为compaction。\n\n- 在hbase中主要存在两种类型的compaction合并\n  - **==minor compaction 小合并==**\n  - **==major compaction 大合并==**\n\n#### 4.3.1 minor compaction 小合并\n\n- 在将Store中多个HFile合并为一个HFile\n\n  在这个过程中会选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，对于超过了TTL的数据、更新的数据、删除的数据仅仅只是做了标记。并没有进行物理删除，一次Minor Compaction的结果是更少并且更大的StoreFile。这种合并的触发频率很高。\n\n- minor compaction触发条件由以下几个参数共同决定：\n\n~~~xml\n<!--表示至少需要三个满足条件的store file时，minor compaction才会启动-->\n<property>\n\t<name>hbase.hstore.compactionThreshold</name>\n\t<value>3</value>\n</property>\n\n<!--表示一次minor compaction中最多选取10个store file-->\n<property>\n\t<name>hbase.hstore.compaction.max</name>\n\t<value>10</value>\n</property>\n\n<!--默认值为128m,\n表示文件大小小于该值的store file 一定会加入到minor compaction的store file中\n-->\n<property>\n\t<name>hbase.hstore.compaction.min.size</name>\n\t<value>134217728</value>\n</property>\n\n<!--默认值为LONG.MAX_VALUE，\n表示文件大小大于该值的store file 一定会被minor compaction排除-->\n<property>\n\t<name>hbase.hstore.compaction.max.size</name>\n\t<value>9223372036854775807</value>\n</property>\n~~~\n\n#### 4.3.2 major compaction 大合并\n\n* 合并Store中所有的HFile为一个HFile\n\n  将所有的StoreFile合并成一个StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL过期数据、版本号超过设定版本号的数据。合并频率比较低，默认7天执行一次，并且性能消耗非常大，建议生产关闭(设置为0)，在应用空闲时间手动触发。一般可以是手动控制进行合并，防止出现在业务高峰期。\n\n* major compaction触发时间条件\n\n  ~~~xml\n  <!--默认值为7天进行一次大合并，-->\n  <property>\n  \t<name>hbase.hregion.majorcompaction</name>\n  \t<value>604800000</value>\n  </property>\n  ~~~\n\n* 手动触发\n\n  ~~~ruby\n  ##使用major_compact命令\n  major_compact tableName\n  ~~~\n\n\n\n## 5. region 拆分机制\n\n- region中存储的是大量的rowkey数据 ,当region中的数据条数过多的时候,直接影响查询效率.当region过大的时候.hbase会拆分region , 这也是Hbase的一个优点 .\n\n- HBase的region split策略一共有以下几种：\n\n* 1、**ConstantSizeRegionSplitPolicy**\n\n  * 0.94版本前默认切分策略\n\n* 当region大小大于某个阈值(hbase.hregion.max.filesize=10G)之后就会触发切分，一个region等分为2个region。\n\n  * 但是在生产线上这种切分策略却有相当大的弊端：切分策略对于大表和小表没有明显的区分。阈值(hbase.hregion.max.filesize)设置较大对大表比较友好，但是小表就有可能不会触发分裂，极端情况下可能就1个，这对业务来说并不是什么好事。如果设置较小则对小表友好，但一个大表就会在整个集群产生大量的region，这对于集群的管理、资源使用、failover来说都不是一件好事。\n\n  \n\n* 2、**IncreasingToUpperBoundRegionSplitPolicy**\n\n  *  0.94版本~2.0版本默认切分策略\n\n  - 切分策略稍微有点复杂，总体看和ConstantSizeRegionSplitPolicy思路相同，一个region大小大于设置阈值就会触发切分。但是这个阈值并不像ConstantSizeRegionSplitPolicy是一个固定的值，而是会在一定条件下不断调整，调整规则和region所属表在当前regionserver上的region个数有关系.\n\n  - region split的计算公式是：\n    regioncount^3 * 128M * 2，当region达到该size的时候进行split\n    例如：\n    第一次split：1^3 * 256 = 256MB \n    第二次split：2^3 * 256 = 2048MB \n    第三次split：3^3 * 256 = 6912MB \n    第四次split：4^3 * 256 = 16384MB > 10GB，因此取较小的值10GB \n    后面每次split的size都是10GB了\n\n* 3、**SteppingSplitPolicy**\n\n  * 2.0版本默认切分策略\n\n  - 这种切分策略的切分阈值又发生了变化，相比 IncreasingToUpperBoundRegionSplitPolicy 简单了一些，依然和待分裂region所属表在当前regionserver上的region个数有关系，如果region个数等于1，\n    切分阈值为flush size * 2，否则为MaxRegionFileSize。这种切分策略对于大集群中的大表、小表会比 IncreasingToUpperBoundRegionSplitPolicy 更加友好，小表不会再产生大量的小region，而是适可而止。\n\n* 4、**KeyPrefixRegionSplitPolicy**\n\n  - 根据rowKey的前缀对数据进行分组，这里是指定rowKey的前多少位作为前缀，比如rowKey都是16位的，指定前5位是前缀，那么前5位相同的rowKey在进行region split的时候会分到相同的region中。\n\n* 5、**DelimitedKeyPrefixRegionSplitPolicy**\n\n  - 保证相同前缀的数据在同一个region中，例如rowKey的格式为：userid_eventtype_eventid，指定的delimiter为 _ ，则split的的时候会确保userid相同的数据在同一个region中。\n\n\n* 6、**DisabledRegionSplitPolicy**\n  * 不启用自动拆分, 需要指定手动拆分\n\n\n\n## 6. HBase表的预分区\n\n- 当一个table刚被创建的时候，Hbase默认的分配一个region给table。也就是说这个时候，所有的读写请求都会访问到同一个regionServer的同一个region中，这个时候就达不到负载均衡的效果了，集群中的其他regionServer就可能会处于比较空闲的状态。\n- 解决这个问题可以用**pre-splitting**,在创建table的时候就配置好，生成多个region。\n\n### 6.1 为何要预分区？\n\n* 增加数据读写效率\n* 负载均衡，防止数据倾斜\n* 方便集群容灾调度region\n* 优化Map数量\n\n### 6.2 预分区原理\n\n- 每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。\n\n### 6.3 手动指定预分区\n\n- 两种方式\n\n- 方式一\n\n~~~ruby\ncreate 'person','info1','info2',SPLITS => ['1000','2000','3000','4000']\n~~~\n\n![personSplit](assets/personSplit.png)\n\n* 方式二：也可以把分区规则创建于文件中\n\n  ~~~shell\n  cd /kfly/doc\n  \n  vim split.txt\n  ~~~\n\n  - 文件内容\n\n  ~~~\n  aaa\n  bbb\n  ccc\n  ddd\n  ~~~\n\n  - hbase shell中，执行命令\n\n  ~~~ruby\n  create 'student','info',SPLITS_FILE => '/kfly/install/split.txt'\n  ~~~\n\n  - 成功后查看web界面\n\n  ![splitFile](assets/splitFile.png)\n\n### 6.2.2 HexStringSplit 算法\n\n- HexStringSplit会将数据从“00000000”到“FFFFFFFF”之间的数据长度按照**n等分**之后算出每一段的其实rowkey和结束rowkey，以此作为拆分点。\n\n- 例如：\n\n  ```ruby\n  create 'mytable', 'base_info',' extra_info', {NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}\n  ```\n\n![hbasePreSplit](assets/hbasePreSplit.png)\n\n\n\n## 7. region 合并\n\n### 7.1 region合并说明\n\n- Region的合并不是为了性能,  而是出于维护的目的 .\n- 比如删除了大量的数据 ,这个时候每个Region都变得很小 ,存储多个Region就浪费了 ,这个时候可以把Region合并起来，进而可以减少一些Region服务器节点 \n\n### 7.2 如何进行region合并\n\n#### 7.2.1 通过Merge类冷合并Region\n\n- 执行合并前，==需要先关闭hbase集群==\n\n- 创建一张hbase表：\n\n```ruby\ncreate 'test','info1',SPLITS => ['1000','2000','3000']\n```\n\n- 查看表region\n\n![testRegion](assets/testRegion.png)\n\n- 需求：\n\n  需要把test表中的2个region数据进行合并：\n  test,,1565940912661.62d28d7d20f18debd2e7dac093bc09d8.\n  test,1000,1565940912661.5b6f9e8dad3880bcc825826d12e81436.\n\n- 这里通过org.apache.hadoop.hbase.util.Merge类来实现，不需要进入hbase shell，直接执行（==需要先关闭hbase集群==）：\n  hbase org.apache.hadoop.hbase.util.Merge test test,,1565940912661.62d28d7d20f18debd2e7dac093bc09d8. test,1000,1565940912661.5b6f9e8dad3880bcc825826d12e81436.\n\n- 成功后界面观察\n\n![testMerge](assets/testMerge.png)\n\n#### 7.2.2  通过online_merge热合并Region\n\n- ==不需要关闭hbase集群==，在线进行合并\n\n- 与冷合并不同的是，online_merge的传参是Region的hash值，而Region的hash值就是Region名称的最后那段在两个.之间的字符串部分。\n\n- 需求：需要把test表中的2个region数据进行合并：\n  test,2000,1565940912661.c2212a3956b814a6f0d57a90983a8515.\n  test,3000,1565940912661.553dd4db667814cf2f050561167ca030.\n\n- 需要进入hbase shell：\n\n  ```ruby\n  merge_region 'c2212a3956b814a6f0d57a90983a8515','553dd4db667814cf2f050561167ca030'\n  ```\n\n- 成功后观察界面\n\n![online_merge](assets/online_merge.png)\n\n## 8. HBase集成MapReduce\n\n* HBase表中的数据最终都是存储在HDFS上，HBase天生的支持MR的操作，我们可以通过MR直接处理HBase表中的数据，并且MR可以将处理后的结果直接存储到HBase表中。\n  * 参考地址：<http://hbase.apache.org/book.html#mapreduce>\n\n### 8.1 实战一\n\n* 需求：==读取HBase当中myuser这张表的数据，将数据写入到另外一张myuser2表里面去==\n\n- 第一步：创建myuser2这张hbase表\n\n  **注意：**列族的名字要与myuser表的列族名字相同\n\n```ruby\nhbase(main):010:0> create 'myuser2','f1'\n```\n\n- 第二步：创建maven工程并导入jar包\n\n```xml\n\t<repositories>\n        <repository>\n            <id>cloudera</id>\n            <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n        </repository>\n    </repositories>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.6.0-mr1-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hbase</groupId>\n            <artifactId>hbase-client</artifactId>\n            <version>1.2.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hbase</groupId>\n            <artifactId>hbase-server</artifactId>\n            <version>1.2.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.testng</groupId>\n            <artifactId>testng</artifactId>\n            <version>6.14.3</version>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                    <encoding>UTF-8</encoding>\n                    <!--    <verbal>true</verbal>-->\n                </configuration>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.2</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <filters>\n                                <filter>\n                                    <artifact>*:*</artifact>\n                                    <excludes>\n                                        <exclude>META-INF/*.SF</exclude>\n                                        <exclude>META-INF/*.DSA</exclude>\n                                        <exclude>META-INF/*/RSA</exclude>\n                                    </excludes>\n                                </filter>\n                            </filters>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n- 第三步：开发MR程序实现功能\n\n~~~java\npackage com.kaikeba;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.hbase.Cell;\nimport org.apache.hadoop.hbase.CellUtil;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.client.Result;\nimport org.apache.hadoop.hbase.client.Scan;\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable;\nimport org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;\nimport org.apache.hadoop.hbase.mapreduce.TableMapper;\nimport org.apache.hadoop.hbase.mapreduce.TableReducer;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\n\nimport java.io.IOException;\n\npublic class HBaseMR {\n\n    public static class HBaseMapper extends TableMapper<Text,Put>{\n        @Override\n        protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException {\n             //获取rowkey的字节数组\n            byte[] bytes = key.get();\n            String rowkey = Bytes.toString(bytes);\n            //构建一个put对象\n            Put put = new Put(bytes);\n            //获取一行中所有的cell对象\n            Cell[] cells = value.rawCells();\n            for (Cell cell : cells) {\n                  // f1列族\n                if(\"f1\".equals(Bytes.toString(CellUtil.cloneFamily(cell)))){\n                    // name列名\n                     if(\"name\".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))){\n                          put.add(cell);\n                     }\n                     // age列名\n                    if(\"age\".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))){\n                        put.add(cell);\n                    }\n                }\n            }\n            if(!put.isEmpty()){\n              context.write(new Text(rowkey),put);\n            }\n        }\n    }\n\n     public  static  class HbaseReducer extends TableReducer<Text,Put,ImmutableBytesWritable>{\n         @Override\n         protected void reduce(Text key, Iterable<Put> values, Context context) throws IOException, InterruptedException {\n             for (Put put : values) {\n                 context.write(null,put);\n             }\n         }\n     }\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        Configuration conf = new Configuration();\n\n        Scan scan = new Scan();\n\n        Job job = Job.getInstance(conf);\n        job.setJarByClass(HBaseMR.class);\n        //使用TableMapReduceUtil 工具类来初始化我们的mapper\n        TableMapReduceUtil.initTableMapperJob(TableName.valueOf(args[0]),scan,HBaseMapper.class,Text.class,Put.class,job);\n        //使用TableMapReduceUtil 工具类来初始化我们的reducer\n        TableMapReduceUtil.initTableReducerJob(args[1],HbaseReducer.class,job);\n        //设置reduce task个数\n         job.setNumReduceTasks(1);\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n~~~\n\n* 打成jar包提交到集群中运行\n\n  ~~~shell\n  hadoop jar hbase_java_api-1.0-SNAPSHOT.jar com.kaikeba.HBaseMR t1 t2\n  ~~~\n\n\n### 8.2 实战二\n\n* 需求 读取hdfs上面的数据，写入到hbase表里面去\n\n  node03执行以下命令准备数据文件，并将数据文件上传到HDFS上面去\n\n  ~~~\n  hdfs dfs -mkdir -p /hbase/input\n  cd /kfly/install\n  vim \n  user.txt\n  \n  0007\tzhangsan\t18\n  0008\tlisi\t25\n  0009\twangwu\t20\n  \n  将文件上传到hdfs的路径下面去\n  hdfs dfs -put kflyb/install/user.txt   /hbase/input/\n  ~~~\n\n* 代码开发\n\n ~~~java\npackage com.kaikeba;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable;\nimport org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;\nimport org.apache.hadoop.hbase.mapreduce.TableReducer;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport java.io.IOException;\n\n\n\npublic class Hdfs2Hbase {\n\n    public static class HdfsMapper extends Mapper<LongWritable,Text,Text,NullWritable> {\n\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            context.write(value,NullWritable.get());\n        }\n    }\n\n    public static class HBASEReducer extends TableReducer<Text,NullWritable,ImmutableBytesWritable> {\n\n        protected void reduce(Text key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {\n            String[] split = key.toString().split(\" \");\n            Put put = new Put(Bytes.toBytes(split[0]));\n            put.addColumn(\"f1\".getBytes(),\"name\".getBytes(),split[1].getBytes());\n            put.addColumn(\"f1\".getBytes(),\"age\".getBytes(), split[2].getBytes());\n            context.write(new ImmutableBytesWritable(Bytes.toBytes(split[0])),put);\n        }\n    }\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        job.setJarByClass(Hdfs2Hbase.class);\n\n        job.setInputFormatClass(TextInputFormat.class);\n        //输入文件路径\n        TextInputFormat.addInputPath(job,new Path(args[0]));\n        job.setMapperClass(HdfsMapper.class);\n        //map端的输出的key value 类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(NullWritable.class);\n\n        //指定输出到hbase的表名\n        TableMapReduceUtil.initTableReducerJob(args[1],HBASEReducer.class,job);\n\n        //设置reduce个数\n        job.setNumReduceTasks(1);\n\n        System.exit(job.waitForCompletion(true)?0:1);\n    }\n}\n\n ~~~\n\n* 创建hbase表 t3\n\n~~~ruby\ncreate 't3','f1'\n~~~\n\n\n\n* 打成jar包提交到集群中运行\n\n~~~shell\nhadoop jar hbase_java_api-1.0-SNAPSHOT.jar com.kaikeba.Hdfs2Hbase /data/user.txt t3\n\n~~~\n\n\n\n### 8.3 实战三\n\n* 需求\n\n  * ==通过bulkload的方式批量加载数据到HBase表中==\n  * ==将我们hdfs上面的这个路径/hbase/input/user.txt的数据文件，转换成HFile格式，然后load到myuser2这张表里面去==\n\n* 知识点描述\n\n  - 加载数据到HBase当中去的方式多种多样，我们可以使用HBase的javaAPI或者使用sqoop将我们的数据写入或者导入到HBase当中去，但是这些方式不是慢就是在导入的过程的占用Region资源导致效率低下\n  - 我们也可以通过MR的程序，将我们的数据直接转换成HBase的最终存储格式HFile，然后直接load数据到HBase当中去即可\n\n* HBase数据正常写流程回顾\n\n  ![hbase-write](assets/hbase-write.png)\n\n* bulkload方式的处理示意图\n\n![](assets/bulkload.png)\n\n\n\n* 好处\n\n  - 导入过程不占用Region资源\n  - 能快速导入海量的数据\n  - 节省内存\n\n* ==1、开发生成HFile文件的代码==\n\n~~~java\npackage com.kaikeba;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.Connection;\nimport org.apache.hadoop.hbase.client.ConnectionFactory;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.client.Table;\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable;\nimport org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class HBaseLoad {\n\n    public static class LoadMapper  extends Mapper<LongWritable,Text,ImmutableBytesWritable,Put> {\n        @Override\n        protected void map(LongWritable key, Text value, Mapper.Context context) throws IOException, InterruptedException {\n            String[] split = value.toString().split(\" \");\n            Put put = new Put(Bytes.toBytes(split[0]));\n            put.addColumn(\"f1\".getBytes(),\"name\".getBytes(),split[1].getBytes());\n            put.addColumn(\"f1\".getBytes(),\"age\".getBytes(), split[2].getBytes());\n            context.write(new ImmutableBytesWritable(Bytes.toBytes(split[0])),put);\n        }\n    }\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n            final String INPUT_PATH=  \"hdfs://node01:8020/hbase/input\";\n            final String OUTPUT_PATH= \"hdfs://node01:8020/hbase/output_file\";\n            Configuration conf = HBaseConfiguration.create();\n\n            Connection connection = ConnectionFactory.createConnection(conf);\n            Table table = connection.getTable(TableName.valueOf(\"t4\"));\n            Job job= Job.getInstance(conf);\n\n            job.setJarByClass(HBaseLoad.class);\n            job.setMapperClass(LoadMapper.class);\n            job.setMapOutputKeyClass(ImmutableBytesWritable.class);\n            job.setMapOutputValueClass(Put.class);\n        \n            //指定输出的类型HFileOutputFormat2\n            job.setOutputFormatClass(HFileOutputFormat2.class);\n\n         HFileOutputFormat2.configureIncrementalLoad(job,table,connection.getRegionLocator(TableName.valueOf(\"t4\")));\n            FileInputFormat.addInputPath(job,new Path(INPUT_PATH));\n            FileOutputFormat.setOutputPath(job,new Path(OUTPUT_PATH));\n            System.exit(job.waitForCompletion(true)?0:1);\n\n\n    }\n}\n\n~~~\n\n* ==2、打成jar包提交到集群中运行==\n\n~~~shell\nhadoop jar hbase_java_api-1.0-SNAPSHOT.jar com.kaikeba.HBaseLoad\n~~~\n\n* ==3、观察HDFS上输出的结果==\n\n![f1](assets/f1.png)\n\n![HFile文件](assets/HFile文件.png)\n\n* ==4、加载HFile文件到hbase表中==\n\n  * 方式一：代码加载\n\n  ~~~java\n  package com.kaikeba;\n  \n  import org.apache.hadoop.conf.Configuration;\n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.hbase.HBaseConfiguration;\n  import org.apache.hadoop.hbase.TableName;\n  import org.apache.hadoop.hbase.client.Admin;\n  import org.apache.hadoop.hbase.client.Connection;\n  import org.apache.hadoop.hbase.client.ConnectionFactory;\n  import org.apache.hadoop.hbase.client.Table;\n  import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;\n  \n  public class LoadData {\n      public static void main(String[] args) throws Exception {\n          Configuration configuration = HBaseConfiguration.create();\n          configuration.set(\"hbase.zookeeper.quorum\", \"node01,node02,node03\");\n      //获取数据库连接\n      Connection connection =  ConnectionFactory.createConnection(configuration);\n      //获取表的管理器对象\n      Admin admin = connection.getAdmin();\n      //获取table对象\n      TableName tableName = TableName.valueOf(\"t4\");\n      Table table = connection.getTable(tableName);\n      //构建LoadIncrementalHFiles加载HFile文件\n      LoadIncrementalHFiles load = new LoadIncrementalHFiles(configuration);\n      load.doBulkLoad(new Path(\"hdfs://node01:8020/hbase/output_file\"), admin,table,connection.getRegionLocator(tableName));\n   }\n  }\n  ~~~\n\n  * 方式二：命令加载\n\n  先将hbase的jar包添加到hadoop的classpath路径下\n\n  ```shell\n  先将hbase的jar包添加到hadoop的classpath路径下\n  export HBASE_HOME=/kfly/install/hbase-1.2.0-cdh5.14.2/\n  export HADOOP_HOME=/kfly/install/hadoop-2.6.0/\n  export HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase mapredcp`\n  ```\n\n- 运行命令\n\n  ```shell\n  yarn jar /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar   completebulkload /hbase/output_hfile myuser2\n  ```\n\n​\t\n\n## 8. HBase集成Hive\n\n- Hive提供了与HBase的集成，使得能够在HBase表上使用hive sql 语句进行查询、插入操作以及进行Join和Union等复杂查询，同时也可以将hive表中的数据映射到Hbase中\n\n### 8.1 HBase与Hive的对比（\n\n#### 8.1.1 Hive\n\n- 数据仓库\n\n  Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。\n\n- 用于数据分析、清洗                \n\n  Hive适用于离线的数据分析和清洗，延迟较高\n\n- 基于HDFS、MapReduce\n\n  Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。（不要钻不需要执行MapReduce代码的情况的牛角尖）\n\n#### 8.1.2 HBase\n\n- 数据库\n\n  是一种面向列存储的非关系型数据库。\n\n- 用于存储结构化和非结构话的数据\n\n  适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。\n\n- 基于HDFS\n\n  数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。\n\n- 延迟较低，接入在线业务使用\n\n  面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。\n\n##### 8.1.3 总结：Hive与HBase\n\n- Hive和Hbase是两种基于Hadoop的不同技术，Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到HBase，或者从HBase写回Hive。\n\n### 9.2 整合配置\n\n#### 9.2.1 拷贝jar包\n\n- 将我们HBase的五个jar包拷贝到hive的lib目录下\n\n- hbase的jar包都在/kfly/install/hbase-1.2.0-cdh5.14.2/lib\n\n- 我们需要拷贝五个jar包名字如下\n\n```\nhbase-client-1.2.0-cdh5.14.2.jar                  \nhbase-hadoop2-compat-1.2.0-cdh5.14.2.jar \nhbase-hadoop-compat-1.2.0-cdh5.14.2.jar  \nhbase-it-1.2.0-cdh5.14.2.jar    \nhbase-server-1.2.0-cdh5.14.2.jar\n```\n\n- 我们直接在node03执行以下命令，通过创建软连接的方式来进行jar包的依赖\n\n```shell\nln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar              /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar   \n\nln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-hadoop2-compat-1.2.0-cdh5.14.2.jar      /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-hadoop2-compat-1.2.0-cdh5.14.2.jar             \nln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-hadoop-compat-1.2.0-cdh5.14.2.jar       /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-hadoop-compat-1.2.0-cdh5.14.2.jar            \nln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-it-1.2.0-cdh5.14.2.jar     /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-it-1.2.0-cdh5.14.2.jar    \n\nln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar          /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar  \n```\n\n#### 9.2.2 修改hive的配置文件\n\n- 编辑**node03**服务器上面的hive的配置文件hive-site.xml\n\n```shell\ncd /kfly/install/hive-1.1.0-cdh5.14.2/conf\nvim hive-site.xml\n```\n\n-  添加以下两个属性的配置\n\n```xml\n<property>\n\t\t<name>hive.zookeeper.quorum</name>\n\t\t<value>node01,node02,node03</value>\n</property>\n <property>\n\t\t<name>hbase.zookeeper.quorum</name>\n\t\t<value>node01,node02,node03</value>\n</property>\n```\n\n#### 9.2.3 修改hive-env.sh配置文件\n\n```shell\ncd /kfly/install/hive-1.1.0-cdh5.14.2/conf\nvim hive-env.sh\n```\n\n- 添加以下配置\n\n```\nexport HADOOP_HOME=/export/servers/hadoop-2.6.0\nexport HBASE_HOME=/export/servers/hbase-1.2.0-cdh5.14.2\nexport HIVE_CONF_DIR=/export/servers/hive-1.1.0-cdh5.14.2/conf\n```\n\n\n\n### 9.3 需求一：将hive表当中分析的结果保存到hbase表当中去\n\n#### 9.3.1 hive当中建表\n\n- node03执行以下命令，进入hive客户端，并创建hive表\n\n```shell\ncd /kfly/install/hive-1.1.0-cdh5.14.2/\nbin/hive\n```\n\n- 创建hive数据库与hive对应的数据库表\n\n```mysql\ncreate database course;\nuse course;\n\ncreate external table if not exists course.score(id int,cname string,score int) row format delimited fields terminated by '\\t' stored as textfile ;\n```\n\n#### 9.3.2 准备数据内容如下并加载到hive表\n\n- node03执行以下命令，创建数据文件\n\n```shell\ncd /kfly/install/hivedatas\nvi hive-hbase.txt\n```\n\n- 文件内容如下\n\n```\n1\tzhangsan\t80\n2\tlisi\t60\n3\twangwu\t30\n4\tzhaoliu\t70\n```\n\n- 进入hive客户端进行加载数据\n\n```mysql\nhive (course)> load data local inpath '/kfly/doc/hive-hbase.txt' into table score;\nhive (course)> select * from score;\n```\n\n#### 9.3.3 创建hive管理表与HBase进行映射\n\n- 我们可以创建一个hive的管理表与hbase当中的表进行映射，hive管理表当中的数据，都会存储到hbase上面去\n\n- hive当中创建内部表\n\n```sql\ncreate table course.hbase_score(id int,cname string,score int) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'  with serdeproperties(\"hbase.columns.mapping\" = \"cf:name,cf:score\") tblproperties(\"hbase.table.name\" = \"hbase_score\");\n```\n\n- 通过insert  overwrite select  插入数据\n\n```mysql\ninsert overwrite table course.hbase_score select id,cname,score from course.score;\n```\n\n#### 9.3.4 hbase当中查看表hbase_score\n\n- 进入hbase的客户端查看表hbase_score，并查看当中的数据\n\n```ruby\nhbase(main):023:0> list\n\nTABLE                                                                                 hbase_score                                                                           myuser                                                                                 myuser2                                                                               student                                                                               user                                                                                   5 row(s) in 0.0210 seconds\n=> [\"hbase_score\", \"myuser\", \"myuser2\", \"student\", \"user\"]\n\nhbase(main):024:0> scan 'hbase_score'\n\nROW                      COLUMN+CELL                                                   \n 1                       column=cf:name, timestamp=1550628395266, value=zhangsan       \n 1                       column=cf:score, timestamp=1550628395266, value=80           \n 2                       column=cf:name, timestamp=1550628395266, value=lisi           \n 2                       column=cf:score, timestamp=1550628395266, value=60           \n 3                       column=cf:name, timestamp=1550628395266, value=wangwu         \n 3                       column=cf:score, timestamp=1550628395266, value=30           \n 4                       column=cf:name, timestamp=1550628395266, value=zhaoliu       \n 4                       column=cf:score, timestamp=1550628395266, value=70           \n4 row(s) in 0.0360 seconds\n```\n\n### 9.4 需求二：创建hive外部表，映射HBase当中已有的表模型（5分钟）\n\n#### 9.4.1 HBase当中创建表并手动插入加载一些数据\n\n- 进入HBase的shell客户端，\n\n```shell\nbin/hbase shell\n```\n\n- 手动创建一张表，并插入加载一些数据进去\n\n```ruby\n# 创建一张表\ncreate 'hbase_hive_score',{ NAME =>'cf'}\n# 通过put插入数据到hbase表\nput 'hbase_hive_score','1','cf:name','zhangsan'\nput 'hbase_hive_score','1','cf:score', '95'\nput 'hbase_hive_score','2','cf:name','lisi'\nput 'hbase_hive_score','2','cf:score', '96'\nput 'hbase_hive_score','3','cf:name','wangwu'\nput 'hbase_hive_score','3','cf:score', '97'\n```\n\n#### 9.4.2 建立hive的外部表，映射HBase当中的表以及字段\n\n- 在hive当中建立外部表\n\n- 进入hive客户端，然后执行以下命令进行创建hive外部表，就可以实现映射HBase当中的表数据\n\n```mysql\nCREATE external TABLE course.hbase2hive(id int, name string, score int) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf:name,cf:score\") TBLPROPERTIES(\"hbase.table.name\" =\"hbase_hive_score\");\n```\n\n","tags":["hadoop","hbase","hbase与hive整合"]},{"title":"大数据开发之Hbase（一）","url":"/2019/11/11/hbase/大数据开发之Hbase（一）/","content":"\n# 大数据开发之HBase\n\n## 1. HBase是什么\n\n- [漫画学习HBase----最易懂的Hbase架构原理解析](<http://developer.51cto.com/art/201904/595698.htm>)\n\n### 1.1 HBase的概念\n\n* HBase基于Google的BigTable论文，是建立的==HDFS==之上，提供**高可靠性**、**高性能**、**列存储**、**可伸缩**、**实时读写**的分布式数据库系统。\n* 在需要==实时读写随机访问==超大规模数据集时，可以使用HBase。\n\n### 1.2 HBase的特点\n\n* ==**海量存储**==\n  * 可以存储大批量的数据\n* ==**列式存储**==\n  * HBase表的数据是基于列族进行存储的，列族是在列的方向上的划分。\n* ==**极易扩展**==\n  * 底层依赖HDFS，当磁盘空间不足的时候，只需要动态增加datanode节点就可以了\n  * 可以通过增加服务器来对集群的存储进行扩容\n* ==**高并发**==\n  * 支持高并发的读写请求\n* ==**稀疏**==\n  * 稀疏主要是针对HBase列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的。\n* ==**数据的多版本**==\n  - HBase表中的数据可以有多个版本值，默认情况下是根据版本号去区分，版本号就是插入数据的时间戳\n* ==**数据类型单一**==\n  * 所有的数据在HBase中是以==字节数组==进行存储\n\n\n\n## 2. HBase集群安装部署\n\n​\t[点击跳转](https://kfly.top/2019/11/26/hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n\n## 3. HBase表的数据模型\n\n![](assets/HBase-data-model.png.png)\n\n### 3.1 rowkey行键\n\n- table的主键，table中的记录按照rowkey 的字典序进行排序\n- Row key行键可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)\n\n### 3.2 Column Family列族\n\n- 列族或列簇\n- HBase表中的每个列，都归属与某个列族\n- 列族是表的schema的一部分(而列不是)，即建表时至少指定一个列族\n- 比如创建一张表，名为`user`，有两个列族，分别是`info`和`data`，建表语句`create 'user', 'info', 'data'`\n\n### 3.3 Column列\n\n- 列肯定是表的某一列族下的一个列，用`列族名:列名`表示，如`info`列族下的`name`列，表示为`info:name`\n- 属于某一个ColumnFamily,类似于我们mysql当中创建的具体的列\n\n### 3.4 cell单元格\n\n- 指定row key行键、列族、列，可以确定的一个cell单元格\n\n- cell中的数据是没有类型的，全部是以字节数组进行存储\n\n![](assets/Image201911072218.png)\n\n### 3.5 Timestamp时间戳\n\n- 可以对表中的Cell多次赋值，每次赋值操作时的时间戳timestamp，可看成Cell值的版本号version number\n- 即一个Cell可以有多个版本的值\n\n\n\n## 4. HBase整体架构\n\n![](assets/HBase-Hmaster-Hregionserver.png)\n\n### 4.1 Client客户端\n\n* Client是操作HBase集群的入口\n  * 对于管理类的操作，如表的增、删、改操纵，Client通过RPC与HMaster通信完成\n  * 对于表数据的读写操作，Client通过RPC与RegionServer交互，读写数据\n* Client类型：\n  * HBase shell\n  * Java编程接口\n  * Thrift、Avro、Rest等等\n\n### 4.2 ZooKeeper集群\n\n* 作用\n  * 实现了HMaster的高可用，多HMaster间进行主备选举\n\n  * 保存了HBase的元数据信息meta表，提供了HBase表中region的寻址入口的线索数据\n\n  * 对HMaster和HRegionServer实现了监控\n\n### 4.3 HMaster\n\n* HBase集群也是主从架构，HMaster是主的角色，是老大\n* 主要负责Table表和Region的相关管理工作：\n* 关于Table\n  * 管理Client对Table的增删改的操作\n  * 关于Region\n    * 在Region分裂后，负责新Region分配到指定的HRegionServer上\n    * 管理HRegionServer间的负载均衡，迁移region分布\n    * 当HRegionServer宕机后，负责其上的region的迁移\n\n### 4.4 HRegionServer\n\n* HBase集群中从的角色，是小弟\n* 作用\n\n- 响应客户端的读写数据请求\n  - 负责管理一系列的Region\n  - 切分在运行过程中变大的region\n\n### 4.5 Region\n\n* HBase集群中分布式存储的最小单元\n* 一个Region对应一个Table表的部分数据\n\n\n\n> HBase使用，主要有两种形式：①命令；②Java编程\n\n## 5. HBase shell 命令基本操作\n\n### 5.1 进入HBase客户端命令操作界面\n\n- node01执行以下命令，进入HBase的shell客户端\n\n```shell\ncd /kfly/install/hbase-1.2.0-cdh5.14.2/\nbin/HBase shell\n\n```\n\n### 5.2 help 帮助命令\n\n```shell\nHBase(main):001:0> help\n\n```\n\n### 5.3 list 查看有哪些表\n\n- 查看当前数据库中有哪些表\n\n```shell\nHBase(main):002:0> list\n\n```\n\n### 5.4 create 创建表\n\n- 创建user表，包含info、data两个列族\n- 使用create命令\n\n```\nHBase(main):010:0> create 'user', 'info', 'data'\n\n或者\n\nHBase(main):010:0> create 'user', {NAME => 'info', VERSIONS => '3'}，{NAME => 'data'}\n\n```\n\n### 5.5 put 插入数据操作\n\n- 向表中插入数据\n- 使用put命令\n\n```\n向user表中插入信息，row key为rk0001，列族info中添加名为name的列，值为zhangsan\nHBase(main):011:0> put 'user', 'rk0001', 'info:name', 'zhangsan'\n\n向user表中插入信息，row key为rk0001，列族info中添加名为gender的列，值为female\nHBase(main):012:0> put 'user', 'rk0001', 'info:gender', 'female'\n\n向user表中插入信息，row key为rk0001，列族info中添加名为age的列，值为20\nHBase(main):013:0> put 'user', 'rk0001', 'info:age', 20\n\n向user表中插入信息，row key为rk0001，列族data中添加名为pic的列，值为picture\nHBase(main):014:0> put 'user', 'rk0001', 'data:pic', 'picture'\n\n```\n\n### 5.6 查询数据操作\n\n#### 5.6.1 通过rowkey进行查询\n\n- 获取user表中row key为rk0001的所有信息（即所有cell的数据）\n- 使用get命令\n\n```\nHBase(main):015:0> get 'user', 'rk0001'\n\n```\n\n#### 5.6.2 查看rowkey下某个列族的信息\n\n- 获取user表中row key为rk0001，info列族的所有信息\n\n```\nHBase(main):016:0> get 'user', 'rk0001', 'info'\n\n```\n\n#### 5.6.3 查看rowkey指定列族指定字段的值\n\n- 获取user表中row key为rk0001，info列族的name、age列的信息\n\n```shell\nHBase(main):017:0> get 'user', 'rk0001', 'info:name', 'info:age'\n\n```\n\n![](assets/Image201911080715.png)\n\n#### 5.6.4 查看rowkey指定多个列族的信息\n\n- 获取user表中row key为rk0001，info、data列族的信息\n\n```shell\nHBase(main):018:0> get 'user', 'rk0001', 'info', 'data'\n\n或者你也可以这样写\nHBase(main):019:0> get 'user', 'rk0001', {COLUMN => ['info', 'data']}\n\n或者你也可以这样写，也行\nHBase(main):020:0> get 'user', 'rk0001', {COLUMN => ['info:name', 'data:pic']}\n\n```\n\n#### 5.6.5 指定rowkey与列值过滤器查询\n\n- 获取user表中row key为rk0001，cell的值为zhangsan的信息\n\n```shell\nHBase(main):030:0> get 'user', 'rk0001', {FILTER => \"ValueFilter(=, 'binary:zhangsan')\"}\n\n```\n\n#### 5.6.6 指定rowkey与列值模糊查询\n\n- 获取user表中row key为rk0001，列标示符中含有a的信息\n\n```shell\nHBase(main):031:0> get 'user', 'rk0001', {FILTER => \"(QualifierFilter(=,'substring:a'))\"}\n\n继续插入一批数据\n\nHBase(main):032:0> put 'user', 'rk0002', 'info:name', 'fanbingbing'\nHBase(main):033:0> put 'user', 'rk0002', 'info:gender', 'female'\nHBase(main):034:0> put 'user', 'rk0002', 'info:nationality', '中国'\nHBase(main):035:0> get 'user', 'rk0002', {FILTER => \"ValueFilter(=, 'binary:中国')\"}\n\n```\n\n#### 5.6.7 查询所有行的数据\n\n- 查询user表中的所有信息\n- 使用scan命令\n\n```\nHBase(main):032:0>  scan 'user'\n\n```\n\n#### 5.6.8 列族查询\n\n- 查询user表中列族为info的信息\n\n```\nscan 'user', {COLUMNS => 'info'}\n\nscan 'user', {COLUMNS => 'info', RAW => true, VERSIONS => 5}\n\nscan 'user', {COLUMNS => 'info', RAW => true, VERSIONS => 3}\n\n```\n\n#### 5.6.9 多列族查询\n\n- 查询user表中列族为info和data的信息\n\n```\nscan 'user', {COLUMNS => ['info', 'data']}\n\n```\n\n#### 5.6.10 指定列族与某个列名查询\n\n- 查询user表中列族为info、列标示符为name的信息\n\n```\nscan 'user', {COLUMNS => 'info:name'}\n\n```\n\n- 查询info:name列、data:pic列的数据\n\n```\nscan 'user', {COLUMNS => ['info:name', 'data:pic']}\n\n```\n\n- 查询user表中列族为info、列标示符为name的信息,并且版本最新的5个\n\n```\nscan 'user', {COLUMNS => 'info:name', VERSIONS => 5}\n\n```\n\n#### 5.6.11 指定多个列族与按照数据值模糊查询\n\n- 查询user表中列族为info和data且列标示符中含有a字符的信息\n\n```\nscan 'user', {COLUMNS => ['info', 'data'], FILTER => \"(QualifierFilter(=,'substring:a'))\"}\n\n```\n\n#### 5.6.12 指定rowkey的范围查询\n\n- 查询user表中列族为info，rk范围是[rk0001, rk0003)的数据\n\n```\nscan 'user', {COLUMNS => 'info', STARTROW => 'rk0001', ENDROW => 'rk0003'}\n\n```\n\n#### 5.6.13 指定rowkey模糊查询\n\n- 查询user表中row key以rk字符开头的数据\n\n```\nscan 'user',{FILTER=>\"PrefixFilter('rk')\"}\n\n```\n\n#### 5.6.14 指定数据版本的范围查询\n\n- 查询user表中指定范围的数据（前闭后开）\n\n```\nscan 'user', {TIMERANGE => [1392368783980, 1392380169184]}\n\n```\n\n\n\n### 5.7 更新数据操作\n\n#### 5.7.1 更新数据值\n\n- 更新操作同插入操作一模一样，只不过有数据就更新，没数据就添加\n- 使用put命令\n\n#### 5.7.2 更新版本号\n\n- 将user表的f1列族版本数改为5\n\n```\nHBase(main):050:0> alter 'user', NAME => 'info', VERSIONS => 5\n\n```\n\n### 5.8 删除数据以及删除表操作\n\n#### 5.8.1 指定rowkey以及列名进行删除\n\n- 删除user表row key为rk0001，列标示符为info:name的数据\n\n```\nHBase(main):045:0> delete 'user', 'rk0001', 'info:name'\n\n```\n\n#### 5.8.2 指定rowkey，列名以及版本号进行删除\n\n- 删除user表row key为rk0001，列标示符为info:name，timestamp为1392383705316的数据\n\n```\ndelete 'user', 'rk0001', 'info:name', 1392383705316\n\n```\n\n#### 5.8.3 删除一个列族\n\n- 删除一个列族：\n\n```\nalter 'user', NAME => 'info', METHOD => 'delete' \n\n或 alter 'user', 'delete' => 'info'\n\n```\n\n#### 5.8.4 清空表数据\n\n```\nHBase(main):017:0> truncate 'user'\n\n```\n\n#### 5.8.5 删除表\n\n- 首先需要先让该表为disable状态，使用命令：\n\n```\nHBase(main):049:0> disable 'user'\n\n```\n\n- 然后使用drop命令删除这个表\n\n```\n HBase(main):050:0> drop 'user'\n\n```\n\n> (注意：如果直接drop表，会报错：Drop the named table. Table must first be disabled)\n\n### 5.9 统计一张表有多少行数据\n\n```\nHBase(main):053:0> count 'user'\n\n```\n\n \n\n## 6. HBase的高级shell管理命令\n\n### 6.1 status\n\n- 例如：显示服务器状态\n\n```\nHBase(main):058:0> status 'node01'\n\n```\n\n### 6.2 whoami\n\n- 显示HBase当前用户，例如：\n\n```\nHBase> whoami\n\n```\n\n### 6.3 list\n\n- 显示当前所有的表\n\n```\nHBase >  list\n\n```\n\n### 6.4 count\n\n- 统计指定表的记录数，例如：\n\n```\nHBase> count 'user' \n\n```\n\n### 6.5 describe\n\n- 展示表结构信息\n\n```\nHBase> describe 'user'\n\n```\n\n### 6.6 exists\n\n- 检查表是否存在，适用于表量特别多的情况\n\n```\nHBase> exists 'user'\n\n```\n\n### 6.7 is_enabled、is_disabled\n\n- 检查表是否启用或禁用\n\n```\nHBase> is_enabled 'user'\nHBase> is_disabled 'user'\n\n```\n\n### 6.8 alter\n\n- 该命令可以改变表和列族的模式，例如：\n\n- **为当前表增加列族：**\n\n```\nHBase> alter 'user', NAME => 'CF2', VERSIONS => 2\n\n```\n\n- **为当前表删除列族：**\n\n```\nHBase(main):002:0>  alter 'user', 'delete' => 'CF2'\n\n```\n\n### 6.9 disable/enable\n\n- 禁用一张表/启用一张表\n\n```\nHBase> disable 'user'\nHBase> enable 'user'\n\n```\n\n### 6.10 drop\n\n- 删除一张表，记得在删除表之前必须先禁用\n\n### 6.11 truncate\n\n- 禁用表-删除表-创建表\n\n\n\n## 7. HBase的JavaAPI操作（重点）\n\n- HBase是一个分布式的NoSql数据库，在实际工作当中，我们一般都可以通过JavaAPI来进行各种数据的操作，包括创建表，以及数据的增删改查等等\n\n### 7.1 创建maven工程\n\n- 讲如下内容作为maven工程中pom.xml的repositories的内容\n- 自动导包（需要从cloudera仓库下载，耗时较长，**耐心等待**）\n\n```xml\n\t<repositories>\n        <repository>\n            <id>cloudera</id>\n            <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n        </repository>\n    </repositories>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.6.0-mr1-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.HBase</groupId>\n            <artifactId>hbase-client</artifactId>\n            <version>1.2.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.HBase</groupId>\n            <artifactId>hbase-server</artifactId>\n            <version>1.2.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.testng</groupId>\n            <artifactId>testng</artifactId>\n            <version>6.14.3</version>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                    <encoding>UTF-8</encoding>\n                </configuration>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.2</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <filters>\n                                <filter>\n                                    <artifact>*:*</artifact>\n                                    <excludes>\n                                        <exclude>META-INF/*.SF</exclude>\n                                        <exclude>META-INF/*.DSA</exclude>\n                                        <exclude>META-INF/*/RSA</exclude>\n                                    </excludes>\n                                </filter>\n                            </filters>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n\n```\n\n### 7.2 创建myuser表\n\n- 创建myuser表，此表有两个列族f1和f2\n\n```java\n\t//操作数据库  第一步：获取连接  第二步：获取客户端对象   第三步：操作数据库  第四步：关闭\n    /**\n     * 创建一张表  myuser  两个列族  f1   f2\n     */\n    @Test\n    public void createTable() throws IOException {\n        Configuration configuration = HBaseConfiguration.create();\n        //连接HBase集群不需要指定HBase主节点的ip地址和端口号\n        configuration.set(\"HBase.zookeeper.quorum\",\"node01:2181,node02:2181,node03:2181\");\n        //创建连接对象\n        Connection connection = ConnectionFactory.createConnection(configuration);\n        //获取连接对象，创建一张表\n        //获取管理员对象，来对手数据库进行DDL的操作\n        Admin admin = connection.getAdmin();\n        //指定我们的表名\n        TableName myuser = TableName.valueOf(\"myuser\");\n        HTableDescriptor hTableDescriptor = new HTableDescriptor(myuser);\n        //指定两个列族\n        HColumnDescriptor f1 = new HColumnDescriptor(\"f1\");\n        HColumnDescriptor f2 = new HColumnDescriptor(\"f2\");\n        hTableDescriptor.addFamily(f1);\n        hTableDescriptor.addFamily(f2);\n        admin.createTable(hTableDescriptor);\n        admin.close();\n        connection.close();\n\n    }\n\n```\n\n### 7.3 向表中添加数据\n\n```java\n \tprivate Connection connection ;\n    private final String TABLE_NAME = \"myuser\";\n    private Table table ;\n\n    @BeforeTest\n    public void initTable () throws IOException {\n        Configuration configuration = HBaseConfiguration.create();\n        configuration.set(\"HBase.zookeeper.quorum\",\"node01:2181,node02:2181\");\n        connection= ConnectionFactory.createConnection(configuration);\n        table = connection.getTable(TableName.valueOf(TABLE_NAME));\n    }\n\n    @AfterTest\n    public void close() throws IOException {\n        table.close();\n        connection.close();\n    }\n\n\n    /**\n     *  向myuser表当中添加数据\n     */\n    @Test\n    public void addData() throws IOException {\n        //获取表\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        Put put = new Put(\"0001\".getBytes());//创建put对象，并指定rowkey值\n        put.addColumn(\"f1\".getBytes(),\"name\".getBytes(),\"zhangsan\".getBytes());\n        put.addColumn(\"f1\".getBytes(),\"age\".getBytes(), Bytes.toBytes(18));\n        put.addColumn(\"f1\".getBytes(),\"id\".getBytes(), Bytes.toBytes(25));\n        put.addColumn(\"f1\".getBytes(),\"address\".getBytes(), Bytes.toBytes(\"地球人\"));\n        table.put(put);\n        table.close();\n    } \n\n```\n\n### 7.4 查询数据\n\n- 初始化一批数据到HBase表当中，用于查询\n\n```java\n \t@Test\n    public void insertBatchData() throws IOException {\n\n        //获取连接\n        Configuration configuration = HBaseConfiguration.create();\n        configuration.set(\"HBase.zookeeper.quorum\", \"node01:2181,node02:2181\");\n        Connection connection = ConnectionFactory.createConnection(configuration);\n        //获取表\n        Table myuser = connection.getTable(TableName.valueOf(\"myuser\"));\n        //创建put对象，并指定rowkey\n        Put put = new Put(\"0002\".getBytes());\n        put.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(1));\n        put.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"曹操\"));\n        put.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(30));\n        put.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"沛国谯县\"));\n        put.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"16888888888\"));\n        put.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"helloworld\"));\n\n        Put put2 = new Put(\"0003\".getBytes());\n        put2.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(2));\n        put2.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"刘备\"));\n        put2.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(32));\n        put2.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put2.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"幽州涿郡涿县\"));\n        put2.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"17888888888\"));\n        put2.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"talk is cheap , show me the code\"));\n\n        Put put3 = new Put(\"0004\".getBytes());\n        put3.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(3));\n        put3.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"孙权\"));\n        put3.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(35));\n        put3.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put3.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"下邳\"));\n        put3.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"12888888888\"));\n        put3.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"what are you 弄啥嘞！\"));\n\n        Put put4 = new Put(\"0005\".getBytes());\n        put4.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(4));\n        put4.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"诸葛亮\"));\n        put4.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(28));\n        put4.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put4.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"四川隆中\"));\n        put4.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"14888888888\"));\n        put4.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"出师表你背了嘛\"));\n\n        Put put5 = new Put(\"0006\".getBytes());\n        put5.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(5));\n        put5.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"司马懿\"));\n        put5.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(27));\n        put5.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put5.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"哪里人有待考究\"));\n        put5.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"15888888888\"));\n        put5.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"跟诸葛亮死掐\"));\n\n\n        Put put6 = new Put(\"0007\".getBytes());\n        put6.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(5));\n        put6.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"xiaobubu—吕布\"));\n        put6.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(28));\n        put6.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put6.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"内蒙人\"));\n        put6.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"15788888888\"));\n        put6.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"貂蝉去哪了\"));\n\n        List<Put> listPut = new ArrayList<Put>();\n        listPut.add(put);\n        listPut.add(put2);\n        listPut.add(put3);\n        listPut.add(put4);\n        listPut.add(put5);\n        listPut.add(put6);\n\n        myuser.put(listPut);\n        myuser.close();\n        connection.close();\n    }\n\n```\n\n#### 7.4.1 Get查询\n\n- 按照rowkey进行查询，获取所有列的所有值\n- 查询主键rowkey为0003的人\n\n```java\n\t /**\n     * 查询rowkey为0003的人\n     */\n    @Test\n    public void getData() throws IOException {\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        //通过get对象，指定rowkey\n        Get get = new Get(Bytes.toBytes(\"0003\"));\n\n        get.addFamily(\"f1\".getBytes());//限制只查询f1列族下面所有列的值\n        //查询f2  列族 phone  这个字段\n        get.addColumn(\"f2\".getBytes(),\"phone\".getBytes());\n        //通过get查询，返回一个result对象，所有的字段的数据都是封装在result里面了\n        Result result = table.get(get);\n        List<Cell> cells = result.listCells();  //获取一条数据所有的cell，所有数据值都是在cell里面 的\n        for (Cell cell : cells) {\n            byte[] family_name = CellUtil.cloneFamily(cell);//获取列族名\n            byte[] column_name = CellUtil.cloneQualifier(cell);//获取列名\n            byte[] rowkey = CellUtil.cloneRow(cell);//获取rowkey\n            byte[] cell_value = CellUtil.cloneValue(cell);//获取cell值\n            //需要判断字段的数据类型，使用对应的转换的方法，才能够获取到值\n            if(\"age\".equals(Bytes.toString(column_name))  || \"id\".equals(Bytes.toString(column_name))){\n                System.out.println(Bytes.toString(family_name));\n                System.out.println(Bytes.toString(column_name));\n                System.out.println(Bytes.toString(rowkey));\n                System.out.println(Bytes.toInt(cell_value));\n            }else{\n                System.out.println(Bytes.toString(family_name));\n                System.out.println(Bytes.toString(column_name));\n                System.out.println(Bytes.toString(rowkey));\n                System.out.println(Bytes.toString(cell_value));\n            }\n        }\n        table.close();\n    }\n```\n\n#### 7.4.2 Scan查询\n\n```java\n\t/**\n     * 不知道rowkey的具体值，我想查询rowkey范围值是0003  到0006\n     * select * from myuser  where age > 30  and id < 8  and name like 'zhangsan'\n     *\n     */\n    @Test\n    public void scanData() throws IOException {\n        //获取table\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        Scan scan = new Scan();//没有指定startRow以及stopRow  全表扫描\n        //只扫描f1列族\n        scan.addFamily(\"f1\".getBytes());\n        //扫描 f2列族 phone  这个字段\n        scan.addColumn(\"f2\".getBytes(),\"phone\".getBytes());\n        scan.setStartRow(\"0003\".getBytes());\n        scan.setStopRow(\"0007\".getBytes());\n        //通过getScanner查询获取到了表里面所有的数据，是多条数据\n        ResultScanner scanner = table.getScanner(scan);\n        //遍历ResultScanner 得到每一条数据，每一条数据都是封装在result对象里面了\n        for (Result result : scanner) {\n            List<Cell> cells = result.listCells();\n            for (Cell cell : cells) {\n                byte[] family_name = CellUtil.cloneFamily(cell);\n                byte[] qualifier_name = CellUtil.cloneQualifier(cell);\n                byte[] rowkey = CellUtil.cloneRow(cell);\n                byte[] value = CellUtil.cloneValue(cell);\n                //判断id和age字段，这两个字段是整形值\n                if(\"age\".equals(Bytes.toString(qualifier_name))  || \"id\".equals(Bytes.toString(qualifier_name))){\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toInt(value));\n                }else{\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toString(value));\n                }\n            }\n        }\n        table.close();\n    }\n```\n\n### 7.5 HBase过滤器查询\n\n#### 7.5.1 过滤器\n\n- 过滤器的作用是在服务端判断数据是否满足条件，然后只将满足条件的数据返回给客户端\n\n- 过滤器的类型很多，但是可以分为两大类\n  - ==比较过滤器==\n  - ==专用过滤器==\n\n- HBase过滤器的**比较运算符**：\n\n```\nLESS  <\nLESS_OR_EQUAL <=\nEQUAL =\nNOT_EQUAL <>\nGREATER_OR_EQUAL >=\nGREATER >\nNO_OP 排除所有\n```\n\n- Hbase过滤器的**比较器**（指定比较机制）：\n\n```\nBinaryComparator  按字节索引顺序比较指定字节数组，采用Bytes.compareTo(byte[])\nBinaryPrefixComparator 跟前面相同，只是比较左端的数据是否相同\nNullComparator 判断给定的是否为空\nBitComparator 按位比较\nRegexStringComparator 提供一个正则的比较器，仅支持 EQUAL 和非EQUAL\nSubstringComparator 判断提供的子串是否出现在中。\n```\n\n#### 7.5.2 比较过滤器使用\n\n##### 1、rowKey过滤器RowFilter\n\n- 通过RowFilter过滤比rowKey  0003小的所有值出来\n\n```java\n\t/**\n     * 查询所有的rowkey比0003小的所有的数据\n     */\n    @Test\n    public void rowFilter() throws IOException {\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        Scan scan = new Scan();\n        //获取我们比较对象\n        BinaryComparator binaryComparator = new BinaryComparator(\"0003\".getBytes());\n        /***\n         * rowFilter需要加上两个参数\n         * 第一个参数就是我们的比较规则\n         * 第二个参数就是我们的比较对象\n         */\n        RowFilter rowFilter = new RowFilter(CompareFilter.CompareOp.GREATER, binaryComparator);\n        //为我们的scan对象设置过滤器\n        scan.setFilter(rowFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        for (Result result : scanner) {\n            List<Cell> cells = result.listCells();\n            for (Cell cell : cells) {\n                byte[] family_name = CellUtil.cloneFamily(cell);\n                byte[] qualifier_name = CellUtil.cloneQualifier(cell);\n                byte[] rowkey = CellUtil.cloneRow(cell);\n                byte[] value = CellUtil.cloneValue(cell);\n                //判断id和age字段，这两个字段是整形值\n                if(\"age\".equals(Bytes.toString(qualifier_name))  || \"id\".equals(Bytes.toString(qualifier_name))){\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toInt(value));\n                }else{\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toString(value));\n                }\n            }\n        }\n    }\n```\n\n##### 2、列族过滤器FamilyFilter\n\n- 查询列族名包含f2的所有列族下面的数据\n\n```JAVA\n\t/**\n     * 通过familyFilter来实现列族的过滤\n     * 需要过滤，列族名包含f2\n     * f1  f2   hello   world\n     */\n    @Test\n    public void familyFilter() throws IOException {\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        Scan scan = new Scan();\n        SubstringComparator substringComparator = new SubstringComparator(\"f2\");\n        //通过familyfilter来设置列族的过滤器\n        FamilyFilter familyFilter = new FamilyFilter(CompareFilter.CompareOp.EQUAL, substringComparator);\n        scan.setFilter(familyFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        for (Result result : scanner) {\n            List<Cell> cells = result.listCells();\n            for (Cell cell : cells) {\n                byte[] family_name = CellUtil.cloneFamily(cell);\n                byte[] qualifier_name = CellUtil.cloneQualifier(cell);\n                byte[] rowkey = CellUtil.cloneRow(cell);\n                byte[] value = CellUtil.cloneValue(cell);\n                //判断id和age字段，这两个字段是整形值\n                if(\"age\".equals(Bytes.toString(qualifier_name))  || \"id\".equals(Bytes.toString(qualifier_name))){\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toInt(value));\n                }else{\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toString(value));\n                }\n            }\n        }\n    }\n```\n\n##### 3、列过滤器QualifierFilter\n\n- 只查询name列的值\n\n```java\n/**\n     * 列名过滤器 只查询包含name列的值\n     */\n    @Test\n    public void  qualifierFilter() throws IOException {\n        Scan scan = new Scan();\n        SubstringComparator substringComparator = new SubstringComparator(\"name\");\n        //定义列名过滤器，只查询列名包含name的列\n        QualifierFilter qualifierFilter = new QualifierFilter(CompareFilter.CompareOp.EQUAL, substringComparator);\n        scan.setFilter(qualifierFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        printlReult(scanner);\n    }\n```\n\n##### 4、列值过滤器ValueFilter\n\n- 查询所有列当中包含8的数据\n\n```java\n\t/**\n     * 查询哪些字段值  包含数字8\n     */\n    @Test\n    public void contains8() throws IOException {\n        Scan scan = new Scan();\n        SubstringComparator substringComparator = new SubstringComparator(\"8\");\n        //列值过滤器，过滤列值当中包含数字8的所有的列\n        ValueFilter valueFilter = new ValueFilter(CompareFilter.CompareOp.EQUAL, substringComparator);\n        scan.setFilter(valueFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        printlReult(scanner);\n    }\n```\n\n#### 7.5.3 专用过滤器使用\n\n##### 1、单列值过滤器 SingleColumnValueFilter\n\n- SingleColumnValueFilter会返回满足条件的整列值的所有字段\n\n- 查询名字为刘备的数据\n\n```java\n\t/**\n     * select  *  from  myuser where name  = '刘备'\n     * 会返回我们符合条件数据的所有的字段\n     *\n     * SingleColumnValueExcludeFilter  列值排除过滤器\n     *  select  *  from  myuser where name  ！= '刘备'\n     */\n    @Test\n    public void singleColumnValueFilter() throws IOException {\n        //查询 f1  列族 name  列  值为刘备的数据\n        Scan scan = new Scan();\n        //单列值过滤器，过滤  f1 列族  name  列  值为刘备的数据\n        SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter(\"f1\".getBytes(), \"name\".getBytes(), CompareFilter.CompareOp.EQUAL, \"刘备\".getBytes());\n        scan.setFilter(singleColumnValueFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        printlReult(scanner);\n    }\n```\n\n##### 2、列值排除过滤器SingleColumnValueExcludeFilter\n\n- 与SingleColumnValueFilter相反，会排除掉指定的列，其他的列全部返回\n\n##### 3、rowkey前缀过滤器PrefixFilter\n\n- 查询以00开头的所有前缀的rowkey\n\n```java\n\t/**\n     * 查询rowkey前缀以  00开头的所有的数据\n     */\n    @Test\n    public  void  prefixFilter() throws IOException {\n        Scan scan = new Scan();\n        //过滤rowkey以  00开头的数据\n        PrefixFilter prefixFilter = new PrefixFilter(\"00\".getBytes());\n        scan.setFilter(prefixFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        printlReult(scanner);\n    }\n```\n\n##### 4、分页过滤器PageFilter（15分钟）\n\n- 通过pageFilter实现分页过滤器\n\n```java\n\t/**\n     * HBase当中的分页\n     */\n    @Test\n    public void hbasePageFilter() throws IOException {\n        int pageNum= 3;\n        int pageSize = 2;\n        Scan scan = new Scan();\n        if(pageNum == 1 ){\n            //获取第一页的数据\n            scan.setMaxResultSize(pageSize);\n            scan.setStartRow(\"\".getBytes());\n            //使用分页过滤器来实现数据的分页\n            PageFilter filter = new PageFilter(pageSize);\n            scan.setFilter(filter);\n            ResultScanner scanner = table.getScanner(scan);\n            printlReult(scanner);\n        }else{\n            String  startRow = \"\";\n            //扫描数据的调试 扫描五条数据\n            int scanDatas = (pageNum - 1) * pageSize + 1;\n            scan.setMaxResultSize(scanDatas);//设置一步往前扫描多少条数据\n            PageFilter filter = new PageFilter(scanDatas);\n            scan.setFilter(filter);\n            ResultScanner scanner = table.getScanner(scan);\n            for (Result result : scanner) {\n                byte[] row = result.getRow();//获取rowkey\n                //最后一次startRow的值就是0005\n                startRow= Bytes.toString(row);//循环遍历我们多有获取到的数据的rowkey\n                //最后一条数据的rowkey就是我们需要的起始的rowkey\n            }\n            //获取第三页的数据\n            scan.setStartRow(startRow.getBytes());\n            scan.setMaxResultSize(pageSize);//设置我们扫描多少条数据\n            PageFilter filter1 = new PageFilter(pageSize);\n            scan.setFilter(filter1);\n            ResultScanner scanner1 = table.getScanner(scan);\n            printlReult(scanner1);\n        }\n    }\n```\n\n#### 3、多过滤器综合查询FilterList\n\n- 需求：使用SingleColumnValueFilter查询f1列族，name为刘备的数据，并且同时满足rowkey的前缀以00开头的数据（PrefixFilter）\n\n```java\n\t/**\n     * 查询  f1 列族  name  为刘备数据值\n     * 并且rowkey 前缀以  00开头数据\n     */\n    @Test\n    public  void filterList() throws IOException {\n        Scan scan = new Scan();\n        SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter(\"f1\".getBytes(), \"name\".getBytes(), CompareFilter.CompareOp.EQUAL, \"刘备\".getBytes());\n        PrefixFilter prefixFilter = new PrefixFilter(\"00\".getBytes());\n        FilterList filterList = new FilterList();\n        filterList.addFilter(singleColumnValueFilter);\n        filterList.addFilter(prefixFilter);\n        scan.setFilter(filterList);\n        ResultScanner scanner = table.getScanner(scan);\n        printlReult(scanner);\n    }\n```\n\n### 7.6 HBase的删除操作\n\n#### 1、根据rowkey删除数据\n\n- 删除rowkey为003的数据\n\n```java\n\t/**\n     * 删除数据\n     */\n    @Test\n    public  void  deleteData() throws IOException {\n        Delete delete = new Delete(\"0003\".getBytes());\n        table.delete(delete);\n    }\n```\n\n#### 2、删除表操作\n\n```java\n \t/**\n     * 删除表\n     */\n    @Test\n    public void deleteTable() throws IOException {\n        //获取管理员对象，用于表的删除\n        Admin admin = connection.getAdmin();\n        //删除一张表之前，需要先禁用表\n        admin.disableTable(TableName.valueOf(TABLE_NAME));\n        admin.deleteTable(TableName.valueOf(TABLE_NAME));\n    }\n```\n\n\n\n# 五、拓展点、未来计划、行业趋势\n\n### 8. Hbase在实际场景中的应用\n\n##### 8.1 交通方面\n\n- 船舶GPS信息，全长江的船舶GPS信息，每天有1千万左右的数据存储。\n\n##### 8.2 金融方面\n\n- 消费信息、贷款信息、信用卡还款信息等\n\n\n##### 8.3 电商方面\n\n- 电商网站的交易信息、物流信息、游览信息等\n\n##### 8.4 电信方面\n\n- 通话信息、语音详单等\n\n==**总结：海量明细数据的存储，并且后期需要有很好的查询性能**==","tags":["hadoop","hbase"]},{"title":"Hadoop数据分析之Hive（四）","url":"/2019/11/11/hadoop/Hadoop数据分析之Hive（四）/","content":"\n## 数据仓库分析及hive中数据倾斜介绍\n\n## 一、数据仓库\n\n### 1. 数据仓库基本介绍\n\n英文名称为==Data Warehouse==，可简写为DW或DWH。数据仓库的目的是==构建面向分析的集成化数据环境==，为企业提供==决策支持==（Decision Support）。它出于分析性报告和决策支持目的而创建。\n\n数据仓库本身并不“生产”任何数据，同时自身也不需要“消费”任何的数据，数据来源于外部，并且开放给外部应用，这也是为什么叫“仓库”，而不叫“工厂”的原因。\n\n\n\n\n\n### 2. 数据仓库的定义\n\n数据仓库是==面向主题的==（Subject-Oriented ）、==集成的==（Integrated）、==稳定性的==（Non-Volatile）和==时变的==（Time-Variant ）数据集合，用以支持管理决策。 \n\n\n\n#### 2.1、面向主题\n\n数据仓库中的数据是按照一定的主题域进行组织。\n\n主题是一个抽象的概念，是指用户使用数据仓库进行决策时所关心的重点方面，一个主题通常与多个操作型信息系统相关。\n\n> 以电商为例：\n>\n> 用户主题：主要是用于分析用户的行为\n>\n> 商品主题：针对商品进行分析    指标：昨日新增商品，昨日下架商品 最近七天流量最高的哪些商品\n>\n> 财务主题：财务分析\n>\n> 订单主题：订单分析\n>\n> 货运主题：针对快递分析\n\n​\t\n\n#### 2.2、集成性\n\n根据决策分析的要求，将分散于各处的源数据进行抽取、筛选、清理、综合等工作，最终集成到数据仓库中。\n\n![](img/2019-08-17_17-04-09.png)\n\n\n\n#### 2.3、稳定性\n\n数据的相对稳定性，数据仓库中的数据只进行新增，没有更新操作、删除操作处理。\n\n反映历史变化，以查询分析为主。\n\n\n\n#### 2.4、时变性\n\n数据仓库的数据一般都带有时间属性，随着时间的推移而发生变化，不断地生成主题的新快照\n\n![](img/2019-08-17_17-09-51.png)\n\n\n\n### 3. 数据仓库与数据库的区别\n\n数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别。\n\n**==OLTP==**： On-Line Transaction Processing  叫==联机事务处理==， 也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理 .\n\n **==OLAP==**：On-Line Analytical Processing  叫==联机分析处理==，一般针对某些主题的历史数据进行分析，支持管理决策。\n\n简而言之，==数据库是面向事务的设计，数据仓库是面向主题设计的==。 \n\n数据库一般存储在线交易数据，有很高的事务要求；数据仓库存储的一般是历史数据。 \n\n数据库设计是尽量避免冗余，一般采用符合范式的规则来设计，数据仓库在设计是有意引入冗余，采用反范式的方式来设计。 \n\n数据库是==为捕获数据而设计==，数据仓库是==为分析数据而设计==，它的两个基本的元素是维表和事实表。维是看问题的角度，比如时间，部门，维表放的就是这些东西的定义，事实表里放着要查询的数据，同时有维的ID。\n\n\n\n| **功能** | **数据仓库**                           | **数据库**                             |\n| -------- | -------------------------------------- | -------------------------------------- |\n| 数据范围 | 存储历史的、完整的、反应历史变化的     | 当前状态数据                           |\n| 数据变化 | 可添加、无删除、无变更的、反应历史变化 | 支持频繁的增、删、改、查操作           |\n| 应用场景 | 面向分析、支持战略决策                 | 面向业务交易流程                       |\n| 设计理论 | 违范式、适当冗余                       | 遵照范式(第一、二、三等范式)、避免冗余 |\n| 处理量   | 非频繁、大批量、高吞吐、有延迟         | 频繁、小批次、高并发、低延迟           |\n\n\n\n### . 构建数据仓库常用手段\n\n• 传统数仓建设更多的基于成熟的商业数据集成平台，比如Teradata、Oracle、Informatica等，技术体系比较成熟完善，但相对比较封闭，对实施者技术面要求也相对专业且单一，一般更多应用于银行、保险、电信等“有钱”行业.\n\n• 基于大数据的数仓建设一般是基于非商业、开源的技术，常见的是基于hadoop生态构建，涉及技术较广泛、复杂，同时相对于商业产品，稳定性、服务支撑较弱，需要自己维护更多的技术框架。在大数据领域，==常用的数据仓库构建手段很多基于hive，sparkSQL，impala等各种技术框架==.\n\n\n\n### 5. 数据仓库分层\n\n\n\n#### 5.1 数据仓库分层描述\n\n* 数据仓库更多代表的是一种对数据的管理和使用的方式，它是一整套包括了etl、调度、建模在内的完整的理论体系。现在所谓的大数据更多的是一种数据量级的增大和工具的上的更新。 两者并无冲突，相反，而是一种更好的结合。数据仓库在构建过程中通常都需要进行分层处理。业务不同，分层的技术处理手段也不同。\n* 分层是数据仓库解决方案中，数据架构设计的一种数据逻辑结构 ，通过分层理念建立的数据仓库，它的可扩展性非常好，这样设计出来的模型架构，可以任意地增减、替换数据仓库中的各个组成部分。\n\n![](img/数据仓库层.png)\n\n\n\n~~~\n从整体的逻辑划分来讲，数据仓库模型实际上就是这三层架构。\n\n接入层：底层的数据源或者是操作数据层，一般在公司的话，统一都是称为ODS层\n\n中间层：是做数据仓库同学需要花费更多精力的一层，这一层包括的内容是最多的、最复杂的。\n\n应用层：对不同的应用提供对应的数据。该层主要是提供数据产品和数据分析使用的数据，\n\t   比如我们经常说的报表数据\n\t\n~~~\n\n\n\n* 针对于这三层架构，这里给出比较典型的一个做数据仓库在实施的时候，具体的层次划分。\n\n![dw](img/dw.png)\n\n\n\n* ==ODS==：\n\n  * Operation Data Store 原始数据层\n\n* ==DWD==\n\n  * data warehouse detail 数据明细层\n\n  * 它主要是针对于接入层的数据进行数据的清洗和转换。还有就是一些维度的补充。\n\n* ==DWS==\n  * data warehouse summary 数据汇总层\n\n  * 它是在DWD明细层之上，也有公司叫DW层\n\n  * 它是按照一定的粒度进行了汇总聚合操作。它是单业务场景。\n\n* ==DWM==\n\n  * data warehouse market 数据集市层\n  * 它是在DWS数据汇总层之上，集市层它是多业务场景的。\n\n* ==APP==\n\n  - Application 应用层\n  - 这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用\n\n~~~\nTMP临时表：在做一些中间层表计算的时候，大量使用tmp临时表。\nDIM维度层：基于ODS层和DWD层抽象出一些公共的维度，\n\t\t  典型的公共维度主要包括城市信息、渠道信息、个人基础属性信息。\n~~~\n\n\n\n#### 5.2 为什么要进行数据仓库分层\n\n* 分层的主要原因是在管理数据的时候，能对数据有一个更加清晰的掌控，主要有下面几个原因：\n  * **空间换时间**\n    * 通过建设多层次的数据模型供用户使用，避免用户直接使用底层操作型数据，可以更高效的访问数据。\n  * **把复杂问题简单化**\n    * 讲一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。\n  * **便于处理业务的变化**\n    * 随着业务的变化，只需要调整底层的数据，对应用层对业务的调整零感知。\n\n\n\n### 6. 数据仓库建模\n\n​\t目前业界较为流行的数据仓库的建模方法非常多，这里主要介绍==范式建模法==，==维度建模法==，==实体建模法==等几种方法，每种方法其实从本质上讲就是从不同的角度看我们业务中的问题，不管从技术层面还是业务层面，其实代表的是哲学上的一种世界观。\n\n\n\n#### 6.1 范式建模法（Third Normal Form 3NF）\n\n~~~\n范式建模法是基于整个关系型数据库的理论基础之上发展而来的，其实是我们在构建数据模型常用的一个方法，主要解决关系型数据库得数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。\n\n从其表达的含义来看，一个符合第三范式的关系必须具有以下三个条件 :\n\n（1）每个属性值唯一，不具有多义性 ;\n（2）每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;\n（3）每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。\n\n~~~\n\n#### 6.2 维度建模法\n\n~~~\n维度建模(dimensional modeling)是专门用于分析型数据库、数据仓库、数据集市建模的方法。维度建模法简单描述就是按照事实表、维度表来构建数仓、集市。\n维度建模从分析决策的需求出发构建模型，为分析需求服务，因此它重点关注用户如何更快速地完成需求分析，同时具有较好的大规模复杂查询的相应性能。\n\n~~~\n\n* 维度表\n\n~~~\n维度表示你要对数据进行分析时所用的一个量,比如你要分析产品销售情况, \n你可以选择按类别来进行分析,或按区域来分析。\n\n通常来说维度表信息比较固定，且数据量小\n~~~\n\n- 事实表\n\n```\n表示对分析主题的度量。\n事实表包含了与各维度表相关联的外键，并通过join方式与维度表关联。事实表的度量通常是数值类型，且记录数会不断增加，表规模迅速增长。\n\n消费事实表：Prod_id(引用商品维度表), TimeKey(引用时间维度表), Place_id(引用地点维度表), Unit(销售量)。\n```\n\n~~~\n总的说来，在数据仓库中不需要严格遵守规范化设计原则。因为数据仓库的主导功能就是面向分析，以查询为主，不涉及数据更新操作。事实表的设计是以能够正确记录历史信息为准则，维度表的设计是以能够以合适的角度来聚合主题内容为准则\n~~~\n\n\n\n##### 6.2.1 维度建模三种模式\n\n基于事实表和维表就可以构建出多种多维模型，包括星形模型、雪花模型和星座模型。\n\n维度建模法最被人广泛知晓的名字就是星型模式。\n\n\n\n* ==星型模式==\n\n  ~~~\n  星形模式(Star Schema)是最常用的维度建模方式。星型模式是以事实表为\n  中心，所有的维度表直接连接在事实表上，像星星一样。\n  星形模式的维度建模由一个事实表和一组维表成，且具有以下特点：\n  a. 维表只和事实表关联，维表之间没有关联；\n  b. 每个维表主键为单列，且该主键放置在事实表中，作为两边连接的外键；\n  c. 以事实表为核心，维表围绕核心呈星形分布；\n  \n  ~~~\n\n  ![星型模型](img/星型模型.png)\n\n\n\n* ==雪花模式==\n\n~~~\n雪花模式是对星形模式的扩展。雪花模式的维度表可以拥有其他维度表的，虽然这种模型相比星型更规范一些，但是由于这种模型不太容易理解，维护成本比较高，而且性能方面需要关联多层维表，性能也比星型模型要低。所以一般不是很常用。\n~~~\n\n![雪花模型](img/雪花模型.png)\n\n\n\n* ==星座模式==\n\n~~~\n星座模式是星型模式延伸而来，星型模式是基于一张事实表的，而星座模式是基于多张事实表的，而且共享维度信息。\n\n前面介绍的两种维度建模方法都是多维表对应单事实表，但在很多时候维度空间内的事实表不止一个，而一个维表也可能被多个事实表用到。在业务发展后期，绝大部分维度建模都采用的是星座模式。\n~~~\n\n![星座模型](img/星座模型.png)\n\n\n\n#### 6.3 实体建模法\n\n~~~\n实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。\n\n从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。\n\n那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。\n~~~\n\n参考文档：<http://www.uml.org.cn/sjjmck/201810163.asp>\n\n\n\n\n\n### 7、数据仓库架构\n\n\n\n![](img/数据仓库架构图.png)\n\n* ==数据采集==\n\n~~~\n数据采集层的任务就是把数据从各种数据源中采集和存储到数据存储上，期间有可能会做一些ETL操作。\n\n数据源种类可以有多种：\n日志：所占份额最大，存储在备份服务器上\n业务数据库：如Mysql、Oracle\n来自HTTP/FTP的数据：合作伙伴提供的接口\n其他数据源：如Excel等需要手工录入的数据\n~~~\n\n\n\n* ==数据存储与分析==\n\n~~~\nHDFS是大数据环境下数据仓库/数据平台最完美的数据存储解决方案。\n\n离线数据分析与计算，也就是对实时性要求不高的部分，Hive是不错的选择。\n使用Hadoop框架自然而然也提供了MapReduce接口，如果真的很乐意开发Java，或者对SQL不熟，那么也可以使用MapReduce来做分析与计算。\nSpark性能比MapReduce好很多，同时使用SparkSQL操作Hive。\n~~~\n\n\n\n* ==数据共享==\n\n~~~\n　　前面使用Hive、MR、Spark、SparkSQL分析和计算的结果，还是在HDFS上，但大多业务和应用不可能直接从HDFS上获取数据，那么就需要一个数据共享的地方，使得各业务和产品能方便的获取数据。\n\n　　这里的数据共享，其实指的是前面数据分析与计算后的结果存放的地方，其实就是关系型数据库和NOSQL数据库。\n~~~\n\n\n\n* ==数据应用==\n\n~~~\n报表：报表所使用的数据，一般也是已经统计汇总好的，存放于数据共享层。\n接口：接口的数据都是直接查询数据共享层即可得到。\n即席查询：即席查询通常是现有的报表和数据共享层的数据并不能满足需求，需要从数据存储层直接查询。一般都是通过直接操作SQL得到。\n~~~\n\n## 二、Hive中数据倾斜\n\n### 1、什么是数据倾斜\n\n~~~\n由于数据分布不均匀，造成数据大量的集中到一点，造成数据热点\n~~~\n\n\n\n### 2、数据倾斜的现象\n\n~~~\n在执行任务的时候，任务进度长时间维持在99%左右，查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。\n\n单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。最长时长远大于平均时长。\n~~~\n\n### 3、数据倾斜的情况\n\n![450330742](img/450330742.png)\n\n### 4、数据倾斜的原因\n\n~~~\n1)、key分布不均匀\n\n2)、业务数据本身的特性\n\n3)、建表时考虑不周\n\n4)、某些SQL语句本身就有数据倾斜\n~~~\n\n\n\n### 5、数据倾斜的解决方案\n\n#### 5.1 map端聚合\n\n~~~sql\n--Map 端部分聚合，相当于Combiner\nhive.map.aggr = true；\n--有数据倾斜的时候进行负载均衡\nhive.groupby.skewindata=true；\n\n--有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。\n~~~\n\n#### 5.2 SQL语句调节\n\n* 如何Join\n\n  ~~~\n  关于驱动表的取，用join key分布最均匀的表作为驱动表\n  做好列裁剪和filter操作，以达到两表做join的时候，数据量相对变小的效果。\n  ~~~\n\n* 大小表Join\n\n  ~~~\n  使用map join让小的维度表（1000条以下的记录条数） 先进内存。在map端完成reduce.\n  ~~~\n\n* 大表Join大表\n\n  ~~~\n  把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。\n  ~~~\n\n* count distinct大量相同特殊值\n\n  ~~~\n  count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。\n  ~~~\n\n* group by维度过小\n\n  ~~~\n  采用sum() group by的方式来替换count(distinct)完成计算。\n  ~~~\n\n* 特殊情况特殊处理\n\n  ~~~\n  在业务逻辑优化效果的不大情况下，一些时候是可以将倾斜的数据单独拿出来处理。最后union回去\n  ~~~\n\n#### 5.3 典型的业务场景\n\n* 空值产生的数据倾斜\n\n  * 场景\n\n    ~~~\n    如日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和 用户表中的user_id 关联，会碰到数据倾斜的问题。\n    ~~~\n\n  * 解决办法\n\n  ~~~sql\n  --user_id为空的不参与关联\n  \n  select * from log a\n    join users b\n    on a.user_id is not null\n    and a.user_id = b.user_id\n  union all\n  select * from log a\n    where a.user_id is null;\n    \n    \n    \n  --赋与空值分新的key值\n  select *\n    from log a\n    left outer join users b\n    on case when a.user_id is null then concat(‘hive’,rand()) else a.user_id end = b.user_id;\n    \n  ~~~\n\n* 不同数据类型关联产生数据倾斜\n\n  * 场景\n\n  ~~~\n  用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。\n  ~~~\n\n  * 解决办法\n\n    * 把数字类型转换成字符串类型\n\n    ~~~sql\n    select * from users a\n      left outer join logs b\n      on a.usr_id = cast(b.user_id as string);\n     \n    ~~~\n\n","tags":["hive","数据仓库","数据倾斜"]},{"title":"hive企业综合案例实战","url":"/2019/11/08/hive企业综合案例实战/","content":"\n# hive的综合案例实战\n\n## 1、需求描述\n\n统计youtube影音视频网站的常规指标，各种TopN指标：\n\n--统计视频观看数Top10\n\n--统计视频类别热度Top10\n\n--统计视频观看数Top20所属类别\n\n--统计视频观看数Top50所关联视频的所属类别Rank\n\n--统计每个类别中的视频热度Top10\n\n--统计每个类别中视频流量Top10\n\n--统计上传视频最多的用户Top10以及他们上传的视频\n\n--统计每个类别视频观看数Top10\n\n## 2、项目表字段\n\n### 1、数据结构\n\n1．视频表\n\n| 字段          | 备注       | 详细描述               |\n| ------------- | ---------- | ---------------------- |\n| video id      | 视频唯一id | 11位字符串             |\n| uploader      | 视频上传者 | 上传视频的用户名String |\n| age           | 视频年龄   | 视频在平台上的整数天   |\n| category      | 视频类别   | 上传视频指定的视频分类 |\n| length        | 视频长度   | 整形数字标识的视频长度 |\n| views         | 观看次数   | 视频被浏览的次数       |\n| rate          | 视频评分   | 满分5分                |\n| ratings       | 流量       | 视频的流量，整型数字   |\n| conments      | 评论数     | 一个视频的整数评论数   |\n| related   ids | 相关视频id | 相关视频的id，最多20个 |\n\n\n\n2．用户表\n\n| 字段     | 备注         | 字段类型 |\n| -------- | ------------ | -------- |\n| uploader | 上传者用户名 | string   |\n| videos   | 上传视频数   | int      |\n| friends  | 朋友数量     | int      |\n\n## 3、ETL原始数据清洗\n\n通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用“\\t”进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用“&”分割，同时去掉两边空格，多个相关视频id也使用“&”进行分割。\n\n三件事情\n\n1. 长度不够9的删掉\n\n2. 视频类别删掉空格\n\n3. 该相关视频的分割符\n\n创建maven工程，并导入jar包\n\n```\n<repositories>\n        <repository>\n            <id>cloudera</id>\n            <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n        </repository>\n    </repositories>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.6.0-mr1-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n            <version>2.6.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-hdfs</artifactId>\n            <version>2.6.0-cdh5.14.2</version>\n        </dependency>\n\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-mapreduce-client-core</artifactId>\n            <version>2.6.0-cdh5.14.2</version>\n        </dependency>\n        <!-- https://mvnrepository.com/artifact/junit/junit -->\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.11</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.testng</groupId>\n            <artifactId>testng</artifactId>\n            <version>RELEASE</version>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.38</version>\n            <scope>compile</scope>\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                    <encoding>UTF-8</encoding>\n                    <!--    <verbal>true</verbal>-->\n                </configuration>\n            </plugin>\n\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.4.3</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <minimizeJar>true</minimizeJar>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n            <!--  <plugin>\n                  <artifactId>maven-assembly-plugin </artifactId>\n                  <configuration>\n                      <descriptorRefs>\n                          <descriptorRef>jar-with-dependencies</descriptorRef>\n                      </descriptorRefs>\n                      <archive>\n                          <manifest>\n                              <mainClass></mainClass>\n                          </manifest>\n                      </archive>\n                  </configuration>\n                  <executions>\n                      <execution>\n                          <id>make-assembly</id>\n                          <phase>package</phase>\n                          <goals>\n                              <goal>single</goal>\n                          </goals>\n                      </execution>\n                  </executions>\n              </plugin>-->\n        </plugins>\n    </build>\n```\n\n1、代码开发：ETLUtil\n\n```\npublic class VideoUtil {\n    /**\n     * 对我们的数据进行清洗的工作，\n     * 数据切割，如果长度小于9 直接丢掉\n     * 视频类别中间空格 去掉\n     * 关联视频，使用 &  进行分割\n     * @param line\n     * @return\n     * FM1KUDE3C3k  renetto\t736\tNews & Politics\t1063\t9062\t4.57\t525\t488\tLnMvSxl0o0A&IKMtzNuKQso&Bq8ubu7WHkY&Su0VTfwia1w&0SNRfquDfZs&C72NVoPsRGw\n     */\n    public  static String washDatas(String line){\n        if(null == line || \"\".equals(line)) {\n            return null;\n        }\n        //判断数据的长度，如果小于9，直接丢掉\n        String[] split = line.split(\"\\t\");\n        if(split.length <9){\n            return null;\n        }\n        //将视频类别空格进行去掉\n        split[3] =  split[3].replace(\" \",\"\");\n        StringBuilder builder = new StringBuilder();\n        for(int i =0;i<split.length;i++){\n            if(i <9){\n                //这里面是前面八个字段\n                builder.append(split[i]).append(\"\\t\");\n            }else if(i >=9  && i < split.length -1){\n                builder.append(split[i]).append(\"&\");\n            }else if( i == split.length -1){\n                builder.append(split[i]);\n            }\n        }\n        return  builder.toString();\n    }\n}\n```\n\n\n\n2、代码开发：ETLMapper\n\n```\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport java.io.IOException;\n\npublic class VideoMapper extends Mapper<LongWritable,Text,Text,NullWritable> {\n    private Text  key2 ;\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n        key2 = new Text();\n    }\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        String s = VideoUtils.washDatas(value.toString());\n        if(null != s ){\n            key2.set(s);\n            context.write(key2,NullWritable.get());\n        }\n    }\n}\n```\n\n\n\n3、代码开发：ETLRunner\n\n```\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\npublic class VideoMain extends Configured implements Tool {\n    @Override\n    public int run(String[] args) throws Exception {\n        Job job = Job.getInstance(super.getConf(), \"washDatas\");\n        job.setJarByClass(VideoMain.class);\n        job.setInputFormatClass(TextInputFormat.class);\n        TextInputFormat.addInputPath(job,new Path(args[0]));\n\n        job.setMapperClass(VideoMapper.class);\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(NullWritable.class);\n        job.setOutputFormatClass(TextOutputFormat.class);\n        TextOutputFormat.setOutputPath(job,new Path(args[1]));\n        //注意，我们这里没有自定义reducer，会使用默认的一个reducer类\n        job.setNumReduceTasks(7);\n        boolean b = job.waitForCompletion(true);\n        return b?0:1;\n    }\n    public static void main(String[] args) throws Exception {\n        int run = ToolRunner.run(new Configuration(), new VideoMain(), args);\n        System.exit(run);\n    }\n}\n```\n\n## 4、项目建表并加载数据\n\n### 1、创建表\n\n创建表：youtubevideo_ori，youtubevideo_user_ori，\n\n创建表：youtubevideo_orc，youtubevideo_user_orc\n\nyoutubevideo_ori：\n\n开启分桶表功能\n\n```\nset hive.enforce.bucketing=true;\nset mapreduce.job.reduces=-1;\n\ncreate database youtube;\nuse youtube;\ncreate table youtubevideo_ori(\n    videoId string, \n    uploader string, \n    age int, \n    category array<string>, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedId array<string>)\nrow format delimited \nfields terminated by \"\\t\"\ncollection items terminated by \"&\"\nstored as textfile;\n```\n\nyoutubevideo_user_ori：\n\n```\ncreate table youtubevideo_user_ori(\n    uploader string,\n    videos int,\n    friends int)\nclustered by (uploader) into 24 buckets\nrow format delimited \nfields terminated by \"\\t\" \nstored as textfile;\n\n```\n\n然后把原始数据插入到orc表中\n\nyoutubevideo_orc：\n\n```\ncreate table youtubevideo_orc(\n    videoId string, \n    uploader string, \n    age int, \n    category array<string>, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedId array<string>)\nclustered by (uploader) into 8 buckets \nrow format delimited fields terminated by \"\\t\" \ncollection items terminated by \"&\" \nstored as orc;\n```\n\nyoutubevideo_user_orc：\n\n```\ncreate table youtubevideo_user_orc(\n    uploader string,\n    videos int,\n    friends int)\nclustered by (uploader) into 24 buckets \nrow format delimited \nfields terminated by \"\\t\" \nstored as orc;\n\n```\n\n### 2、导入ETL之后的数据\n\nyoutubevideo_ori：\n\n```\nload data inpath \"/youtubevideo/output/video/2008/0222\" into table youtubevideo_ori;\n```\n\nyoutubevideo_user_ori：\n\n```\nload data inpath \"/youtubevideo/user/2008/0903\" into table youtubevideo_user_ori;\n```\n\n### 3、向ORC表插入数据\n\nyoutubevideo_orc：\n\n```\ninsert overwrite table youtubevideo_orc select * from youtubevideo_ori;\n```\n\nyoutubevideo_user_orc：\n\n```\ninsert into table youtubevideo_user_orc select * from youtubevideo_user_ori;\n```\n\n## 5、业务分析\n\n### 1、统计视频观看数Top10\n\n思路：使用order by按照views字段做一个全局排序即可，同时我们设置只显示前10条。\n\n最终代码：\n\n```sql\nselect \n    videoId, \n    uploader, \n    age, \n    category, \n    length, \n    views, \n    rate, \n    ratings, \n    comments \nfrom \n    youtubevideo_orc \norder by \n    views \ndesc limit \n    10;\n\n```\n\n\n\n### 2、统计视频类别热度Top10\n\n思路：\n\n1) 即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。\n\n2) 我们需要按照类别group by聚合，然后count组内的videoId个数即可。\n\n3) 因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。\n\n4) 最后按照热度排序，显示前10条。\n\n最终代码：\n\n```sql\nselect \n    category_name as category, \n    count(t1.videoId) as hot \nfrom (\n    select \n        videoId,\n        category_name \n    from \n        youtubevideo_orc lateral view explode(category) t_catetory as category_name) t1 \ngroup by \n    t1.category_name \norder by \n    hot \ndesc limit \n    10;\n\n```\n\n### 3、统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数\n\n思路：\n\n1) 先找到观看数最高的20个视频所属条目的所有信息，降序排列\n\n2) 把这20条信息中的category分裂出来(列转行)\n\n3) 最后查询视频分类名称和该分类下有多少个Top20的视频\n\n最终代码：\n\n```sql\nselect \n    category_name as category, \n    count(t2.videoId) as hot_with_views \nfrom (\n    select \n        videoId, \n        category_name \n    from (\n        select \n            * \n        from \n            youtubevideo_orc \n        order by \n            views \n        desc limit \n            20) t1 lateral view explode(category) t_catetory as category_name) t2 \ngroup by \n    category_name \norder by \n    hot_with_views \ndesc;\n```\n\n### 4、 统计视频观看数Top50所关联视频的所属类别Rank\n\n思路：\n\n1)       查询出观看数最多的前50个视频的所有信息(当然包含了每个视频对应的关联视频)，记为临时表t1\n\nt1：观看数前50的视频\n\n```sql\nselect \n    * \nfrom \n    youtubevideo_orc \norder by \n    views \ndesc limit \n    50;\n```\n\n2)       将找到的50条视频信息的相关视频relatedId列转行，记为临时表t2\n\nt2：将相关视频的id进行列转行操作\n\n```\nselect \n    explode(relatedId) as videoId \nfrom \n\tt1;\n\n\n```\n\n3)       将相关视频的id和youtubevideo_orc表进行inner join操作\n\nt5：得到两列数据，一列是category，一列是之前查询出来的相关视频id\n\n```\n(select \n    distinct(t2.videoId), \n    t3.category \nfrom \n    t2\ninner join \n    youtubevideo_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name;\n\n\n```\n\n4) 按照视频类别进行分组，统计每组视频个数，然后排行\n\n最终代码：\n\n```\nselect \n    category_name as category, \n    count(t5.videoId) as hot \nfrom (\n    select \n        videoId, \n        category_name \n    from (\n        select \n            distinct(t2.videoId), \n            t3.category \n        from (\n            select \n                explode(relatedId) as videoId \n            from (\n                select \n                    * \n                from \n                    youtubevideo_orc \n                order by \n                    views \n                desc limit \n                    50) t1) t2 \n        inner join \n            youtubevideo_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name) t5\ngroup by \n    category_name \norder by \n    hot \ndesc;\n\n\n```\n\n\n\n### 5、统计每个类别中的视频热度Top10，以Music为例\n\n思路：\n\n1) 要想统计Music类别中的视频热度Top10，需要先找到Music类别，那么就需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。\n\n2) 向category展开的表中插入数据。\n\n3) 统计对应类别（Music）中的视频热度。\n\n最终代码：\n\n创建表类别表：\n\n```\ncreate table youtubevideo_category(\n    videoId string, \n    uploader string, \n    age int, \n    categoryId string, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int, \n    relatedId array<string>)\nrow format delimited \nfields terminated by \"\\t\" \ncollection items terminated by \"&\" \nstored as orc;\n\n\n```\n\n向类别表中插入数据：\n\n```\ninsert into table youtubevideo_category  \n    select \n        videoId,\n        uploader,\n        age,\n        categoryId,\n        length,\n        views,\n        rate,\n        ratings,\n        comments,\n        relatedId \n    from \n        youtubevideo_orc lateral view explode(category) catetory as categoryId;\n\n```\n\n统计Music类别的Top10（也可以统计其他）\n\n```\nselect \n    videoId, \n    views\nfrom \n    youtubevideo_category \nwhere \n    categoryId = \"Music\" \norder by \n    views \ndesc limit\n    10;\n\n```\n\n\n\n### 6、 统计每个类别中视频流量Top10，以Music为例\n\n思路：\n\n1) 创建视频类别展开表（categoryId列转行后的表）\n\n2) 按照ratings排序即可\n\n最终代码：\n\n```\nselect videoid,views,ratings \nfrom youtubevideo_category \nwhere categoryid = \"Music\" order by ratings desc limit 10;\n\n```\n\n### 7、 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频\n\n思路：\n\n1) 先找到上传视频最多的10个用户的用户信息\n\n```sql\nselect \n    * \nfrom \n    youtubevideo_user_orc \norder by \n    videos \ndesc limit \n    10;\n```\n\n2) 通过uploader字段与youtubevideo_orc表进行join，得到的信息按照views观看次数进行排序即可。\n\n最终代码：\n\n```\nselect \n    t2.videoId, \n    t2.views,\n    t2.ratings,\n    t1.videos,\n    t1.friends \nfrom (\n    select \n        * \n    from \n        youtubevideo_user_orc \n    order by \n        videos desc \n    limit \n        10) t1 \njoin \n    youtubevideo_orc t2\non \n    t1.uploader = t2.uploader \norder by \n    views desc \nlimit \n    20;\n```\n\n### 8、统计每个类别视频观看数Top10\n\n思路：\n\n1) 先得到categoryId展开的表数据\n\n2) 子查询按照categoryId进行分区，然后分区内排序，并生成递增数字，该递增数字这一列起名为rank列\n\n3) 通过子查询产生的临时表，查询rank值小于等于10的数据行即可。\n\n最终代码：\n\n```\nselect \n    t1.* \nfrom (\n    select \n        videoId,\n        categoryId,\n        views,\n        row_number() over(partition by categoryId order by views desc) rank from youtubevideo_category) t1 \nwhere \n    rank <= 10;\n\n```\n\n\n\n","tags":["hadoop","hive","项目练习"]},{"title":"Hadoop数据分析之Hive（三）","url":"/2019/11/05/hadoop/Hadoop数据分析之Hive（三）/","content":"\n### 1. hive表的文件存储格式\n\nHive支持的存储数的格式主要有；TEXTFILE（行式存储） 、SEQUENCEFILE(行式存储)、ORC（列式存储）、PARQUET（列式存储）。\n\n#### 1、列式存储和行式存储\n\n![img](img/clip_image002.jpg)\n\n上图左边为逻辑表，右边第一个为行式存储，第二个为列式存储。\n\n**行存储的特点：** 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。select  *  \n\n**列存储的特点：** 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。  select   某些字段效率更高\n\nTEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；\n\nORC和PARQUET是基于列式存储的。\n\n#### 2 、TEXTFILE格式\n\n默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。\n\n#### 3 、ORC格式\n\nOrc (Optimized Row Columnar)是hive 0.11版里引入的新的存储格式。\n\n可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB->250MB，这样能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data,Row Data,Stripe Footer：\n\n![img](img/clip_image003.png)\n\n一个orc文件可以分为若干个Stripe\n\n一个stripe可以分为三个部分\n\nindexData：某些列的索引数据\n\nrowData :真正的数据存储\n\nStripFooter：stripe的元数据信息\n\n   1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引只是记录某行的各字段在Row Data中的offset。\n\n​    2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。\n\n​    3）Stripe Footer：存的是各个stripe的元数据信息\n\n每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。\n\n#### 4 、PARQUET格式\n\nParquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。\n\nParquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。\n\n通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。\n\n![Parquet文件格式](img/clip_image005.jpg)\n\n上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。\n\n\n\n#### 5 主流文件存储格式对比实验\n\n从存储文件的压缩比和查询速度两个角度对比。\n\n**存储文件的压缩比测试：**\n\n测试数据 参见log.data\n\n##### 1）TextFile\n\n（1）创建表，存储数据格式为TEXTFILE\n\n```sql\nuse kfly;\ncreate table log_text (\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE ;\n```\n\n（2）向表中加载数据\n\n```sql\nload data local inpath '/kkb/install/hivedatas/log.data' into table log_text ;\n```\n\n \n\n（3）查看表中数据大小，大小为18.1M\n\n```sql\ndfs -du -h /user/hive/warehouse/myhive.db/log_text;\n18.1 M  /user/hive/warehouse/log_text/log.data\n```\n\n##### 2）ORC\n\n（1）创建表，存储数据格式为ORC\n\n```sql\nas\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nSTORED AS orc ;\n```\n\n（2）向表中加载数据\n\n```sql\ninsert into table log_orc select * from log_text ;\n```\n\n（3）查看表中数据大小\n\n```sql\ndfs -du -h /user/hive/warehouse/myhive.db/log_orc;\n\n2.8 M  /user/hive/warehouse/log_orc/123456_0\n```\n\norc这种存储格式，默认使用了zlib压缩方式来对数据进行压缩，所以数据会变成了2.8M，非常小\n\n##### 3）Parquet\n\n（1）创建表，存储数据格式为parquet\n\n```sql\ncreate table log_parquet(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nSTORED AS PARQUET ;  \n```\n\n（2）向表中加载数据\n\n```sql\ninsert into table log_parquet select * from log_text ;\n```\n\n（3）查看表中数据大小\n\n```sql\ndfs -du -h /user/hive/warehouse/myhive.db/log_parquet;\n\n13.1 M  /user/hive/warehouse/log_parquet/123456_0\n\n```\n\n存储文件的压缩比总结：\n\n```sql\nORC >  Parquet >  textFile\n\n```\n\n**存储文件的查询速度测试：**\n\n```sql\n1）TextFile\nhive (default)> select count(*) from log_text;\n_c0\n100000\nTime taken: 21.54 seconds, Fetched: 1 row(s)  \n\n2）ORC\nhive (default)> select count(*) from log_orc;\n_c0\n100000\nTime taken: 20.867 seconds, Fetched: 1 row(s)  \n\n3）Parquet\nhive (default)> select count(*) from log_parquet; \n_c0\n100000\nTime taken: 22.922 seconds, Fetched: 1 row(s)\n\n存储文件的查询速度总结：\nORC > TextFile > Parquet\n```\n\n### 2、存储和压缩结合\n\n官网：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC\n\nORC存储方式的压缩：\n\n| Key                      | Default    | Notes                                                        |\n| ------------------------ | ---------- | ------------------------------------------------------------ |\n| orc.compress             | ZLIB       | high level   compression (one of NONE, ZLIB, SNAPPY)         |\n| orc.compress.size        | 262,144    | number of bytes in   each compression chunk                  |\n| orc.stripe.size          | 67,108,864 | number of bytes in   each stripe                             |\n| orc.row.index.stride     | 10,000     | number of rows   between index entries (must be >= 1000)     |\n| orc.create.index         | true       | whether to create row   indexes                              |\n| orc.bloom.filter.columns | \"\"         | comma separated list of column names for which bloom filter   should be created |\n| orc.bloom.filter.fpp     | 0.05       | false positive probability for bloom filter (must >0.0 and   <1.0) |\n\n#### 1）创建一个非压缩的的ORC存储方式\n\n（1）建表语句\n\n```sql\ncreate table log_orc_none(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nSTORED AS orc tblproperties (\"orc.compress\"=\"NONE\");\n\n```\n\n（2）插入数据\n\n```sql\ninsert into table log_orc_none select * from log_text ;\n\n```\n\n（3）查看插入后数据\n\n```sql\ndfs -du -h /user/hive/warehouse/myhive.db/log_orc_none;\n\n7.7 M  /user/hive/warehouse/log_orc_none/123456_0\n\n```\n\n#### 2）创建一个SNAPPY压缩的ORC存储方式\n\n（1）建表语句\n\n```sql\ncreate table log_orc_snappy(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nSTORED AS orc tblproperties (\"orc.compress\"=\"SNAPPY\");\n\n```\n\n（2）插入数据\n\n```sql\ninsert into table log_orc_snappy select * from log_text ;\n```\n\n（3）查看插入后数据\n\n```sql\ndfs -du -h /user/hive/warehouse/myhive.db/log_orc_snappy ;\n3.8 M  /user/hive/warehouse/log_orc_snappy/123456_0\n```\n\n3）上一节中默认创建的ORC存储方式，导入数据后的大小为\n\n```sql\n2.8 M  /user/hive/warehouse/log_orc/123456_0\n```\n\n比Snappy压缩的还小。原因是orc存储文件默认采用ZLIB压缩。比snappy压缩的小。\n\n4）存储方式和压缩总结：\n\n​        在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy。\n\n\n\n### 3. hive的SerDe\n\n#### 1 hive的SerDe是什么\n\n​\tSerde是 ==Serializer/Deserializer==的简写。hive使用Serde进行行对象的序列与反序列化。最后实现把文件内容映射到 hive 表中的字段数据类型。\n\n​\t为了更好的阐述使用 SerDe 的场景，我们需要了解一下 Hive 是如何读数据的(类似于 HDFS 中数据的读写操作)：\n\n```sql\nHDFS files –> InputFileFormat –> <key, value> –> Deserializer –> Row object\n\nRow object –> Serializer –> <key, value> –> OutputFileFormat –> HDFS files\n```\n\n\n\n#### 2 hive的SerDe 类型\n\n- Hive 中内置==org.apache.hadoop.hive.serde2== 库，内部封装了很多不同的SerDe类型。\n- hive创建表时， 通过自定义的SerDe或使用Hive内置的SerDe类型指定数据的序列化和反序列化方式。\n\n```sql\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name \n[(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] \n[CLUSTERED BY (col_name, col_name, ...) \n[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] \n[ROW FORMAT row_format] \n[STORED AS file_format] \n[LOCATION hdfs_path]\n\n\n```\n\n- 如上创建表语句， 使用==row format 参数说明SerDe的类型。==\n\n- 你可以创建表时使用用户**自定义的Serde或者native Serde**， **如果 ROW FORMAT没有指定或者指定了 ROW FORMAT DELIMITED就会使用native Serde**。\n- [Hive SerDes](https://cwiki.apache.org/confluence/display/Hive/SerDe): \n  - Avro (Hive 0.9.1 and later) \n  - ORC (Hive 0.11 and later) \n  - RegEx \n  - Thrift \n  - Parquet (Hive 0.13 and later) \n  - CSV (Hive 0.14 and later) \n  - MultiDelimitSerDe \n\n#### 3  企业实战\n\n##### 1 通过MultiDelimitSerDe 解决多字符分割场景\n\n- 1、创建表\n\n```sql\nuse kfly;\ncreate  table kfly_mul (id String, name string)\nrow format serde 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe'\nWITH SERDEPROPERTIES (\"field.delim\"=\"##\");\n\n\n```\n\n- 2、准备数据 t1.txt\n\n```sql\ncd /kkb/install/hivedatas\nvim t1.txt\n\n\n1##xiaoming\n2##xiaowang\n3##xiaozhang\n\n```\n\n- 3、加载数据\n\n```sql\nload data local inpath '/kkb/install/hivedatas/t1.txt' into table t1;\n\n```\n\n- 4、查询数据\n\n```sql\n0: jdbc:hive2://node1:10000> select * from t1;\n+--------+------------+--+\n| t1.id  |  t1.name   |\n+--------+------------+--+\n| 1      | xiaoming   |\n| 2      | xiaowang   |\n| 3      | xiaozhang  |\n+--------+------------+--+\n\n```\n\n##### 2 通过RegexSerDe 解决多字符分割场景\n\n- 1、创建表\n\n```sql\ncreate  table t2(id int, name string)\nrow format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' \nWITH SERDEPROPERTIES (\"input.regex\" = \"^(.*)\\\\#\\\\#(.*)$\");\n\n```\n\n- 2、准备数据 t1.txt\n\n```sql\n1##xiaoming\n2##xiaowang\n3##xiaozhang\n\n```\n\n- 3、加载数据\n\n```sql\nload data local inpath '/kkb/install/hivedatas/t1.txt' into table t2;\n\n```\n\n- 4、查询数据\n\n```sql\n0: jdbc:hive2://node1:10000> select * from t2;\n+--------+------------+--+\n| t2.id  |  t2.name   |\n+--------+------------+--+\n| 1      | xiaoming   |\n| 2      | xiaowang   |\n| 3      | xiaozhang  |\n+--------+------------+--+\n\n```\n\n\n- \n\n### 4. hive的企业级调优\n\n#### 1、Fetch抓取\n\n- Fetch抓取是指，==Hive中对某些情况的查询可以不必使用MapReduce计算==\n\n  - 例如：select * from score;\n  - 在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台\n\n- 在hive-default.xml.template文件中 ==hive.fetch.task.conversion默认是more==，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。\n\n- 案例实操\n\n  - 把 hive.fetch.task.conversion设置成**==none==**，然后执行查询语句，都会执行mapreduce程序\n\n  ```sql\n  set hive.fetch.task.fen=none;\n  select * from score;\n  select s_id from score;\n  select s_id from score limit 3;\n  \n  ```\n\n  - 把hive.fetch.task.conversion设置成==**more**==，然后执行查询语句，如下查询方式都不会执行mapreduce程序。\n\n  ```sql\n  set hive.fetch.task.conversion=more;\n  select * from score;\n  select s_id from score;\n  select s_id from score limit 3;\n  \n  ```\n\n#### 2、本地模式\n\n- 在Hive客户端测试时，默认情况下是启用hadoop的job模式,把任务提交到集群中运行，这样会导致计算非常缓慢；\n\n- Hive可以通过本地模式在单台机器上处理任务。对于小数据集，执行时间可以明显被缩短。\n\n- 案例实操\n\n  ```sql\n  --开启本地模式，并执行查询语句\n  set hive.exec.mode.local.auto=true;  //开启本地mr\n  \n  --设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，\n  --默认为134217728，即128M\n  set hive.exec.mode.local.auto.inputbytes.max=50000000;\n  \n  --设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，\n  --默认为4\n  set hive.exec.mode.local.auto.input.files.max=5;\n  \n  \n  --执行查询的sql语句\n  select * from student cluster by s_id;\n  ```\n\n\n\n```sql\n--关闭本地运行模式\nset hive.exec.mode.local.auto=false;\nselect * from student cluster by s_id;\n```\n\n\n\n#### 3、表的优化\n\n##### 1 小表、大表 join\n\n- 将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。\n\n  ```sql\n  select  count(distinct s_id)  from score;\n  \n  select count(s_id) from score group by s_id; 在map端进行聚合，效率更高\n  ```\n\n- 实际测试发现：新版的hive已经对小表 join 大表和大表 join 小表进行了优化。小表放在左边和右边已经没有明显区别。\n\n- 多个表关联时，最好分拆成小段，避免大sql（无法控制中间Job）\n\n##### 2 大表 join 大表\n\n- 1．空 key 过滤\n\n  - 有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。\n\n  - 此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。\n\n  - 测试环境准备：\n\n    ```sql\n    use myhive;\n    create table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n    \n    create table nullidtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n    \n    create table jointable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n    \n    load data local inpath '/kkb/install/hivedatas/hive_big_table/*' into table ori; \n    load data local inpath '/kkb/install/hivedatas/hive_have_null_id/*' into table nullidtable;\n    \n    ```\n\n    过滤空key与不过滤空key的结果比较\n\n    ```sql\n    不过滤：\n    INSERT OVERWRITE TABLE jointable\n    SELECT a.* FROM nullidtable a JOIN ori b ON a.id = b.id;\n    结果：\n    No rows affected (152.135 seconds)\n    \n    过滤：\n    INSERT OVERWRITE TABLE jointable\n    SELECT a.* FROM (SELECT * FROM nullidtable WHERE id IS NOT NULL ) a JOIN ori b ON a.id = b.id;\n    结果：\n    No rows affected (141.585 seconds)\n    ```\n\n- 2、空 key 转换\n\n  - 有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在 join 的结果中，此时我们可以表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上。\n\n    不随机分布：\n\n    ```sql\n    set hive.exec.reducers.bytes.per.reducer=32123456;\n    set mapreduce.job.reduces=7;\n    INSERT OVERWRITE TABLE jointable\n    SELECT a.*\n    FROM nullidtable a\n    LEFT JOIN ori b ON CASE WHEN a.id IS NULL THEN 'hive' ELSE a.id END = b.id;\n    No rows affected (41.668 seconds)  \n    \n    ```\n\n    **结果：这样的后果就是所有为null值的id全部都变成了相同的字符串，及其容易造成数据的倾斜（所有的key相同，相同key的数据会到同一个reduce当中去）**\n\n    **为了解决这种情况，我们可以通过hive的rand函数，随记的给每一个为空的id赋上一个随机值，这样就不会造成数据倾斜**\t\t\n\n  ​\t\t随机分布：\n\n  ```sql\n  set hive.exec.reducers.bytes.per.reducer=32123456;\n  set mapreduce.job.reduces=7;\n  INSERT OVERWRITE TABLE jointable\n  SELECT a.*\n  FROM nullidtable a\n  LEFT JOIN ori b ON CASE WHEN a.id IS NULL THEN concat('hive', rand()) ELSE a.id END = b.id;\n  \n  No rows affected (42.594 seconds)              \n  ```\n\n\n\n##### 3、大表join小表与小表join大表实测\n\n需求：测试大表JOIN小表和小表JOIN大表的效率 （新的版本当中已经没有区别了，旧的版本当中需要使用小表）\n\n（1）建大表、小表和JOIN后表的语句\n\n```sql\ncreate table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\ncreate table smalltable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\ncreate table jointable2(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\n```\n\n（2）分别向大表和小表中导入数据\n\n```sql\nhive (default)> load data local inpath '/kkb/install/hivedatas/big_data' into table bigtable;\n\nhive (default)>load data local inpath '/kkb/install/hivedatas/small_data' into table smalltable;\n```\n\n##### 3 map  join \n\n- 如果不指定MapJoin 或者不符合 MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在map端进行join，避免reducer处理。\n\n- 1、开启MapJoin参数设置\n\n  ```sql\n   --默认为true\n  set hive.auto.convert.join = true;\n  ```\n\n- 2、大表小表的阈值设置（默认25M一下认为是小表）\n\n```sql\nset hive.mapjoin.smalltable.filesize=26214400;\n\n```\n\n- 3、MapJoin工作机制\n\n![xxx](img/xxx-1570506631515.jpg)\n\n首先是Task A，它是一个Local Task（在客户端本地执行的Task），负责扫描小表b的数据，将其转换成一个HashTable的数据结构，并写入本地的文件中，之后将该文件加载到DistributeCache中。\n\n接下来是Task B，该任务是一个没有Reduce的MR，启动MapTasks扫描大表a,在Map阶段，根据a的每一条记录去和DistributeCache中b表对应的HashTable关联，并直接输出结果。\n\n由于MapJoin没有Reduce，所以由Map直接输出结果文件，有多少个Map Task，就有多少个结果文件。\n\n**案例实操：**\n\n（1）开启Mapjoin功能\n\n```sql\nset hive.auto.convert.join = true; 默认为true\n```\n\n（2）执行小表JOIN大表语句\n\n```sql\nINSERT OVERWRITE TABLE jointable2\nSELECT b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nFROM smalltable s\nJOIN bigtable  b\nON s.id = b.id;\n\nTime taken: 31.814 seconds\n```\n\n（3）执行大表JOIN小表语句\n\n```shell\nINSERT OVERWRITE TABLE jointable2\nSELECT b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nFROM bigtable  b\nJOIN smalltable  s\nON s.id = b.id;\n\nTime taken: 28.46 seconds\n```\n\n\n\n##### 4 group By\n\n- 默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。\n\n- 并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。\n\n- 开启Map端聚合参数设置\n\n  ```sql\n  --是否在Map端进行聚合，默认为True\n  set hive.map.aggr = true;\n  --在Map端进行聚合操作的条目数目\n  set hive.groupby.mapaggr.checkinterval = 100000;\n  --有数据倾斜的时候进行负载均衡（默认是false）\n  set hive.groupby.skewindata = true;\n  \n  当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。\n  ```\n\n\n\n##### 5 count(distinct) \n\n- 数据量小的时候无所谓，数据量大的情况下，由于count distinct 操作需要用一个reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般count distinct使用先group by 再count的方式替换\n\n  环境准备：\n\n  \n\n  ```sql\n  create table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n  \n  load data local inpath '/kkb/install/hivedatas/data/100万条大表数据（id除以10取整）/bigtable' into table bigtable;\n  \n  \n  --每个reduce任务处理的数据量 默认256000000（256M）\n   set hive.exec.reducers.bytes.per.reducer=32123456;\n   \n   select  count(distinct ip )  from log_text;\n   \n   转换成\n   set hive.exec.reducers.bytes.per.reducer=32123456;\n   select count(ip) from (select ip from log_text group by ip) t;\n   \n   \n   虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。\n  ```\n\n##### 6 笛卡尔积\n\n- 尽量避免笛卡尔积，即避免join的时候不加on条件，或者无效的on条件\n- Hive只能使用1个reducer来完成笛卡尔积。\n\n#### 4、使用分区剪裁、列剪裁\n\n- 尽可能早地过滤掉尽可能多的数据量，避免大量数据流入外层SQL。\n- **列剪裁**\n  - 只获取需要的列的数据，减少数据输入。\n- **分区裁剪**\n  - 分区在hive实质上是目录，分区裁剪可以方便直接地过滤掉大部分数据。\n  - 尽量使用分区过滤，少用select  *\n\n\n\n​\t环境准备：\n\n```sql\ncreate table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\ncreate table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\nload data local inpath '/home/admin/softwares/data/加递增id的原始数据/ori' into table ori;\n\nload data local inpath '/home/admin/softwares/data/100万条大表数据（id除以10取整）/bigtable' into table bigtable;\n\n```\n\n先关联再Where：\n\n```sql\nSELECT a.id\nFROM bigtable a\nLEFT JOIN ori b ON a.id = b.id\nWHERE b.id <= 10;\n\n```\n\n正确的写法是写在ON后面：先Where再关联\n\n```sql\nSELECT a.id\nFROM ori a\nLEFT JOIN bigtable b ON (a.id <= 10 AND a.id = b.id);\n\n```\n\n或者直接写成子查询：\n\n```sql\nSELECT a.id\nFROM bigtable a\nRIGHT JOIN (SELECT id\nFROM ori\nWHERE id <= 10\n) b ON a.id = b.id;\n\n```\n\n\n\n#### 5、并行执行\n\n- 把一个sql语句中没有相互依赖的阶段并行去运行。提高集群资源利用率\n\n```sql\n--开启并行执行\nset hive.exec.parallel=true;\n--同一个sql允许最大并行度，默认为8。\nset hive.exec.parallel.thread.number=16;\n```\n\n\n\n#### 6、严格模式\n\n- Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。\n\n- 通过设置属性hive.mapred.mode值为默认是非严格模式**nonstrict** 。开启严格模式需要修改hive.mapred.mode值为**strict**，开启严格模式可以禁止3种类型的查询。\n\n  ```sql\n  --设置非严格模式（默认）\n  set hive.mapred.mode=nonstrict;\n  \n  --设置严格模式\n  set hive.mapred.mode=strict;\n  ```\n\n\n\n- （1）对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行\n\n  ```sql\n  --设置严格模式下 执行sql语句报错； 非严格模式下是可以的\n  select * from order_partition；\n  \n  异常信息：Error: Error while compiling statement: FAILED: SemanticException [Error 10041]: No partition predicate found for Alias \"order_partition\" Table \"order_partition\" \n  ```\n\n- （2）对于使用了order by语句的查询，要求必须使用limit语句\n\n  ```sql\n  --设置严格模式下 执行sql语句报错； 非严格模式下是可以的\n  select * from order_partition where month='2019-03' order by order_price; \n  \n  异常信息：Error: Error while compiling statement: FAILED: SemanticException 1:61 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'order_price'\n  ```\n\n- （3）限制笛卡尔积的查询\n\n  - 严格模式下，避免出现笛卡尔积的查询\n\n\n\n#### 7、JVM重用\n\n- JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。\n\n  Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。\n\n  ```xml\n  <property>\n    <name>mapreduce.job.jvm.numtasks</name>\n    <value>10</value>\n    <description>How many tasks to run per jvm. If set to -1, there is\n    no limit. \n    </description>\n  </property>\n  \n  ```\n\n  我们也可以在hive当中通过\n\n  ```sql\n   set  mapred.job.reuse.jvm.num.tasks=10;\n  ```\n\n  这个设置来设置我们的jvm重用\n\n  这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。\n\n#### 8、推测执行\n\n- 在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。\n\n  设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置\n\n```xml\n<property>\n  <name>mapreduce.map.speculative</name>\n  <value>true</value>\n  <description>If true, then multiple instances of some map tasks \n               may be executed in parallel.</description>\n</property>\n\n<property>\n  <name>mapreduce.reduce.speculative</name>\n  <value>true</value>\n  <description>If true, then multiple instances of some reduce tasks \n               may be executed in parallel.</description>\n</property>\n\n```\n\n不过hive本身也提供了配置项来控制reduce-side的推测执行：\n\n```xml\n  <property>\n    <name>hive.mapred.reduce.tasks.speculative.execution</name>\n    <value>true</value>\n    <description>Whether speculative execution for reducers should be turned on. </description>\n  </property>\n\n```\n\n关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。\n\n\n\n#### 9、压缩\n\n​\t参见数据的压缩\n\n- Hive表中间数据压缩\n\n  ```shell\n  #设置为true为激活中间数据压缩功能，默认是false，没有开启\n  set hive.exec.compress.intermediate=true;\n  #设置中间数据的压缩算法\n  set mapred.map.output.compression.codec= org.apache.hadoop.io.compress.SnappyCodec;\n  \n  ```\n\n- Hive表最终输出结果压缩\n\n  ```shell\n  set hive.exec.compress.output=true;\n  set mapred.output.compression.codec= \n  org.apache.hadoop.io.compress.SnappyCodec;\n  ```\n\n#### 10、使用EXPLAIN（执行计划）\n\n查看hql执行计划\n\n\n\n#### 11、数据倾斜\n\n##### 1 合理设置Map数\n\n- 1)  通常情况下，作业会通过input的目录产生一个或者多个map任务。\n\n  ```sql\n  主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。\n  \n  举例：\n  a)  假设input目录下有1个文件a，大小为780M，那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数。\n  b) 假设input目录下有3个文件a，b，c大小分别为10m，20m，150m，那么hadoop会分隔成4个块（10m，20m，128m，22m），从而产生4个map数。即，如果文件大于块大小(128m)，那么会拆分，如果小于块大小，则把该文件当成一个块。\n  \n  ```\n\n- 2） 是不是map数越多越好？\n\n  ```shell\n    答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。\n  ```\n\n- 3） 是不是保证每个map处理接近128m的文件块，就高枕无忧了？\n\n  ```shell\n  答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。\n  \n  针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；\n  \n  ```\n\n\n\n##### 2 小文件合并\n\n- 在map执行前合并小文件，减少map数：\n\n- CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）\n\n  ```sql\n  set mapred.max.split.size=112345600;\n  set mapred.min.split.size.per.node=112345600;\n  set mapred.min.split.size.per.rack=112345600;\n  set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n  \n  ```\n\n  这个参数表示执行前进行小文件合并，前面三个参数确定合并文件块的大小，大于文件块大小128m的，按照128m来分隔，小于128m，大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），进行合并。\n\n##### 3 复杂文件增加Map数\n\n- 当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。\n\n- 增加map的方法为\n\n  - 根据 ==computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))==公式\n  - ==调整maxSize最大值==。让maxSize最大值低于blocksize就可以增加map的个数。\n\n  ```shell\n  mapreduce.input.fileinputformat.split.minsize=1 默认值为1\n  \n  mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue 默认值Long.MAXValue因此，默认情况下，切片大小=blocksize \n  \n  maxsize（切片最大值): 参数如果调到比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。\n  \n  minsize(切片最小值): 参数调的比blockSize大，则可以让切片变得比blocksize还大。\n  \n  ```\n\n  - 例如\n\n  ```sql\n  --设置maxsize大小为10M，也就是说一个fileSplit的大小为10M\n  set mapreduce.input.fileinputformat.split.maxsize=10485760;\n  ```\n\n\n\n##### 4 合理设置Reduce数\n\n- 1、调整reduce个数方法一\n\n  - 1）每个Reduce处理的数据量默认是256MB\n\n    ```sql\n    set hive.exec.reducers.bytes.per.reducer=256000000;\n    ```\n\n  - 2) 每个任务最大的reduce数，默认为1009\n\n    ```sql\n    set hive.exec.reducers.max=1009;\n    ```\n\n  - 3) 计算reducer数的公式\n\n    ```shell\n    N=min(参数2，总输入数据量/参数1)\n    ```\n\n- 2、调整reduce个数方法二\n\n  ```sql\n  --设置每一个job中reduce个数\n  set mapreduce.job.reduces=3;\n  ```\n\n\n\n- 3、reduce个数并不是越多越好\n\n  - 过多的启动和初始化reduce也会消耗时间和资源；\n\n  - 同时过多的reduce会生成很多个文件，也有可能出现小文件问题\n\n","tags":["hadoop","hive","调优"]},{"title":"hadoop之数据分析Hive（二）","url":"/2019/11/03/hadoop/hadoop之数据分析Hive（二）/","content":"\n### 1、hive的参数传递\n\n#### 1、Hive命令行\n\nhive [-hiveconf x=y]* [<-i filename>]* [<-f filename>|<-e query-string>] [-S]\n\n说明：\n\n1、   -i 从文件初始化HQL。\n\n2、   -e从命令行执行指定的HQL \n\n3、   -f 执行HQL脚本 \n\n4、   -v 输出执行的HQL语句到控制台 \n\n5、   -p <port> connect to Hive Server on port number \n\n6、   -hiveconf x=y Use this to set hive/hadoop configuration variables.  设置hive运行时候的参数配置\n\n#### 2、Hive参数配置方式\n\nHive参数大全：\n\nhttps://cwiki.apache.org/confluence/display/Hive/Configuration+Properties\n\n开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么设定的参数没有起作用？这通常是错误的设定方式导致的。\n\n**对于一般参数，有以下三种设定方式：**\n\n```\n配置文件  hive-site.xml\n\n命令行参数  启动hive客户端的时候可以设置参数\n\n参数声明   进入客户单以后设置的一些参数  set  \n```\n\n**配置文件**：Hive的配置文件包括\n\n 用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml \n\n默认配置文件：$HIVE_CONF_DIR/hive-default.xml \n\n用户自定义配置会覆盖默认配置。\n\n另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。\n\n**命令行参数**：启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如：\n\n```shell\nbin/hive -hiveconf hive.root.logger=INFO,console\n```\n\n这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效。\n\n**参数声明**：可以在HQL中使用SET关键字设定参数，例如：\n\n```\nset mapred.reduce.tasks=100;\n```\n\n这一设定的作用域也是session级的。\n\n上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。\n\n```\n参数声明  >   命令行参数   >  配置文件参数（hive）\n```\n\n#### 3、使用变量传递参数\n\n实际工作当中，我们一般都是将hive的hql语法开发完成之后，就写入到一个脚本里面去，然后定时的通过命令 hive  -f  去执行hive的语法即可，然后通过定义变量来传递参数到hive的脚本当中去，那么我们接下来就来看看如何使用hive来传递参数。\n\nhive0.9以及之前的版本是不支持传参的\n hive1.0版本之后支持  hive -f 传递参数\n\n在hive当中我们一般可以使用hivevar或者hiveconf来进行参数的传递\n\n##### hiveconf使用说明\n\nhiveconf用于定义HIVE执行上下文的属性(配置参数)，可覆盖覆盖hive-site.xml（hive-default.xml）中的参数值，如用户执行目录、日志打印级别、执行队列等。例如我们可以使用hiveconf来覆盖我们的hive属性配置，\n\nhiveconf变量取值必须要使用hiveconf作为前缀参数，具体格式如下:\n\n```sql\n${hiveconf:key} \nbin/hive --hiveconf \"mapred.job.queue.name=root.default\"\n```\n\n##### hivevar使用说明\n\nhivevar用于定义HIVE运行时的变量替换，类似于JAVA中的“PreparedStatement”，与\\${key}配合使用或者与 ${hivevar:key}\n\n对于hivevar取值可以不使用前缀hivevar，具体格式如下：\n\n```sql\n使用前缀:\n ${hivevar:key}\n不使用前缀:\n ${key}\n--hivevar  name=zhangsan    ${hivevar:name}  \n也可以这样取值  ${name}\n```\n\n##### define使用说明\n\n```sql\ndefine与hivevar用途完全一样，还有一种简写“-d\nbin/hive --hiveconf \"mapred.job.queue.name=root.default\" -d my=\"201809\" --database mydb\n执行SQL\nselect * from mydb where concat(year, month) = ${my} limit 10;\n```\n\n##### hiveconf与hivevar使用实战\n\n需求：hive当中执行以下hql语句，并将参数全部都传递进去\n\n```sql\nselect * from student left join score on student.s_id = score.s_id where score.month = '201807' and score.s_score > 80 and score.c_id = 03;\n```\n\n###### 第一步：创建student表并加载数据\n\n```sql\nhive (myhive)> create external table student\n(s_id string,s_name string,s_birth string , s_sex string ) row format delimited\nfields terminated by '\\t';\n\nhive (myhive)> load data local inpath '/kkb/install/hivedatas/student.csv' overwrite into table student;\n\n```\n\n###### 第二步：定义hive脚本\n\n开发hql脚本，并使用hiveconf和hivevar进行参数穿肚\n\nnode03执行以下命令定义hql脚本\n\n```sql\ncd /kkb/instal/hivedatas\n\nvim hivevariable.hql\nuse myhive;\nselect * from student left join score on student.s_id = score.s_id where score.month = ${hiveconf:month} and score.s_score > ${hivevar:s_score} and score.c_id = ${c_id};   \n```\n\n###### 第三步：调用hive脚本并传递参数\n\nnode03执行以下命令并\n\n```sql\n[root@node03 hive-1.1.0-cdh5.14.2]# bin/hive --hiveconf month=201807 --hivevar s_score=80 --hivevar c_id=03  -f /kkb/install/hivedatas/hivevariable.hql\n```\n\n### 2、hive的常用函数介绍\n\n#### 系统内置函数\n\n```\n1．查看系统自带的函数\nhive> show functions;\n2．显示自带的函数的用法\nhive> desc function upper;\n3．详细显示自带的函数的用法\nhive> desc function extended upper;\n```\n\n\n\n#### 1、数值计算\n\n##### 1、取整函数: round \n\n**语法**: round(double a)\n **返回值**: BIGINT\n **说明**: 返回double类型的整数值部分 （遵循四舍五入）\n\n```\nhive> select round(3.1415926) from tableName;\n3\nhive> select round(3.5) from tableName;\n4\nhive> create table tableName as select round(9542.158) from tableName;\n\n```\n\n\n\n##### 2、指定精度取整函数: round \n\n**语法**: round(double a, int d)\n **返回值**: DOUBLE\n **说明**: 返回指定精度d的double类型\n\n```\nhive> select round(3.1415926,4) from tableName;\n3.1416\n\n```\n\n##### 3、向下取整函数: floor \n\n**语法**: floor(double a)\n **返回值**: BIGINT\n **说明**: 返回等于或者小于该double变量的最大的整数\n\n```\nhive> select floor(3.1415926) from tableName;\n3\nhive> select floor(25) from tableName;\n25\n\n\n```\n\n##### 4、向上取整函数: ceil \n\n**语法**: ceil(double a)\n **返回值**: BIGINT\n **说明**: 返回等于或者大于该double变量的最小的整数\n\n```\nhive> select ceil(3.1415926) from tableName;\n4\nhive> select ceil(46) from tableName;\n46\n\n\n```\n\n##### 5、向上取整函数: ceiling \n\n**语法**: ceiling(double a)\n **返回值**: BIGINT\n **说明**: 与ceil功能相同\n\n\n\n```\nhive> select ceiling(3.1415926) from tableName;\n4\nhive> select ceiling(46) from tableName;\n46\n\n```\n\n##### 6、取随机数函数: rand \n\n**语法**: rand(),rand(int seed)\n **返回值**: double\n **说明**: 返回一个0到1范围内的随机数。如果指定种子seed，则会等到一个稳定的随机数序列\n\n```\nhive> select rand() from tableName;\n0.5577432776034763\nhive> select rand() from tableName;\n0.6638336467363424\nhive> select rand(100) from tableName;\n0.7220096548596434\nhive> select rand(100) from tableName;\n0.7220096548596434\n```\n\n\n\n#### 2、日期函数\n\n##### 1、UNIX时间戳转日期函数: from_unixtime  \n\n**语法**: from_unixtime(bigint unixtime[, string format])\n **返回值**: string\n **说明**: 转化UNIX时间戳（从1970-01-01 00:00:00 UTC到指定时间的秒数）到当前时区的时间格式\n\n```sql\nhive> select from_unixtime(1323308943,'yyyyMMdd') from tableName;\n20111208\n\n\n```\n\n##### 2、获取当前UNIX时间戳函数: unix_timestamp\n\n**语法**: unix_timestamp()\n **返回值**: bigint\n **说明**: 获得当前时区的UNIX时间戳\n\n```sql\nhive> select unix_timestamp() from tableName;\n1323309615\n\n```\n\n##### 3、日期转UNIX时间戳函数: unix_timestamp \n\n**语法**: unix_timestamp(string date)\n **返回值**: bigint\n **说明**: 转换格式为\"yyyy-MM-dd HH:mm:ss\"的日期到UNIX时间戳。如果转化失败，则返回0。\n\n```   sql\nhive> select unix_timestamp('2011-12-07 13:01:03') from tableName;\n1323234063\n```\n\n##### 4、指定格式日期转UNIX时间戳函数: unix_timestamp \n\n**语法**: unix_timestamp(string date, string pattern)\n **返回值**: bigint\n **说明**: 转换pattern格式的日期到UNIX时间戳。如果转化失败，则返回0。\n\n```sql\nhive> select unix_timestamp('20111207 13:01:03','yyyyMMdd HH:mm:ss') from tableName;\n1323234063\n\n```\n\n##### 5、日期时间转日期函数: to_date  \n\n**语法**: to_date(string timestamp)\n **返回值**: string\n **说明**: 返回日期时间字段中的日期部分。\n\n```sql\nhive> select to_date('2011-12-08 10:03:01') from tableName;\n2011-12-08\n```\n\n##### 6、日期转年函数: year \n\n**语法**: year(string date)\n **返回值**: int\n **说明**: 返回日期中的年。\n\n```sql\nhive> select year('2011-12-08 10:03:01') from tableName;\n2011\nhive> select year('2012-12-08') from tableName;\n2012\n\n\n```\n\n##### 7、日期转月函数: month \n\n**语法**: month (string date)\n **返回值**: int\n **说明**: 返回日期中的月份。\n\n```sql\nhive> select month('2011-12-08 10:03:01') from tableName;\n12\nhive> select month('2011-08-08') from tableName;\n8\n\n\n```\n\n##### 8、日期转天函数: day \n\n**语法**: day (string date)\n **返回值**: int\n **说明**: 返回日期中的天。\n\n```sql\nhive> select day('2011-12-08 10:03:01') from tableName;\n8\nhive> select day('2011-12-24') from tableName;\n24\n\n\n```\n\n##### 9、日期转小时函数: hour \n\n**语法**: hour (string date)\n **返回值**: int\n **说明**: 返回日期中的小时。\n\n```sql\nhive> select hour('2011-12-08 10:03:01') from tableName;\n10\n\n```\n\n##### 10、日期转分钟函数: minute\n\n**语法**: minute (string date)\n **返回值**: int\n **说明**: 返回日期中的分钟。\n\n```sql\nhive> select minute('2011-12-08 10:03:01') from tableName;\n3\n\nhive> select second('2011-12-08 10:03:01') from tableName;\n1\n```\n\n\n\n##### 12、日期转周函数: weekofyear\n\n**语法**: weekofyear (string date)\n **返回值**: int\n **说明**: 返回日期在当前的周数。\n\n```sql\nhive> select weekofyear('2011-12-08 10:03:01') from tableName;\n49\n\n```\n\n##### 13、日期比较函数: datediff \n\n**语法**: datediff(string enddate, string startdate)\n **返回值**: int\n **说明**: 返回结束日期减去开始日期的天数。\n\n```sql\nhive> select datediff('2012-12-08','2012-05-09') from tableName;\n213\n\n```\n\n##### 14、日期增加函数: date_add \n\n**语法**: date_add(string startdate, int days)\n **返回值**: string\n **说明**: 返回开始日期startdate增加days天后的日期。\n\n```\nhive> select date_add('2012-12-08',10) from tableName;\n2012-12-18\n\n```\n\n##### 15、日期减少函数: date_sub \n\n**语法**: date_sub (string startdate, int days)\n **返回值**: string\n **说明**: 返回开始日期startdate减少days天后的日期。\n\n```\nhive> select date_sub('2012-12-08',10) from tableName;\n2012-11-28\n\n```\n\n\n\n#### 3、条件函数\n\n##### 1、If函数: if \n\n**语法**: if(boolean testCondition, T valueTrue, T valueFalseOrNull)\n **返回值**: T\n **说明**: 当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull\n\n```\nhive> select if(1=2,100,200) from tableName;\n200\nhive> select if(1=1,100,200) from tableName;\n100\n\n\n```\n\n##### 2、非空查找函数: COALESCE\n\n**语法**: COALESCE(T v1, T v2, …)\n **返回值**: T\n **说明**: 返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL\n\n```\nhive> select COALESCE(null,'100','50') from tableName;\n100\n\n\n```\n\n##### 3、条件判断函数：CASE \n\n**语法**: CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END\n **返回值**: T\n **说明**：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f\n\n```\nhive> Select case 100 when 50 then 'tom' when 100 then 'mary' else 'tim' end from tableName;\nmary\nhive> Select case 200 when 50 then 'tom' when 100 then 'mary' else 'tim' end from tableName;\ntim\n\n\n```\n\n##### 4、条件判断函数：CASE  \n\n**语法**: CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END\n **返回值**: T\n **说明**：如果a为TRUE,则返回b；如果c为TRUE，则返回d；否则返回e\n\n```\nhive> select case when 1=2 then 'tom' when 2=2 then 'mary' else 'tim' end from tableName;\nmary\nhive> select case when 1=1 then 'tom' when 2=2 then 'mary' else 'tim' end from tableName;\ntom\n\n\n```\n\n\n\n#### 4、字符串函数\n\n##### 1、字符串长度函数：length\n\n**语法**: length(string A)\n **返回值**: int\n **说明**：返回字符串A的长度\n\n```\nhive> select length('abcedfg') from tableName;\n\n```\n\n##### 2、字符串反转函数：reverse\n\n**语法**: reverse(string A)\n **返回值**: string\n **说明**：返回字符串A的反转结果\n\n```\nhive> select reverse('abcedfg') from tableName;\ngfdecba\n```\n\n##### 3、字符串连接函数：concat\n\n**语法**: concat(string A, string B…)\n **返回值**: string\n **说明**：返回输入字符串连接后的结果，支持任意个输入字符串\n\n```\nhive> select concat('abc','def','gh') from tableName;\nabcdefgh\n\n```\n\n##### 4、字符串连接并指定字符串分隔符：concat_ws\n\n**语法**: concat_ws(string SEP, string A, string B…)\n **返回值**: string\n **说明**：返回输入字符串连接后的结果，SEP表示各个字符串间的分隔符\n\n```\nhive> select concat_ws(',','abc','def','gh')from tableName;\nabc,def,gh\n\n```\n\n##### 5、字符串截取函数：substr\n\n**语法**: substr(string A, int start),substring(string A, int start)\n **返回值**: string\n **说明**：返回字符串A从start位置到结尾的字符串\n\n```\nhive> select substr('abcde',3) from tableName;\ncde\nhive> select substring('abcde',3) from tableName;\ncde\nhive>  select substr('abcde',-1) from tableName;  （和ORACLE相同）\ne\n\n```\n\n\n\n##### 6、字符串截取函数：substr,substring \n\n**语法**: substr(string A, int start, int len),substring(string A, int start, int len)\n **返回值**: string\n **说明**：返回字符串A从start位置开始，长度为len的字符串\n\n```\nhive> select substr('abcde',3,2) from tableName;\ncd\nhive> select substring('abcde',3,2) from tableName;\ncd\nhive>select substring('abcde',-2,2) from tableName;\nde\n\n```\n\n\n\n##### 7、字符串转大写函数：upper,ucase  \n\n**语法**: upper(string A) ucase(string A)\n **返回值**: string\n **说明**：返回字符串A的大写格式\n\n```\nhive> select upper('abSEd') from tableName;\nABSED\nhive> select ucase('abSEd') from tableName;\nABSED\n\n```\n\n##### 8、字符串转小写函数：lower,lcase  \n\n**语法**: lower(string A) lcase(string A)\n **返回值**: string\n **说明**：返回字符串A的小写格式\n\n```\nhive> select lower('abSEd') from tableName;\nabsed\nhive> select lcase('abSEd') from tableName;\nabsed\n\n```\n\n\n\n##### 9、去空格函数：trim \n\n**语法**: trim(string A)\n **返回值**: string\n **说明**：去除字符串两边的空格\n\n```\nhive> select trim(' abc ') from tableName;\nabc\n\n```\n\n##### 10、url解析函数  parse_url\n\n**语法**:\nparse_url(string urlString, string partToExtract [, string keyToExtract])\n**返回值**: string\n**说明**：返回URL中指定的部分。partToExtract的有效值为：HOST, PATH,\nQUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO.\n\n```\nhive> select parse_url\n('https://www.tableName.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST') \nfrom tableName;\nwww.tableName.com \nhive> select parse_url\n('https://www.tableName.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k1')\n from tableName;\nv1\n\n```\n\n\n\n##### 11、json解析  get_json_object \n\n**语法**: get_json_object(string json_string, string path)\n **返回值**: string\n **说明**：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。\n\n```\nhive> select  get_json_object('{\"store\":{\"fruit\":\\[{\"weight\":8,\"type\":\"apple\"},{\"weight\":9,\"type\":\"pear\"}], \"bicycle\":{\"price\":19.95,\"color\":\"red\"} },\"email\":\"amy@only_for_json_udf_test.net\",\"owner\":\"amy\"}','$.owner') from tableName;\n\n```\n\n\n\n##### 12、重复字符串函数：repeat \n\n**语法**: repeat(string str, int n)\n **返回值**: string\n **说明**：返回重复n次后的str字符串\n\n```\nhive> select repeat('abc',5) from tableName;\nabcabcabcabcabc\n\n```\n\n##### 13、分割字符串函数: split   \n\n**语法**: split(string str, string pat)\n **返回值**: array\n **说明**: 按照pat字符串分割str，会返回分割后的字符串数组\n\n```\nhive> select split('abtcdtef','t') from tableName;\n[\"ab\",\"cd\",\"ef\"]\n\n```\n\n#### 5、集合统计函数\n\n##### 1、个数统计函数: count  \n\n**语法**: count(*), count(expr), count(DISTINCT expr[, expr_.])\n**返回值**：Int\n\n**说明**: count(*)统计检索出的行的个数，包括NULL值的行；count(expr)返回指定字段的非空值的个数；count(DISTINCT\nexpr[, expr_.])返回指定字段的不同的非空值的个数\n\n```\nhive> select count(*) from tableName;\n20\nhive> select count(distinct t) from tableName;\n10\n\n\n```\n\n##### 2、总和统计函数: sum \n\n**语法**: sum(col), sum(DISTINCT col)\n **返回值**: double\n **说明**: sum(col)统计结果集中col的相加的结果；sum(DISTINCT col)统计结果中col不同值相加的结果\n\n```\nhive> select sum(t) from tableName;\n100\nhive> select sum(distinct t) from tableName;\n70\n\n\n```\n\n##### 3、平均值统计函数: avg   \n\n**语法**: avg(col), avg(DISTINCT col)\n **返回值**: double\n **说明**: avg(col)统计结果集中col的平均值；avg(DISTINCT col)统计结果中col不同值相加的平均值\n\n```\nhive> select avg(t) from tableName;\n50\nhive> select avg (distinct t) from tableName;\n30\n\n\n```\n\n##### 4、最小值统计函数: min \n\n**语法**: min(col)\n **返回值**: double\n **说明**: 统计结果集中col字段的最小值\n\n```\nhive> select min(t) from tableName;\n20\n\n\n```\n\n##### 5、最大值统计函数: max  \n\n**语法**: maxcol)\n **返回值**: double\n **说明**: 统计结果集中col字段的最大值\n\n```\nhive> select max(t) from tableName;\n120\n\n```\n\n#### 6、复合类型构建函数\n\n##### 1、Map类型构建: map  \n\n**语法**: map (key1, value1, key2, value2, …)\n **说明**：根据输入的key和value对构建map类型\n\n```\ncreate table score_map(name string, score map<string,int>)\nrow format delimited fields terminated by '\\t' \ncollection items terminated by ',' map keys terminated by ':';\n\n创建数据内容如下并加载数据\ncd /kkb/install/hivedatas/\nvim score_map.txt\n\nzhangsan\t数学:80,语文:89,英语:95\nlisi\t语文:60,数学:80,英语:99\n\n加载数据到hive表当中去\nload data local inpath '/kkb/install/hivedatas/score_map.txt' overwrite into table score_map;\n\nmap结构数据访问：\n获取所有的value：\nselect name,map_values(score) from score_map;\n\n获取所有的key：\nselect name,map_keys(score) from score_map;\n\n按照key来进行获取value值\nselect name,score[\"数学\"]  from score_map;\n\n查看map元素个数\nselect name,size(score) from score_map;\n\n```\n\n##### 2、Struct类型构建: struct\n\n**语法**: struct(val1, val2, val3, …)\n **说明**：根据输入的参数构建结构体struct类型，似于C语言中的结构体，内部数据通过X.X来获取，假设我们的数据格式是这样的，电影ABC，有1254人评价过，打分为7.4分\n\n```\n创建struct表\nhive> create table movie_score( name string,  info struct<number:int,score:float> )row format delimited fields terminated by \"\\t\"  collection items terminated by \":\"; \n\n加载数据\ncd /kkb/install/hivedatas/\nvim struct.txt\n\nABC\t1254:7.4  \nDEF\t256:4.9  \nXYZ\t456:5.4\n\n加载数据\nload data local inpath '/kkb/install/hivedatas/struct.txt' overwrite into table movie_score;\n\n\nhive当中查询数据\nhive> select * from movie_score;  \nhive> select info.number,info.score from movie_score;  \nOK  \n1254    7.4  \n256     4.9  \n456     5.4  \n\n```\n\n##### 3、array类型构建: array\n\n**语法**: array(val1, val2, …)\n **说明**：根据输入的参数构建数组array类型\n\n```sql\nhive> create table  person(name string,work_locations array<string>)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nCOLLECTION ITEMS TERMINATED BY ',';\n\n加载数据到person表当中去\ncd /kkb/install/hivedatas/\nvim person.txt\n\n数据内容格式如下\nbiansutao\tbeijing,shanghai,tianjin,hangzhou\nlinan\tchangchu,chengdu,wuhan\n\n加载数据\nhive > load  data local inpath '/kkb/install/hivedatas/person.txt' overwrite into table person;\n\n查询所有数据数据\nhive > select * from person;\n\n按照下表索引进行查询\nhive > select work_locations[0] from person;\n\n查询所有集合数据\nhive  > select work_locations from person; \n\n查询元素个数\nhive >  select size(work_locations) from person;   \n```\n\n#### 7、复杂类型长度统计函数\n\n##### 1.Map类型长度函数: size(Map<k .V>)\n\n**语法**: size(Map<k .V>)\n **返回值**: int\n **说明**: 返回map类型的长度\n\n```\nhive> select size(t) from map_table2;\n2\n```\n\n##### 2.array类型长度函数: size(Array<T>)\n\n**语法**: size(Array<T>)\n **返回值**: int\n **说明**: 返回array类型的长度\n\n```\nhive> select size(t) from arr_table2;\n4\n\n\n```\n\n##### 3.类型转换函数  \n\n**类型转换函数**: cast\n **语法**: cast(expr as <type>)\n **返回值**: Expected \"=\" to follow \"type\"\n **说明**: 返回转换后的数据类型\n\n```\nhive> select cast('1' as bigint) from tableName;\n1\n\n```\n\n#### 8、hive当中的lateral view 与 explode以及reflect和分析函数\n\n##### 1、使用explode函数将hive表中的Map和Array字段数据进行拆分\n\nlateral view用于和split、explode等UDTF一起使用的，能将一行数据拆分成多行数据，在此基础上可以对拆分的数据进行聚合，lateral view首先为原始表的每行调用UDTF，UDTF会把一行拆分成一行或者多行，lateral view在把结果组合，产生一个支持别名表的虚拟表。\n其中explode还可以用于将hive一列中复杂的array或者map结构拆分成多行\n\n```\n需求：现在有数据格式如下\nzhangsan\tchild1,child2,child3,child4\tk1:v1,k2:v2\nlisi\tchild5,child6,child7,child8\t k3:v3,k4:v4\n\n字段之间使用\\t分割，需求将所有的child进行拆开成为一列\n \n+----------+--+\n| mychild  |\n+----------+--+\n| child1   |\n| child2   |\n| child3   |\n| child4   |\n| child5   |\n| child6   |\n| child7   |\n| child8   |\n+----------+--+\n\n将map的key和value也进行拆开，成为如下结果\n\n+-----------+-------------+--+\n| mymapkey  | mymapvalue  |\n+-----------+-------------+--+\n| k1        | v1          |\n| k2        | v2          |\n| k3        | v3          |\n| k4        | v4          |\n+-----------+-------------+--+\n```\n\n###### 第一步：创建hive数据库\n\n创建hive数据库\n\n```\nhive (default)> create database hive_explode;\nhive (default)> use hive_explode;\n```\n\n###### 第二步：创建hive表，然后使用explode拆分map和array\n\n```\nhive (hive_explode)> create  table hive_explode.t3(name string,children array<string>,address Map<string,string>) row format delimited fields terminated by '\\t'  collection items    terminated by ','  map keys terminated by ':' stored as textFile;\n```\n\n###### 第三步：加载数据\n\nnode03执行以下命令创建表数据文件\n\n```\ncd  /kkb/install/hivedatas/\n\nvim maparray\n数据内容格式如下\n\nzhangsan\tchild1,child2,child3,child4\tk1:v1,k2:v2\nlisi\tchild5,child6,child7,child8\tk3:v3,k4:v4\n```\n\nhive表当中加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kkb/install/hivedatas/maparray' into table hive_explode.t3;\n\n```\n\n###### 第四步：使用explode将hive当中数据拆开\n\n将array当中的数据拆分开\n\n```\nhive (hive_explode)> SELECT explode(children) AS myChild FROM hive_explode.t3;\n\n```\n\n将map当中的数据拆分开\n\n```\nhive (hive_explode)> SELECT explode(address) AS (myMapKey, myMapValue) FROM hive_explode.t3;\n\n```\n\n##### 2、使用explode拆分json字符串\n\n需求：现在有一些数据格式如下：\n\n```\na:shandong,b:beijing,c:hebei|1,2,3,4,5,6,7,8,9|[{\"source\":\"7fresh\",\"monthSales\":4900,\"userCount\":1900,\"score\":\"9.9\"},{\"source\":\"jd\",\"monthSales\":2090,\"userCount\":78981,\"score\":\"9.8\"},{\"source\":\"jdmart\",\"monthSales\":6987,\"userCount\":1600,\"score\":\"9.0\"}]\n\n```\n\n其中字段与字段之间的分隔符是 | \n\n我们要解析得到所有的monthSales对应的值为以下这一列（行转列）\n\n```\n4900\n2090\n6987\n\n```\n\n###### 第一步：创建hive表\n\n```\nhive (hive_explode)> create table hive_explode.explode_lateral_view  (area string, goods_id string, sale_info string)  ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS textfile;\n```\n\n###### 第二步：准备数据并加载数据\n\n准备数据如下\n\n```\ncd /kkb/install/hivedatas\nvim explode_json\n\na:shandong,b:beijing,c:hebei|1,2,3,4,5,6,7,8,9|[{\"source\":\"7fresh\",\"monthSales\":4900,\"userCount\":1900,\"score\":\"9.9\"},{\"source\":\"jd\",\"monthSales\":2090,\"userCount\":78981,\"score\":\"9.8\"},{\"source\":\"jdmart\",\"monthSales\":6987,\"userCount\":1600,\"score\":\"9.0\"}]\n\n \n\n```\n\n加载数据到hive表当中去\n\n```\nhive (hive_explode)> load data local inpath '/kkb/install/hivedatas/explode_json' overwrite into table hive_explode.explode_lateral_view;\n```\n\n###### 第三步：使用explode拆分Array\n\n```\nhive (hive_explode)> select explode(split(goods_id,',')) as goods_id from hive_explode.explode_lateral_view;\n```\n\n \n\n###### 第四步：使用explode拆解Map\n\n```\nhive (hive_explode)> select explode(split(area,',')) as area from hive_explode.explode_lateral_view;\n```\n\n\n\n###### 第五步：拆解json字段\n\n```\nhive (hive_explode)> select explode(split(regexp_replace(regexp_replace(sale_info,'\\\\[\\\\{',''),'}]',''),'},\\\\{')) as  sale_info from hive_explode.explode_lateral_view;\n```\n\n \n\n然后我们想用get_json_object来获取key为monthSales的数据：\n\n```\nhive (hive_explode)> select get_json_object(explode(split(regexp_replace(regexp_replace(sale_info,'\\\\[\\\\{',''),'}]',''),'},\\\\{')),'$.monthSales') as  sale_info from hive_explode.explode_lateral_view;\n\n\n然后出现异常FAILED: SemanticException [Error 10081]: UDTF's are not supported outside the SELECT clause, nor nested in expressions\nUDTF explode不能写在别的函数内\n如果你这么写，想查两个字段，select explode(split(area,',')) as area,good_id from explode_lateral_view;\n会报错FAILED: SemanticException 1:40 Only a single expression in the SELECT clause is supported with UDTF's. Error encountered near token 'good_id'\n使用UDTF的时候，只支持一个字段，这时候就需要LATERAL VIEW出场了\n```\n\n##### 3、配合LATERAL  VIEW使用\n\n配合lateral view查询多个字段\n\n```\nhive (hive_explode)> select goods_id2,sale_info from explode_lateral_view LATERAL VIEW explode(split(goods_id,','))goods as goods_id2;\n\n```\n\n其中LATERAL VIEW explode(split(goods_id,','))goods相当于一个虚拟表，与原表explode_lateral_view笛卡尔积关联。\n\n也可以多重使用\n\n```\nhive (hive_explode)> select goods_id2,sale_info,area2 from explode_lateral_view  LATERAL VIEW explode(split(goods_id,','))goods as goods_id2 LATERAL VIEW explode(split(area,','))area as area2;\n\n```\n\n也是三个表笛卡尔积的结果\n\n最终，我们可以通过下面的句子，把这个json格式的一行数据，完全转换成二维表的方式展现\n\n```\nhive (hive_explode)> select get_json_object(concat('{',sale_info_1,'}'),'$.source') as source, get_json_object(concat('{',sale_info_1,'}'),'$.monthSales') as monthSales, get_json_object(concat('{',sale_info_1,'}'),'$.userCount') as monthSales,  get_json_object(concat('{',sale_info_1,'}'),'$.score') as monthSales from explode_lateral_view   LATERAL VIEW explode(split(regexp_replace(regexp_replace(sale_info,'\\\\[\\\\{',''),'}]',''),'},\\\\{'))sale_info as sale_info_1;\n\n```\n\n总结：\n\nLateral View通常和UDTF一起出现，为了解决UDTF不允许在select字段的问题。 \n Multiple Lateral View可以实现类似笛卡尔乘积。 \n Outer关键字可以把不输出的UDTF的空结果，输出成NULL，防止丢失数据。\n\n\n\n#### 9、列转行\n\n##### 1．相关函数说明\n\nCONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;\n\nCONCAT_WS(separator, str1, str2,...)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;\n\nCOLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。\n\n##### 2．数据准备\n\n表6-6 数据准备\n\n| name   | constellation | blood_type |\n| ------ | ------------- | ---------- |\n| 孙悟空 | 白羊座        | A          |\n| 老王   | 射手座        | A          |\n| 宋宋   | 白羊座        | B          |\n| 猪八戒 | 白羊座        | A          |\n| 冰冰   | 射手座        | A          |\n\n##### 3．需求\n\n把星座和血型一样的人归类到一起。结果如下：\n\n```\n射手座,A            老王|冰冰\n白羊座,A            孙悟空|猪八戒\n白羊座,B            宋宋\n\n```\n\n\n\n##### 4．创建本地constellation.txt，导入数据\n\nnode03服务器执行以下命令创建文件，注意数据使用\\t进行分割\n\n```\ncd /kkb/install/hivedatas\nvim constellation.txt\n```\n\n```\n孙悟空\t白羊座\tA\n老王\t射手座\tA\n宋宋\t白羊座\tB       \n猪八戒\t白羊座\tA\n凤姐\t射手座\tA\n```\n\n##### 5．创建hive表并导入数据\n\n创建hive表并加载数据\n\n```\nhive (hive_explode)> create table person_info(  name string,  constellation string,  blood_type string)  row format delimited fields terminated by \"\\t\";\n```\n\n加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kkb/install/hivedatas/constellation.txt' into table person_info;\n```\n\n##### 6．按需求查询数据\n\n```\nhive (hive_explode)> select t1.base, concat_ws('|', collect_set(t1.name)) name from    (select name, concat(constellation, \",\" , blood_type) base from person_info) t1 group by  t1.base;\n```\n\n\n\n#### 10、行转列\n\n##### 1．函数说明\n\nEXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。\n\nLATERAL VIEW\n\n用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias\n\n解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。\n\n##### 2．数据准备\n\n数据内容如下，字段之间都是使用\\t进行分割\n\n```\ncd /kkb/install/hivedatas\n\nvim movie.txt\n《疑犯追踪》\t悬疑,动作,科幻,剧情\n《Lie to me》\t悬疑,警匪,动作,心理,剧情\n《战狼2》\t战争,动作,灾难\n```\n\n\n\n##### 3．需求\n\n将电影分类中的数组数据展开。结果如下：\n\n```\n《疑犯追踪》\t悬疑\n《疑犯追踪》\t动作\n《疑犯追踪》\t科幻\n《疑犯追踪》\t剧情\n《Lie to me》\t悬疑\n《Lie to me》\t警匪\n《Lie to me》\t动作\n《Lie to me》\t心理\n《Lie to me》\t剧情\n《战狼2》\t战争\n《战狼2》\t动作\n《战狼2》\t灾难\n```\n\n\n\n##### 4．创建hive表并导入数据\n\n创建hive表\n\n```\nhive (hive_explode)> create table movie_info(movie string, category array<string>) row format delimited fields terminated by \"\\t\" collection items terminated by \",\";\n```\n\n加载数据\n\n```\nload data local inpath \"/kkb/install/hivedatas/movie.txt\" into table movie_info;\n```\n\n \n\n##### 5．按需求查询数据\n\n```\nhive (hive_explode)>  select movie, category_name  from  movie_info lateral view explode(category) table_tmp as category_name;\n```\n\n#### 11、reflect函数\n\nreflect函数可以支持在sql中调用java中的自带函数，秒杀一切udf函数。\n\n##### 使用java.lang.Math当中的Max求两列中最大值\n\n创建hive表\n\n```\nhive (hive_explode)>  create table test_udf(col1 int,col2 int) row format delimited fields terminated by ',';\n```\n\n准备数据并加载数据\n\n```\ncd /kkb/install/hivedatas\n\nvim test_udf\n\n1,2\n4,3\n6,4\n7,5\n5,6\n```\n\n加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kkb/install/hivedatas/test_udf' overwrite into table test_udf;\n```\n\n使用java.lang.Math当中的Max求两列当中的最大值\n\n```\nhive (hive_explode)> select reflect(\"java.lang.Math\",\"max\",col1,col2) from test_udf;\n```\n\n##### 不同记录执行不同的java内置函数\n\n创建hive表\n\n```\nhive (hive_explode)> create table test_udf2(class_name string,method_name string,col1 int , col2 int) row format delimited fields terminated by ',';\n```\n\n准备数据\n\n```\ncd /export/servers/hivedatas\n\nvim test_udf2\n\njava.lang.Math,min,1,2\njava.lang.Math,max,2,3\n\n```\n\n加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kkb/install/hivedatas/test_udf2' overwrite into table test_udf2;\n```\n\n执行查询\n\n```\nhive (hive_explode)> select reflect(class_name,method_name,col1,col2) from test_udf2;\n```\n\n##### 判断是否为数字\n\n使用apache commons中的函数，commons下的jar已经包含在hadoop的classpath中，所以可以直接使用。\n\n使用方式如下：\n\n```\nhive (hive_explode)> select reflect(\"org.apache.commons.lang.math.NumberUtils\",\"isNumber\",\"123\");\n```\n\n\n\n#### 12、hive当中的分析函数—分组求topN\n\n##### 1、分析函数的作用介绍\n\n对于一些比较复杂的数据求取过程，我们可能就要用到分析函数，分析函数主要用于分组求topN，或者求取百分比，或者进行数据的切片等等，我们都可以使用分析函数来解决\n\n\n\n##### 2、常用的分析函数介绍\n\n1、ROW_NUMBER()：\n\n从1开始，按照顺序，生成分组内记录的序列,比如，按照pv降序排列，生成分组内每天的pv名次,ROW_NUMBER()的应用场景非常多，再比如，获取分组内排序第一的记录;获取一个session中的第一条refer等。 \n\n2、RANK() ：\n\n生成数据项在分组中的排名，排名相等会在名次中留下空位 \n\n3、DENSE_RANK() ：\n\n生成数据项在分组中的排名，排名相等会在名次中不会留下空位 \n\n4、CUME_DIST ：\n\n小于等于当前值的行数/分组内总行数。比如，统计小于等于当前薪水的人数，所占总人数的比例 \n\n5、PERCENT_RANK ：\n\n分组内当前行的RANK值/分组内总行数\n\n6、NTILE(n) ：\n\n用于将分组数据按照顺序切分成n片，返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。NTILE不支持ROWS BETWEEN，比如 NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)。\n\n##### 3、需求描述\n\n现有数据内容格式如下，分别对应三个字段，cookieid，createtime ，pv，求取每个cookie访问pv前三名的数据记录，其实就是分组求topN，求取每组当中的前三个值\n\n```\ncookie1,2015-04-10,1\ncookie1,2015-04-11,5\ncookie1,2015-04-12,7\ncookie1,2015-04-13,3\ncookie1,2015-04-14,2\ncookie1,2015-04-15,4\ncookie1,2015-04-16,4\ncookie2,2015-04-10,2\ncookie2,2015-04-11,3\ncookie2,2015-04-12,5\ncookie2,2015-04-13,6\ncookie2,2015-04-14,3\ncookie2,2015-04-15,9\ncookie2,2015-04-16,7\n```\n\n###### 第一步：创建数据库表\n\n在hive当中创建数据库表\n\n```\nCREATE EXTERNAL TABLE cookie_pv (\ncookieid string,\ncreatetime string, \npv INT\n) ROW FORMAT DELIMITED \nFIELDS TERMINATED BY ',' ;\n```\n\n###### 第二步：准备数据并加载\n\nnode03执行以下命令，创建数据，并加载到hive表当中去\n\n```\ncd /kkb/install/hivedatas\nvim cookiepv.txt\n\ncookie1,2015-04-10,1\ncookie1,2015-04-11,5\ncookie1,2015-04-12,7\ncookie1,2015-04-13,3\ncookie1,2015-04-14,2\ncookie1,2015-04-15,4\ncookie1,2015-04-16,4\ncookie2,2015-04-10,2\ncookie2,2015-04-11,3\ncookie2,2015-04-12,5\ncookie2,2015-04-13,6\ncookie2,2015-04-14,3\ncookie2,2015-04-15,9\ncookie2,2015-04-16,7\n```\n\n加载数据到hive表当中去\n\n```\nload  data  local inpath '/kkb/install/hivedatas/cookiepv.txt'  overwrite into table  cookie_pv \n```\n\n###### 第三步：使用分析函数来求取每个cookie访问PV的前三条记录\n\n```sql\nSELECT * FROM (\n\tSELECT \n  cookieid,\n  createtime,\n  pv,\n  RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn1,\n  DENSE_RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn2,\n  ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY pv DESC) AS rn3 \n  FROM cookie_pv) temp \nWHERE temp.rn1 <=  3 ;\n```\n\n\n\n\n\n#### 13、hive自定义函数\n\n##### 1、自定义函数的基本介绍\n\n1）Hive 自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。\n\n2）当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。\n\n3）根据用户自定义函数类别分为以下三种：\n\n        （1）UDF（User-Defined-Function）\n    \n                一进一出\n    \n        （2）UDAF（User-Defined Aggregation Function）\n    \n                聚集函数，多进一出\n    \n                类似于：count/max/min\n    \n        （3）UDTF（User-Defined Table-Generating Functions）\n    \n                一进多出\n    \n                如lateral view explode()\n\n4）官方文档地址\n\nhttps://cwiki.apache.org/confluence/display/Hive/HivePlugins\n\n5）编程步骤：\n\n        （1）继承org.apache.hadoop.hive.ql.UDF\n    \n        （2）需要实现evaluate函数；evaluate函数支持重载；\n\n6）注意事项\n\n        （1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void；\n    \n        （2）UDF中常用Text/LongWritable等类型，不推荐使用java类型；\n\n \n\n\n\n\n#####  2、自定义函数开发\n\n###### 第一步：创建maven java 工程，并导入jar包\n\n```\n<repositories>\n    <repository>\n        <id>cloudera</id>\n <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n    </repository>\n</repositories>\n<dependencies>\n    <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-common</artifactId>\n        <version>2.6.0-cdh5.14.2</version>\n    </dependency>\n    <dependency>\n        <groupId>org.apache.hive</groupId>\n        <artifactId>hive-exec</artifactId>\n        <version>1.1.0-cdh5.14.2</version>\n    </dependency>\n</dependencies>\n<build>\n<plugins>\n    <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>3.0</version>\n        <configuration>\n            <source>1.8</source>\n            <target>1.8</target>\n            <encoding>UTF-8</encoding>\n        </configuration>\n    </plugin>\n     <plugin>\n         <groupId>org.apache.maven.plugins</groupId>\n         <artifactId>maven-shade-plugin</artifactId>\n         <version>2.2</version>\n         <executions>\n             <execution>\n                 <phase>package</phase>\n                 <goals>\n                     <goal>shade</goal>\n                 </goals>\n                 <configuration>\n                     <filters>\n                         <filter>\n                             <artifact>*:*</artifact>\n                             <excludes>\n                                 <exclude>META-INF/*.SF</exclude>\n                                 <exclude>META-INF/*.DSA</exclude>\n                                 <exclude>META-INF/*/RSA</exclude>\n                             </excludes>\n                         </filter>\n                     </filters>\n                 </configuration>\n             </execution>\n         </executions>\n     </plugin>\n</plugins>\n</build>\n```\n\n \n\n \n\n###### 第二步：开发java类继承UDF，并重载evaluate 方法\n\n```\npublic class MyUDF extends UDF {\n     public Text evaluate(final Text s) {\n         if (null == s) {\n             return null;\n         }\n         //**返回大写字母         return new Text(s.toString().toUpperCase());\n     }\n }\n```\n\n###### 第三步：将我们的项目打包，并上传到hive的lib目录下\n\n使用maven的package进行打包，将我们打包好的jar包上传到node03服务器的/kkb/install/hive-1.1.0-cdh5.14.2/lib 这个路径下\n\n###### 第四步：添加我们的jar包\n\n重命名我们的jar包名称\n\n```\ncd /kkb/install/hive-1.1.0-cdh5.14.2/lib\nmv original-day_hive_udf-1.0-SNAPSHOT.jar udf.jar\n```\n\nhive的客户端添加我们的jar包\n\n```\n0: jdbc:hive2://node03:10000> add jar /kkb/install/hive-1.1.0-cdh5.14.2/lib/udf.jar;\n```\n\n \n\n###### 第五步：设置函数与我们的自定义函数关联\n\n```\n0: jdbc:hive2://node03:10000> create temporary function tolowercase as 'com.kkb.udf.MyUDF';\n```\n\n###### 第六步：使用自定义函数\n\n```\n0: jdbc:hive2://node03:10000>select tolowercase('abc');\n```\n\n \n\nhive当中如何创建永久函数\n\n在hive当中添加临时函数，需要我们每次进入hive客户端的时候都需要添加以下，退出hive客户端临时函数就会失效，那么我们也可以创建永久函数来让其不会失效\n\n创建永久函数\n\n```\n1、指定数据库，将我们的函数创建到指定的数据库下面\n0: jdbc:hive2://node03:10000>use myhive;\n\n2、使用add jar添加我们的jar包到hive当中来\n0: jdbc:hive2://node03:10000>add jar /kkb/install/hive-1.1.0-cdh5.14.2/lib/udf.jar;\n\n3、查看我们添加的所有的jar包\n0: jdbc:hive2://node03:10000>list  jars;\n\n4、创建永久函数，与我们的函数进行关联\n0: jdbc:hive2://node03:10000>create  function myuppercase as 'com.kkb.udf.MyUDF';\n\n5、查看我们的永久函数\n0: jdbc:hive2://node03:10000>show functions like 'my*';\n\n6、使用永久函数\n0: jdbc:hive2://node03:10000>select myhive.myuppercase('helloworld');\n\n7、删除永久函数\n0: jdbc:hive2://node03:10000>drop function myhive.myuppercase;\n\n8、查看函数\n show functions like 'my*';\n```\n\n\n\n### 3. hive表的数据压缩 \n\n#### 1、数据的压缩说明\n\n- 压缩模式评价\n  - 可使用以下三种标准对压缩方式进行评价\n    - 1、压缩比：压缩比越高，压缩后文件越小，所以压缩比越高越好\n    - 2、压缩时间：越快越好\n    - 3、已经压缩的格式文件是否可以再分割：可以分割的格式允许单一文件由多个Mapper程序处理，可以更好的并行化\n\n- 常见压缩格式\n\n| 压缩方式 | 压缩比 | 压缩速度 | 解压缩速度 | 是否可分割 |\n| :------: | :----: | :------: | :--------: | :--------: |\n|   gzip   | 13.4%  | 21 MB/s  |  118 MB/s  |     否     |\n|  bzip2   | 13.2%  | 2.4MB/s  |  9.5MB/s   |     是     |\n|   lzo    | 20.5%  | 135 MB/s |  410 MB/s  |     是     |\n|  snappy  | 22.2%  | 172 MB/s |  409 MB/s  |     否     |\n\n- Hadoop编码/解码器方式\n\n| 压缩格式 |             对应的编码/解码器              |\n| :------: | :----------------------------------------: |\n| DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |\n|   Gzip   |  org.apache.hadoop.io.compress.GzipCodec   |\n|  BZip2   |  org.apache.hadoop.io.compress.BZip2Codec  |\n|   LZO    |     com.hadoop.compress.lzo.LzopCodec      |\n|  Snappy  | org.apache.hadoop.io.compress.SnappyCodec  |\n\n \t压缩性能的比较\n\n| 压缩算法 | 原始文件大小 | 压缩文件大小 | 压缩速度 | 解压速度 |\n| -------- | ------------ | ------------ | -------- | -------- |\n| gzip     | 8.3GB        | 1.8GB        | 17.5MB/s | 58MB/s   |\n| bzip2    | 8.3GB        | 1.1GB        | 2.4MB/s  | 9.5MB/s  |\n| LZO      | 8.3GB        | 2.9GB        | 49.3MB/s | 74.6MB/s |\n\nhttp://google.github.io/snappy/\n\nOn a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.\n\n#### 2、压缩配置参数\n\n要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：\n\n| 参数                                                 | 默认值                                                       | 阶段        | 建议                                         |\n| ---------------------------------------------------- | ------------------------------------------------------------ | ----------- | -------------------------------------------- |\n| io.compression.codecs      （在core-site.xml中配置） | org.apache.hadoop.io.compress.DefaultCodec,   org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,   org.apache.hadoop.io.compress.Lz4Codec | 输入压缩    | Hadoop使用文件扩展名判断是否支持某种编解码器 |\n| mapreduce.map.output.compress                        | false                                                        | mapper输出  | 这个参数设为true启用压缩                     |\n| mapreduce.map.output.compress.codec                  | org.apache.hadoop.io.compress.DefaultCodec                   | mapper输出  | 使用LZO、LZ4或snappy编解码器在此阶段压缩数据 |\n| mapreduce.output.fileoutputformat.compress           | false                                                        | reducer输出 | 这个参数设为true启用压缩                     |\n| mapreduce.output.fileoutputformat.compress.codec     | org.apache.hadoop.io.compress. DefaultCodec                  | reducer输出 | 使用标准工具或者编解码器，如gzip和bzip2      |\n| mapreduce.output.fileoutputformat.compress.type      | RECORD                                                       | reducer输出 | SequenceFile输出使用的压缩类型：NONE和BLOCK  |\n\n#### 3、开启Map输出阶段压缩\n\n开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：\n\n**案例实操：**\n\n```\n1）开启hive中间传输数据压缩功能\nhive (default)>set hive.exec.compress.intermediate=true;\n\n2）开启mapreduce中map输出压缩功能\nhive (default)>set mapreduce.map.output.compress=true;\n\n3）设置mapreduce中map输出数据的压缩方式\nhive (default)>set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec;\n\n4）执行查询语句\n   select count(1) from score;\n```\n\n\n\n#### 4、 开启Reduce输出阶段压缩\n\n当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。\n\n**案例实操：**\n\n```\n1）开启hive最终输出数据压缩功能\nhive (default)>set hive.exec.compress.output=true;\n\n2）开启mapreduce最终输出数据压缩\nhive (default)>set mapreduce.output.fileoutputformat.compress=true;\n\n3）设置mapreduce最终数据输出压缩方式\nhive (default)> set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;\n\n4）设置mapreduce最终数据输出压缩为块压缩\nhive (default)>set mapreduce.output.fileoutputformat.compress.type=BLOCK;\n\n5）测试一下输出结果是否是压缩文件\ninsert overwrite local directory '/kkb/install/hivedatas/snappy' select * from score distribute by s_id sort by s_id desc;\n```\n\n","tags":["hadoop","hive","function"]},{"title":"Hadoop之数据分析Hive（一）","url":"/2019/11/01/hadoop/Hadoop之数据分析Hive（一）/","content":"\n### 1.数据仓库的基本概念\n\n#### 1.数据仓库的基本概念\n\n英文名称为Data Warehouse，可简写为DW或DWH。数据仓库的目的是构建面向分析的集成化数据环境，为企业提供决策支持（Decision Support）。它出于分析性报告和决策支持目的而创建。\n\n数据仓库本身并不“生产”任何数据，同时自身也不需要“消费”任何的数据，数据来源于外部，并且开放给外部应用，这也是为什么叫“仓库”，而不叫“工厂”的原因。\n\n#### 2.数据仓库的主要特征\n\n数据仓库是面向主题的（Subject-Oriented）、集成的（Integrated）、非易失的（Non-Volatile）和时变的（Time-Variant ）数据集合，用以支持管理决策。\n\n#### 3. 数据仓库与数据库区别 \n\n数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别。 \n\n操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing，），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。 \n\n分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持管理决策。\n\n首先要明白，数据仓库的出现，并不是要取代数据库。\n\n数据库是面向事务的设计，数据仓库是面向主题设计的。\n\n数据库一般存储业务数据，数据仓库存储的一般是历史数据。\n\n数据库设计是尽量避免冗余，一般针对某一业务应用进行设计，比如一张简单的User表，记录用户名、密码等简单数据即可，符合业务应用，但是不符合分析。数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计。\n\n数据库是为捕获数据而设计，数据仓库是为分析数据而设计。\n\n以银行业务为例。数据库是事务系统的数据平台，客户在银行做的每笔交易都会写入数据库，被记录下来，这里，可以简单地理解为用数据库记账。数据仓库是分析系统的数据平台，它从事务系统获取数据，并做汇总、加工，为决策者提供决策的依据。比如，某银行某分行一个月发生多少交易，该分行当前存款余额是多少。如果存款又多，消费交易又多，那么该地区就有必要设立ATM了。 \n\n显然，银行的交易量是巨大的，通常以百万甚至千万次来计算。事务系统是实时的，这就要求时效性，客户存一笔钱需要几十秒是无法忍受的，这就要求数据库只能存储很短一段时间的数据。而分析系统是事后的，它要提供关注时间段内所有的有效数据。这些数据是海量的，汇总计算起来也要慢一些，但是，只要能够提供有效的分析数据就达到目的了。 \n\n数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它决不是所谓的“大型数据库”。\n\n#### 4.数据仓库分层架构\n\n按照数据流入流出的过程，数据仓库架构可分为三层——**源数据**、**数据仓库**、**数据应用。**                            \n\n数据仓库的数据来源于不同的源数据，并提供多样的数据应用，数据自下而上流入数据仓库后向上层开放应用，而数据仓库只是中间集成化数据管理的一个平台。\n\n源数据层（ODS）：此层数据无任何更改，直接沿用外围系统数据结构和数据，不对外开放；为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。\n\n数据仓库层（DW）：也称为细节层，DW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。\n\n数据应用层（DA或APP）：前端应用直接读取的数据源；根据报表、专题分析需求而计算生成的数据。\n\n数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。\n\n为什么要对数据仓库分层？\n\n用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。\n\n通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。\n\n### 2. Hive是什么\n\n#### 1 hive的概念\n\nHive是基于Hadoop的一个数据仓库工具，==可以将结构化的数据文件映射为一张数据库表==，并提供类SQL查询功能。其本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据的存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更进一步可以说hive就是一个MapReduce的客户端\n\n![](/Users/dingchuangshi/Downloads/hive_day01_课前资料/hive_day01课程设计.assets/Snipaste_2019-07-10_23-23-31.png)\n\n\n\n#### 2 Hive与数据库的区别\n\n![](/Users/dingchuangshi/Downloads/hive_day01_课前资料/hive_day01课程设计.assets/2018040319335283.png)\n\n* Hive 具有 SQL 数据库的外表，但应用场景完全不同。\n* ==Hive 只适合用来做海量离线数据统计分析，也就是数据仓库==。\n\n\n\n#### 3 Hive的优缺点\n\n\n\n* ==优点==\n  * **操作接口采用类SQL语法**，提供快速开发的能力（简单、容易上手）。\n\n  * **避免了去写MapReduce**，减少开发人员的学习成本。\n\n  * **Hive支持用户自定义函数**，用户可以根据自己的需求来实现自己的函数。\n\n* ==缺点==\n  * **Hive 不支持记录级别的增删改操作**\n  * **Hive 的查询延迟很严重**\n    * hadoop jar  xxxx.jar  xxx.class /input /output\n      * 进行任务的划分，然后进行计算资源的申请\n      * map 0%  reduce 0%\n      * map 10%  reduce 0%\n  * **Hive 不支持事务**\n\n\n\n#### 4 Hive架构原理\n\n\n\n![](assets/2019-07-11_11-08-35.png)\n\n* 1、用户接口：Client\n\n- CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）\n\n* 2、元数据：Metastore\n  * 元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；\n\n    * 默认存储在自带的derby数据库中，==推荐使用MySQL存储Metastore==\n\n* 3、Hadoop集群\n  * 使用HDFS进行存储，使用MapReduce进行计算。\n\n* 4、Driver：驱动器\n  * 解析器（SQL Parser） \n    * 将SQL字符串转换成抽象语法树AST\n    * 对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误\n  * 编译器（Physical Plan）：将AST编译生成逻辑执行计划\n  * 优化器（Query Optimizer）：对逻辑执行计划进行优化\n  * 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说默认就是mapreduce任务\n\n\n\n![hive1](assets/hive1.png)\n\n\n\n\n\n### 3. Hive的安装部署\n\n​\t[hive安装部署](https://kfly.top/2019/11/26/hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\t\n\n### 4. hive的交互方式 \n\n* ==先启动hadoop集群和mysql服务==\n\n#### 1 Hive交互shell\n\n~~~shell\ncd /kkb/install/hive-1.1.0-cdh5.14.2\nbin/hive\n~~~\n\n#### 2 Hive JDBC服务\n\n* 启动hiveserver2服务\n\n  * 前台启动\n\n    ~~~shell\n    cd /kkb/install/hive-1.1.0-cdh5.14.2\n    bin/hive --service hiveserver2\n    ~~~\n\n  * 后台启动\n\n  ~~~shell\n  cd /kkb/install/hive-1.1.0-cdh5.14.2\n  nohup  bin/hive --service hiveserver2  &\n  ~~~\n\n* beeline连接hiveserver2\n\n  重新开启一个会话窗口，然后使用beeline连接hive\n\n  ~~~shell\n  cd /kkb/install/hive-1.1.0-cdh5.14.2\n  bin/beeline\n  beeline> !connect jdbc:hive2://node03:10000\n  ~~~\n\n\n#### 3  Hive的命令\n\n* hive  ==-e== sql语句\n  * 使用 –e  参数来直接执行hql的语句\n\n~~~\ncd /kkb/install/hive-1.1.0-cdh5.14.2/\nbin/hive -e \"show databases\"\n~~~\n\n* hive  ==-f==  sql文件\n\n  * 使用 –f  参数执行包含hql语句的文件\n\n  * node03执行以下命令准备hive执行脚本\n\n  * ```\n    cd /kkb/install/\n    vim hive.sql\n    \n    文件内容如下\n    create database if not exists myhive;\n    \n    通过以下命令来执行我们的hive脚本\n    cd /kkb/install/hive-1.1.0-cdh5.14.2/\n    bin/hive -f /kkb/install/hive.sql \n    ```\n\n### 5、Hive的数据类型\n\n#### 1 基本数据类型\n\n\n\n|    类型名称    |              描述               |    举例    |\n| :------------: | :-----------------------------: | :--------: |\n|    boolean     |           true/false            |    true    |\n|    tinyint     |        1字节的有符号整数        |     1      |\n|    smallint    |        2字节的有符号整数        |     1      |\n|  ==**int**==   |        4字节的有符号整数        |     1      |\n| **==bigint==** |        8字节的有符号整数        |     1      |\n|     float      |        4字节单精度浮点数        |    1.0     |\n| **==double==** |        8字节单精度浮点数        |    1.0     |\n| **==string==** |        字符串(不设长度)         |   “abc”    |\n|    varchar     | 字符串（1-65355长度，超长截断） |   “abc”    |\n|   timestamp    |             时间戳              | 1563157873 |\n|      date      |              日期               |  20190715  |\n\n\n\n#### 2 复合数据类型\n\n| 类型名称 |                         描述                          |       举例        |\n| :------: | :---------------------------------------------------: | :---------------: |\n|  array   | 一组有序的字段，字段类型必须相同 array(元素1，元素2)  |  Array（1,2,3）   |\n|   map    |           一组无序的键值对 map(k1,v1,k2,v2)           | Map(‘a’,1,'b',2)  |\n|  struct  | 一组命名的字段，字段类型可以不同 struct(元素1，元素2) | Struct('a',1,2,0) |\n\n* array字段的元素访问方式：\n\n  * 下标获取元素，下标从0开始\n\n    * 获取第一个元素\n\n      * array[0]\n\n* map字段的元素访问方式\n\n  * 通过键获取值\n\n    * 获取a这个key对应的value\n\n      * map['a']\n\n\n* struct字段的元素获取方式\n  * 定义一个字段c的类型为struct{a int;b string}\n    * 获取a和b的值\n      * 使用c.a 和c.b 获取其中的元素值\n        * 这里可以把这种类型看成是一个对象\n\n~~~sql\ncreate table complex(\n         col1 array<int>,\n         col2 map<string,int>,\n         col3 struct<a:string,b:int,c:double>\n)\n\n~~~\n\n\n\n### 6、Hive的DDL操作\n\n#### 1 hive的数据库DDL操作\n\n##### 1、创建数据库\n\n```sql\nhive > create database db_hive;\n# 或者\nhive > create database if not exists db_hive;\n```\n\n- 数据库在HDFS上的默认存储路径是==/user/hive/warehouse/*.db==\n\n\n##### 2、显示所有数据库\n\n```sql\n  hive> show databases;\n```\n\n##### 3、查询数据库\t\n\n```sql\nhive> show databases like 'db_hive*';\n```\n\n##### 4、查看数据库详情\n\n```sql\nhive> desc database db_hive;\n```\n\n##### 5、显示数据库详细信息\n\n```sql\nhive> desc database extended db_hive;\n```\n\n##### 6、切换当前数据库\n\n```sql\nhive > use db_hive;\n```\n\n##### 7、删除数据库\n\n```sql\n#删除为空的数据库\nhive> drop database db_hive;\n\n#如果删除的数据库不存在，最好采用if exists 判断数据库是否存在\nhive> drop database if exists db_hive;\n\n#如果数据库中有表存在，这里需要使用cascade强制删除数据库\nhive> drop database if exists db_hive cascade;\n```\n\n#### 6.2 hive的表DDL操作\n\n##### 1 、建表语法介绍\n\n```sql\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name \n[(col_name data_type [COMMENT col_comment], ...)] \n[COMMENT table_comment] \n[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] 分区\n[CLUSTERED BY (col_name, col_name, ...) 分桶\n[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] \n[ROW FORMAT row_format]  row format delimited fields terminated by “分隔符”\n[STORED AS file_format] \n[LOCATION hdfs_path]\n```\n\n官网地址：<https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL>\n\n##### 2 、字段解释说明\n\n- create table \n\n  - 创建一个指定名字的表\n- EXTERNAL  \n\n  - 创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），指定表的数据保存在哪里\n- COMMENT\n\n  - 为表和列添加注释\n- PARTITIONED BY\n\n  - 创建分区表\n- CLUSTERED BY\n\n  - 创建分桶表\n- SORTED BY\n\n  - 按照字段排序（一般不常用）\n- ROW FORMAT\n  - 指定每一行中字段的分隔符\n\n    - row format delimited fields terminated by ‘\\t’\n\n* STORED AS\n  * 指定存储文件类型\n    - 常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）\n    - 如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE\n\n- LOCATION \n\n  - 指定表在HDFS上的存储位置。\n\n##### 3、 创建内部表\n\n- 1、==直接建表==\n  - 使用标准的建表语句\n\n```sql\nuse myhive;\ncreate table stu(id int,name string);\n\n可以通过insert  into  向hive表当中插入数据，但是不建议工作当中这么做\ninsert  into stu(id,name) values(1,\"zhangsan\");\nselect * from  stu;\n```\n\n- 2、==查询建表法==\n  - 通过AS 查询语句完成建表：将子查询的结果存在新表里，有数据 \n\n```sql\ncreate table if not exists myhive.stu1 as select id, name from stu;\n```\n\n- 3、==like建表法==\n  - 根据已经存在的表结构创建表\n\n```sql\ncreate table if not exists myhive.stu2 like stu;\n```\n\n- 4、查询表的类型\n\n```sql\nhive > desc formatted myhive.stu;\n```\n\n![2019-07-12_14-33-00](/Users/dingchuangshi/Downloads/hive_day01_课前资料/hive_day01课程设计.assets/2019-07-12_14-33-00-1563156555810.png)\n\n\n\n创建内部表并指定字段之间的分隔符，指定文件的存储格式，以及数据存放的位置\n\n```\ncreate  table if not exists myhive.stu3(id int ,name string)\nrow format delimited fields terminated by '\\t' stored as textfile location       '/user/stu2';\n```\n\n##### 4、 创建外部表\n\n外部表因为是指定其他的hdfs路径的数据加载到表当中来，所以hive表会认为自己不完全独占这份数据，所以删除hive表的时候，数据仍然存放在hdfs当中，不会删掉\n\n```\ncreate external table myhive.teacher (t_id string,t_name string) row format delimited fields terminated by '\\t';\n```\n\n- 创建外部表的时候需要加上**==external==** 关键字\n- location字段可以指定，也可以不指定\n  - 指定就是数据存放的具体目录\n  - 不指定就是使用默认目录 ==/user/hive/warehouse==\n\n![2019-07-12_14-51-53](assets/2019-07-12_14-51-53.png)\n\n\n\n向外部表当中加载数据：\n\n我们前面已经看到过通过insert的方式向内部表当中插入数据，外部表也可以通过insert的方式进行插入数据，只不过insert的方式，我们一般都不推荐，实际工作当中我们都是使用load的方式来加载数据到内部表或者外部表\n\nload数据可以从本地文件系统加载或者也可以从hdfs上面的数据进行加载\n\n- 从本地文件系统加载数据到teacher表当中去，==将我们附件当汇总的数据资料都上传到node03服务器的/kkb/install/hivedatas路径下面去==\n\n  ```\n  mkdir -p /kkb/install/hivedatas\n  #将数据都上传到/kkb/install/hivedatas路径下，然后在hive客户端下执行以下操作\n  load data local inpath '/kkb/install/hivedatas/teacher.csv' into table myhive.teacher;\n  ```\n\n- 从hdfs上面加载文件到teacher表里面去(将teacher.csv文件上传到==hdfs的/kkb/hdfsload/hivedatas==路径下)\n\n  ```shell\n  cd /kkb/install/hivedatas\n  hdfs dfs -mkdir -p /kkb/hdfsload/hivedatas\n  hdfs dfs -put teacher.csv /kkb/hdfsload/hivedatas\n  # 在hive的客户端当中执行\n  load  data  inpath  '/kkb/hdfsload/hivedatas'  overwrite into table myhive.teacher;\n  ```\n\n##### 5、 内部表与外部表的互相转换\n\n- 1、内部表转换为外部表\n\n```sql\n#将stu内部表改为外部表\nalter table stu set tblproperties('EXTERNAL'='TRUE');\n```\n\n- 2、外部表转换为内部表\n\n```sql\n#把emp外部表改为内部表\nalter table teacher set tblproperties('EXTERNAL'='FALSE');\n```\n\n##### 6、 内部表与外部表的区别\n\n- 1、建表语法的区别\n  - 外部表在创建的时候需要加上==external==关键字\n\n- 2、删除表之后的区别\n  - 内部表删除后，表的元数据和真实数据都被删除了\n- 外部表删除后，仅仅只是把该表的元数据删除了，真实数据还在，后期还是可以恢复出来\n\n##### 7、内部表与外部表的使用时机\n\n​\t内部表由于删除表的时候会同步删除HDFS的数据文件，所以确定如果一个表仅仅是你独占使用，其他人不适用的时候就可以创建内部表，如果一个表的文件数据，其他人也要使用，那么就创建外部表\n\n一般外部表都是用在数据仓库的ODS层\n\n内部表都是用在数据仓库的DW层\n\n##### 8、hive的分区表\n\n如果hive当中所有的数据都存入到一个文件夹下面，那么在使用MR计算程序的时候，读取一整个目录下面的所有文件来进行计算，就会变得特别慢，因为数据量太大了，实际工作当中一般都是计算前一天的数据，所以我们只需要将前一天的数据挑出来放到一个文件夹下面即可，专门去计算前一天的数据。这样就可以使用hive当中的分区表，通过分文件夹的形式，将每一天的数据都分成为一个文件夹，然后我们计算数据的时候，通过指定前一天的文件夹即可只计算前一天的数据。\n\n在大数据中，最常用的一种思想就是分治，我们可以把大的文件切割划分成一个个的小的文件，这样每次操作一个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每天，或者每小时进行切分成一个个的小的文件，这样去操作小的文件就会容易得多了\n\n\n\n![2019-07-15_11-35-37](assets/2019-07-15_11-35-37.png)\n\n\n\n```\n在文件系统上建立文件夹，把表的数据放在不同文件夹下面，加快查询速度。\n```\n\n创建分区表语法\n\n```\nhive (myhive)> create table score(s_id string,c_id string, s_score int) partitioned by (month string) row format delimited fields terminated by '\\t';\n```\n\n创建一个表带多个分区\n\n```\nhive (myhive)> create table score2 (s_id string,c_id string, s_score int) partitioned by (year string,month string,day string) row format delimited fields terminated by '\\t';\n```\n\n 加载数据到分区表当中去\n\n```\n hive (myhive)>load data  local inpath '/kkb/install/hivedatas/score.csv' into table score partition  (month='201806');\n```\n\n加载数据到多分区表当中去\n\n```\nhive (myhive)> load data local inpath '/kkb/install/hivedatas/score.csv' into table score2 partition(year='2018',month='06',day='01');\n```\n\n查看分区\n\n```\nhive (myhive)> show  partitions  score;\n```\n\n添加一个分区\n\n```\nhive (myhive)> alter table score add partition(month='201805');\n```\n\n同时添加多个分区\n\n```\nhive (myhive)> alter table score add partition(month='201804') partition(month = '201803');\n```\n\n注意：添加分区之后就可以在hdfs文件系统当中看到表下面多了一个文件夹\n\n删除分区\n\n```\nhive (myhive)> alter table score drop partition(month = '201806');\n```\n\n\n\n外部分区表综合练习：\n\n需求描述：现在有一个文件score.csv文件，里面有三个字段，分别是s_id string, c_id string,s_score int，字段都是使用 \\t进行分割，存放在集群的这个目录下/scoredatas/day=20180607，这个文件每天都会生成，存放到对应的日期文件夹下面去，文件别人也需要公用，不能移动。需求，创建hive对应的表，并将数据加载到表中，进行数据统计分析，且删除表之后，数据不能删除\n\n需求实现:\n\n数据准备:\n\nnode03执行以下命令，将数据上传到hdfs上面去\n\n将我们的score.csv上传到node03服务器的/kkb/install/hivedatas目录下，然后将score.csv文件上传到/kkb/install/hivedatas目录下去\n\n```\ncd /kkb/install/hivedatas/\nhdfs dfs -mkdir -p /scoredatas/day=20180607\nhdfs dfs -put score.csv /scoredatas/day=20180607/\n```\n\n创建外部分区表，并指定文件数据存放目录\n\n```\nhive (myhive)> create external table score4(s_id string, c_id string,s_score int) partitioned by (day string) row format delimited fields terminated by '\\t' location '/scoredatas';\n```\n\n进行表的修复,说白了就是建立我们表与我们数据文件之间的一个关系映射\n\n```\nhive (myhive)> msck  repair   table  score4;\n```\n\n修复成功之后即可看到数据已经全部加载到表当中去了\n\n##### 9、hive的分桶表\n\n![](assets/2019-07-16_17-01-51.png)\n\n- 分桶是相对分区进行更细粒度的划分。\n\n- ==分桶将整个数据内容安装某列属性值取hash值进行区分，具有相同hash值的数据进入到同一个文件中==\n\n  - 比如按照name属性分为3个桶，就是对name属性值的hash值对3取摸，按照取模结果对数据分桶。\n    - 取模结果为==0==的数据记录存放到一个文件\n    - 取模结果为==1==的数据记录存放到一个文件\n    - 取模结果为==2==的数据记录存放到一个文件\n    - 取模结果为==3==的数据记录存放到一个文件\n\n- **==作用==**\n\n  - 1、取样sampling更高效。没有分区的话需要扫描整个数据集。\n  - 2、提升某些查询操作效率，例如map side join\n\n- 案例演示\n\n  - 1、创建分桶表\n\n    - 在创建分桶表之前要执行的命令\n      - ==set hive.enforce.bucketing=true;==  开启对分桶表的支持\n      - ==set mapreduce.job.reduces=4;==      设置与桶相同的reduce个数（默认只有一个reduce）\n\n    ```sql\n    # 进入hive客户端然后执行以下命令\n    use myhive;\n    set mapreduce.job.reduces=4;  \n    set hive.enforce.bucketing=true; \n    --分桶表\n    create table myhive.user_buckets_demo(id int, name string)\n    clustered by(id) \n    into 4 buckets \n    row format delimited fields terminated by '\\t';\n    \n    --普通表\n    create table user_demo(id int, name string)\n    row format delimited fields terminated by '\\t';\n    \n    ```\n\n  - 2、准备数据文件 buckets.txt\n\n    ```\n    #在linux当中执行以下命令\n    cd /kkb/install/hivedatas/\n    vim user_bucket.txt\n    \n    1\tlaowang1\n    2\tlaowang2\n    3\tlaowang3\n    4\tlaowang4\n    5\tlaowang5\n    6\tlaowang6\n    7\tlaowang7\n    8\tlaowang8\n    9\tlaowang9\n    10\tlaowang10\n    ```\n\n  ```\n  - 3、加载数据到普通表 user_demo 中\n  \n  load data local inpath '/kkb/install/hivedatas/user_bucket.txt'  overwrite into table user_demo; \n  \n  \n  ```\n\n```sql\n  #在hive客户端当中加载数据\nload data local inpath '/kkb/install/hivedatas/user_bucket.txt' into table user_demo;\n```\n\n  - 4、加载数据到桶表user_buckets_demo中\n\n  ```sql\n  insert into table user_buckets_demo select * from user_demo;\n  ```\n\n- 5、hdfs上查看表的数据目录\n\n  ![](assets/2019-07-16_16-30-09.png)\n\n\n\n- 6、抽样查询桶表的数据\n  - tablesample抽样语句，语法：tablesample(bucket  x  out  of  y)\n    - x表示从第几个桶开始取数据\n    - y表示桶数的倍数，一共需要从 ==桶数/y==  个桶中取数据\n\n```sql\nselect * from user_buckets_demo tablesample(bucket 1 out of 2)\n\n-- 需要的总桶数=4/2=2个\n-- 先从第1个桶中取出数据\n-- 再从第1+2=3个桶中取出数据\n```\n\n### 7、Hive修改表结构 \n\n修改表名称语法\n\n```\nalter table  old_table_name  rename to  new_table_name;\n```\n\n#### 7.1 修改表的名称\n\n~~~sql\nhive> alter table stu3 rename to stu4;\n~~~\n\n#### 7.2 表的结构信息\n\n~~~sql\nhive> desc stu4;\n\nhive> desc formatted stu4;\n\n~~~\n\n#### 7.3 增加/修改/替换列信息\n\n* 增加列\n\n~~~sql\nhive> alter table stu4 add columns(address string);\n\n~~~\n\n* 修改列\n\n~~~sql\nhive> alter table stu4 change column address address_id int;\n\n~~~\n\n### 8. Hive数据导入\n\n#### 8.1、直接向表中插入数据（强烈不推荐使用）\n\n```sql\nhive (myhive)> create table score3 like score;\nhive (myhive)> insert into table score3 partition(month ='201807') values ('001','002','100');\n\n```\n\n#### 8.2、通过load方式加载数据（必须掌握）\n\n语法：\n\n```sql\n hive> load data [local] inpath 'dataPath' overwrite | into table student [partition (partcol1=val1,…)]; \n\n```\n\n通过load方式加载数据\n\n```sql\nhive (myhive)> load data local inpath '/kkb/install/hivedatas/score.csv' overwrite into table score3 partition(month='201806');\n\n```\n\n#### 8.3、通过查询方式加载数据（必须掌握）\n\n通过查询方式加载数据\n\n```sql\nhive (myhive)> create table score5 like score;\nhive (myhive)> insert overwrite table score5 partition(month = '201806') select s_id,c_id,s_score from score;\n\n```\n\n#### 8.4、查询语句中创建表并加载数据（as select）\n\n将查询的结果保存到一张表当中去\n\n```\nhive (myhive)> create table score6 as select * from score;\n\n```\n\n#### 8.5、创建表时通过location指定加载数据路径\n\n1）创建表，并指定在hdfs上的位置\n\n```sql\nhive (myhive)> create external table score7 (s_id string,c_id string,s_score int) row format delimited fields terminated by '\\t' location '/myscore7';\n\n```\n\n2）上传数据到hdfs上，我们也可以直接在hive客户端下面通过dfs命令来进行操作hdfs的数据\n\n```\nhive (myhive)>  dfs -mkdir -p /myscore7;\nhive (myhive)> dfs -put /kkb/install/hivedatas/score.csv /myscore7;\n\n```\n\n3）查询数据\n\n```\nhive (myhive)> select * from score7;\n\n```\n\n#### 8.6、export导出与import 导入 hive表数据（内部表操作）\n\n```\nhive (myhive)> create table teacher2 like teacher;\nhive (myhive)> export table teacher to  '/kkb/teacher';\nhive (myhive)> import table teacher2 from '/kkb/teacher';\n\n```\n\n### 9、Hive数据导出\n\n#### 9.1 insert 导出\n\n* 1、将查询的结果导出到本地\n\n~~~sql\ninsert overwrite local directory '/kkb/install/hivedatas/stu' select * from stu;\n\n~~~\n\n* 2、将查询的结果格式化导出到本地\n\n~~~sql\ninsert overwrite local directory '/kkb/install/hivedatas/stu2' row format delimited fields terminated by  ',' select * from stu;\n\n~~~\n\n* 3、将查询的结果导出到HDFS上==(没有local)==\n\n~~~sql\ninsert overwrite  directory '/kkb/hivedatas/stu'  row format delimited fields terminated by  ','  select * from stu;\n\n~~~\n\n#### 9.2、 Hive Shell 命令导出\n\n* 基本语法：\n\n  * hive -e \"sql语句\" >   file\n  * hive -f  sql文件   >    file\n\n  ~~~shell\n  bin/hive -e 'select * from myhive.stu;' > /kkb/install/hivedatas/student1.txt\n  \n  ~~~\n\n\n\n#### 9.3、export导出到HDFS上\n\n~~~sql\nexport table  myhive.stu to '/kkb/install/hivedatas/stuexport';\n\n~~~\n\n\n\n### 10、hive的静态分区和动态分区\n\n#### 10.1 静态分区\n\n- 表的分区字段的值需要开发人员手动给定\n\n  - 1、创建分区表\n\n  ~~~sql\n  use myhive;\n  create table order_partition(\n  order_number string,\n  order_price  double,\n  order_time string\n  )\n  partitioned BY(month string)\n  row format delimited fields terminated by '\\t';\n  \n  ~~~\n\n~~~\n  \n- 10.2、准备数据\torder.txt内容如下\n  \ncd /kkb/install/hivedatas\nvim order.txt \n10001\t100\t2019-03-02\n10002\t200\t2019-03-02\n10003\t300\t2019-03-02\n10004\t400\t2019-03-03\n10005\t500\t2019-03-03\n10006\t600\t2019-03-03\n10007\t700\t2019-03-04\n10008\t800\t2019-03-04\n10009\t900\t2019-03-04\n\n~~~\n\n\n\n  - 3、加载数据到分区表\n\n  ~~~sql\nload data local inpath '/kkb/install/hivedatas/order.txt' overwrite into table order_partition partition(month='2019-03');\n\n  ~~~\n\n  - 4、查询结果数据\t\n\n  ~~~sql\nselect * from order_partition where month='2019-03';\n结果为：\n  \n10001   100.0   2019-03-02      2019-03\n10002   200.0   2019-03-02      2019-03\n10003   300.0   2019-03-02      2019-03\n10004   400.0   2019-03-03      2019-03\n10005   500.0   2019-03-03      2019-03\n10006   600.0   2019-03-03      2019-03\n10007   700.0   2019-03-04      2019-03\n10008   800.0   2019-03-04      2019-03\n10009   900.0   2019-03-04      2019-03\n\n  ~~~\n\n\n\n\n#### .2 动态分区\n\n- 按照需求实现把数据自动导入到表的不同分区中，==不需要手动指定==\n\n  - **需求：按照不同部门作为分区导数据到目标表** \n\n    1、创建表\n\n    ~~~sql\n    --创建普通表\n    create table t_order(\n        order_number string,\n        order_price  double, \n        order_time   string\n    )row format delimited fields terminated by '\\t';\n    \n    --创建目标分区表\n    create table order_dynamic_partition(\n        order_number string,\n        order_price  double    \n    )partitioned BY(order_time string)\n    row format delimited fields terminated by '\\t';\n    \n    \n    ~~~\n\n    2、准备数据\t order_created.txt内容如下\n\n    ~~~sql\n    cd /kkb/install/hivedatas\n    vim order_partition.txt\n    \n    10001\t100\t2019-03-02 \n    10002\t200\t2019-03-02\n    10003\t300\t2019-03-02\n    10004\t400\t2019-03-03\n    10005\t500\t2019-03-03\n    10006\t600\t2019-03-03\n    10007\t700\t2019-03-04\n    10008\t800\t2019-03-04\n    10009\t900\t2019-03-04\n    \n    ~~~\n\n    3、向普通表t_order加载数据\n\n    ```\n    load data local inpath '/kkb/install/hivedatas/order_partition.txt' overwrite into table t_order;\n    \n    ```\n\n    4、动态加载数据到分区表中\n\n    ```\n    要想进行动态分区，需要设置参数\n    //开启动态分区功能\n    hive> set hive.exec.dynamic.partition=true; \n    //设置hive为非严格模式\n    hive> set hive.exec.dynamic.partition.mode=nonstrict; \n    hive> insert into table order_dynamic_partition partition(order_time) select order_number,order_price,order_time from t_order;\n    \n    ```\n\n    5、查看分区\n\n    ```\n    bin/hive>  show partitions order_dynamic_partition;\n    \n    ```\n\n    ![1569313506031](assets/1569313506031.png)\n\n### 11、hive的基本查询语法\n\n#### 1. 基本查询\n\n- 注意\n  - SQL 语言==大小写不敏感==\n  - SQL 可以写在一行或者多行\n  - ==关键字不能被缩写也不能分行==\n  - 各子句一般要分行写\n  - 使用缩进提高语句的可读性\n\n\n\n##### 1.1 全表和特定列查询\n\n- 全表查询\n\n```sql\nselect * from stu;\n\n```\n\n- 选择特定列查询\n\n```sql\nselect id,name from stu;\n\n```\n\n##### 1.2 列起别名\n\n- 重命名一个列\n\n  - 紧跟列名，也可以在列名和别名之间加入关键字 ‘as’ \n\n- 案例实操\n\n  ```sql\n  select id,name as stuName from stu;\n  \n  ```\n\n\n\n1.3 常用函数\n\n- 1．求总行数（count）\n\n```sql\n select count(*) cnt from score;\n\n```\n\n- 2、求分数的最大值（max）\n\n```sql\nselect max(s_score) from score;\n\n```\n\n- 3、求分数的最小值（min）\n\n```sql\nselect min(s_score) from score;\n\n```\n\n- 4、求分数的总和（sum）\n\n```sql\nselect sum(s_score) from score;\n\n```\n\n- 5、求分数的平均值（avg）\n\n```sql\nselect avg(s_score) from score;\n\n```\n\n\n\n##### 1.4 limit 语句\n\n- 典型的查询会返回多行数据。limit子句用于限制返回的行数。\n\n```sql\n select  * from score limit 5;\n\n```\n\n\n\n##### 1.5 where 语句\n\n- 1、使用 where 子句，将不满足条件的行过滤掉\n- 2、==where 子句紧随from子句==\n- 3、案例实操\n\n```sql\nselect  * from score where s_score > 60;\n\n```\n\n\n\n##### 1.6 算术运算符\n\n\n\n| 运算符 | 描述           |\n| ------ | -------------- |\n| A+B    | A和B 相加      |\n| A-B    | A减去B         |\n| A*B    | A和B 相乘      |\n| A/B    | A除以B         |\n| A%B    | A对B取余       |\n| A&B    | A和B按位取与   |\n| A\\|B   | A和B按位取或   |\n| A^B    | A和B按位取异或 |\n| ~A     | A按位取反      |\n\n\n\n##### 1.7 比较运算符\n\n\n\n|         操作符          | 支持的数据类型 |                             描述                             |\n| :---------------------: | :------------: | :----------------------------------------------------------: |\n|           A=B           |  基本数据类型  |             如果A等于B则返回true，反之返回false              |\n|          A<=>B          |  基本数据类型  | 如果A和B都为NULL，则返回true，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL |\n|       A<>B, A!=B        |  基本数据类型  | A或者B为NULL则返回NULL；如果A不等于B，则返回true，反之返回false |\n|           A<B           |  基本数据类型  | A或者B为NULL，则返回NULL；如果A小于B，则返回true，反之返回false |\n|          A<=B           |  基本数据类型  | A或者B为NULL，则返回NULL；如果A小于等于B，则返回true，反之返回false |\n|           A>B           |  基本数据类型  | A或者B为NULL，则返回NULL；如果A大于B，则返回true，反之返回false |\n|          A>=B           |  基本数据类型  | A或者B为NULL，则返回NULL；如果A大于等于B，则返回true，反之返回false |\n| A [NOT] BETWEEN B AND C |  基本数据类型  | 如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为true，反之为false。如果使用NOT关键字则可达到相反的效果。 |\n|        A IS NULL        |  所有数据类型  |           如果A等于NULL，则返回true，反之返回false           |\n|      A IS NOT NULL      |  所有数据类型  |          如果A不等于NULL，则返回true，反之返回false          |\n|    IN(数值1, 数值2)     |  所有数据类型  |                  使用 IN运算显示列表中的值                   |\n|     A [NOT] LIKE B      |  STRING 类型   | B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回true；反之返回false。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。like不是正则，而是通配符 |\n|  A RLIKE B, A REGEXP B  |  STRING 类型   | B是一个正则表达式，如果A与其匹配，则返回true；反之返回false。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。 |\n\n\n\n##### 1.8 逻辑运算符\n\n\n\n|  操作符  |  操作  |                   描述                    |\n| :------: | :----: | :---------------------------------------: |\n| A AND  B | 逻辑并 |    如果A和B都是true则为true，否则false    |\n| A  OR  B | 逻辑或 | 如果A或B或两者都是true则为true，否则false |\n|  NOT  A  | 逻辑否 |      如果A为false则为true,否则false       |\n\n\n\n#### 2. 分组 \n\n##### 2.1 Group By 语句\n\n​\tGroup By 语句通常会和==聚合函数==一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。\n\n- 案例实操：\n\n  - （1）计算每个学生的平均分数\n\n  ```sql\n  select s_id,avg(s_score) from score group by s_id;\n  \n  ```\n\n  - （2）计算每个学生最高的分数\n\n  ```sql\n  select s_id,max(s_score) from score group by s_id;\n  \n  ```\n\n\n\n##### 2.2 Having语句\n\n- having 与 where 不同点\n\n  - where针对==表中的列发挥作用==，查询数据；==having针对查询结果中的列==发挥作用，筛选数据\n  - where后面==不能写分组函数==，而having后面可以==使用分组函数==\n  - having只用于group by分组统计语句\n\n- 案例实操\n\n  - 求每个学生的平均分数\n\n  ```sql\n  select s_id,avg(s_score) from score group by s_id;\n  \n  ```\n\n  - 求每个学生平均分数大于60的人\n\n  ```sql\n  select s_id,avg(s_score) as avgScore from score group by s_id having avgScore > 60;\n  \n  ```\n\n#### 3. join语句\n\n##### 11.3.1 等值 join\n\n- Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。\n\n- 案例实操\n\n  - 根据学生和成绩表，查询学生姓名对应的成绩\n\n  ```sql\n  select * from stu left join score on stu.id = score.s_id;\n  \n  ```\n\n\n\n##### 11.3.2 表的别名\n\n- 好处\n\n  - 使用别名可以简化查询。\n  - 使用表名前缀可以提高执行效率。\n\n- 案例实操\n\n  - 合并老师与课程表\n\n  ```sql\n  #hive当中创建course表并加载数据\n  create table course (c_id string,c_name string,t_id string) row format delimited fields terminated by '\\t';\n  load data local inpath '/kkb/install/hivedatas/course.csv' overwrite into table course;\n  \n  select * from teacher t join course c on t.t_id = c.t_id;\n  \n  ```\n\n##### 11.3.3 内连接 inner join\n\n- 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。\n  - join默认是inner  join\n- 案例实操\n\n```sql\nselect * from teacher t inner join course c  on t.t_id = c.t_id;\n\n```\n\n\n\n##### 11.3.4 左外连接 left outer join\n\n- 左外连接：join操作符==左边表中==符合where子句的所有记录将会被返回。\n\n- 案例实操\n\n  - 查询老师对应的课程\n\n  ```sql\n   select * from teacher t left outer join course c  on t.t_id = c.t_id;\n  \n  ```\n\n\n\n##### 3.5 右外连接 right outer join\n\n- 右外连接：join操作符==右边表中==符合where子句的所有记录将会被返回。\n\n- 案例实操\n\n  ```sql\n   select * from teacher t right outer join course c  on t.t_id = c.t_id;\n  \n  ```\n\n##### 11.3.6 满外连接 full outer join\n\n- 满外连接：将会返回==所有表中==符合where语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用null值替代。\n\n- 案例实操\n\n  ```sql\n  select * from teacher t full outer join course c  on t.t_id = c.t_id;\n  \n  ```\n\n##### 11.3.7 多表连接 \n\n- **多个表使用join进行连接**\n\n- 注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。\n\n- 案例实操\n\n  - 多表连接查询，查询老师对应的课程，以及对应的分数，对应的学生\n\n  ```sql\n  select * from teacher t left join course c on t.t_id = c.t_id \n  left join score s on c.c_id = s.c_id \n  left join stu on s.s_id = stu.id;\n  \n  ```\n\n#### 11.4. 排序\n\n##### 11.4.1 order by 全局排序\n\n- order by 说明\n\n  - 全局排序，只有一个reduce\n  - 使用 ORDER BY 子句排序\n    - asc ( ascend)\n      - 升序 (默认)\n    - desc (descend)\n      - 降序\n  - order by 子句在select语句的结尾\n\n- 案例实操\n\n  - 查询学生的成绩，并按照分数降序排列\n\n  ```sql\n  select * from score  s order by s_score desc ;\n  \n  ```\n\n##### 11.4.2 按照别名排序\n\n- 按照学生分数的平均值排序\n\n```sql\nselect s_id,avg(s_score) avgscore  from score group by s_id order by avgscore desc; \n\n```\n\n##### 11.4.3 每个MapReduce内部排序（Sort By）局部排序\n\n- sort by：每个reducer内部进行排序，对全局结果集来说不是排序。\n\n  1、设置reduce个数\n\n  ```\n  set mapreduce.job.reduces=3;\n  \n  ```\n\n  2、查看reduce的个数\n\n  ```sql\n  set mapreduce.job.reduces;\n  \n  ```\n\n  3、查询成绩按照成绩降序排列\n\n  ```sql\n   select * from score s sort by s.s_score;\n  \n  ```\n\n  4、将查询结果导入到文件中（按照成绩降序排列）\n\n  ```sql\n  insert overwrite local directory '/kkb/install/hivedatas/sort' select * from score s sort by s.s_score;\n  \n  ```\n\n\n\n##### 11.4.4 distribute by 分区排序\n\n- distribute by：类似MR中partition，==采集hash算法，在map端将查询的结果中hash值相同的结果分发到对应的reduce文件中==。结合sort by使用。\n\n- 注意\n\n  - Hive要求 **distribute by** 语句要写在 **sort by** 语句之前。\n\n- 案例实操\n\n  - 先按照学生 sid 进行分区，再按照学生成绩进行排序\n\n    - 设置reduce的个数\n\n    ```sql\n    set mapreduce.job.reduces=3;\n    \n    ```\n\n    - 通过distribute by  进行数据的分区,，将不同的sid 划分到对应的reduce当中去\n\n    ```sql\n    insert overwrite local directory '/kkb/install/hivedatas/distribute' select * from score distribute by s_id sort by s_score;\n    \n    ```\n\n\n\n##### 11.4.5 cluster by\n\n- 当distribute by和sort by字段相同时，可以使用cluster by方式\n\n- 除了distribute by 的功能外，还会对该字段进行排序，所以cluster by = distribute by + sort by\n\n  ```sql\n  --以下两种写法等价\n  \n  insert overwrite local directory '/kkb/install/hivedatas/distribute_sort' select * from score distribute  by s_score sort  by s_score;\n  \n  \n  insert overwrite local directory '/kkb/install/hivedatas/cluster' select * from score  cluster by s_score;\n  \n  \n  ```\n\n\n\n### 12、hive客户端jdbc操作\n\n#### 第一步：启动hiveserver2的服务端\n\nnode03执行以下命令启动hiveserver2的服务端\n\n```\ncd /kkb/install/hive-1.1.0-cdh5.14.2/\nnohup bin/hive --service hiveserver2 2>&1 &\n\n```\n\n#### 第二步：引入依赖\n\n```xml\n   <repositories>\n        <repository>\n            <id>cloudera</id>\n            <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n        </repository>\n    </repositories>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-exec</artifactId>\n            <version>1.1.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-jdbc</artifactId>\n            <version>1.1.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-cli</artifactId>\n            <version>1.1.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n            <version>2.6.0-cdh5.14.2</version>\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                    <encoding>UTF-8</encoding>\n                    <!--    <verbal>true</verbal>-->\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n```\n\n#### 第三步：代码开发\n\n```java\nimport java.sql.*;\n\npublic class HiveJDBC {\n    private static String url=\"jdbc:hive2://192.168.52.120:10000/myhive\";\n    public static void main(String[] args) throws Exception {\n        Class.forName(\"org.apache.hive.jdbc.HiveDriver\");\n        //获取数据库连接\n        Connection connection = DriverManager.getConnection(url, \"hadoop\",\"\");\n        //定义查询的sql语句\n        String sql=\"select * from stu\";\n        try {\n            PreparedStatement ps = connection.prepareStatement(sql);\n            ResultSet rs = ps.executeQuery();\n            while (rs.next()){\n                //获取id字段值\n                int id = rs.getInt(1);\n                //获取deptid字段\n                String name = rs.getString(2);\n                System.out.println(id+\"\\t\"+name);\n            }\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n    }\n}\n\n```\n\n\n\n### 、hive的可视化工具dbeaver介绍以及使用\n\n#### 1、dbeaver的基本介绍\n\ndbeaver是一个图形化的界面工具，专门用于与各种数据库的集成，通过dbeaver我们可以与各种数据库进行集成通过图形化界面的方式来操作我们的数据库与数据库表，类似于我们的sqlyog或者navicate\n\n\n\n#### 2、dbeaver的下载安装\n\nhttps://github.com/dbeaver/dbeaver/releases\n\n我们可以直接从github上面下载我们需要的对应的安装包即可\n\n#### 3、dbeaver的安装与使用\n\n这里我们使用的版本是6.15这个版本，下载zip的压缩包，直接解压就可以使用，然后双击dbeaver.exe即可启动\n\n##### 第一步：双击dbeaver.exe然后启动dbeaver图形化界面\n\n![1569407489837](assets/1569407489837.png)\n\n\n\n\n\n##### 第二步：配置我们的主机名与端口号\n\n\n\n![20181113110722636](assets/20181113110722636.png)\n\n![1569407697691](assets/1569407697691.png)\n\n\n\n\n\n\n\n\n\n**注意：⚠️**\n\n企业hive可视化工具：Hub\n\n","tags":["hive"]},{"title":"zookeeper分布式协调框架（二）hadoop ha高可用安装","url":"/2019/10/28/zookeeper/zookeeper分布式协调框架（二）hadoop ha高可用安装/","content":"\n# 集群规划hadoop高可用搭建\n\n>  说明：\n>\n>  - 集群共5个节点，主机名分别是node01、node02、node03、node04、node05\n>\n>  - 初始启动集群\n>    - node01上运行active namenode即主namenode；node02上运行standby namenode即从namenode\n>    - node04上运行主resourcemanager；node05上运行从resourcemanager\n\n- 每个节点运行的进程如下表\n\n| 机器名 | 运行进程                                                    |\n| ------ | ----------------------------------------------------------- |\n| node01 | NameNode/zkfc/Zookeeper/Journalnode/DataNode/NodeManager    |\n| node02 | NameNode/zkfc/Zookeeper/Journalnode/DataNode/NodeManager    |\n| node03 | Zookeeper/Journalnode/DataNode/NodeManager/JobHistoryServer |\n| node04 | ResourceManager                                             |\n| node05 | ResourceManager                                             |\n\n\n\n# Hadoop HA搭建\n\n## 1. 虚拟机环境准备\n\n- 准备**5台**虚拟机\n- 在做五节点hadoop HA集群搭建之前，要求先完成**每台**虚拟机的**基本环境准备**\n  - 每个节点都要做好“在node01上开始解压hadoop的tar.gz包之前的环境配置”\n  - 主要包括如下步骤（三节点Hadoop集群搭建时已讲解过，不再赘述）\n    - windows|mac安装VMWare虚拟化软件\n    - VMWare下安装CenoOS7\n    - 虚拟机关闭防火墙\n    - 禁用selinux\n    - 配置虚拟网卡\n    - 配置虚拟机网络\n    - 安装JDK\n    - 配置时间同步\n    - 修改主机名\n    - 修改ip地址\n    - 修改/etc/hosts\n    - 各节点免密钥登陆\n    - 重启虚拟机\n\n\n\n## 2. 安装ZooKeeper集群\n\n> Hadoop高可用集群需要使用ZooKeeper集群做分布式协调；所以先安装ZooKeeper集群\n\n- 在node01、node02、node03上安装ZooKeeper集群（详见三节点ZooKeeper集群搭建，不再赘述）\n\n\n\n## 3. 五节点Hadoop HA搭建\n\n> **注意：**\n>\n> ①3.1到3.8在**node01**上操作\n>\n> ②**此文档使用<font color=red>普通用户</font>操作，如hadoop**\n>\n> ③**hadoop安装到用户主目录下，如/kkb/install**\n>\n> <font color=red>**请根据自己的实际情况修改**</font>\n\n\n\n### 3.1 解压hadoop压缩包\n\n- hadoop压缩包hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz上传到node01的/kkb/soft路径中\n\n- 解压hadoop压缩包到/kkb/install\n\n```shell\n#解压hadoop压缩包到/kkb/install\n[hadoop@node01 ~]$ cd\n[hadoop@node01 ~]$ cd /kkb/soft/\n[hadoop@node01 soft]$ tar -xzvf hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz -C /kkb/install/\n```\n\n\n### 3.2 修改hadoop-env.sh\n\n- 进入hadoop配置文件路径$HADOOP_HOME/etc/hadoop\n\n```shell\n[hadoop@node01 soft]$ cd /kkb/install/hadoop-2.6.0-cdh5.14.2/\n[hadoop@node01 hadoop-2.6.0-cdh5.14.2]$ cd etc/hadoop/\n```\n\n- 修改hadoop-env.sh，修改JAVA_HOME值为jdk解压路径；保存退出\n\n```shell\nexport JAVA_HOME=/kkb/install/jdk1.8.0_141\n```\n\n> 注意：JAVA_HOME值修改为<font color=red>**自己jdk的实际目录**</font>\n\n### 3.3 修改core-site.xml\n\n> **注意：**\n>\n> **情况一：值/kkb/install/hadoop-2.6.0-cdh5.14.2/tmp根据实际情况修改**\n>\n> **情况二：值node01:2181,node02:2181,node03:2181根据实际情况修改，修改成安装了zookeeper的虚拟机的主机名**\n\n```xml\n<configuration>\n\t<!-- 指定hdfs的nameservice id为ns1 -->\n\t<property>\n\t\t<name>fs.defaultFS</name>\n\t\t<value>hdfs://ns1</value>\n\t</property>\n\t<!-- 指定hadoop临时文件存储的基目录 -->\n\t<property>\n\t\t<name>hadoop.tmp.dir</name>\n\t\t<value>/kkb/install/hadoop-2.6.0-cdh5.14.2/tmp</value>\n\t</property>\n\t<!-- 指定zookeeper地址，ZKFailoverController使用 -->\n\t<property>\n\t\t<name>ha.zookeeper.quorum</name>\n\t\t<value>node01:2181,node02:2181,node03:2181</value>\n\t</property>\n</configuration>\n```\n\n### 3.4 修改hdfs-site.xml\n\n> **注意：**\n>\n> **情况一：属性值qjournal://node01:8485;node02:8485;node03:8485/ns1中的主机名，修改成实际安装zookeeper的虚拟机的主机名**\n>\n> **情况二：属性值/kkb/install/hadoop-2.6.0-cdh5.14.2/journal中”/kkb/install/hadoop-2.6.0-cdh5.14.2”替换成实际hadoop文件夹的路径**\n>\n> **情况三：属性值/home/hadoop/.ssh/id_rsa中/home/hadoop根据实际情况替换**\n\n```xml\n<configuration>\n\t<!--指定hdfs的nameservice列表，多个之前逗号分隔；此处只有一个ns1，需要和core-site.xml中的保持一致 -->\n\t<property>\n\t\t<name>dfs.nameservices</name>\n\t\t<value>ns1</value>\n\t</property>\n\t<!-- ns1下面有两个NameNode，分别是nn1，nn2 -->\n\t<property>\n\t\t<name>dfs.ha.namenodes.ns1</name>\n\t\t<value>nn1,nn2</value>\n\t</property>\n\t<!-- nn1的RPC通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.ns1.nn1</name>\n\t\t<value>node01:8020</value>\n\t</property>\n\t<!-- nn1的http通信地址,web访问地址 -->\n\t<property>\n\t\t<name>dfs.namenode.http-address.ns1.nn1</name>\n\t\t<value>node01:50070</value>\n\t</property>\n\t<!-- nn2的RPC通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.ns1.nn2</name>\n\t\t<value>node02:8020</value>\n\t</property>\n\t<!-- nn2的http通信地址,web访问地址 -->\n\t<property>\n\t\t<name>dfs.namenode.http-address.ns1.nn2</name>\n\t\t<value>node02:50070</value>\n\t</property>\n\t<!-- 指定NameNode的元数据在JournalNode上的存放位置 -->\n\t<property>\n\t\t<name>dfs.namenode.shared.edits.dir</name>\n\t\t<value>qjournal://node01:8485;node02:8485;node03:8485/ns1</value>\n\t</property>\n\t<!-- 指定JournalNode在本地磁盘存放数据的位置 -->\n\t<property>\n\t\t<name>dfs.journalnode.edits.dir</name>\n\t\t<value>/kkb/install/hadoop-2.6.0-cdh5.14.2/journal</value>\n\t</property>\n\t<!-- 开启NameNode失败自动切换 -->\n\t<property>\n\t\t<name>dfs.ha.automatic-failover.enabled</name>\n\t\t<value>true</value>\n\t</property>\n\t<!-- 此类决定哪个namenode是active，切换active和standby -->\n\t<property>\n\t\t<name>dfs.client.failover.proxy.provider.ns1</name>\n\t\t<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n\t</property>\n\t<!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行-->\n\t<property>\n\t\t<name>dfs.ha.fencing.methods</name>\n\t\t<value>\n\t\tsshfence\n\t\tshell(/bin/true)\n\t\t</value>\n\t</property>\n\t<!-- 使用sshfence隔离机制时需要ssh免密登陆到目标机器 -->\n\t<property>\n\t\t<name>dfs.ha.fencing.ssh.private-key-files</name>\n\t\t<value>/home/hadoop/.ssh/id_rsa</value>\n\t</property>\n\t<!-- 配置sshfence隔离机制超时时间 -->\n\t<property>\n\t\t<name>dfs.ha.fencing.ssh.connect-timeout</name>\n\t\t<value>30000</value>\n\t</property>\n</configuration>\n```\n\n### 3.5 修改mapred-site.xml\n\n- 重命名文件\n\n```shell\n[hadoop@node01 hadoop]$ mv mapred-site.xml.template mapred-site.xml\n```\n\n- 修改mapred-site.xml\n\n```xml\n<configuration>\n\t<!-- 指定运行mr job的运行时框架为yarn -->\n\t<property>\n\t\t<name>mapreduce.framework.name</name>\n\t\t<value>yarn</value>\n\t</property>\n    <!-- MapReduce JobHistory Server IPC host:port -->\n\t<property>\n\t\t<name>mapreduce.jobhistory.address</name>\n\t\t<value>node03:10020</value>\n\t</property>\n\t<!-- MapReduce JobHistory Server Web UI host:port -->\n\t<property>\n\t\t<name>mapreduce.jobhistory.webapp.address</name>\n\t\t<value>node03:19888</value>\n\t</property>\n</configuration>\n```\n\n### 3.6 修改yarn-site.xml\n\n> **注意：**\n>\n> **情况一：属性yarn.resourcemanager.hostname.rm1的值node04根据实际情况替换**\n>\n> **情况二：属性yarn.resourcemanager.hostname.rm2的值node05根据实际情况替换**\n>\n> **情况三：属性值node01:2181,node02:2181,node03:2181根据实际情况替换；替换成实际安装zookeeper的虚拟机的主机名**\n\n```xml\n<configuration>\n    <!-- 是否启用日志聚合.应用程序完成后,日志汇总收集每个容器的日志,这些日志移动到文件系统,例如HDFS. -->\n\t<!-- 用户可以通过配置\"yarn.nodemanager.remote-app-log-dir\"、\"yarn.nodemanager.remote-app-log-dir-suffix\"来确定日志移动到的位置 -->\n\t<!-- 用户可以通过应用程序时间服务器访问日志 -->\n\t<!-- 启用日志聚合功能，应用程序完成后，收集各个节点的日志到一起便于查看 -->\n\t<property>\n\t\t\t<name>yarn.log-aggregation-enable</name>\n\t\t\t<value>true</value>\n\t</property>\n\t<!-- 开启RM高可靠 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.ha.enabled</name>\n\t\t<value>true</value>\n\t</property>\n\t<!-- 指定RM的cluster id为yrc，意为yarn cluster -->\n\t<property>\n\t\t<name>yarn.resourcemanager.cluster-id</name>\n\t\t<value>yrc</value>\n\t</property>\n\t<!-- 指定RM的名字 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.ha.rm-ids</name>\n\t\t<value>rm1,rm2</value>\n\t</property>\n\t<!-- 指定第一个RM的地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.hostname.rm1</name>\n\t\t<value>node04</value>\n\t</property>\n    <!-- 指定第二个RM的地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.hostname.rm2</name>\n\t\t<value>node05</value>\n\t</property>\n    <!-- 配置第一台机器的resourceManager通信地址 -->\n\t<!--客户端通过该地址向RM提交对应用程序操作-->\n\t<property>\n\t\t<name>yarn.resourcemanager.address.rm1</name>\n\t\t<value>node04:8032</value>\n\t</property>\n\t<!--向RM调度资源地址--> \n\t<property>\n\t\t<name>yarn.resourcemanager.scheduler.address.rm1</name>\n\t\t<value>node04:8030</value>\n\t</property>\n\t<!--NodeManager通过该地址交换信息-->\n\t<property>\n\t\t<name>yarn.resourcemanager.resource-tracker.address.rm1</name>\n\t\t<value>node04:8031</value>\n\t</property>\n\t<!--管理员通过该地址向RM发送管理命令-->\n\t<property>\n\t\t<name>yarn.resourcemanager.admin.address.rm1</name>\n\t\t<value>node04:8033</value>\n\t</property>\n\t<!--RM HTTP访问地址,查看集群信息-->\n\t<property>\n\t\t<name>yarn.resourcemanager.webapp.address.rm1</name>\n\t\t<value>node04:8088</value>\n\t</property>\n\t<!-- 配置第二台机器的resourceManager通信地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.address.rm2</name>\n\t\t<value>node05:8032</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.scheduler.address.rm2</name>\n\t\t<value>node05:8030</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.resource-tracker.address.rm2</name>\n\t\t<value>node05:8031</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.admin.address.rm2</name>\n\t\t<value>node05:8033</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.webapp.address.rm2</name>\n\t\t<value>node05:8088</value>\n\t</property>\n    <!--开启resourcemanager自动恢复功能-->\n\t<property>\n\t\t<name>yarn.resourcemanager.recovery.enabled</name>\n\t\t<value>true</value>\n\t</property>\t\n    <!--在node4上配置rm1,在node5上配置rm2,注意：一般都喜欢把配置好的文件远程复制到其它机器上，但这个在YARN的另一个机器上一定要修改，其他机器上不配置此项-->\n\t<!--\n    <property>       \n\t\t<name>yarn.resourcemanager.ha.id</name>\n\t\t<value>rm1</value>\n\t   <description>If we want to launch more than one RM in single node, we need this configuration</description>\n\t</property>\n\t-->\n\t<!--用于持久存储的类。尝试开启-->\n\t<property>\n\t\t<name>yarn.resourcemanager.store.class</name>\n\t\t<!-- 基于zookeeper的实现 -->\n\t\t<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\n\t</property>\n    <!-- 单个任务可申请最少内存，默认1024MB -->\n\t<property>\n\t\t<name>yarn.scheduler.minimum-allocation-mb</name>\n\t\t<value>512</value>\n\t</property>\n\t<!--多长时间聚合删除一次日志 此处-->\n\t<property>\n\t\t<name>yarn.log-aggregation.retain-seconds</name>\n\t\t<value>2592000</value><!--30 day-->\n\t</property>\n\t<!--时间在几秒钟内保留用户日志。只适用于如果日志聚合是禁用的-->\n\t<property>\n\t\t<name>yarn.nodemanager.log.retain-seconds</name>\n\t\t<value>604800</value><!--7 day-->\n\t</property>\n\t<!-- 指定zk集群地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.zk-address</name>\n\t\t<value>node01:2181,node02:2181,node03:2181</value>\n\t</property>\n    <!-- 逗号隔开的服务列表，列表名称应该只包含a-zA-Z0-9_,不能以数字开始-->\n\t<property>\n\t\t<name>yarn.nodemanager.aux-services</name>\n\t\t<value>mapreduce_shuffle</value>\n\t</property>\n</configuration>\n```\n\n### 3.7 修改slaves\n\n> node01、node02、node03上运行了datanode、nodemanager，所以修改slaves内容**替换**为：\n\n```shell\nnode01\nnode02\nnode03\n\n```\n\n### 3.8 远程拷贝hadoop文件夹\n\n> 拷贝到node02~node05\n\n```shell\n[hadoop@node01 hadoop]$ scp -r /kkb/install/hadoop-2.6.0-cdh5.14.2/ node02:/kkb/install/\n[hadoop@node01 hadoop]$ scp -r /kkb/install/hadoop-2.6.0-cdh5.14.2/ node03:/kkb/install/\n[hadoop@node01 hadoop]$ scp -r /kkb/install/hadoop-2.6.0-cdh5.14.2/ node04:/kkb/install/\n[hadoop@node01 hadoop]$ scp -r /kkb/install/hadoop-2.6.0-cdh5.14.2/ node05:/kkb/install/\n\n```\n\n### 3.9 修改两个RM的yarn-site.xml\n\n- 在**node04**上，找到属性`yarn.resourcemanager.ha.id`去除注释①、②\n\n```shell\n[hadoop@node04 ~]$ cd /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop\n[hadoop@node04 hadoop]$ vim yarn-site.xml \n\n```\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201909232016.png)\n\n- 在**node05**上\n  - 找到属性`yarn.resourcemanager.ha.id`去除注释**①、②**\n  - **③**修改成rm2\n\n```shell\n[hadoop@node05 ~]$ cd /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/\n[hadoop@node05 hadoop]$ vim yarn-site.xml\n\n```\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201909232022.png)\n\n- 修改后，结果如下\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201909232024.png)\n\n### 3.10 配置环境变量\n\n- **node01到node05<font color='red'>五个节点都配置环境变量</font>**\n\n```shell\n#将hadoop添加到环境变量中\nvim /etc/profile\n\n```\n\n- 添加内容如下（注意：若HADOOP_HOME已经存在，则修改）：\n\n```shell\nexport HADOOP_HOME=/kkb/install/hadoop-2.6.0-cdh5.14.2/\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n\n```\n\n- 编译文件，使新增环境变量生效\n\n```shell\nsource /etc/profile\n\n```\n\n## 4. 启动与初始化hadoop集群\n\n>  **注意：**严格按照下面的步骤 先检查各台hadoop环境变量是否设置好\n\n### 4.1 启动zookeeper集群\n\n>  注意：根据zookeeper实际安装情况，启动zookeeper\n\n分别在node01、node02、node03上启动zookeeper\n\n```shell\nzkServer.sh start\n\n```\n\n#查看状态：一个为leader，另外两个为follower\n\n```shell\nzkServer.sh status\n\n```\n\n### 4.2 启动HDFS\n\n#### 4.2.1 格式化ZK\n\n> 在**node01**上执行即可\n>\n> - 集群有两个namenode，分别在node01、node02上\n>\n> - 每个namenode对应一个zkfc进程；\n>\n> - 在主namenode node01上格式化zkfc\n\n```shell\nhdfs zkfc -formatZK\n\n```\n\n#### 4.2.2 启动journalnode\n\n- 在**node01**上执行\n  - 会启动node01、node02、node03上的journalnode\n  - 因为使用的是hadoop-daemon**s**.sh\n\n```shell\nhadoop-daemons.sh start journalnode\n\n```\n\n- 运行jps命令检验，node01、node02、node03上多了JournalNode进程\n\n####  4.2.3 格式化HDFS\n\n- 在node01上执行\n- 根据集群规划node01、node02上运行namenode；所以<font color='red'>**只在主namenode节点**</font>即node01上执行命令:\n  - 此命令慎用；只在集群搭建（初始化）时使用一次；\n  - 一旦再次使用，会将HDFS上之前的数据格式化删除掉\n\n```shell\nhdfs namenode -format\n\n```\n\n- 下图表示格式化成功\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201909241056.png)\n\n#### 4.2.4 初始化元数据、启动主NN\n\n- node01上执行（主namenode）\n\n```shell\nhdfs namenode -initializeSharedEdits -force\n#启动HDFS\nstart-dfs.sh\n\n```\n\n#### 4.2.5 同步元数据信息、启动从NN\n\n- **node02**上执行（从namenode）\n- 同步元数据信息，并且设置node02上namenode为standBy状态\n\n```shell\nhdfs namenode -bootstrapStandby\nhadoop-daemon.sh start namenode\n\n```\n\n#### 4.2.5 JPS查看进程\n\n- node01上\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201909241118.png)\n\n- node02上\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201909241119.png)\n\n- node03上\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201909241120.png)\n\n### 4.3 启动YARN\n\n#### 4.6.1 **主resourcemanager**\n\n- **node04**上执行（**<font color='red'>主resourcemanager</font>**）\n  - 把namenode和resourcemanager部署在不同节点，是因为性能问题，因为他们都要占用大量资源\n  - <font color='red'>坑</font>：在node04上启动yarn之前，先依次从node04 ssh远程连接到node01、node02、node03、node04、node05；因为初次ssh时，需要交互，输入yes，回车\n\n```shell\nstart-yarn.sh\n\n```\n\n#### 4.6.2 从resourcemanager\n\n- 在<font color='red'>从resourcemanager</font>即**node05**上启动rm\n\n```shell\nyarn-daemon.sh start resourcemanager\n\n```\n\n#### 4.6.3 查看resourceManager状态\n\n- node04上，它的resourcemanager的Id是rm1\n\n```shell\nyarn rmadmin -getServiceState rm1\n\n```\n\n- node05上，它的resourcemanager的Id是rm2\n\n```shell\nyarn rmadmin -getServiceState rm2\n\n```\n\n### 4.4 启动JobHistory\n\n- **node03**上执行\n\n```shell\nmr-jobhistory-daemon.sh start historyserver\n\n```\n\n\n\n## 5. 验证集群是否可用\n\n### 5.1 验证HDFS HA\n\n#### 5.1.1 访问WEB UI\n\n> node01、node02一主一备\n\n```html\nhttp://node01:50070\n\n```\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201907271415.png)\n\n```\nhttp://node02:50070\n\n```\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201907271416.png)\n\n#### 5.1.2 模拟主备切换\n\n- 在主namenode节点，运行\n\n```shell\nhadoop-daemon.sh stop namenode\n\n```\n\n- 访问之前为\"备namenode\"的WEB UI；发现状态更新为active\n\n- 或者使用命令查看状态\n\n```shell\nhdfs haadmin -getServiceState nn2\n\n```\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201907271417.png)\n\n- 启动刚才手动停掉的namenode\n\n```shell\nhadoop-daemon.sh start namenode\n\n```\n\n- 访问它的WEB UI，发现状态更新为standby\n\n- 或者使用命令查看状态\n\n```\nhdfs haadmin -getServiceState nn1\n\n```\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201907271419.png)\n\n### 5.2 验证Yarn HA\n\n> node04、node05主备切换\n\n#### 5.2.1 访问WEB UI\n\n- node04浏览器访问\n\n```\nhttp://node04:8088/cluster/cluster\n\n```\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201907271519.png)\n\n- node05浏览器访问\n\n```\nhttp://node05:8088/cluster/cluster\n\n```\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201907271520.png)\n\n#### 5.2.2 模拟主备切换\n\n- 在主resourcemanager节点，运行\n\n```shell\nyarn-daemon.sh stop resourcemanager\n\n```\n\n- 访问之前为\"备resourcemanager\"的WEB UI；发现状态更新为active\n\n- 或者命令查看状态\n\n```shell\nyarn rmadmin -getServiceState rm2\n```\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201907271524.png)\n\n- 启动刚才手动停掉的resourcemanager\n\n```shell\nyarn-daemon.sh start resourcemanager\n```\n\n- 访问它的WEB UI，发现状态更新为standby\n\n- 或者命令查看状态\n\n```shell\nyarn rmadmin -getServiceState rm1\n```\n\n![](/Users/dingchuangshi/Downloads/Hadoop-2.6.0-cdh5.14.2 HA搭建/assets/Image201907271526.png)\n\n#### 5.2.3 运行MR示例\n\n- 运行一下hadoop示例中的WordCount程序：\n\n```shell\nhadoop fs -put /kkb/install/hadoop-2.6.0-cdh5.14.2/LICENSE.txt /\nhadoop jar /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar wordcount /LICENSE.txt /w0727\n\n```\n\n\n\n## 6. 集群常用命令\n\n### 6.1 关闭Hadoop HA集群\n\n> 正确指令执行顺序如下\n\n- 主namenode上运行\n\n```shell\nstop-dfs.sh\n```\n\n- 主resoucemanager上运行\n\n```shell\nstop-yarn.sh\n```\n\n- 从resoucemanager上运行\n\n```shell\nyarn-daemon.sh stop resourcemanager\n```\n\n- 关闭zookeeper集群；每个zk服务器运行\n\n```shell\nzkServer.sh stop\n```\n\n### 6.2 常用命令\n\n- 单独启动namenode\n\n```shell\nhadoop-daemon.sh start namenode\n```\n\n- 单独启动datanode\n\n```shell\nhadoop-daemon.sh start datanode\n```\n\n- 单独启动journalnode\n\n```shell\nhadoop-daemon.sh start journalnode\n```\n\n- 启动zookeeper\n\n```shell\n./zkServer.sh start\n```\n\n- 启动hdfs\n\n```shell\nstart-dfs.sh\n```\n\n- 启动yarn\n\n```shell\nstart-yarn.sh\n```\n\n- 单独启动resorucemanager\n\n```shell\nyarn-daemon.sh start resouremanger\n```\n\n- 查看namenode状态（namenode1）\n\n```shell\nhdfs haadmin -getServiceState nn1\n```\n\n- 查看resourcemanager状态（resourcemanager2）\n\n```shell\nyarn rmadmin -getServiceState rm2\n```\n\n","tags":["hadoop","环境搭建","zookeeper","zookeeper ha"]},{"title":"zookeeper分布式协调框架（一）","url":"/2019/10/27/zookeeper/zookeeper分布式协调框架（一）/","content":"\n# zookeeper分布式协调框架（一）\n\n\n\n## 1. 为什么要用ZooKeeper\n\n- 分布式框架多个独立的程序协同工作比较复杂\n  - 开发人员容易花较多的精力实现如何使多个程序协同工作的逻辑\n  - 导致没有时间更好的思考实现程序本身的逻辑\n  - 或者开发人员对程序间的协同工作关注不够，造成协调问题\n  - 且这个分布式框架中协同工作的逻辑是共性的需求\n- ZooKeeper简单易用，能够很好的解决分布式框架在运行中，出现的各种协调问题。比如集群master主备切换、节点的上下线感知、统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等等\n\n\n\n## 2. 什么是ZooKeeper\n\n- 是Google的Chubby的一个开源实现版\n- ZooKeeper\n  - 一个分布式的，开源的，用于分布式应用程序的协调服务（service）\n  - 主从架构\n- Zookeeper 作为一个分布式的服务框架\n  - 主要用来解决分布式集群中应用系统的一致性问题\n  - 它能提供基于类似于文件系统的**目录节点树**方式的数据存储，\n  - Zookeeper 作用主要是用来维护和监控存储的数据的状态变化，通过监控这些数据状态的变化，从而达到基于数据的集群管理\n\n![](/Users/dingchuangshi/Downloads/20191023-ZooKeeper课件/20191023-ZooKeeper第一次/assets/Image201906091839.png)\n\n\n\n## 3. ZooKeeper应用初体验\n\n> 从下图观察：ZooKeeper集群目前有两种角色：leader、follower；\n>\n> ZooKeeper集群也是主从架构的：leader为主；follower为从\n\n![](assets/zkservice.jpg)\n\n> 通过客户端操作ZooKeeper集群，有两种类型的客户端\n>\n> ①命令行zkCli\n>\n> ②Java编程\n\n### 3.1 zkCli命令行\n\n- 集群命令（**每个节点运行此命令**）\n\n![](/Users/dingchuangshi/Downloads/20191023-ZooKeeper课件/20191023-ZooKeeper第一次/assets/Image201906101409.png)\n\n```shell\n# 启动ZooKeeper集群；在ZooKeeper集群中的每个节点执行此命令\n${ZK_HOME}/bin/zkServer.sh start\n# 停止ZooKeeper集群（每个节点执行以下命令）\n${ZK_HOME}/bin/zkServer.sh stop\n# 查看集群状态（每个节点执行此命令）\n${ZK_HOME}/bin/zkServer.sh status\n```\n\n- 客户端连接zkServer服务器\n\n\n![](assets/Image201906101413.png)\n\n```shell\n# 使用ZooKeeper自带的脚本，连接ZooKeeper的服务器\nzkCli.sh -server node01:2181,node02:2181,node03:2181\n```\n\n> -server选项后指定参数node01:2181,node02:2181,node03:2181\n>\n> 客户端随机的连接三个服务器中的一个\n\n- 客户端发出对ZooKeeper集群的读写请求\n\n  - ZooKeeper集群中有类似于linux文件系统的一个简版的文件系统；目录结构也是树状结构（目录树）\n\n![](/Users/dingchuangshi/Downloads/20191023-ZooKeeper课件/20191023-ZooKeeper第一次/assets/Image201910231127.png)\n\n  - 重要技巧：<font color='red'>不会就喊**help**</font>\n\n  - 还记得其它框架中help的使用吗？\n\n![](/Users/dingchuangshi/Downloads/20191023-ZooKeeper课件/20191023-ZooKeeper第一次/assets/Image201905291936.png)\n\n- 常用命令\n\n```shell\n#查看ZooKeeper根目录/下的文件列表\nls /\n```\n\n![](assets/Image201910231148.png)\n\n```shell\n#创建节点，并指定数据\ncreate /kkb\tkkb\n```\n\n```shell\n#获得某节点的数据\nget /kkb\n```\n\n![](/Users/dingchuangshi/Downloads/20191023-ZooKeeper课件/20191023-ZooKeeper第一次/assets/Image201910231151.png)\n\n```shell\n#修改节点的数据\nset /kkb kkb01\n\n#删除节点\ndelete /kkb\n```\n\n\n\n### 3.2 Java API编程\n\n> IDE可以是eclipse，或IDEA；此处以IDEA演示\n>\n> 编程分两类：原生API编程；curator编程\n\n- [Curator官网](< http://curator.apache.org/ >)\n- Curator编程\n  - Curator对ZooKeeper的api做了封装，提供简单易用的api；\n  - 它的风格是Curator链式编程\n  - 参考《使用curator做zk编程》\n\n#### 3.2.1 常用api接口\n\n ```java\n\n\t\t/**\n     * 连接参数\n     */\n    private static final String ZK_ADDRESS = \"node01:2181,node02:2181,node03:2181\";\n    private static final String ZK_PATH = \"/zk_test\";\n    /**\n     * 初始化，建立连接\n     */\n    public static void init() {\n        //重试连接策略，失败重试次数；每次休眠5000毫秒\n        //RetryPolicy policy = new ExponentialBackoffRetry(3000, 3);\n        RetryNTimes retryPolicy = new RetryNTimes(10, 5000);\n\n        // 1.设置客户端参数，参数1：指定连接的服务器集端口列表；参数2：重试策略\n        client = CuratorFrameworkFactory.newClient(ZK_ADDRESS, retryPolicy);\n        //启动客户端，连接到zk集群\n        client.start();\n\n        System.out.println(\"zk client start successfully!\");\n    }\n\n\t\t/**\n     * 创建永久节点\n     * @throws Exception\n     */\n    public static void createPersistentZNode() throws Exception {\n        String zNodeData = \"火辣的\";\n\n        ///a/b/c\n        client.create().\n                creatingParentsIfNeeded().\n                withMode(CreateMode.PERSISTENT).\n                forPath(\"/kfly/top/orchid\", zNodeData.getBytes());\n    }\n\n// 查询节点列表\nclient.getChildren().forPath(\"/\")\n  \n// 删除节点\nclient.delete().forPath(ZK_PATH);\n\n// 查询节点数据\nclient.getData().forPath(ZK_PATH)\n  \n// 修改节点数据\nclient.setData().forPath(ZK_PATH, data2.getBytes())\n  \n  \n    /**\n     * 监听ZNode， 一次监听，多次生效（zk client命令行，一次监听，生效一次）\n     */\n    public static void watchZNode() throws Exception {\n        //设置节点的cache\n        TreeCache treeCache = new TreeCache(client, \"/zk_test\");\n        //设置监听器和处理过程\n        treeCache.getListenable().addListener(new TreeCacheListener() {\n            @Override\n            public void childEvent(CuratorFramework client, TreeCacheEvent event) throws Exception {\n                ChildData data = event.getData();\n                if(data !=null){\n                    switch (event.getType()) {\n                        case NODE_ADDED: // 新增\n                            break;\n                        case NODE_REMOVED: // 删除\n                            break;\n                        case NODE_UPDATED: // 修改\n                            break;\n                        default:\n                            break;\n                    }\n                }\n            }\n        });\n        //开始监听\n        treeCache.start();\n        Thread.sleep(60000);\n        //关闭cache\n        treeCache.close();\n    }\n\n ```\n\n\n\n## 4.基本概念和操作（25分钟）\n\n> 分布式通信有几种方式\n>\n> 1、直接通过网络连接的方式进行通信；\n>\n> 2、通过共享存储的方式，来进行通信或数据的传输\n>\n> ZooKeeper使用第二种方式，提供分布式协调服务\n\n### 4.1 ZooKeeper数据结构\n\n> ZooKeeper主要由以下三个部分实现\n\n**ZooKeeper=①简版文件系统(Znode)+②原语+③通知机制(Watcher)。**\n\n- ZK文件系统\n  - 基于类似于文件系统的**目录节点树**方式的数据存储\n- 原语\n  - 可简单理解成ZooKeeper的基本的命令\n- Watcher（监听器）\n\n![img](assets/fcfaaf51f3deb48f36625a57fa1f3a292df57834.jpg)\n\n![](assets/Image201909181739.png)\n\n\n\n### **4.2 数据节点**ZNode\n\n**4.2.1 什么是ZNode**\n\n- ZNode 分为四类：\n\n|            | 持久节点  | 临时节点     |\n| ---------- | --------- | ------------ |\n| 非有序节点 | create    | create -e    |\n| 有序节点   | create -s | create -s -e |\n\n**4.2.2 持久节点**\n\n- 类比，文件夹\n\n```shell\n# 创建节点/zk_test，并设置数据my_data\ncreate /zk_test my_data\n# 持久节点，只有显示的调用命令，才能删除永久节点\ndelete /zk_test\n```\n\n**4.2.3 临时节点**\n\n- 临时节点的生命周期跟客户端**会话**session绑定，一旦会话失效，临时节点被删除。\n\n```shell\n# client1上创建临时节点\ncreate -e /tmp tmpdata\n\n# client2上查看client1创建的临时节点\nls /\n\n# client1断开连接\nclose\n\n# client2上观察现象，发现临时节点被自动删除\nls /\n```\n\n**4.2.4 有序节点**\n\n- ZNode也可以设置为**有序节点**\n\n- 为什么设计临时节点？\n\n- 防止多个不同的客户端在同一目录下，创建同名ZNode，由于重名，导致创建失败\n\n- 如何创建临时节点\n\n  - 命令行使用-s选项：create -s /kkb kkb\n\n  - Curator编程，可添加一个特殊的属性：CreateMode.EPHEMERAL\n\n    ```java\n    \n        /**\n         * 创建临时节点\n         * @throws Exception\n         */\n        public static void createEphemeralZNode() throws Exception {\n            // 创建临时节点\n            String zNodeData2 = \"kfly\";\n            client.create().\n                    creatingParentsIfNeeded().\n              \t\t\t// 创建临时节点 EPHEMERAL\n                    withMode(CreateMode.EPHEMERAL).\n                  \n              forPath(\"/top/ding\", zNodeData2.getBytes());\n            TimeUnit.SECONDS.sleep(10);\n        }\n    ```\n    \n    \n\n- 一旦节点被标记上这个属性，那么在这个节点被创建时，ZooKeeper 就会自动在其节点后面追加上一个整型数字\n\n  - 这个整数是一个由父节点维护的自增数字。\n  - 提供了创建唯一名字的ZNode的方式\n\n  ```shell\n  # 创建持久、有序节点\n  create -s /test01 test01-data\n  # Created /test010000000009\n  \n  ```\n\n\n\n\n### 4.3 会话（Session)\n\n![](assets/ZooKeeper2.png)\n\n**4.4.1 什么是会话** \n\n- 客户端要对ZooKeeper集群进行读写操作，得先与某一ZooKeeper服务器建立TCP长连接；此TCP长连接称为建立一个会话Session。\n\n- 每个会话有超时时间：SessionTimeout\n  - 当客户端与集群建立会话后，如果超过SessionTimeout时间，两者间没有通信，会话超时\n\n**4.4.2 会话的特点**\n\n- 客户端打开一个Session中的请求以FIFO（先进先出）的顺序执行；\n  - 如客户端client01与集群建立会话后，先发出一个create请求，再发出一个get请求；\n  - 那么在执行时，会先执行create，再执行get\n- 若打开两个Session，无法保证Session间，请求FIFO执行；只能保证一个session中请求的FIFO\n\n**4.4.3 会话的生命周期**\n\n![](/Users/dingchuangshi/Downloads/20191023-ZooKeeper课件/20191023-ZooKeeper第一次/assets/Image201905311514.png)\n\n- 会话的生命周期\n  - 未建立连接\n  - 正在连接\n  - 已连接\n  - 关闭连接\n\n### **4.4 请求**\n\n- 读写请求\n  - 通过客户端向ZooKeeper集群中写数据\n  - 通过客户端从ZooKeeper集群中读数据\n\n![ZooKeeper官网架构图](assets/zkservice.jpg)\n\n### 4.5 事务zxid\n\n- 事务\n  - 客户端的写请求，会对ZooKeeper中的数据做出更改；如增删改的操作\n  - 每次写请求，会生成一次事务\n  - 每个事务有一个全局唯一的事务ID，用 ZXID 表示；全局自增\n\n- 事务特点\n  - ACID：\n  - 原子性atomicity | 一致性consistency | 隔离性isolation | 持久性durability\n\n- ZXID结构：\n  - 通常是一个64位的数字。由**epoch+counter**组成\n  - epoch、counter各32位\n\n![](assets/Image201906140813.png)\n\n### 4.6 Watcher监视与通知 \n\n**4.6.1 为什么要有Watcher**\n\n- 问：客户端如何获取ZooKeeper服务器上的最新数据？\n\n  - **方式一**轮询：ZooKeeper以远程服务的方式，被客户端访问；客户端以轮询的方式获得znode数据，效率会比较低（代价比较大）\n\n  ![](/Users/dingchuangshi/Downloads/20191023-ZooKeeper课件/20191023-ZooKeeper第一次/assets/Image201905291811.png)\n\n  - **方式二**基于通知的机制：\n    - 客户端在znode上注册一个Watcher监视器\n    - 当znode上数据出现变化，watcher监测到此变化，通知客户端\n\n  ![](/Users/dingchuangshi/Downloads/20191023-ZooKeeper课件/20191023-ZooKeeper第一次/assets/Image201905291818.png)\n\n- 对比，那种好？\n\n**4.6.2 什么是Watcher?**\n\n- 客户端在服务器端，注册的事件监听器；\n- watcher用于监听znode上的某些事件\n  - 比如znode数据修改、节点增删等；\n  - 当监听到事件后，watcher会触发通知客户端\n\n**4.6.3 如何设置Watcher**\n\n> 注意：**Watcher是一个<font color='red'>单次触发的操作</font>**\n\n- 可以设置watcher的命令如下：\n\n![](/Users/dingchuangshi/Downloads/20191023-ZooKeeper课件/20191023-ZooKeeper第一次/assets/Image201905291977.png)\n\n- **示例1**\n\n```shell\n#ls path [watch]\n#node01 上执行\nls /zk_test watch\n\n#node02 上执行\ncreate /zk_test/dir01 dir01-data\n\n#观察node-01上变化\n[zk: node-01:2181,node-02:2181,node-03:2181(CONNECTED) 87] \nWATCHER::\n\nWatchedEvent state:SyncConnected type:NodeChildrenChanged path:/zk_test\n\n```\n\n图示：\n\n- client1上执行步骤1\n\n- client2上执行步骤2；\n\n- client1上观察现象3\n\n![](assets/Image201905311334.png)\n\n![](assets/Image201905311335.png)\n\n- **示例2**\n\n```shell\n#监控节点数据的变化；\n#node02上\nget /zk_test watch\n\n#node03上\nset /zk_test \"junk01\"\n#观察node2上cli的输出，检测到变化\n\n```\n\n- **示例3**：节点上下线监控\n\n  - **原理：**\n\n    1. 节点1（client1）创建临时节点\n    2. 节点2（client2）在临时节点，注册监听器watcher\n    3. 当client1与zk集群断开连接，临时节点会被删除\n    4. watcher发送消息，通知client2，临时节点被删除的事件\n\n  - **用到的zk特性：**\n\n    ​\tWatcher+临时节点\n\n  - **好处：**\n\n    ​\t通过这种方式，检测和被检测系统不需要直接关联（如client1与client2），而是通过ZK上的某个节点进行关联，大大减少了系统**耦合**。\n\n  - **实现：**\n\n    client1操作\n\n    ```shell\n    # 创建临时节点\n    create -e /zk_tmp tmp-data\n    \n    ```\n\n    client2操作\n\n    ```shell\n    # 在/zk_tmp注册监听器\n    ls /zk_tmp watch\n    \n    ```\n\n    client1操作\n\n    ```shell\n    # 模拟节点下线\n    close\n    \n    ```\n\n    观察client2\n\n    ```shell\n    WATCHER::\n    \n    WatchedEvent state:SyncConnected type:NodeDeleted path:/zk_tmp\n    \n    ```\n\n  - **图示：**\n\n    client1：\n\n    ![](assets/Image201905311401.png)\n\n    client2：\n\n![](assets/Image201905311402.png)\n\n\n\n\n\n## 5. ZooKeeper工作原理\n\n- ZooKeeper使用原子广播协议叫做Zab(ZooKeeper Automic Broadcast)协议\n- Zab协议有两种模式\n  - **恢复模式（选主）**：因为ZooKeeper也是主从架构；当ZooKeeper集群没有主的角色leader时，从众多服务器中选举leader时，处于此模式\n  - **广播模式（同步）**：当集群有了leader后，客户端向ZooKeeper集群读写数据时，集群处于此模式\n- 为了保证事务的顺序一致性，ZooKeeper采用了递增的事务id号（zxid）来标识事务，所有提议（proposal）都有zxid\n\n\n\n## 6. ZooKeeper应用场景\n\n- ZooKeeper应用场景\n\n![](assets/20170221224856838.png)\n\n1. NameNode使用ZooKeeper实现高可用.\n\n2. Yarn ResourceManager使用ZooKeeper实现高可用.\n\n3. 利用ZooKeeper对HBase集群做高可用配置\n\n4. kafka使用ZooKeeper\n\n   - 保存消息消费信息比如offset.\n   \n   - 用于检测崩溃\n   \n- 主题topic发现\n  \n   - 保持主题的生产和消费状态\n   \n## 7. ACL访问控制列表\n\n### 7.1 为什么要用ACL\n\nzk做为分布式架构中的重要中间件，通常会在上面以节点的方式存储一些关键信息，默认情况下，所有应用都可以读写任何节点，在复杂的应用中，这不太安全，ZK通过ACL机制来解决访问权限问题\n\n### 7.2 什么是ACL\n\nACL(Access Control List)可以设置某些客户端，对zookeeper服务器上节点的权限，如增删改查等\n\n### 7.3 ACL种类\n\nZooKeeper 采用 ACL（Access Control Lists）策略来进行权限控制。ZooKeeper 定义了如下5种权限。\n\n- CREATE: 创建**子节点**的权限。\n\n- READ: 获取节点数据和子节点列表的权限。\n\n- WRITE：更新节点数据的权限。\n\n- DELETE: 删除**子节点**的权限。\n\n- ADMIN: 设置节点ACL的权限。\n\n>  注意：CREATE 和 DELETE 都是针对子节点的权限控制。\n\n### 7.4 如何设置ACL\n\n1. 五种权限简称\n\n   ```shell\n   CREATE -> 增 -> c\n   READ -> 查 -> r\n   WRITE -> 改 -> w\n   DELETE -> 删 -> d\n   ADMIN -> 管理 -> a\n   这5种权限简写为**crwda**\n   ```\n\n2. 鉴权模式\n\n```shell\n- world：默认方式，相当于全世界都能访问\n- auth：代表已经认证通过的用户(cli中可以通过addauth digest user:pwd 来添加当前上下文中的授权用户)\n- digest：即用户名:密码这种方式认证，这也是业务系统中最常用的\n- ip：使用Ip地址认证\n```\n\n3. 演示auth方式\n\n```shell\n# 1）增加一个认证用户\n# addauth digest 用户名:密码明文\naddauth digest kkb:kkb\n\n# 2）设置权限\n# setAcl /path auth:用户名:密码明文:权限\nsetAcl /zk_test auth:kkb:kkb:rw\n\n# 3）查看ACL设置\ngetAcl /zk_test\n```\n\n\n\n## 8. HDFS HA方案\n\n### 8.1 ZooKeeper监听器\n\n- 关于ZooKeeper监听器有三个重要的逻辑：\n\n   - **注册**：客户端向ZooKeeper集群注册监听器\n\n   - **监听事件**：监听器负责监听特定的事件\n\n   - **回调函数**：当监听器监听到事件的发生后，调用注册监听器时定义的回调函数\n\n### 8.2 类比举例\n\n- 为了便于理解，举例：旅客住店无房可住的情况\n\n   - 一哥们去酒店办理入住，但是被告知目前无空房\n   - 这哥们告诉客服：你给我记住了，帮我留意一下有没有空出的房间，如果有，及时通知我（**类似注册监听器，监听特定事件**）\n   - 将近12点，有房客退房，有空闲的房间（**事件**）\n   - 客服发现有空房（**监听到事件**）\n   - 及时通知这哥们\n   - 这哥们收到通知后，**做一些事**，比如马上从附近酒吧赶回酒店（**调用回调函数**）\n   \n   ![](assets/Image201910242342.png)\n   \n\n### 8.3 HDFS HA原理\n\n> 关键逻辑：\n>\n> ①监听器：**注册、监听事件、回调函数**\n>\n> ②共享存储：JournalNode\n\n![](assets/Image201905211519.png)   \n\n- 在Hadoop 1.x版本，HDFS集群的NameNode一直存在单点故障问题：\n  - 集群只存在一个NameNode节点，它维护了HDFS所有的元数据信息\n  - 当该节点所在服务器宕机或者服务不可用，整个HDFS集群处于不可用状态\n  \n- Hadoop 2.x版本提出了高可用 (High Availability, HA) 解决方案\n  \n> HDFS HA方案，主要分两部分：\n  >\n> ①元数据同步\n  >\n  > ②主备切换\n\n- 元数据同步\n- 在同一个HDFS集群，运行两个互为主备的NameNode节点。\n  - 一台为主Namenode节点，处于Active状态，一台为备NameNode节点，处于Standby状态。\n  - 其中只有Active NameNode对外提供读写服务，Standby NameNode会根据Active NameNode的状态变化，在必要时**切换**成Active状态。\n  - **JournalNode集群**\n    - 在主备切换过程中，新的Active NameNode必须确保与原Active NamNode元数据同步完成，才能对外提供服务\n    - 所以用JournalNode集群作为共享存储系统；\n    - 当客户端对HDFS做操作，会在Active NameNode中edits.log文件中作日志记录，同时日志记录也会写入JournalNode集群；负责存储HDFS新产生的元数据\n    - 当有新数据写入JournalNode集群时，Standby NameNode能监听到此情况，将新数据同步过来\n    - Active NameNode(写入)和Standby NameNode(读取)实现元数据同步\n    - 另外，所有datanode会向两个主备namenode做block report\n\n![](assets/Image201909200732.png)\n\n- <font color='blue'>②主备切换</font>\n\n- **ZKFC涉及角色**\n\n  - 每个NameNode节点上各有一个ZKFC进程\n  - ZKFC即ZKFailoverController，作为独立进程存在，负责控制NameNode的主备切换\n  - ZKFC会监控NameNode的健康状况，当发现Active NameNode异常时，通过Zookeeper集群进行namenode主备选举，完成Active和Standby状态的切换\n    - ZKFC在启动时，同时会初始化HealthMonitor和ActiveStandbyElector服务\n    - ZKFC同时会向HealthMonitor和ActiveStandbyElector注册相应的回调方法（如上图的①回调、②回调）\n    - **HealthMonitor**定时调用NameNode的HAServiceProtocol RPC接口(monitorHealth和getServiceStatus)，监控NameNode的健康状态并向ZKFC反馈\n    - **ActiveStandbyElector**接收ZKFC的选举请求，通过Zookeeper自动完成namenode主备选举\n    - 选举完成后回调ZKFC的主备切换方法对NameNode进行Active和Standby状态的切换\n  \n- **主备选举过程：**两个ZKFC通过各自ActiveStandbyElector发起NameNode的主备选举，这个过程利用Zookeeper的写一致性和临时节点机制实现\n\n  - 当发起一次**主备**选举时，ActiveStandbyElector会尝试在Zookeeper创建临时节点`/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock`，Zookeeper的写一致性保证最终只会有一个ActiveStandbyElector创建成功\n- ActiveStandbyElector从ZooKeeper获得选举结果\n  \n  - 创建成功的 ActiveStandbyElector回调ZKFC的回调方法②，将对应的NameNode切换为Active NameNode状态\n- 而创建失败的ActiveStandbyElector回调ZKFC的回调方法②，将对应的NameNode切换为Standby NameNode状态\n  \n- 不管是否选举成功，所有ActiveStandbyElector都会在临时节点ActiveStandbyElectorLock上注册一个Watcher监听器，来监听这个节点的状态变化事件\n  \n  - 如果Active NameNode对应的HealthMonitor检测到NameNode状态异常时，通知对应ZKFC\n- ZKFC会调用 ActiveStandbyElector 方法，删除在Zookeeper上创建的临时节点ActiveStandbyElectorLock\n  \n  - 此时，Standby NameNode的ActiveStandbyElector注册的Watcher就会监听到此节点的 NodeDeleted事件。\n- 收到这个事件后，此ActiveStandbyElector发起主备选举，成功创建临时节点ActiveStandbyElectorLock，如果创建成功，则Standby NameNode被选举为Active NameNode（过程同上）\n  \n- **如何防止脑裂**\n\n  - 脑裂\n\n    在分布式系统中双主现象又称为脑裂，由于Zookeeper的“假死”、长时间的垃圾回收或其它原因都可能导致双Active NameNode现象，此时两个NameNode都可以对外提供服务，无法保证数据一致性\n\n  - 隔离\n\n    对于生产环境，这种情况的出现是毁灭性的，必须通过自带的**隔离（Fencing）**机制预防此类情况\n\n  - 原理\n    - ActiveStandbyElector成功创建ActiveStandbyElectorLock临时节点后，会创建另一个ActiveBreadCrumb持久节点\n  \n    - ActiveBreadCrumb持久节点保存了Active NameNode的地址信息\n  \n    - 当Active NameNode在正常的状态下断开Zookeeper Session，会一并删除临时节点ActiveStandbyElectorLock、持久节点ActiveBreadCrumb\n  \n    - 但是如果ActiveStandbyElector在异常的状态下关闭Zookeeper Session，那么持久节点ActiveBreadCrumb会保留下来（此时有可能由于active NameNode与ZooKeeper通信不畅导致，所以此NameNode**还处于active状态**）\n  \n    - 当另一个NameNode要由standy变成active状态时，会发现上一个Active NameNode遗留下来的ActiveBreadCrumb节点，那么会回调ZKFailoverController的方法对旧的Active NameNode进行fencing\n  \n      ①首先ZKFC会尝试调用旧Active NameNode的HAServiceProtocol RPC接口的transitionToStandby方法，看能否将其状态切换为Standby\n  \n      ②如果transitionToStandby方法切换状态失败，那么就需要执行Hadoop自带的隔离措施，Hadoop目前主要提供两种隔离措施：\n      sshfence：SSH to the Active NameNode and kill the process；\n      shellfence：run an arbitrary shell command to fence the Active NameNode\n  \n      ③只有成功地fencing之后，选主成功的ActiveStandbyElector才会回调ZKFC的becomeActive方法将对应的NameNode切换为Active，开始对外提供服务\n  \n\n\n\n>前情回顾：\n>\n>- ZooKeeper使用原子广播协议Zab(ZooKeeper Automic Broadcast)，保证分布式一致性\n>- 协议Zab协议有两种模式，它们分别是\n>  - ①**恢复模式（选主）**：因为ZooKeeper也是主从架构；当ZooKeeper集群没有主的角色leader时，从众多服务器中选举leader时，处于此模式；主要处理内部矛盾，我们称之为**安其内**\n>  - ②**广播模式（同步）**：当集群有了leader后，客户端向ZooKeeper集群读写数据时，集群处于此模式；主要处理外部矛盾，我们称之为**攘其外**\n>- 事务\n>  - 为了保证事务的顺序一致性，ZooKeeper采用了递增的事务id号（zxid）来标识事务，所有提议（proposal）都有zxid\n>  - 每次事务的提交，必须符合quorum多数派\n\n## 9. ZooKeeper读写\n\n### 9.1 ZooKeeper集群架构图\n\n- ZooKeeper集群也是主从架构\n  - 主角色：leader\n  - 从角色：follower或observer；统称为learner\n\n![](assets/zkservice.jpg)\n\n\n\n> 客户端与ZK集群交互，主要分两大类操作\n\n### 9.2 读操作\n\n![](assets/Image201910251149.png)\n\n- 常见的读取操作，如ls /查看目录；get /zktest查询ZNode数据\n\n- 读操作\n\n  - 客户端先与某个ZK服务器建立Session\n\n  - 然后，直接从此ZK服务器读取数据，并返回客户端即可\n\n  - 关闭Session\n\n### 9.3 写操作\n\n- 写操作比较复杂；为了便于理解，先举个生活中的例子：去银行存钱\n  - 银行柜台共有5个桂圆姐姐，编程从①到⑤，其中③是**领导leader**\n  - 有两个客户\n  - 客户①找到桂圆①，说：昨天少给我存了1000万，现在需要给我加进去\n  - 桂圆①说，对不起先生，我没有这么大的权限，请你稍等一下，我向领导**leader**③汇报一下\n  - 领导③收到消息后，为了做出英明的决策，要征询下属的意见(**proposal**)①②④⑤\n  - 只要有**过半数quorum**（5/2+1=3，包括leader自己）同意，则leader做出决定(**commit**)，同意此事\n  - leader告知所有下属follower，你们都记下此事生效\n  - 桂圆①答复客户①，说已经给您账号里加了1000万\n\n![](assets/Image201906121126.png)\n\n![](assets/Image2019061212537.png)\n\n![](assets/Image201910251203.png)\n\n- 客户端写操作\n  \n  - ①客户端向zk集群写入数据，如create /kkb；与一个follower建立Session连接，从节点follower01\n  \n  - ②follower将写请求转发给leader\n  \n  - ③leader收到消息后，发出**proposal提案**（创建/kkb），每个follower先**记录下**要创建/kkb\n  \n  - ④超过**半数quorum**（包括leader自己）同意提案，则leader提交**commit提案**，leader本地创建/kkb节点ZNode\n  \n  - ⑤leader通知所有follower，也commit提案；follower各自在本地创建/kkb\n  \n  - ⑥follower01响应client\n  \n    \n  \n\n## 10.  ZooKeeper服务\n\n### 10.1 **架构问题**\n\n![](assets/zkservice.jpg)\n\n- leader很重要？\n- 如果没有leader怎么办？\n  - 开始选举新的leader\n\n- **ZooKeeper服务器四种状态：**\n    - looking：服务器处于寻找Leader群首的状态\n\n    - leading：服务器作为群首时的状态\n\n    - following：服务器作为follower跟随者时的状态\n\n    - observing：服务器作为观察者时的状态\n\n\n\n> leader选举分**两种情况**\n>\n> - 全新集群leader选举\n>\n> - 非全新集群leader选举\n\n### 10.2 全新集群leader选举\n\n![](assets/Image201906130749.png)\n\n  - 以3台机器组成的ZooKeeper集群为例 \n\n  - 原则：集群中过**半数**（多数派quorum）Server启动后，才能选举出Leader；\n\n      - 此处quorum数是多少？3/2+1=2\n      - 即quorum=集群服务器数除以2，再加1\n\n  - 理解leader选举前，先了解几个概念\n\n        - 选举过程中，每个server需发出投票；投票信息**vote信息**结构为(sid, zxid)\n\n            全新集群，server1~3初始投票信息分别为：\n      \n            ​\tserver1 ->  **(1, 0)**\n          ​\t​server2 ->  **(2, 0)**\n          ​\tserver3 ->  **(3, 0)**\n        \n    - **leader选举公式**：\n    \n      ​\tserver1 vote信息 (sid1,zxid1)\n    \n      ​\tserver2 vote信息 (sid2,zxid2)\n    \n      ​\t**①zxid大的server胜出；**\n    \n      ​\t**②若zxid相等，再根据判断sid判断，sid大的胜出**\n  \n  - 选举leader流程：\n\n    > 假设按照ZK1、ZK2、ZK3的依次启动\n    \n    - 启动ZK1后，投票给自己，vote信息(1,0)，没有过半数，选举不出leader\n    \n    - 再启动ZK2；ZK1和ZK2票投给自己及其他服务器；ZK1的投票为(1, 0)，ZK2的投票为(2, 0)\n    \n    - 处理投票。每个server将收到的多个投票做处理\n      - 如ZK1投给自己的票(1,0)与ZK2传过来的票(2,0)比较；\n      - 利用leader选举公式，因为zxid都为0，相等；所以判断sid最大值；2>1；ZK1更新自己的投票为(2, 0)\n      - ZK2也是如此逻辑，ZK2更新自己的投票为(2,0)\n    \n    - 再次发起投票\n      - ZK1、ZK2上的投票都是(2,0)\n      - 发起投票后，ZK1上有一个自己的票(2,0)和一票来自ZK2的票(2,0)，这两票都选ZK2为leader\n      - ZK2上有一个自己的票(2,0)和一票来自ZK1的票(2,0)，这两票都选ZK2为leader\n      - 统计投票。server统计投票信息，是否有半数server投同一个服务器为leader；\n        - ZK2当选2票；多数\n      - 改变服务器状态。确定Leader后，各服务器更新自己的状态\n        - 更改ZK2状态从looking到leading，为Leader\n        - 更改ZK1状态从looking到following，为Follower\n    \n    - 当K3启动时，发现已有Leader，不再选举，直接从LOOKING改为FOLLOWING\n\n### 10.3 非全新集群leader选举\n\n![](assets/Image201906131101.png)\n\n- 选举原理同上比较zxid、sid\n- 不再赘述\n\n\n\n## 11. ZAB算法\n\n### 11.1 仲裁quorum\n\n- 什么是仲裁quorum？\n\n  - 发起proposal时，只要多数派同意，即可生效\n\n- 为什么要仲裁？\n\n  - 多数据派不需要所有的服务器都响应，proposal就能生效\n  - 且能提高集群的响应速度\n\n- quorum数如何选择？\n\n  -    **集群节点数 / 2 + 1**\n  - 如3节点的集群：quorum数=3/2+1=2\n\n### 11.2 网络分区、脑裂\n\n  - 网络分区：网络通信故障，集群被分成了2部分\n\n  - 脑裂：\n\n    - 原leader处于一个分区；\n    - 另外一个分区选举出新的leader \n    - 集群出现2个leader\n\n### 11.3 ZAB算法\n\n> [raft算法动图地址](<http://thesecretlivesofdata.com/raft/#replication>)\n\n- **ZAB与RAFT相似，区别如下：**\n\n  1、zab心跳从follower到leader；raft相反\n\n  2、zab任期叫epoch\n\n- 一下以RAFT算法动图为例，分析ZAB算法\n\n![](assets/脑裂.gif)\n\n![](assets/脑裂-1560463867696.png)\n\n\n\n### 11.4 ZooKeeper服务器个数\n\n- 仲裁模式下，服务器个数最好为奇数个。**why?**\n\n\n![](assets/Image201906131311.png)\n\n  - 5节点的比6节点的集群\n      - 容灾能力一样，\n      - quorum小，响应快\n\n\n\n## 12. ZooKeeper工作原理\n\n### 12.1写操作流程图\n\n![](assets/ZooKeeper3.png)\n\n1. 在Client向Follwer发出一个写的请求\n2. Follwer把请求发送给Leader\n3. Leader接收到以后开始发起投票并通知Follwer进行投票\n4. Follwer把投票结果发送给Leader\n5. Leader将结果汇总，如果多数同意，则开始写入同时把写入操作通知给Follwer，然后commit\n6. Follwer把请求结果返回给Client\n\n### 12.2 ZooKeeper状态同步\n\n完成leader选举后，zk就进入ZooKeeper之间状态同步过程\n\n1. leader构建NEWLEADER封包，包含leader中最大的zxid值；广播给其它follower\n2. follower收到后，如果自己的最大zxid小于leader的，则需要与leader状态同步；否则不需要\n3. leader给需要同步的每个follower创建LearnerHandler线程，负责数据同步请求\n4. leader主线程等待LearnHandler线程处理结果\n5. 只有多数follower完成同步，leader才开始对外服务，响应写请求\n6. LearnerHandler线程处理逻辑\n   1. 接收follower封包FOLLOWERINFO，包含此follower最大zxid（代称f-max-zxid）\n   2. f-max-zxid与leader最大zxid（代称l-max-zxid）比较\n   3. 若相等，说明当前follower是最新的\n   4. 另外，若在判断期间，有没有新提交的proposal\n      1. 如果有那么会发送DIFF封包将有差异的数据同步过去.同时将follower没有的数据逐个发送COMMIT封包给follower要求记录下来.\n      2. 如果follower数据id更大,那么会发送TRUNC封包告知截除多余数据.\n      3. 如果这一阶段内没有提交的提议值,直接发送SNAP封包将快照同步发送给follower.\n   5. 以上消息完毕之后,发送UPTODATE封包告知follower当前数据就是最新的了\n   6. 再次发送NEWLEADER封包宣称自己是leader,等待follower的响应.\n\n![](assets/Image201906140856.png)\n\n\n\n## 分布式锁\n\n![](assets/Image201906121639.png)\n\n```shell\ncreate -s -e /locker/node_ ndata\n```\n","tags":["zookeeper","zookeeper ha"]},{"title":"Hadoop架构原理之Yarn","url":"/2019/10/21/hadoop/Hadoop架构原理之Yarn/","content":"\n# Hadoop架构原理之Yarn\n\n\n\n## 1. YARN介绍\n\n![img](assets/a19a61bc-9378-3e38-944a-899a09f37908.jpg)\n\n- Apache Hadoop YARN(Yet Another Resource Negotiator)是Hadoop的子项目，为分离Hadoop2.0资源管理和计算组件而引入\n- YRAN具有足够的通用性，可以支持其它的分布式计算模式\n\n![img](assets/99b59921-9a97-3199-8c39-d3b77dfdceaf.jpg)\n\n\n\n## 2. YARN架构\n\n- 类似HDFS，YARN也是经典的**主从（master/slave）架构**\n  - YARN服务由一个ResourceManager（RM）和多个NodeManager（NM）构成\n  - ResourceManager为主节点（master）\n  - NodeManager为从节点（slave）\n\n![yarn的体系结构](assets/Figure3Architecture-of-YARN.png)\n\n\n\n\n\n- ApplicationMaster可以在容器内运行任何类型的任务。例如，MapReduce ApplicationMaster请求容器启动map或reduce任务，而Giraph ApplicationMaster请求容器运行Giraph任务。\n\n| 组件名                 | 作用                                                         |\n| :--------------------- | ------------------------------------------------------------ |\n| **ApplicationManager** | 相当于这个Application的监护人和管理者，负责监控、管理这个Application的所有Attempt在cluster中各个节点上的具体运行，同时负责向Yarn ResourceManager申请资源、返还资源等； |\n| **NodeManager**        | 是Slave上一个独立运行的进程，负责上报节点的状态(磁盘，内存，cpu等使用信息)； |\n| **Container**          | 是yarn中分配资源的一个单位，包涵内存、CPU等等资源，YARN以Container为单位分配资源； |\n\nResourceManager 负责对各个 NodeManager 上资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的 ApplicationMaster，它负责向 ResourceManager 申请资源，并要求 NodeManger 启动可以占用一定资源的任务。由于不同的 ApplicationMaster 被分布到不同的节点上，因此它们之间不会相互影响。\n\nClient 向 ResourceManager 提交的每一个应用程序都必须有一个 ApplicationMaster，它经过 ResourceManager 分配资源后，运行于某一个 Slave 节点的 Container 中，具体做事情的 Task，同样也运行与某一个 Slave 节点的 Container 中。\n\n### 2.1 **ResourceManager**\n\n- RM是一个全局的资源管理器，集群只有一个\n  - 负责整个系统的资源管理和分配\n  - 包括处理客户端请求\n  - 启动/监控 ApplicationMaster\n  - 监控 NodeManager、资源的分配与调度\n- 它主要由两个组件构成：\n  - 调度器（Scheduler）\n  - 应用程序管理器（Applications Manager，ASM）\n\n- 调度器\n  - 调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。\n  - 需要注意的是，该调度器是一个“纯调度器”\n    - 它不从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。\n    - 调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。\n\n- 应用程序管理器\n  - 应用程序管理器主要负责管理整个系统中所有应用程序\n  - 接收job的提交请求\n  - 为应用分配第一个 Container 来运行 ApplicationMaster，包括应用程序提交、与调度器协商资源以启动 ApplicationMaster、监控 ApplicationMaster 运行状态并在失败时重新启动它等\n\n### 2.2 **NodeManager**\n\n![nodemanager架构](assets/20190103113256851.png)\n\n- NodeManager 是一个 slave 服务，整个集群有多个\n\n- NodeManager ：\n  - 它负责接收 ResourceManager 的资源分配请求，分配具体的 Container 给应用。\n  - 负责监控并报告 Container 使用信息给 ResourceManager。\n\n- 功能：\n\n  - NodeManager 本节点上的资源使用情况和各个 Container 的运行状态（cpu和内存等资源）\n  - 接收及处理来自 ResourceManager 的命令请求，分配 Container 给应用的某个任务；\n  - 定时地向RM汇报以确保整个集群平稳运行，RM 通过收集每个 NodeManager 的报告信息来追踪整个集群健康状态的，而 NodeManager 负责监控自身的健康状态；\n  - 处理来自 ApplicationMaster 的请求；\n  - 管理着所在节点每个 Container 的生命周期；\n  - 管理每个节点上的日志；\n\n  - 当一个节点启动时，它会向 ResourceManager 进行注册并告知 ResourceManager 自己有多少资源可用。\n  - 在运行期，通过 NodeManager 和 ResourceManager 协同工作，这些信息会不断被更新并保障整个集群发挥出最佳状态。\n\n  - NodeManager 只负责管理自身的 Container，它并不知道运行在它上面应用的信息。负责管理应用信息的组件是 ApplicationMaster\n\n### 2.3 Container\n\n- Container 是 YARN 中的资源抽象\n  - 它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等\n  - 当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。\n  - YARN 会为每个任务分配一个 Container，且该任务只能使用该 Container 中描述的资源。\n\n- Container 和集群NodeManager节点的关系是：\n  - 一个NodeManager节点可运行多个 Container\n  - 但一个 Container 不会跨节点。\n  - 任何一个 job 或 application 必须运行在一个或多个 Container 中\n  - 在 Yarn 框架中，ResourceManager 只负责告诉 ApplicationMaster 哪些 Containers 可以用\n  - ApplicationMaster 还需要去找 NodeManager 请求分配具体的 Container。\n\n- 需要注意的是\n  - Container 是一个动态资源划分单位，是根据应用程序的需求动态生成的\n  - 目前为止，YARN 仅支持 CPU 和内存两种资源，且使用了轻量级资源隔离机制 Cgroups 进行资源隔离。\n\n- 功能：\n  - 对task环境的抽象；\n\n  - 描述一系列信息；\n\n  - 任务运行资源的集合（cpu、内存、io等）；\n\n  - 任务运行环境\n\n### 2.4 **ApplicationMaster**\n\n- 功能：\n  - 数据切分；\n  - 为应用程序申请资源并进一步分配给内部任务（TASK）；\n  - 任务监控与容错；\n  - 负责协调来自ResourceManager的资源，并通过NodeManager监视容器的执行和资源使用情况。\n\n- ApplicationMaster 与 ResourceManager 之间的通信\n  - 是整个 Yarn 应用从提交到运行的最核心部分，是 Yarn 对整个集群进行动态资源管理的根本步骤\n  - Yarn 的动态性，就是来源于多个Application 的 ApplicationMaster 动态地和 ResourceManager 进行沟通，不断地申请、释放、再申请、再释放资源的过程。\n\n### 2.5 Resource Request\n\n[引用连接](https://www.jianshu.com/p/f50e85bdb9ce)\n\n- Yarn的设计目标\n  - 允许我们的各种应用以共享、安全、多租户的形式使用整个集群。\n  - 并且，为了保证集群资源调度和数据访问的高效性，Yarn还必须能够感知整个集群拓扑结构。\n\n- 为了实现这些目标，ResourceManager的调度器Scheduler为应用程序的资源请求定义了一些灵活的协议，**Resource Request**和**Container**。\n  - 一个应用先向ApplicationMaster发送一个满足自己需求的资源请求\n  - 然后ApplicationMaster把这个资源请求以resource-request的形式发送给ResourceManager的Scheduler\n  - Scheduler再在这个原始的resource-request中返回分配到的资源描述Container。\n\n- 每个ResourceRequest可看做一个可序列化Java对象，包含的字段信息如下：\n\n```xml\n<!--\n- resource-name：资源名称，现阶段指的是资源所在的host和rack，后期可能还会支持虚拟机或者更复杂的网络结构\n- priority：资源的优先级\n- resource-requirement：资源的具体需求，现阶段指内存和cpu需求的数量\n- number-of-containers：满足需求的Container的集合\n-->\n<resource-name, priority, resource-requirement, number-of-containers>\n```\n\n### 2.6 JobHistoryServer \n\n- 作业历史服务\n\n  - 记录在yarn中调度的作业历史运行情况情况 ,\n\n  - 通过命令启动\n\n    ```shell\n    mr-jobhistory-daemon.sh start historyserver\n    ```\n\n  - 在集群中的数据节点机器上单独使用命令启动直接启动即可,\n\n  - 启动成功后会出现JobHistoryServer进程(使用jps命令查看，下面会有介绍) ,\n\n  - 并且可以从19888端口进行查看日志详细信息\n\n    ```\n    node01:19888\n    ```\n\n    点击链接，查看job日志\n\n    ![](assets/Image201910202320.png)\n\n- 如果没有启动jobhistoryserver，无法查看应用的日志\n\n![1563002086000](assets/1563002086000.png)\n\n- 打开如下图界面，在下图中点击History，页面会进行一次跳转\n\n![1563002731472](assets/1563002731472.png)\n\n- 点击History之后 跳转后的页面如下图是空白的，因为没有启动jobhistoryserver\n\n![1563002773601](assets/1563002773601.png)\n\n- jobhistoryserver启动后，在此运行MR程序，如wordcount\n\n![1563004024903](assets/1563004024903.png)\n\n- 点击History连接，跳转一个赞新的页面\n  - TaskType中列举的map和reduce，Total表示此次运行的mapreduce程序执行所需要的map和reduce的任务数\n\n![1563004057197](assets/1563004057197.png)\n\n- 点击TaskType列中Map连接\n\n![1563004490476](assets/1563004490476.png)\n\n- 看到map任务的相关信息比如执行状态,启动时间，完成时间。\n\n![1563004598290](assets/1563004598290.png)\n\n- 可以使用同样的方式我们查看reduce任务执行的详细信息，这里不再赘述.\n\n- jobhistoryserver就是进行作业运行过程中历史运行信息的记录，方便我们对作业进行分析.\n\n### 2.7 Timeline Server \n\n- 用来写日志服务数据 , 一般来写与第三方结合的日志服务数据(比如spark等)\n- 它是对jobhistoryserver功能的有效补充，jobhistoryserver只能对mapreduce类型的作业信息进行记录\n- 它记录除了jobhistoryserver能够进行对作业运行过程中信息进行记录之外\n- 还记录更细粒度的信息，比如任务在哪个队列中运行，运行任务时设置的用户是哪个用户。\n\n- 根据官网的解释jobhistoryserver只能记录mapreduce应用程序的记录，timelineserver功能更强大,但不是替代jobhistory两者是功能间的互补关系.\n\n![1563006522419](assets/1563006522419.png)\n\n- [官网教程](<http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/TimelineServer.html>)\n\n\n\n## 3. YARN应用运行原理\n\n![yarn架构图](assets/yarn_architecture.gif)\n\n\n### 3.1 YARN应用提交过程\n\n- Application在Yarn中的执行过程，整个执行过程可以总结为三步：\n\n  - 应用程序提交\n  - 启动应用的ApplicationMaster实例\n  - ApplicationMaster 实例管理应用程序的执行\n\n- **具体提交过程为：**\n\n  ![](assets/Image201909161351.png)\n\n  - 客户端程序向 ResourceManager 提交应用，并请求一个 ApplicationMaster 实例；\n  - ResourceManager 找到一个可以运行一个 Container 的 NodeManager，并在这个 Container 中启动 ApplicationMaster 实例；\n  - ApplicationMaster 向 ResourceManager 进行注册，注册之后客户端就可以查询 ResourceManager 获得自己 ApplicationMaster 的详细信息，以后就可以和自己的 ApplicationMaster 直接交互了（这个时候，客户端主动和 ApplicationMaster 交流，应用先向 ApplicationMaster 发送一个满足自己需求的资源请求）；\n  - ApplicationMaster 根据 resource-request协议 向 ResourceManager 发送 resource-request请求；\n  - 当 Container 被成功分配后，ApplicationMaster 通过向 NodeManager 发送 **container-launch-specification**信息 来启动Container，container-launch-specification信息包含了能够让Container 和 ApplicationMaster 交流所需要的资料；\n  - 应用程序的代码以 task 形式在启动的 Container 中运行，并把运行的进度、状态等信息通过 **application-specific**协议 发送给ApplicationMaster；\n  - 在应用程序运行期间，提交应用的客户端主动和 ApplicationMaster 交流获得应用的运行状态、进度更新等信息，交流协议也是 **application-specific**协议；\n  - 应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster 向 ResourceManager 取消注册然后关闭，用到所有的 Container 也归还给系统。\n\n- **精简版的：**\n\n  - 步骤1：用户将应用程序提交到 ResourceManager 上；\n  - 步骤2：ResourceManager为应用程序 ApplicationMaster 申请资源，并与某个 NodeManager 通信启动第一个 Container，以启动ApplicationMaster；\n  - 步骤3：ApplicationMaster 与 ResourceManager 注册进行通信，为内部要执行的任务申请资源，一旦得到资源后，将于 NodeManager 通信，以启动对应的 Task；\n  - 步骤4：所有任务运行完成后，ApplicationMaster 向 ResourceManager 注销，整个应用程序运行结束。\n\n### 3.2 MapReduce on YARN\n\n![img](assets/820234-20160604233916133-2026396104.jpg)\n\n- 提交作业\n\n  - ①程序打成jar包，在客户端运行hadoop jar命令，提交job到集群运行\n  - job.waitForCompletion(true)中调用Job的submit()，此方法中调用JobSubmitter的submitJobInternal()方法；\n    - ②submitClient.getNewJobID()向resourcemanager请求一个MR作业id\n    - 检查输出目录：如果没有指定输出目录或者目录已经存在，则报错\n    - 计算作业分片；若无法计算分片，也会报错\n    - ③运行作业的相关资源，如作业的jar包、配置文件、输入分片，被上传到HDFS上一个以作业ID命名的目录（jar包副本默认为10，运行作业的任务，如map任务、reduce任务时，可从这10个副本读取jar包）\n    - ④调用resourcemanager的submitApplication()提交作业\n  - 客户端**每秒**查询一下作业的进度（map 50% reduce 0%），进度如有变化，则在控制台打印进度报告；\n  - 作业如果成功执行完成，则打印相关的计数器\n  - 但如果失败，在控制台打印导致作业失败的原因（要学会查看日志，定位问题，分析问题，解决问题）\n\n- **初始化作业**\n\n  - 当ResourceManager(一下简称RM)收到了submitApplication()方法的调用通知后，请求传递给RM的scheduler（调度器）；调度器分配container（容器）\n  - ⑤a RM与指定的NodeManager通信，通知NodeManager启动容器；NodeManager收到通知后，创建占据特定资源的container；\n  - ⑤b 然后在container中运行MRAppMaster进程\n  - ⑥MRAppMaster需要接受任务（各map任务、reduce任务的）的进度、完成报告，所以appMaster需要创建多个簿记对象，记录这些信息\n  - ⑦从HDFS获得client计算出的输入分片split\n    - 每个分片split创建一个map任务\n    - 通过 mapreduce.job.reduces 属性值(编程时，jog.setNumReduceTasks()指定)，知道当前MR要创建多少个reduce任务\n    - 每个任务(map、reduce)有task id\n\n- **Task 任务分配**\n\n  - 如果小作业，appMaster会以uberized的方式运行此MR作业；appMaster会决定在它的JVM中顺序此MR的任务；\n\n    - 原因是，若每个任务运行在一个单独的JVM时，都需要单独启动JVM，分配资源（内存、CPU），需要时间；多个JVM中的任务再在各自的JVM中并行运行\n\n    - 若将所有任务在appMaster的JVM中顺序执行的话，更高效，那么appMaster就会这么做 ，任务作为uber任务运行\n\n    - 小作业判断依据：①小于10个map任务；②只有一个reduce任务；③MR输入大小小于一个HDFS块大小\n\n    - 如何开启uber?设置属性 mapreduce.job.ubertask.enable 值为true\n\n      ```java\n      configuration.set(\"mapreduce.job.ubertask.enable\", \"true\");\n      ```\n\n    - 在运行任何task之前，appMaster调用setupJob()方法，创建OutputCommitter，创建作业的最终输出目录（一般为HDFS上的目录）及任务输出的临时目录（如map任务的中间结果输出目录）\n\n  - ⑧若作业不以uber任务方式运行，那么appMaster会为作业中的每一个任务（map任务、reduce任务）向RM请求container\n\n    - 由于reduce任务在进入排序阶段之前，所有的map任务必须执行完成；所以，为map任务申请容器要优先于为reduce任务申请容器\n    - 5%的map任务执行完成后，才开始为reduce任务申请容器\n    - 为map任务申请容器时，遵循数据本地化，调度器尽量将容器调度在map任务的输入分片所在的节点上（移动计算，不移动数据）\n\n    - reduce任务能在集群任意计算节点运行\n    - 默认情况下，为每个map任务、reduce任务分配1G内存、1个虚拟内核，由属性决定mapreduce.map.memory.mb、mapreduce.reduce.memory.mb、mapreduce.map.cpu.vcores、mapreduce.reduce.reduce.cpu.vcores\n\n- **Task 任务执行**\n\n  - 当调度器为当前任务分配了一个NodeManager（暂且称之为NM01）的容器，并将此信息传递给appMaster后；appMaster与NM01通信，告知NM01启动一个容器，并此容器占据特定的资源量（内存、CPU）\n  - NM01收到消息后，启动容器，此容器占据指定的资源量\n  - 容器中运行YarnChild，由YarnChild运行当前任务（map、reduce）\n  - ⑩在容器中运行任务之前，先将运行任务需要的资源拉取到本地，如作业的JAR文件、配置文件、分布式缓存中的文件\n\n- **作业运行进度与状态更新**\n\n  - 作业job以及它的每个task都有状态（running、successfully completed、failed），当前任务的运行进度、作业计数器\n  - 任务在运行期间，每隔3秒向appMaster汇报执行进度、状态（包括计数器）\n  - appMaster汇总目前运行的所有任务的上报的结果\n  - 客户端每个1秒，轮询访问appMaster获得作业执行的最新状态，若有改变，则在控制台打印出来\n\n- 完成作业\n\n  - appMaster收到最后一个任务完成的报告后，将作业状态设置为成功\n  - 客户端轮询appMaster查询进度时，发现作业执行成功，程序从waitForCompletion()退出\n  - 作业的所有统计信息打印在控制台\n  - appMaster及运行任务的容器，清理中间的输出结果\n  - 作业信息被历史服务器保存，留待以后用户查询\n\n  \n\n\n### 3.3 yarn应用生命周期\n\n- RM: Resource Manager\n- AM: Application Master\n- NM: Node Manager\n\n1. Client向RM提交应用，包括AM程序及启动AM的命令。\n\n2. RM为AM分配第一个容器，并与对应的NM通信，令其在容器上启动应用的AM。\n\n3. AM启动时向RM注册，允许Client向RM获取AM信息然后直接和AM通信。\n\n4. AM通过资源请求协议，为应用协商容器资源。\n\n5. 如容器分配成功，AM要求NM在容器中启动应用，应用启动后可以和AM独立通信。\n\n6. 应用程序在容器中执行，并向AM汇报。\n\n7. 在应用执行期间，Client和AM通信获取应用状态。\n\n8. 应用执行完成，AM向RM注销并关闭，释放资源。\n\n   **申请资源->启动appMaster->申请运行任务的container->分发Task->运行Task->Task结束->回收container**\n\n\n\n## 4. 如何使用YARN\n\n### 4.1 配置文件\n\n```xml\n<!-- $HADOOP_HOME/etc/hadoop/mapred-site.xml -->\n<configuration>\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n</configuration>\n```\n\n```xml\n<!-- $HADOOP_HOME/etc/hadoop/yarn-site.xml -->\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n</configuration>\n```\n\n### 4.2 YARN启动停止\n\n- 启动 ResourceManager 和 NodeManager （以下分别简称RM、NM）\n\n```shell\n#主节点运行命令\n$HADOOP_HOME/sbin/start-yarn.sh\n```\n\n- 停止 RM 和 NM \n\n```shell\n#主节点运行命令\n$HADOOP_HOME/sbin/stop-yarn.sh\n```\n\n- 若RM没有启动起来，可以单独启动\n\n```shell\n#若RM没有启动，在主节点运行命令\n$HADOOP_HOME/sbin/yarn-daemon.sh start resouremanager\n#相反，可单独关闭\n$HADOOP_HOME/sbin/yarn-daemon.sh stop resouremanager\n\n```\n\n- 若NM没有启动起来，可以单独启动\n\n```shell\n#若NM没有启动，在相应节点运行命令\n$HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager\n#相反，可单独关闭\n$HADOOP_HOME/sbin/yarn-daemon.sh stop nodemanager\n\n```\n\n### 4.3 YARN常用命令\n\n**4.3.1 YARN命令列表**\n\n![](assets/Image201907162219.png)\n\n**4.3.2 yarn application命令**\n\n![](assets/Image201907162224.png)\n\n```shell\n#1.查看正在运行的任务\nyarn application -list\n\n```\n\n```shell\n#2.杀掉正在运行任务\nyarn application -kill 任务id\n\n```\n\n```shell\n#3.查看节点列表\nyarn node -list\n\n```\n\n![](assets/Image201907162252.png)\n\n```shell\n#4.查看节点状况；所有端口号与上图中端口号要一致（随机分配）\nyarn node -status node-03:45568\n\n```\n\n![](assets/Image201907171511.png)\n\n```shell\n#5.查看yarn依赖jar的环境变量\nyarn classpath\n\n```\n\n\n\n## 5. YARN调度器\n\n- 试想一下，你现在所在的公司有**一个**hadoop的集群。但是A项目组经常做一些定时的BI报表，B项目组则经常使用一些软件做一些临时需求。那么他们肯定会遇到同时提交任务的场景，这个时候到底如何分配资源满足这两个任务呢？是先执行A的任务，再执行B的任务，还是同时跑两个？\n\n- 在Yarn框架中，调度器是一块很重要的内容。有了合适的调度规则，就可以保证多个应用可以在同一时间有条不紊的工作。最原始的调度规则就是FIFO，即按照用户提交任务的时间来决定哪个任务先执行，先提交的先执行。但是这样很可能一个大任务独占资源，其他的资源需要不断的等待。也可能一堆小任务占用资源，大任务一直无法得到适当的资源，造成饥饿。所以FIFO虽然很简单，但是并不能满足我们的需求。\n\n- 理想情况下，yarn应用发出的资源请求应该立刻给予满足。然而现实中的资源有限，在一个繁忙的集群上，一个应用经常需要等待才能得到所需的资源。yarn调度器的工作就是根据既定的策略为应用分配资源。调度通常是一个难题，并且**没有一个所谓的“最好”的策略**，这也是为什么yarn提供了多重调度器和可配置策略供我们选择的原因。\n\n**yarn分为一级调度管理和二级调度管理**\n一级调度管理(更近底层,更接近于操作资源, 更偏向于应用层和底层结合)\n    计算资源管理(cpu,内存等,计算复杂消耗的cpu多)\n    App生命周期管理\n二级调度管理(自己代码的算法等, 更偏向于应用层)\n    App内部的计算模型管理\n    多样化的计算模型\n\n### 5.1 调度器\n\n- 在YARN中有三种调度器可以选择：FIFO Scheduler ，Capacity Scheduler，FairS cheduler\n\n![三种调度模型](assets/20180912140209122.png)\n\n### 5.2 FIFO Scheduler\n\n- FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。\n\n- FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。在共享集群中，更适合采用Capacity Scheduler或Fair Scheduler，这两个调度器都允许大任务和小任务在提交的同时获得一定的系统资源。\n\n- 上图展示了这几个调度器的区别，从图中可以看出，在FIFO 调度器中，小任务会被大任务阻塞。\n\n### 5.3 Capacity Scheduler\n\n- CDH版本默认使用Fair Scheduler公平调度器\n\n![](assets/Image201909241610.png)\n\n- 观察yarn web界面；使用的是fair scheduler\n\n![](assets/Image201909241637.png)\n\n- 若要使用capacity scheduler，需要修改yarn-site.xml文件；node01上\n\n  ```xml\n  <property>\n  \t<name>yarn.resourcemanager.scheduler.class</name>\n  \t<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>\n  </property>\n  \n  ```\n\n- 并分发到各节点\n\n  ```shell\n  [hadoop@node01 hadoop]$ pwd\n  /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop\n  [hadoop@node01 hadoop]$ scp yarn-site.xml node02:$PWD\n  [hadoop@node01 hadoop]$ scp yarn-site.xml node03:$PWD\n  \n  ```\n\n- 重启yarn\n\n  ```shell\n  [hadoop@node01 hadoop]$ stop-yarn.sh\n  [hadoop@node01 hadoop]$ start-yarn.sh\n  \n  ```\n\n- 而对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间\n\n- 如何配置容量调度器\n\n  - 队列层级结构如下\n\n    ```\n    root \n    ├── prod \n    └── dev \n    \t├── spark \n    \t└── hdp\n    \n    ```\n\n  - 主节点上，将$HADOOP_HOME/etc/hadoop/中的对应capacity-scheduler.xml配置文件备份到其它目录\n\n  - 目录$HADOOP_HOME/etc/hadoop/中建立一个新的capacity-scheduler.xml；内容如下\n\n    ```xml\n    <?xml version=\"1.0\" encoding=\"utf-8\"?>\n    \n    <configuration> \n      <property> \n        <name>yarn.scheduler.capacity.root.queues</name>  \n        <value>prod,dev</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.queues</name>  \n        <value>hdp,spark</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.prod.capacity</name>  \n        <value>40</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.capacity</name>  \n        <value>60</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.maximum-capacity</name>  \n        <value>75</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.hdp.capacity</name>  \n        <value>50</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.spark.capacity</name>  \n        <value>50</value> \n      </property> \n    </configuration>\n    \n    ```\n\n  - 将此xml文件，远程拷贝到相同目录下\n\n  - 将应用放置在哪个队列中，取决于应用本身。\n\n    例如MR，可以通过设置属性**mapreduce.job.queuename**指定相应队列。以WordCount为例，如下\n\n    如果指定的队列不存在，则发生错误。如果不指定，默认使用\"default\"队列，如下图\n\n![](assets/Image201907171118.png)\n\n![](assets/Image201907171138.png)\n\n- 动态更新配置：容量调度器的配置在运行时，可以随时重新加载，调整资源分配参数；你需要编辑conf/capacity-scheduler.xml 并在yarn主节点运行命令让配置文件生效\n  - 另外，除非重启resourcemanager，否则队列只能添加不能删除；但允许关闭\n\n```shell\n[hadoop@node01 hadoop]$ yarn rmadmin -refreshQueues\n\n```\n\n- 程序打包，提交集群运行\n\n```shell\n[hadoop@node01 target]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.wordcount.WordCountMain /README.txt /w24\n\n```\n\n![](assets/capacity scheduler.gif)\n\n\n\n### 5.4 Fair Scheduler\n\n![](assets/Image201907171437 (38).png)\n\n- Apache Hadoop默认使用Capacity Scheduler容量调度器\n\n- CDH版本默认使用Fair Scheduler公平调度器\n\n\n\n- 若要用Fair Scheduler的话，需要配置yarn-site.xml，将属性\"yarn.resourcemanager.scheduler.class\"的值修改成\"org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler\"，如下\n\n```xml\n<property>\n\t<name>yarn.resourcemanager.scheduler.class</name>\n\t<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>\n</property>\n\n```\n\n![](assets/Image201907171501.png)\n\n- 注意：同样，集群中所有yarn-site.xml文件要同步更新\n\n- 在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如下图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。\n\n- 需要注意的是，在下图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成.\n\n- 支持资源抢占\n\n在yarn-site.xml中设置yarn.scheduler.fair.preemption为true\n\n- 可通过一个名为fair-scheduler.xml文件对公平调度器进行配置\n- 此文件可放置在${HADOOP_HOME}/etc/hadoop/目录下\n- 当没有设置此配置文件时，每个应用放置在以当前用户名命名的队列中\n- 队列是用户提交第一个应用时动态创建的\n\n![](assets/Image201909161716.png)\n\n\n\n## 6. YARN应用状态\n\n我们在yarn 的web ui上能够看到yarn 应用程序分为如下几个状态:\n\n- NEW -----新建状态\n- NEW_SAVING-----新建保存状态\n- SUBMITTED-----提交状态\n- ACCEPTED-----接受状态\n- RUNNING-----运行状态\n- FINISHED-----完成状态\n- FAILED-----失败状态\n- KILLED-----杀掉状态\n\n![](assets/1558703612265.png)\n\n\n\n# 7、拓展点、未来计划、行业趋势（5分钟）\n\n1. [查看官网capacity scheduler内容](<https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html>)\n2. [capacity scheduler参考资料](<https://blog.csdn.net/u014589856/article/details/78119504>)\n3. [官网查看fair scheduler内容](<https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/FairScheduler.html>)\n4. 《Hadoop权威指南 第4版》\n   - 4.3 YARN中的调度\n   - 7.1 剖析MapReduce运行机制\n","tags":["hadoop","Yarn"]},{"title":"MapReduce编程（三）","url":"/2019/10/21/hadoop/MapReduce编程（三）/","content":"\n# MapReduce编程模型\n\n### 1. 自定义OutputFormat\n\n#### 1.1 需求\n\n- 现在有一些订单的评论数据，要将订单的好评与其它级别的评论（中评、差评）进行区分开来，将最终的数据分开到不同的文件夹下面去\n\n- 数据第九个字段表示评分等级：0 好评，1 中评，2 差评\n\n  ![](assets/Image201909111129.png)\n\n#### 1.2 逻辑分析\n\n- 程序的关键点是在一个mapreduce程序中，根据数据的不同(好评的评级不同)，输出两类结果到不同**目录**\n- 这类灵活的输出，需求通过自定义OutputFormat来实现\n\n#### 1.3 实现要点\n\n- 在mapreduce中访问外部资源\n- 自定义OutputFormat类，覆写getRecordWriter()方法\n- 自定义RecordWriter类，覆写具体输出数据的方法write()\n\n#### 1.4 MR代码\n\n- 自定义OutputFormat\n\n```java\npackage com.kaikeba.hadoop.outputformat;\n\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.RecordWriter;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n *\n * 本例使用框架默认的Reducer，它将Mapper输入的kv对，原样输出；所以reduce输出的kv类型分别是Text, NullWritable\n * 自定义OutputFormat的类，泛型表示reduce输出的键值对类型；要保持一致;\n * map--(kv)-->reduce--(kv)-->OutputFormat\n */\npublic class MyOutPutFormat extends FileOutputFormat<Text, NullWritable> {\n\n    /**\n     * 两个输出文件;\n     * good用于保存好评文件；其它评级保存到bad中\n     * 根据实际情况修改path;node01及端口号8020\n     */\n    String bad = \"hdfs://node01:8020/outputformat/bad/r.txt\";\n    String good = \"hdfs://node01:8020/outputformat/good/r.txt\";\n\n    /**\n     *\n     * @param context\n     * @return\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    public RecordWriter<Text, NullWritable> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException {\n        //获得文件系统对象\n        FileSystem fs = FileSystem.get(context.getConfiguration());\n        //两个输出文件路径\n        Path badPath = new Path(bad);\n        Path goodPath = new Path(good);\n        FSDataOutputStream badOut = fs.create(badPath);\n        FSDataOutputStream goodOut = fs.create(goodPath);\n        return new MyRecordWriter(badOut,goodOut);\n    }\n\n    /**\n     * 泛型表示reduce输出的键值对类型；要保持一致\n     */\n    static class MyRecordWriter extends RecordWriter<Text, NullWritable>{\n\n        FSDataOutputStream badOut = null;\n        FSDataOutputStream goodOut = null;\n\n        public MyRecordWriter(FSDataOutputStream badOut, FSDataOutputStream goodOut) {\n            this.badOut = badOut;\n            this.goodOut = goodOut;\n        }\n\n        /**\n         * 自定义输出kv对逻辑\n         * @param key\n         * @param value\n         * @throws IOException\n         * @throws InterruptedException\n         */\n        @Override\n        public void write(Text key, NullWritable value) throws IOException, InterruptedException {\n            if (key.toString().split(\"\\t\")[9].equals(\"0\")){//好评\n                goodOut.write(key.toString().getBytes());\n                goodOut.write(\"\\r\\n\".getBytes());\n            }else{//其它评级\n                badOut.write(key.toString().getBytes());\n                badOut.write(\"\\r\\n\".getBytes());\n            }\n        }\n\n        /**\n         * 关闭流\n         * @param context\n         * @throws IOException\n         * @throws InterruptedException\n         */\n        @Override\n        public void close(TaskAttemptContext context) throws IOException, InterruptedException {\n            if(goodOut !=null){\n                goodOut.close();\n            }\n            if(badOut !=null){\n                badOut.close();\n            }\n        }\n    }\n}\n```\n\n- main方法\n\n```java\npackage com.kaikeba.hadoop.outputformat;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\nimport java.io.IOException;\n\npublic class MyOwnOutputFormatMain extends Configured implements Tool {\n\n    public int run(String[] args) throws Exception {\n        Configuration conf = super.getConf();\n        Job job = Job.getInstance(conf, MyOwnOutputFormatMain.class.getSimpleName());\n        job.setJarByClass(MyOwnOutputFormatMain.class);\n\n        //默认项，可以省略或者写出也可以\n        //job.setInputFormatClass(TextInputFormat.class);\n        //设置输入文件\n        TextInputFormat.addInputPath(job, new Path(args[0]));\n        job.setMapperClass(MyMapper.class);\n        //job.setMapOutputKeyClass(Text.class);\n        //job.setMapOutputValueClass(NullWritable.class);\n\n        //设置自定义的输出类\n        job.setOutputFormatClass(MyOutPutFormat.class);\n        //设置一个输出目录，这个目录会输出一个success的成功标志的文件\n        MyOutPutFormat.setOutputPath(job, new Path(args[1]));\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        //默认项，即默认有一个reduce任务，所以可以省略\n//        job.setNumReduceTasks(1);\n//        //Reducer将输入的键值对原样输出\n//        job.setReducerClass(Reducer.class);\n\n        boolean b = job.waitForCompletion(true);\n        return b ? 0: 1;\n    }\n\n    /**\n     *\n     * Mapper输出的key、value类型\n     * 文件每行的内容作为输出的key，对应Text类型\n     * 输出的value为null，对应NullWritable\n     */\n    public static class MyMapper extends Mapper<LongWritable, Text, Text, NullWritable> {\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            //把当前行内容作为key输出；value为null\n            context.write(value, NullWritable.get());\n        }\n    }\n\n    /**\n     *\n     * @param args /ordercomment.csv /ofo\n     * @throws Exception\n     */\n    public static void main(String[] args) throws Exception {\n        Configuration configuration = new Configuration();\n        ToolRunner.run(configuration, new MyOwnOutputFormatMain(), args);\n    }\n}\n```\n\n#### 1.5 总结\n\n- 自定义outputformat\n  - 泛型与reduce输出的键值对类型保持一致\n  - 覆写getRecordWriter()方法\n- 自定义RecordWriter\n  - 泛型与reduce输出的键值对类型保持一致\n  - 覆写具体输出数据的方法write()、close()\n\n- main方法\n  - job.setOutputFormatClass使用自定义在输出类\n\n\n\n### 2. 二次排序\n\n#### 2.1 需求\n\n- 数据：有一个简单的关于员工工资的记录文件salary.txt\n\n  - 每条记录如下，有3个字段，分别表示name、age、salary\n\n  - nancy\t22\t8000\n\n    ![](assets/Image201910181039.png)\n\n- 使用MR处理记录，实现结果中\n\n  - 按照工资从高到低的降序排序\n  - 若工资相同，则按年龄升序排序\n\n#### 2.2 逻辑分析\n\n- 利用MR中key具有可比较的特点\n\n- MapReduce中，根据key进行分区、排序、分组\n\n- 有些MR的输出的key可以直接使用hadoop框架的可序列化可比较类型表示，如Text、IntWritable等等，而这些类型本身是可比较的；如IntWritable默认升序排序\n\n  ![](assets/Image201909111209.png)\n\n- 但有时，使用MR编程，输出的key，若使用hadoop自带的key类型无法满足需求\n\n  - 此时，需要自定义的key类型（包含的是非单一信息，如此例包含工资、年龄）；\n  - 并且也得是**<font color='red'>可序列化、可比较的</font>**\n\n- 需要自定义key，定义排序规则\n\n  - 实现：按照人的salary降序排序，若相同，则再按age升序排序；若salary、age相同，则放入同一组\n\n#### 2.3 MR代码\n\n- 详见工程代码\n- 自定义key类型Person类\n\n```java\npackage com.kaikeba.hadoop.secondarysort;\n\nimport org.apache.hadoop.io.WritableComparable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n//根据输入文件格式，定义JavaBean，作为MR时，Map的输出key类型；要求此类可序列化、可比较\npublic class Person implements WritableComparable<Person> {\n    private String name;\n    private int age;\n    private int salary;\n\n    public Person() {\n    }\n\n    public Person(String name, int age, int salary) {\n        //super();\n        this.name = name;\n        this.age = age;\n        this.salary = salary;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    public void setAge(int age) {\n        this.age = age;\n    }\n\n    public int getSalary() {\n        return salary;\n    }\n\n    public void setSalary(int salary) {\n        this.salary = salary;\n    }\n\n    @Override\n    public String toString() {\n        return this.salary + \"  \" + this.age + \"    \" + this.name;\n    }\n\n    //两个Person对象的比较规则：①先比较salary，高的排序在前；②若相同，age小的在前\n    public int compareTo(Person other) {\n        int compareResult= this.salary - other.salary;\n        if(compareResult != 0) {//若两个人工资不同\n            //工资降序排序；即工资高的排在前边\n            return -compareResult;\n        } else {//若工资相同\n            //年龄升序排序；即年龄小的排在前边\n            return this.age - other.age;\n        }\n    }\n\n    //序列化，将NewKey转化成使用流传送的二进制\n    public void write(DataOutput dataOutput) throws IOException {\n        //注意：①使用正确的write方法；②记住此时的序列化的顺序，name、age、salary\n        dataOutput.writeUTF(name);\n        dataOutput.writeInt(age);\n        dataOutput.writeInt(salary);\n    }\n\n    //使用in读字段的顺序，要与write方法中写的顺序保持一致：name、age、salary\n    public void readFields(DataInput dataInput) throws IOException {\n        //read string\n        //注意：①使用正确的read方法；②读取顺序与write()中序列化的顺序保持一致\n        this.name = dataInput.readUTF();\n        this.age = dataInput.readInt();\n        this.salary = dataInput.readInt();\n    }\n}\n```\n\n- main类、mapper、reducer\n\n```java\npackage com.kaikeba.hadoop.secondarysort;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\nimport java.net.URI;\n\npublic class SecondarySort {\n\n\t/**\n\t *\n\t * @param args /salary.txt /secondarysort\n\t * @throws Exception\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\t\tConfiguration configuration = new Configuration();\n\t\t//configuration.set(\"mapreduce.job.jar\",\"/home/bruce/project/kkbhdp01/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n\t\tJob job = Job.getInstance(configuration, SecondarySort.class.getSimpleName());\n\n\t\tFileSystem fileSystem = FileSystem.get(URI.create(args[1]), configuration);\n\t\t//生产中慎用\n\t\tif (fileSystem.exists(new Path(args[1]))) {\n\t\t\tfileSystem.delete(new Path(args[1]), true);\n\t\t}\n\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tjob.setMapperClass(MyMap.class);\n\t\t//由于mapper与reducer输出的kv类型分别相同，所以，下两行可以省略\n//\t\tjob.setMapOutputKeyClass(Person.class);\n//\t\tjob.setMapOutputValueClass(NullWritable.class);\n\t\t\n\t\t//设置reduce的个数;默认为1\n\t\t//job.setNumReduceTasks(1);\n\n\t\tjob.setReducerClass(MyReduce.class);\n\t\tjob.setOutputKeyClass(Person.class);\n\t\tjob.setOutputValueClass(NullWritable.class);\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n\t\tjob.waitForCompletion(true);\n\n\t}\n\n\t//MyMap的输出key用自定义的Person类型；输出的value为null\n\tpublic static class MyMap extends Mapper<LongWritable, Text, Person, NullWritable> {\n\t\t@Override\n\t\tprotected void map(LongWritable key, Text value, Context context)\n\t\t\t\tthrows IOException, InterruptedException {\n\n\t\t\tString[] fields = value.toString().split(\"\\t\");\n\t\t\tString name = fields[0];\n\t\t\tint age = Integer.parseInt(fields[1]);\n\t\t\tint salary = Integer.parseInt(fields[2]);\n\t\t\t//在自定义类中进行比较\n\t\t\tPerson person = new Person(name, age, salary);\n\t\t\t//person对象作为输出的key\n\t\t\tcontext.write(person, NullWritable.get());\n\t\t}\n\t}\n\n\tpublic static class MyReduce extends Reducer<Person, NullWritable, Person, NullWritable> {\n\t\t@Override\n\t\tprotected void reduce(Person key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {\n\t\t\t//输入的kv对，原样输出\n\t\t\tcontext.write(key, NullWritable.get());\n\t\t}\n\t}\n}\n```\n\n#### 2.4 总结\n\n- 如果MR时，key的排序规则比较复杂，比如需要根据字段1排序，若字段1相同，则需要根据字段2排序...，此时，可以使用自定义key实现\n- 将自定义的key作为MR中，map输出的key的类型（reduce输入的类型）\n- 自定义的key\n  - 实现WritableComparable接口\n  - 实现compareTo比较方法\n  - 实现write序列化方法\n  - 实现readFields反序列化方法\n\n\n\n### 3. 自定义分组求topN\n\n#### 3.1 需求\n\n- 现有一个淘宝用户订单历史记录文件；每条记录有6个字段，分别表示\n\n  - userid、datetime、title商品标题、unitPrice商品单价、purchaseNum购买量、productId商品ID\n\n  ![](assets/Image201909111241.png)\n\n- 现使用MR编程，求出每个用户、每个月消费金额最多的两笔订单，花了多少钱\n\n  - 所以得相同用户、同一个年月的数据，分到同一组\n\n#### 3.2 逻辑分析\n\n- 根据文件格式，自定义JavaBean类OrderBean\n  - 实现WritableComparable接口\n  - 包含6个字段分别对应文件中的6个字段\n  - 重点实现compareTo方法\n    - 先比较userid是否相等；若不相等，则userid升序排序\n    - 若相等，比较两个Bean的日期是否相等；若不相等，则日期升序排序\n    - 若相等，再比较总开销，降序排序\n  - 实现序列化方法write()\n  - 实现反序列化方法readFields()\n- 自定义分区类\n  - 继承Partitioner类\n  - getPartiton()实现，userid相同的，处于同一个分区\n- 自定义Mapper类\n  - 输出key是当前记录对应的Bean对象\n  - 输出的value对应当前下单的总开销\n- 自定义分组类\n  - 决定userid相同、日期（年月）相同的记录，分到同一组中，调用一次reduce()\n- 自定义Reduce类\n  - reduce()中求出当前一组数据中，开销头两笔的信息\n- main方法\n  - job.setMapperClass\n  - job.setPartitionerClass\n  - job.setReducerClass\n  - job.setGroupingComparatorClass\n\n#### 3.3 MR代码\n\n> 详细代码见代码工程\n\n- OrderBean\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.WritableComparable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n//实现WritableComparable接口\npublic class OrderBean implements WritableComparable<OrderBean> {\n\n    //用户ID\n    private String userid;\n    //年月\n    //year+month -> 201408\n    private String datetime;\n    //标题\n    private String title;\n    //单价\n    private double unitPrice;\n    //购买量\n    private int purchaseNum;\n    //商品ID\n    private String produceId;\n\n    public OrderBean() {\n    }\n\n    public OrderBean(String userid, String datetime, String title, double unitPrice, int purchaseNum, String produceId) {\n        super();\n        this.userid = userid;\n        this.datetime = datetime;\n        this.title = title;\n        this.unitPrice = unitPrice;\n        this.purchaseNum = purchaseNum;\n        this.produceId = produceId;\n    }\n\n    //key的比较规则\n    public int compareTo(OrderBean other) {\n        //OrderBean作为MR中的key；如果对象中的userid相同，即ret1为0；就表示两个对象是同一个用户\n        int ret1 = this.userid.compareTo(other.userid);\n\n        if (ret1 == 0) {\n            //如果userid相同，比较年月\n            String thisYearMonth = this.getDatetime();\n            String otherYearMonth = other.getDatetime();\n            int ret2 = thisYearMonth.compareTo(otherYearMonth);\n\n            if(ret2 == 0) {//若datetime相同\n                //如果userid、年月都相同，比较单笔订单的总开销\n                Double thisTotalPrice = this.getPurchaseNum()*this.getUnitPrice();\n                Double oTotalPrice = other.getPurchaseNum()*other.getUnitPrice();\n                //总花销降序排序；即总花销高的排在前边\n                return -thisTotalPrice.compareTo(oTotalPrice);\n            } else {\n                //若datatime不同，按照datetime升序排序\n                return ret2;\n            }\n        } else {\n            //按照userid升序排序\n            return ret1;\n        }\n    }\n\n    /**\n     * 序列化\n     * @param dataOutput\n     * @throws IOException\n     */\n    public void write(DataOutput dataOutput) throws IOException {\n        dataOutput.writeUTF(userid);\n        dataOutput.writeUTF(datetime);\n        dataOutput.writeUTF(title);\n        dataOutput.writeDouble(unitPrice);\n        dataOutput.writeInt(purchaseNum);\n        dataOutput.writeUTF(produceId);\n    }\n\n    /**\n     * 反序列化\n     * @param dataInput\n     * @throws IOException\n     */\n    public void readFields(DataInput dataInput) throws IOException {\n        this.userid = dataInput.readUTF();\n        this.datetime = dataInput.readUTF();\n        this.title = dataInput.readUTF();\n        this.unitPrice = dataInput.readDouble();\n        this.purchaseNum = dataInput.readInt();\n        this.produceId = dataInput.readUTF();\n    }\n\n    /**\n     * 使用默认分区器，那么userid相同的，落入同一分区；\n     * 另外一个方案：此处不覆写hashCode方法，而是自定义分区器，getPartition方法中，对OrderBean的userid求hashCode值%reduce任务数\n     * @return\n     */\n//    @Override\n//    public int hashCode() {\n//        return this.userid.hashCode();\n//    }\n\n    @Override\n    public String toString() {\n        return \"OrderBean{\" +\n                \"userid='\" + userid + '\\'' +\n                \", datetime='\" + datetime + '\\'' +\n                \", title='\" + title + '\\'' +\n                \", unitPrice=\" + unitPrice +\n                \", purchaseNum=\" + purchaseNum +\n                \", produceId='\" + produceId + '\\'' +\n                '}';\n    }\n\n    public String getUserid() {\n        return userid;\n    }\n\n    public void setUserid(String userid) {\n        this.userid = userid;\n    }\n\n    public String getDatetime() {\n        return datetime;\n    }\n\n    public void setDatetime(String datetime) {\n        this.datetime = datetime;\n    }\n\n    public String getTitle() {\n        return title;\n    }\n\n    public void setTitle(String title) {\n        this.title = title;\n    }\n\n    public double getUnitPrice() {\n        return unitPrice;\n    }\n\n    public void setUnitPrice(double unitPrice) {\n        this.unitPrice = unitPrice;\n    }\n\n    public int getPurchaseNum() {\n        return purchaseNum;\n    }\n\n    public void setPurchaseNum(int purchaseNum) {\n        this.purchaseNum = purchaseNum;\n    }\n\n    public String getProduceId() {\n        return produceId;\n    }\n\n    public void setProduceId(String produceId) {\n        this.produceId = produceId;\n    }\n}\n\n```\n\n- MyPartitioner\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\n//mapper的输出key类型是自定义的key类型OrderBean；输出value类型是单笔订单的总开销double -> DoubleWritable\npublic class MyPartitioner extends Partitioner<OrderBean, DoubleWritable> {\n    @Override\n    public int getPartition(OrderBean orderBean, DoubleWritable doubleWritable, int numReduceTasks) {\n        //userid相同的，落入同一分区\n        return (orderBean.getUserid().hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n}\n\n```\n\n- MyMapper\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * 输出kv，分别是OrderBean、用户每次下单的总开销\n */\npublic class MyMapper extends Mapper<LongWritable, Text, OrderBean, DoubleWritable> {\n    DoubleWritable valueOut = new DoubleWritable();\n    DateUtils dateUtils = new DateUtils();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        //13764633023     2014-12-01 02:20:42.000 全视目Allseelook 原宿风暴显色美瞳彩色隐形艺术眼镜1片 拍2包邮    33.6    2       18067781305\n        String record = value.toString();\n        String[] fields = record.split(\"\\t\");\n        if(fields.length == 6) {\n            String userid = fields[0];\n            String datetime = fields[1];\n            String yearMonth = dateUtils.getYearMonthString(datetime);\n            String title = fields[2];\n            double unitPrice = Double.parseDouble(fields[3]);\n            int purchaseNum = Integer.parseInt(fields[4]);\n            String produceId = fields[5];\n\n            //生成OrderBean对象\n            OrderBean orderBean = new OrderBean(userid, yearMonth, title, unitPrice, purchaseNum, produceId);\n\n            //此订单的总开销\n            double totalPrice = unitPrice * purchaseNum;\n            valueOut.set(totalPrice);\n\n            context.write(orderBean, valueOut);\n        }\n    }\n}\n\n```\n\n- MyReducer\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\n//输出的key为userid拼接上年月的字符串，对应Text；输出的value对应单笔订单的金额\npublic class MyReducer extends Reducer<OrderBean, DoubleWritable, Text, DoubleWritable> {\n    /**\n     * ①由于自定义分组逻辑，相同用户、相同年月的订单是一组，调用一次reduce()；\n     * ②由于自定义的key类OrderBean中，比较规则compareTo规定，相同用户、相同年月的订单，按总金额降序排序\n     * 所以取出头两笔，就实现需求\n     * @param key\n     * @param values\n     * @param context\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    protected void reduce(OrderBean key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException {\n        //求每个用户、每个月、消费金额最多的两笔多少钱\n        int num = 0;\n        for(DoubleWritable value: values) {\n            if(num < 2) {\n                String keyOut = key.getUserid() + \"  \" + key.getDatetime();\n                context.write(new Text(keyOut), value);\n                num++;\n            } else {\n                break;\n            }\n        }\n\n    }\n}\n```\n\n- MyGroup\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\n\n//自定义分组类：reduce端调用reduce()前，对数据做分组；每组数据调用一次reduce()\npublic class MyGroup extends WritableComparator {\n\n    public MyGroup() {\n        //第一个参数表示key class\n        super(OrderBean.class, true);\n    }\n\n  \t// 注意： 分组实现的方法是这个\n  \t// compare（Object a,Object b） 这个方法不可以\n    //分组逻辑\n    @Override\n    public int compare(WritableComparable a, WritableComparable b) {\n        //userid相同，且同一月的分成一组\n        OrderBean aOrderBean = (OrderBean)a;\n        OrderBean bOrderBean = (OrderBean)b;\n\n        String aUserId = aOrderBean.getUserid();\n        String bUserId = bOrderBean.getUserid();\n\n        //userid、年、月相同的，作为一组\n        int ret1 = aUserId.compareTo(bUserId);\n        if(ret1 == 0) {//同一用户\n            //年月也相同返回0，在同一组；\n            return aOrderBean.getDatetime().compareTo(bOrderBean.getDatetime());\n        } else {\n            return ret1;\n        }\n    }\n}\n\n```\n\n- CustomGroupingMain\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport com.kaikeba.hadoop.wordcount.WordCountMain;\nimport com.kaikeba.hadoop.wordcount.WordCountMap;\nimport com.kaikeba.hadoop.wordcount.WordCountReduce;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\nimport java.io.IOException;\n\npublic class CustomGroupingMain extends Configured implements Tool {\n\n    ///tmall-201412-test.csv /cgo\n    public static void main(String[] args) throws Exception {\n        int exitCode = ToolRunner.run(new CustomGroupingMain(), args);\n        System.exit(exitCode);\n    }\n\n    @Override\n    public int run(String[] args) throws Exception {\n        //判断以下，输入参数是否是两个，分别表示输入路径、输出路径\n        if (args.length != 2 || args == null) {\n            System.out.println(\"please input Path!\");\n            System.exit(0);\n        }\n\n        Configuration configuration = new Configuration();\n        //告诉程序，要运行的jar包在哪\n        //configuration.set(\"mapreduce.job.jar\",\"/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n\n        //调用getInstance方法，生成job实例\n        Job job = Job.getInstance(configuration, CustomGroupingMain.class.getSimpleName());\n\n        //设置jar包，参数是包含main方法的类\n        job.setJarByClass(CustomGroupingMain.class);\n\n        //通过job设置输入/输出格式\n        //MR的默认输入格式是TextInputFormat，所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n        //设置输入/输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        //设置处理Map阶段的自定义的类\n        job.setMapperClass(MyMapper.class);\n        //设置map combine类，减少网路传出量\n        //job.setCombinerClass(MyReducer.class);\n        job.setPartitionerClass(MyPartitioner.class);\n        //设置处理Reduce阶段的自定义的类\n        job.setReducerClass(MyReducer.class);\n        job.setGroupingComparatorClass(MyGroup.class);\n\n        //如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n        job.setMapOutputKeyClass(OrderBean.class);\n        job.setMapOutputValueClass(DoubleWritable.class);\n\n        //设置reduce task最终输出key/value的类型\n        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(DoubleWritable.class);\n\n        // 提交作业\n        return job.waitForCompletion(true) ? 0 : 1;\n    }\n}\n\n```\n\n- DateUtils\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport java.time.LocalDateTime;\nimport java.time.format.DateTimeFormatter;\n\npublic class DateUtils {\n\n    public static void main(String[] args) {\n        //test1\n//        String str1 = \"13764633024  2014-10-01 02:20:42.000\";\n//        String str2 = \"13764633023  2014-11-01 02:20:42.000\";\n//        System.out.println(str1.compareTo(str2));\n\n        //test2\n//        String datetime = \"2014-12-01 02:20:42.000\";\n//        LocalDateTime localDateTime = parseDateTime(datetime);\n//        int year = localDateTime.getYear();\n//        int month = localDateTime.getMonthValue();\n//        int day = localDateTime.getDayOfMonth();\n//        System.out.println(\"year-> \" + year + \"; month -> \" + month + \"; day -> \" + day);\n\n        //test3\n//        String datetime = \"2014-12-01 02:20:42.000\";\n//        System.out.println(getYearMonthString(datetime));\n    }\n\n    public LocalDateTime parseDateTime(String dateTime) {\n        DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\");\n        LocalDateTime localDateTime = LocalDateTime.parse(dateTime, formatter);\n        return localDateTime;\n    }\n\n    //日期格式转换工具类：将2014-12-14 20:42:14.000转换成201412\n    public String getYearMonthString(String dateTime) {\n        DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\");\n        LocalDateTime localDateTime = LocalDateTime.parse(dateTime, formatter);\n        int year = localDateTime.getYear();\n        int month = localDateTime.getMonthValue();\n        return year + \"\" + month;\n    }\n\n\n\n}\n\n```\n\n\n\n#### 3.4 总结\n\n- 要实现自定义分组逻辑\n  - 一般会自定义JavaBean，作为map输出的key\n    - 实现其中的compareTo方法，设置key的比较逻辑\n    - 实现序列化方法write()\n    - 实现反序列化方法readFields()\n  - 自定义mapper类、reducer类\n  - 自定义partition类，getPartition方法，决定哪些key落入哪些分区\n  - 自定义group分组类，决定reduce阶段，哪些kv对，落入同一组，调用一次reduce()\n  - 写main方法，设置自定义的类\n    - job.setMapperClass\n    - job.setPartitionerClass\n    - job.setReducerClass\n    - job.setGroupingComparatorClass\n\n\n\n### 4. MapReduce数据倾斜(20分钟)\n\n- 什么是数据倾斜？\n  - 数据中不可避免地会出现离群值（outlier），并导致数据倾斜。这些离群值会显著地拖慢MapReduce的执行。\n- 常见的数据倾斜有以下几类：\n  - 数据频率倾斜——某一个区域的数据量要远远大于其他区域。比如某一个key对应的键值对远远大于其他键的键值对。\n  - 数据大小倾斜——部分记录的大小远远大于平均值。\n\n- 在map端和reduce端都有可能发生数据倾斜\n  - 在map端的数据倾斜可以考虑使用combine\n  - 在reduce端的数据倾斜常常来源于MapReduce的默认分区器\n\n- 数据倾斜会导致map和reduce的任务执行时间大为延长，也会让需要缓存数据集的操作消耗更多的内存资源\n\n#### 4.1 如何诊断是否存在数据倾斜（10分钟）\n\n2. 如何诊断哪些键存在数据倾斜？\n   - 发现倾斜数据之后，有必要诊断造成数据倾斜的那些键。有一个简便方法就是在代码里实现追踪每个键的**最大值**。\n   - 为了减少追踪量，可以设置数据量阀值，只追踪那些数据量大于阀值的键，并输出到日志中。实现代码如下\n   - 运行作业后就可以从日志中判断发生倾斜的键以及倾斜程度；跟踪倾斜数据是了解数据的重要一步，也是设计MapReduce作业的重要基础\n   \n```java\n   package com.kaikeba.hadoop.dataskew;\n   \n   import org.apache.hadoop.io.IntWritable;\n   import org.apache.hadoop.io.Text;\n   import org.apache.hadoop.mapreduce.Reducer;\n   import org.apache.log4j.Logger;\n   \n   import java.io.IOException;\n   \n   public class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {\n   \n       private int maxValueThreshold;\n   \n       //日志类\n       private static final Logger LOGGER = Logger.getLogger(WordCountReduce.class);\n   \n       @Override\n       protected void setup(Context context) throws IOException, InterruptedException {\n   \n           //一个键达到多少后，会做数据倾斜记录\n           maxValueThreshold = 10000;\n       }\n   \n       /*\n               (hello, 1)\n               (hello, 1)\n               (hello, 1)\n               ...\n               (spark, 1)\n   \n               key: hello\n               value: List(1, 1, 1)\n           */\n       public void reduce(Text key, Iterable<IntWritable> values,\n                             Context context) throws IOException, InterruptedException {\n           int sum = 0;\n           //用于记录键出现的次数\n           int i = 0;\n   \n           for (IntWritable count : values) {\n               sum += count.get();\n               i++;\n           }\n   \n           //如果当前键超过10000个，则打印日志\n           if(i > maxValueThreshold) {\n               LOGGER.info(\"Received \" + i + \" values for key \" + key);\n           }\n   \n           context.write(key, new IntWritable(sum));// 输出最终结果\n       };\n   }\n```\n\n\n\n\n#### 4.2 减缓数据倾斜\n\n- Reduce数据倾斜一般是指map的输出数据中存在数据频率倾斜的状况，即部分输出键的数据量远远大于其它的输出键\n\n- 如何减小reduce端数据倾斜的性能损失？常用方式有：\n  - 一、自定义分区\n\n    - 基于输出键的背景知识进行自定义分区。\n\n    - 例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的一部分reduce实例。而将其他的都发送给剩余的reduce实例。\n\n  - 二、Combine\n\n    - 使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。\n    - combine的目的就是聚合并精简数据。\n\n  - 三、抽样和范围分区\n\n    - Hadoop默认的分区器是HashPartitioner，基于map输出键的哈希值分区。这仅在数据分布比较均匀时比较好。**在有数据倾斜时就很有问题**。\n\n    - 使用分区器需要首先了解数据的特性。**TotalOrderPartitioner**中，可以通过对原始数据进行抽样得到的结果集来**预设分区边界值**。\n    - TotalOrderPartitioner中的范围分区器可以通过预设的分区边界值进行分区。因此它也可以很好地用在矫正数据中的部分键的数据倾斜问题。\n\n  - 四、数据大小倾斜的自定义策略\n\n    - 在map端或reduce端的数据大小倾斜都会对缓存造成较大的影响，乃至导致OutOfMemoryError异常。处理这种情况并不容易。可以参考以下方法。\n\n    - 设置mapreduce.input.linerecordreader.line.maxlength来限制RecordReader读取的最大长度。\n    - RecordReader在TextInputFormat和KeyValueTextInputFormat类中使用。默认长度没有上限。\n\n\n\n### 5. MR调优\n\n- 见后续文章\n\n\n\n\n\n### 6. 抽样、范围分区\n\n#### 6.1 数据\n\n   - 数据：气象站气象数据，来源美国国家气候数据中心（NCDC）（1900-2000年数据，每年一个文件）\n\n        - 气候数据record的格式如下\n\n\n![](assets/Image201907151554.png)\n\n#### 6.2 需求\n\n- 对气象数据，按照气温进行排序（气温符合正太分布）\n\n#### 6.3 实现方案\n\n- 三种实现思路\n\n  - 方案一：\n    - 设置一个分区，即一个reduce任务；在一个reduce中对结果进行排序；\n    - 失去了MR框架并行计算的优势\n  - 方案二：\n    - 自定义分区，人为指定各温度区间的记录，落入哪个分区；如分区温度边界值分别是-15、0、20，共4个分区\n    - 但由于对整个数据集的气温分布不了解，可能某些分区的数据量大，其它的分区小，数据倾斜\n  - 方案三：\n    - 通过对键空间采样\n    - 只查看一小部分键，获得键的近似分布（好温度的近似分布）\n    - 进而据此结果创建分区，实现尽可能的均匀的划分数据集；\n    - Hadoop内置了采样器；InputSampler\n\n#### 6.4 MR代码\n\n> 分两大步\n\n- 一、先将数据按气温对天气数据集排序。结果存储为sequencefile文件，气温作为输出键，数据行作为输出值\n\n- 代码\n\n  > 此代码处理原始日志文件\n  >\n  > 结果用SequenceFile格式存储；\n  >\n  > 温度作为SequenceFile的key；记录作为value\n\n``` java\npackage com.kaikeba.hadoop.totalorder;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n * 此代码处理原始日志文件 1901\n * 结果用SequenceFile格式存储；\n * 温度作为SequenceFile的key；记录作为value\n */\npublic class SortDataPreprocessor {\n\n  //输出的key\\value分别是气温、记录\n  static class CleanerMapper extends Mapper<LongWritable, Text, IntWritable, Text> {\n  \n    private NcdcRecordParser parser = new NcdcRecordParser();\n    private IntWritable temperature = new IntWritable();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context)\n        throws IOException, InterruptedException {\n      //0029029070999991901010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n      parser.parse(value);\n      if (parser.isValidTemperature()) {//是否是有效的记录\n        temperature.set(parser.getAirTemperature());\n        context.write(temperature, value);\n      }\n    }\n  }\n\n\n  //两个参数：/ncdc/input /ncdc/sfoutput\n  public static void main(String[] args) throws Exception {\n\n    if (args.length != 2) {\n      System.out.println(\"<input> <output>\");\n    }\n\n    Configuration conf = new Configuration();\n\n    Job job = Job.getInstance(conf, SortDataPreprocessor.class.getSimpleName());\n    job.setJarByClass(SortDataPreprocessor.class);\n    //\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n    job.setMapperClass(CleanerMapper.class);\n    //最终输出的键、值类型\n    job.setOutputKeyClass(IntWritable.class);\n    job.setOutputValueClass(Text.class);\n    //reduce个数为0\n    job.setNumReduceTasks(0);\n    //以sequencefile的格式输出\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    //开启job输出压缩功能\n    //方案一\n    conf.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n    conf.set(\"mapreduce.output.fileoutputformat.compress.type\",\"RECORD\");\n    //指定job输出使用的压缩算法\n    conf.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n\n    //方案二\n    //设置sequencefile的压缩、压缩算法、sequencefile文件压缩格式block\n    //SequenceFileOutputFormat.setCompressOutput(job, true);\n    //SequenceFileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);\n    //SequenceFileOutputFormat.setOutputCompressorClass(job, SnappyCodec.class);\n    //SequenceFileOutputFormat.setOutputCompressionType(job, SequenceFile.CompressionType.BLOCK);\n\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n```\n\n- 二、全局排序\n\n  > 使用全排序分区器TotalOrderPartitioner\n  >\n\n```java\npackage com.kaikeba.hadoop.totalorder;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.filecache.DistributedCache;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.partition.InputSampler;\nimport org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner;\n\nimport java.net.URI;\n\n/**\n * 使用TotalOrderPartitioner全局排序一个SequenceFile文件的内容；\n * 此文件是SortDataPreprocessor的输出文件；\n * key是IntWritble，气象记录中的温度\n */\npublic class SortByTemperatureUsingTotalOrderPartitioner{\n\n  /**\n   * 两个参数：/ncdc/sfoutput /ncdc/totalorder\n   * 第一个参数是SortDataPreprocessor的输出文件\n   */\n  public static void main(String[] args) throws Exception {\n    if (args.length != 2) {\n      System.out.println(\"<input> <output>\");\n    }\n\n    Configuration conf = new Configuration();\n\n    Job job = Job.getInstance(conf, SortByTemperatureUsingTotalOrderPartitioner.class.getSimpleName());\n    job.setJarByClass(SortByTemperatureUsingTotalOrderPartitioner.class);\n\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n    //输入文件是SequenceFile\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    //Hadoop提供的方法来实现全局排序，要求Mapper的输入、输出的key必须保持类型一致\n    job.setOutputKeyClass(IntWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    //分区器：全局排序分区器\n    job.setPartitionerClass(TotalOrderPartitioner.class);\n\n    //分了3个区；且分区i-1中的key小于i分区中所有的键\n    job.setNumReduceTasks(3);\n\n    /**\n     * 随机采样器从所有的分片中采样\n     * 每一个参数：采样率；\n     * 第二个参数：总的采样数\n     * 第三个参数：采样的最大分区数；\n     * 只要numSamples和maxSplitSampled（第二、第三参数）任一条件满足，则停止采样\n     */\n    InputSampler.Sampler<IntWritable, Text> sampler =\n            new InputSampler.RandomSampler<IntWritable, Text>(0.1, 5000, 10);\n//    TotalOrderPartitioner.setPartitionFile();\n    /**\n     * 存储定义分区的键；即整个数据集中温度的大致分布情况；\n     * 由TotalOrderPartitioner读取，作为全排序的分区依据，让每个分区中的数据量近似\n     */\n    InputSampler.writePartitionFile(job, sampler);\n\n    //根据上边的SequenceFile文件（包含键的近似分布情况），创建分区\n    String partitionFile = TotalOrderPartitioner.getPartitionFile(job.getConfiguration());\n    URI partitionUri = new URI(partitionFile);\n\n//    JobConf jobConf = new JobConf();\n\n    //与所有map任务共享此文件，添加到分布式缓存中\n    DistributedCache.addCacheFile(partitionUri, job.getConfiguration());\n//    job.addCacheFile(partitionUri);\n\n    //方案一：输出的文件RECORD级别，使用BZip2Codec进行压缩\n    conf.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n    conf.set(\"mapreduce.output.fileoutputformat.compress.type\",\"RECORD\");\n    //指定job输出使用的压缩算法\n    conf.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n\n    //方案二\n    //SequenceFileOutputFormat.setCompressOutput(job, true);\n    //SequenceFileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);\n    //SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);\n\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n\n```\n\n#### 6.5 总结\n\n- 对大量数据进行全局排序\n\n  - 先使用InputSampler.Sampler采样器，对整个key空间进行采样，得到key的近似分布\n\n  - 保存到key分布情况文件中\n\n  - 使用TotalOrderPartitioner，利用上边的key分布情况文件，进行分区；每个分区的数据量近似，从而防止数据倾斜\n\n\n\n## 注意\n\n1. 描述MR的shuffle全流程（面试）\n2. 搭建MAVEN工程，统计词频，并提交集群运行，查看结果\n3. 利用搜狗数据，找出所有独立的uid并写入HDFS\n4. 利用搜狗数据，找出所有独立的uid出现次数，并写入HDFS，并要求使用Map端的Combine操作\n5. 谈谈什么是数据倾斜，什么情况会造成数据倾斜？（面试）\n6. 对MR数据倾斜，如何解决？（面试）\n  ","tags":["hadoop","MapReduce"]},{"title":"MapReduce编程（二）","url":"/2019/10/20/hadoop/MapReduce编程（二）/","content":"\n# MapReduce编程模型（二）\n\n### 1. 自定义分区\n\n#### 1.1 分区原理\n\n- 根据之前讲的shuffle，我们知道在map任务中，从环形缓冲区溢出写磁盘时，会先对kv对数据进行分区操作\n\n- 分区操作是由MR中的分区器负责的\n\n- MapReduce有自带的默认分区器\n\n  - **HashPartitioner**\n  - 关键方法getPartition返回当前键值对的**分区索引**(partition index)\n\n  ```java\n  public class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n  \n    public void configure(JobConf job) {}\n  \n    /** Use {@link Object#hashCode()} to partition. */\n    public int getPartition(K2 key, V2 value, int numReduceTasks) {\n      return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n  }\n  ```\n\n- 环形缓冲区溢出写磁盘前，将每个kv对，作为getPartition()的参数传入；\n\n- 先对键值对中的key求hash值（int类型），与MAX_VALUE按位与；再模上reduce task个数，假设reduce task个数设置为4（可在程序中使用job.setNumReduceTasks(4)指定reduce task个数为4）\n\n  - 那么map任务溢出文件有**4个分区**，分区index分别是0、1、2、3\n  - getPartition()结果有四种：0、1、2、3\n  - 根据计算结果，决定当前kv对，落入哪个分区，如结果是0，则当前kv对落入溢出文件的0分区中\n  - 最终被相应的reduce task通过http获得\n\n![](assets/Image201906280826.png)\n\n![](assets/Image201906272145.png)\n\n- 若是MR默认分区器，不满足需求；可根据业务逻辑，设计自定义分区器，比如实现图上的功能\n\n#### 1.2 默认分区\n\n> 程序执行略\n>\n> 代码详见工程com.kaikeba.hadoop.partitioner包\n\n- MR读取三个文件part1.txt、part2.txt、part3.txt；三个文件放到HDFS目录：/customParttitioner中\n\n  ![](assets/Image201909061640.png)\n  \n- part1.txt内容如下：\n\n  ```\n  Dear Bear River\n  Dear Car\n  ```\n  \n- part2.txt内容如下：\n\n  ```\n  Car Car River\n  Dear Bear\n  ```\n\n- part3.txt内容如下：\n\n  ```\n  Dear Car Bear\n  Car Car\n  ```\n\n- 默认HashPartitioner分区时，查看结果（看代码）\n\n![](assets/Image201906272204.png)\n\n- 运行参数：\n\n```shell\n/customParttitioner /cp01\n```\n\n- 打jar包运行，结果如下：\n\n![](assets/Image201906272210.png)\n\n> 只有part-r-00001、part-r-00003有数据；另外两个没有数据\n>\n> HashPartitioner将Bear分到index=1的分区；将Car|Dear|River分到index=3分区\n\n#### 1.3 自定义分区\n\n**1.3.1** 需求\n\n- 自定义分区，使得文件中，分别以Dear、Bear、River、Car为键的键值对，分别落到index是0、1、2、3的分区中\n\n**1.3.2** 逻辑分析\n\n- 若要实现以上的分区策略，需要自定义分区类\n  - 此类实现Partitioner接口\n  - 在getPartition()中实现分区逻辑\n- main方法中\n  - **设定reduce个数**为4\n  - 设置自定义的分区类，调用job.setPartitionerClass方法\n\n**1.3.3** MR代码\n\n> 完整代码见代码工程\n\n- 自定义分区类如下\n\n```java\npackage com.kaikeba.hadoop.partitioner;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\nimport java.util.HashMap;\n\npublic class CustomPartitioner extends Partitioner<Text, IntWritable> {\n    public static HashMap<String, Integer> dict = new HashMap<String, Integer>();\n\n    //定义每个键对应的分区index，使用map数据结构完成\n    static{\n        dict.put(\"Dear\", 0);\n        dict.put(\"Bear\", 1);\n        dict.put(\"River\", 2);\n        dict.put(\"Car\", 3);\n    }\n\n    public int getPartition(Text text, IntWritable intWritable, int i) {\n        //\n        int partitionIndex = dict.get(text.toString());\n        return partitionIndex;\n    }\n}\n```\n\n\n\n![](assets/Image201906272213.png)\n\n- 运行结果\n\n![](assets/Image201906272217.png)\n\n> 结果满足需求\n\n#### 1.4 总结\n\n- 如果默认分区器不满足业务需求，可以自定义分区器\n  - 自定义分区器的类继承Partitioner类\n  - 覆写getPartition()，在方法中，定义自己的分区策略\n  - 在main()方法中调用job.setPartitionerClass()\n  - main()中设置reduce任务数\n\n\n\n### 2. 自定义Combiner\n\n#### 2.1 需求\n\n- 普通的MR是reduce通过http，取得map任务的分区结果；具体的聚合出结果是在reduce端进行的；\n\n- 以单词计数为例：\n  - 下图中的第一个map任务(map1)，本地磁盘中的结果有5个键值对：(Dear, 1)、(Bear, 1)、(River, 1)、(Dear, 1)、(Car, 1)\n  - 其中，map1中的两个相同的键值对(Dear, 1)、(Dear, 1)，会被第一个reduce任务(reduce1)通过网络拉取到reduce1端\n  - 那么假设map1中(Dear, 1)有1亿个呢？按原思路，map1端需要存储1亿个(Dear, 1)，再将1亿个(Dear, 1)通过网络被reduce1获得，然后再在reduce1端汇总\n  - 这样做map端本地磁盘IO、数据从map端到reduce端传输的网络IO比较大\n  - 那么想，能不能在reduce1从map1拉取1亿个(Dear, 1)之前，在map端就提前先做下reduce汇总，得到结果(Dear, 100000000)，然后再将这个结果（一个键值对）传输到reduce1呢？\n  - 答案是可以的\n  - 我们称之为combine操作\n  \n- map端combine本地聚合（**本质是reduce**）\n\n  ![](assets/Image201906280906.png)\n\n#### 2.2 逻辑分析\n\n- **<font color='red'>注意：</font>**\n\n  - **不论运行多少次Combine操作，都不能影响最终的结果**\n\n  - **并非**所有的mr都适合combine操作，比如求平均值 \n\n    **参考：《并非所有MR都适合combine.txt》**\n\n- 原理图\n\n  > 看原图\n\n![](assets/Image201909091014.png)\n\n- 当每个map任务的环形缓冲区添满80%，开始溢写磁盘文件\n\n- 此过程会分区、每个分区内按键排序、再combine操作（若设置了combine的话）、若设置map输出压缩的话则再压缩\n\n  - 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作；\n  - 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销）\n\n- combine本质上也是reduce；因为自定义的combine类继承自Reducer父类\n\n- map: (K1, V1) -> list(K2, V2)\n\n- combiner: (K2, list(V2)) -> (K2, V2)\n\n- reduce: (K2, list(V2)) -> (K3, V3)\n\n  - reduce函数与combine函数通常是一样的\n  - K3与K2类型相同；\n  - V3与V2类型相同\n  - 即reduce的输入的kv类型分别与输出的kv类型相同\n  \n  \n\n#### 2.3 MR代码\n\n> 对原词频统计代码做修改；\n>\n> 详细代码见代码工程\n\n- WordCountMap、WordCountReduce代码保持不变\n- 唯一需要做的修改是在WordCountMain中，增加job.**setCombinerClass**(WordCountReduce.class);\n- 修改如下：\n\n![](assets/Image201906272006.png)\n\n#### 2.4 小结\n\n- 使用combine时，首先考虑当前MR是否适合combine\n- 总原则是不论使不使用combine不能影响最终的结果\n- 在MR时，发生数据倾斜，且可以使用combine时，可以使用combine缓解数据倾斜\n\n\n\n### 3. MR压缩\n\n#### 3.1 需求\n\n- 作用：在MR中，为了减少磁盘IO及网络IO，可考虑在map端、reduce端设置压缩功能\n- 给“MapReduce编程：用户搜索次数”代码，增加压缩功能\n\n#### 3.2 逻辑分析\n\n- 那么如何设置压缩功能呢？只需在main方法中，给Configuration对象增加如下设置即可\n\n\n```java\n//开启map输出进行压缩的功能\nconfiguration.set(\"mapreduce.map.output.compress\", \"true\");\n//设置map输出的压缩算法是：BZip2Codec，它是hadoop默认支持的压缩算法，且支持切分\nconfiguration.set(\"mapreduce.map.output.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n//开启job输出压缩功能\nconfiguration.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n//指定job输出使用的压缩算法\nconfiguration.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n```\n\n#### 3.3 MR代码\n\n- 给“MapReduce编程：用户搜索次数”代码，增加压缩功能，代码如下\n\n  > 如何打jar包，已演示过，此处不再赘述\n\n```java\npackage com.kaikeba.hadoop.mrcompress;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n * 本MR示例，用于统计每个用户搜索并查看URL链接的次数\n */\npublic class UserSearchCount {\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        //判断以下，输入参数是否是两个，分别表示输入路径、输出路径\n        if (args.length != 2 || args == null) {\n            System.out.println(\"please input Path!\");\n            System.exit(0);\n        }\n\n        Configuration configuration = new Configuration();\n        //configuration.set(\"mapreduce.job.jar\",\"/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n        //开启map输出进行压缩的功能\n        configuration.set(\"mapreduce.map.output.compress\", \"true\");\n        //设置map输出的压缩算法是：BZip2Codec，它是hadoop默认支持的压缩算法，且支持切分\n        configuration.set(\"mapreduce.map.output.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n        //开启job输出压缩功能\n        configuration.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n        //指定job输出使用的压缩算法\n        configuration.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n\n        //调用getInstance方法，生成job实例\n        Job job = Job.getInstance(configuration, UserSearchCount.class.getSimpleName());\n\n        //设置jar包，参数是包含main方法的类\n        job.setJarByClass(UserSearchCount.class);\n\n        //通过job设置输入/输出格式\n        //MR的默认输入格式是TextInputFormat，所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n        //设置输入/输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n//        FileOutputFormat.setCompressOutput(job, true);\n//        FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);\n\n        //设置处理Map阶段的自定义的类\n        job.setMapperClass(SearchCountMapper.class);\n        //设置map combine类，减少网路传出量\n        //job.setCombinerClass(WordCountReduce.class);\n        //设置处理Reduce阶段的自定义的类\n        job.setReducerClass(SearchCountReducer.class);\n\n        //如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n//        job.setMapOutputKeyClass(Text.class);\n//        job.setMapOutputValueClass(IntWritable.class);\n\n        //设置reduce task最终输出key/value的类型\n        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // 提交作业\n        job.waitForCompletion(true);\n    }\n\n    public static class SearchCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n        //定义共用的对象，减少GC压力\n        Text userIdKOut = new Text();\n        IntWritable vOut = new IntWritable(1);\n\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            //获得当前行的数据\n            //样例数据：20111230111645  169796ae819ae8b32668662bb99b6c2d        塘承高速公路规划线路图  1       1       http://auto.ifeng.com/roll/20111212/729164.shtml\n            String line = value.toString();\n\n            //切分，获得各字段组成的数组\n            String[] fields = line.split(\"\\t\");\n\n            //因为要统计每个user搜索并查看URL的次数，所以将userid放到输出key的位置\n            //注意：MR编程中，根据业务需求设计key是很重要的能力\n            String userid = fields[1];\n\n            //设置输出的key的值\n            userIdKOut.set(userid);\n            //输出结果\n            context.write(userIdKOut, vOut);\n        }\n    }\n\n    public static class SearchCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n        //定义共用的对象，减少GC压力\n        IntWritable totalNumVOut = new IntWritable();\n\n        @Override\n        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n            int sum = 0;\n\n            for(IntWritable value: values) {\n                sum += value.get();\n            }\n\n            //设置当前user搜索并查看总次数\n            totalNumVOut.set(sum);\n            context.write(key, totalNumVOut);\n        }\n    }\n}\n```\n\n- 生成jar包，并运行jar包\n\n```shell\n[hadoop@node01 target]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.mrcompress.UserSearchCount /sogou.2w.utf8 /compressed\n```\n\n- 查看结果\n\n  > 可增加数据量，查看使用压缩算法前后的系统各计数器的数据量变化\n\n```shell\n[hadoop@node01 target]$ hadoop fs -ls -h /compressed\n```\n\n![](assets/Image201908241707.png)\n\n#### 3.4 总结\n\n- MR过程中使用压缩可减少数据量，进而减少磁盘IO、网络IO数据量\n- 可设置map端输出的压缩\n- 可设置job最终结果的压缩\n- 通过相应的配置项即可实现\n\n\n\n### 4. 自定义InputFormat\n\n#### 4.1 MapReduce执行过程\n\n![](assets/Image201905211621.png)\n\n- 上图也描述了mapreduce的一个完整的过程；我们主要看map任务是如何从hdfs读取分片数据的部分\n\n  - 涉及3个关键的类\n\n  - ①InputFormat输入格式类\n    \n    ②InputSplit输入分片类：getSplits()\n    \n    - InputFormat输入格式类将输入文件分成一个个分片InputSplit\n    - 每个Map任务对应一个split分片\n    \n    ③RecordReader记录读取器类：createRecordReader()\n    \n    - RecordReader（记录读取器）读取分片数据，一行记录生成一个键值对\n    - 传入map任务的map()方法，调用map()\n    \n    ![](assets/Image201910161117.png)\n    \n    \n\n- 所以，如果需要根据自己的业务情况，自定义输入的话，需要自定义两个类：\n  - InputFormat类\n  - RecordReader类\n\n- 详细流程：\n\n  - 客户端调用InputFormat的**getSplits()**方法，获得输入文件的分片信息\n\n    ![](assets/Image201909111008.png)\n\n  - 针对每个MR job会生成一个相应的app master，负责map\\reduce任务的调度及监控执行情况\n\n  - 将分片信息传递给MR job的app master\n\n  - app master根据分片信息，尽量将map任务尽量调度在split分片数据所在节点（**移动计算不移动数据**）\n\n    ![](assets/Image201909111013.png)\n\n  - 有几个分片，就生成几个map任务\n  \n  - 每个map任务将split分片传递给createRecordReader()方法，生成此分片对应的RecordReader\n  \n  - RecordReader用来读取分片的数据，生成记录的键值对\n  \n    - nextKeyValue()判断是否有下一个键值对，如果有，返回true；否则，返回false\n    - 如果返回true，调用getCurrentKey()获得当前的键\n    - 调用getCurrentValue()获得当前的值\n  \n  - map任务运行过程\n  \n    ![](assets/Image201909111022.png)\n  \n    - map任务运行时，会调用run()\n  \n    - 首先运行一次setup()方法；只在map任务启动时，运行一次；一些初始化的工作可以在setup方法中完成；如要连接数据库之类的操作\n  \n    - while循环，调用context.nextKeyValue()；会委托给RecordRecord的nextKeyValue()，判断是否有下一个键值对\n  \n    - 如果有下一个键值对，调用context.getCurrentKey()、context.getCurrentValue()获得当前的键、值的值（也是调用RecordReader的同名方法）\n  \n      ![](assets/Image201909111045.png)\n  \n    - 作为参数传入map(key, value, context)，调用一次map()\n  \n    - 当读取分片尾，context.nextKeyValue()返回false；退出循环\n  \n    - 调用cleanup()方法，只在map任务结束之前，调用一次；所以，一些回收资源的工作可在此方法中实现，如关闭数据库连接\n\n#### 4.2 需求\n\n- 无论hdfs还是mapreduce，处理小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案\n\n#### 4.3 逻辑分析\n\n- 小文件的优化无非以下几种方式：\n  - 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS(SequenceFile方案)\n  - 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并；可使用**自定义InputFormat**实现\n  - 在mapreduce处理时，可采用**CombineFileInputFormat**提高效率\n- 本例使用第二种方案，自定义输入格式\n\n#### 4.4 MR代码\n\n- 自定义InputFormat\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.JobContext;\n  import org.apache.hadoop.mapreduce.RecordReader;\n  import org.apache.hadoop.mapreduce.TaskAttemptContext;\n  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n  \n  import java.io.IOException;\n  \n  /**\n   * 自定义InputFormat类；\n   * 泛型：\n   *  键：因为不需要使用键，所以设置为NullWritable\n   *  值：值用于保存小文件的内容，此处使用BytesWritable\n   */\n  public class WholeFileInputFormat extends FileInputFormat<NullWritable, BytesWritable> {\n  \n      /**\n       *\n       * 返回false，表示输入文件不可切割\n       * @param context\n       * @param file\n       * @return\n       */\n      @Override\n      protected boolean isSplitable(JobContext context, Path file) {\n          return false;\n      }\n  \n      /**\n       * 生成读取分片split的RecordReader\n       * @param split\n       * @param context\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public RecordReader<NullWritable, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException,InterruptedException {\n          //使用自定义的RecordReader类\n          WholeFileRecordReader reader = new WholeFileRecordReader();\n          //初始化RecordReader\n          reader.initialize(split, context);\n          return reader;\n      }\n  }\n  ```\n\n- 自定义RecordReader\n\n  实现6个相关方法\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.conf.Configuration;\n  import org.apache.hadoop.fs.FSDataInputStream;\n  import org.apache.hadoop.fs.FileSystem;\n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.IOUtils;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.RecordReader;\n  import org.apache.hadoop.mapreduce.TaskAttemptContext;\n  import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n  \n  import java.io.IOException;\n  \n  /**\n   *\n   * RecordReader的核心工作逻辑：\n   * 通过nextKeyValue()方法去读取数据构造将返回的key   value\n   * 通过getCurrentKey 和 getCurrentValue来返回上面构造好的key和value\n   *\n   * @author\n   */\n  public class WholeFileRecordReader extends RecordReader<NullWritable, BytesWritable> {\n  \n      //要读取的分片\n      private FileSplit fileSplit;\n      private Configuration conf;\n  \n      //读取的value数据\n      private BytesWritable value = new BytesWritable();\n      /**\n       *\n       * 标识变量，分片是否已被读取过；因为小文件设置成了不可切分，所以一个小文件只有一个分片；\n       * 而这一个分片的数据，只读取一次，一次读完所有数据\n       * 所以设置此标识\n       */\n      private boolean processed = false;\n  \n      /**\n       * 初始化\n       * @param split\n       * @param context\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public void initialize(InputSplit split, TaskAttemptContext context)\n              throws IOException, InterruptedException {\n          this.fileSplit = (FileSplit) split;\n          this.conf = context.getConfiguration();\n      }\n  \n      /**\n       * 判断是否有下一个键值对。若有，则读取分片中的所有的数据\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public boolean nextKeyValue() throws IOException, InterruptedException {\n          if (!processed) {\n              byte[] contents = new byte[(int) fileSplit.getLength()];\n              Path file = fileSplit.getPath();\n              FileSystem fs = file.getFileSystem(conf);\n              FSDataInputStream in = null;\n              try {\n                  in = fs.open(file);\n                  IOUtils.readFully(in, contents, 0, contents.length);\n                  value.set(contents, 0, contents.length);\n              } finally {\n                  IOUtils.closeStream(in);\n              }\n              processed = true;\n              return true;\n          }\n          return false;\n      }\n  \n      /**\n       * 获得当前的key\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public NullWritable getCurrentKey() throws IOException,\n              InterruptedException {\n          return NullWritable.get();\n      }\n  \n      /**\n       * 获得当前的value\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public BytesWritable getCurrentValue() throws IOException,\n              InterruptedException {\n          return value;\n      }\n  \n      /**\n       * 获得分片读取的百分比；因为如果读取分片数据的话，会一次性的读取完；所以进度要么是1，要么是0\n       * @return\n       * @throws IOException\n       */\n      @Override\n      public float getProgress() throws IOException {\n          //因为一个文件作为一个整体处理，所以，如果processed为true，表示已经处理过了，进度为1；否则为0\n          return processed ? 1.0f : 0.0f;\n      }\n  \n      @Override\n      public void close() throws IOException {\n      }\n  }\n  ```\n\n- main方法\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.conf.Configuration;\n  import org.apache.hadoop.conf.Configured;\n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.io.Text;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.Job;\n  import org.apache.hadoop.mapreduce.Mapper;\n  import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n  import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\n  import org.apache.hadoop.util.Tool;\n  import org.apache.hadoop.util.ToolRunner;\n  \n  import java.io.IOException;\n  \n  /**\n   * 让主类继承Configured类，实现Tool接口\n   * 实现run()方法\n   * 将以前main()方法中的逻辑，放到run()中\n   * 在main()中，调用ToolRunner.run()方法，第一个参数是当前对象；第二个参数是输入、输出\n   */\n  public class SmallFiles2SequenceFile extends Configured implements Tool {\n  \n      /**\n       * 自定义Mapper类\n       * mapper类的输入键值对类型，与自定义InputFormat的输入键值对保持一致\n       * mapper类的输出的键值对类型，分别是文件名、文件内容\n       */\n      static class SequenceFileMapper extends\n              Mapper<NullWritable, BytesWritable, Text, BytesWritable> {\n  \n          private Text filenameKey;\n  \n          /**\n           * 取得文件名\n           * @param context\n           * @throws IOException\n           * @throws InterruptedException\n           */\n          @Override\n          protected void setup(Context context) throws IOException,\n                  InterruptedException {\n              InputSplit split = context.getInputSplit();\n              //获得当前文件路径\n              Path path = ((FileSplit) split).getPath();\n              filenameKey = new Text(path.toString());\n          }\n  \n          @Override\n          protected void map(NullWritable key, BytesWritable value,\n                             Context context) throws IOException, InterruptedException {\n              context.write(filenameKey, value);\n          }\n      }\n  \n      public int run(String[] args) throws Exception {\n          Configuration conf = new Configuration();\n          Job job = Job.getInstance(conf,\"combine small files to sequencefile\");\n          job.setJarByClass(SmallFiles2SequenceFile.class);\n  \n          //设置自定义输入格式\n          job.setInputFormatClass(WholeFileInputFormat.class);\n  \n          WholeFileInputFormat.addInputPath(job,new Path(args[0]));\n          //设置输出格式SequenceFileOutputFormat及输出路径\n          job.setOutputFormatClass(SequenceFileOutputFormat.class);\n          SequenceFileOutputFormat.setOutputPath(job,new Path(args[1]));\n  \n          job.setOutputKeyClass(Text.class);\n          job.setOutputValueClass(BytesWritable.class);\n          job.setMapperClass(SequenceFileMapper.class);\n          return job.waitForCompletion(true) ? 0 : 1;\n      }\n  \n      public static void main(String[] args) throws Exception {\n          int exitCode = ToolRunner.run(new SmallFiles2SequenceFile(),\n                  args);\n          System.exit(exitCode);\n  \n      }\n  }\n  ```\n\n#### 4.5 总结\n\n- 若要自定义InputFormat的话\n  - 需要自定义InputFormat类，并覆写getRecordReader()方法\n  - 自定义RecordReader类，实现方法\n    - initialize()\n    - nextKeyValue()\n    - getCurrentKey()\n    - getCurrentValue()\n    - getProgress()\n    - close()\n\n\n\n## 5、拓展点、未来计划、行业趋势\n\n1. MR中还有一些自带的输入格式，扩展阅读：《Hadoop权威指南 第4版》8.2 输入格式\n\n   ![](assets/Image201909091251.png)\n\n   \n","tags":["hadoop","MapReduce"]},{"title":"MapRedecer编程（一）","url":"/2019/10/15/hadoop/MapRedecer编程（一）/","content":"\n# MapReduce编程模型\n\n\n## 一、知识要点\n\n### 1. MapReduce编程模型\n\n- Hadoop架构图\n\n  Hadoop由HDFS分布式存储、**MapReduce分布式计算**、Yarn资源调度三部分组成\n\n![](assets/Image201906191834-1562922704761.png)\n\n- MapReduce是采用一种**分而治之**的思想设计出来的分布式计算框架\n- MapReduce由两个阶段组成：\n  - Map阶段（切分成一个个小的任务）\n  - Reduce阶段（汇总小任务的结果）\n- 那什么是分而治之呢？\n  - 比如一复杂、计算量大、耗时长的的任务，暂且称为“大任务”；\n  - 此时使用单台服务器无法计算或较短时间内计算出结果时，可将此大任务切分成一个个小的任务，小任务分别在不同的服务器上**并行**的执行\n  - 最终再汇总每个小任务的结果\n\n![](assets/Image201906251747.png)\n\n#### 1.1 Map阶段\n\n- map阶段有一个关键的map()函数；\n- 此函数的输入是**键值对**\n- 输出是一系列**键值对**，输出写入**本地磁盘**。\n\n#### 1.2 Reduce阶段\n\n- reduce阶段有一个关键的函数reduce()函数\n\n- 此函数的输入也是键值对（即map的输出（kv对））\n\n- 输出也是一系列键值对，结果最终写入HDFS\n\n#### 1.3 Map&Reduce\n\n![](assets/Image201906251807.png)\n\n\n\n### 2. MapReduce编程示例\n\n- 以**MapReduce的词频统计**为例：统计一批英文文章当中，每个单词出现的总次数\n\n#### 2.1 MapReduce原理图\n\n![](assets/Image201906271715.png)\n\n- Map阶段\n  - 假设MR的输入文件“**Gone With The Wind**”有三个block；block1、block2、block3 \n  - MR编程时，每个block对应一个分片split\n  - 每一个split对应一个map任务（map task）\n  - 如图共3个map任务（map1、map2、map3）；这3个任务的逻辑一样，所以以第一个map任务（map1）为例分析 \n  - map1读取block1的数据；一次读取block1的一行数据；\n    - 产生键值对(key/value)，作为map()的参数传入，调用map()；\n    - 假设当前所读行是第一行\n    - 将当前所读行的行首相对于当前block开始处的字节偏移量作为key（0）\n    - 当前行的内容作为value（Dear Bear River）\n  - map()内\n    - (按需求，写业务代码)，将value当前行内容按空格切分，得到三个单词Dear | Bear | River\n    - 将每个单词变成键值对，输出出去(Dear, 1) | (Bear, 1) | (River, 1)；最终结果写入map任务所在节点的本地磁盘中（内里还有细节，讲到shuffle时，再细细展开）\n    - block的第一行的数据被处理完后，接着处理第二行；逻辑同上\n    - 当map任务将当前block中所有的数据全部处理完后，此map任务即运行结束\n  - 其它的每一个map任务都是如上逻辑，不再赘述\n- Reduce阶段\n  - reduce任务（reduce task）的个数由自己写的程序编程指定，main()内的job.setNumReduceTasks(4)指定reduce任务是4个（reduce1、reduce2、reduce3、reduce4）\n  - 每一个reduce任务的逻辑一下，所以以第一个reduce任务（reduce1）为例分析\n  - map1任务完成后，reduce1通过网络，连接到map1，将map1输出结果中属于reduce1的分区的数据，通过网络获取到reduce1端（拷贝阶段）\n  - 同样也如此连接到map2、map3获取结果\n  - 最终reduce1端获得4个(Dear, 1)键值对；由于key键相同，它们分到同一组；\n  - 4个(Dear, 1)键值对，转换成[Dear, Iterable(1, 1, 1, )]，作为两个参数传入reduce()\n  - 在reduce()内部，计算Dear的总数为4，并将(Dear, 4)作为键值对输出\n  - 每个reduce任务最终输出文件（内里还有细节，讲到shuffle时，再细细展开），文件写入到HDFS\n\n#### 2.2 MR中key的作用\n\n- <font color='red'>**MapReduce编程中，key有特殊的作用**</font>\n\n  - **①数据中，若要针对某个值进行分组、聚合时，需将此值作为MR中的reduce的输入的key**\n\n  - **如当前的词频统计例子，按单词进行分组，每组中对出现次数做聚合（计算总和）；所以需要将每个单词作为reduce输入的key，MapReduce框架自动按照单词分组，进而求出每组即每个单词的总次数**\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201910141101.png)\n\n  - **②另外，key还具有可排序的特性，因为MR中的key类需要实现WritableComparable接口；而此接口又继承Comparable接口（可查看源码）**\n\n  - **MR编程时，要充分利用以上两点；结合实际业务需求，设置合适的key**\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201908221717.png)\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201908221718.png)\n\n\n\n#### 2.4 MR参考代码\n\n**2.4.1 Mapper代码**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * 类Mapper<LongWritable, Text, Text, IntWritable>的四个泛型分别表示\n * map方法的输入的键的类型kin、值的类型vin；输出的键的类型kout、输出的值的类型vout\n * kin指的是当前所读行行首相对于split分片开头的字节偏移量,所以是long类型，对应序列化类型LongWritable\n * vin指的是当前所读行，类型是String，对应序列化类型Text\n * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text\n * vout根据需求，输出值指的是单词的个数，1，类型是int，对应序列化类型是IntWritable\n *\n */\npublic class WordCountMap extends Mapper<LongWritable, Text, Text, IntWritable> {\n\n    /**\n     * 处理分片split中的每一行的数据；针对每行数据，会调用一次map方法\n     * 在一次map方法调用时，从一行数据中，获得一个个单词word，再将每个单词word变成键值对形式(word, 1)输出出去\n     * 输出的值最终写到本地磁盘中\n     * @param key 当前所读行行首相对于split分片开头的字节偏移量\n     * @param value  当前所读行\n     * @param context\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public void map(LongWritable key, Text value, Context context)\n            throws IOException, InterruptedException {\n        //当前行的示例数据(单词间空格分割)：Dear Bear River\n        //取得当前行的数据\n        String line = value.toString();\n        //按照\\t进行分割，得到当前行所有单词\n        String[] words = line.split(\"\\t\");\n\n        for (String word : words) {\n            //将每个单词word变成键值对形式(word, 1)输出出去\n            //同样，输出前，要将kout, vout包装成对应的可序列化类型，如String对应Text，int对应IntWritable\n            context.write(new Text(word), new IntWritable(1));\n        }\n    }\n}\n\n```\n\n**2.4.2 Reducer代码**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\n/**\n *\n * Reducer<Text, IntWritable, Text, IntWritable>的四个泛型分别表示\n * reduce方法的输入的键的类型kin、输入值的类型vin；输出的键的类型kout、输出的值的类型vout\n * 注意：因为map的输出作为reduce的输入，所以此处的kin、vin类型分别与map的输出的键类型、值类型相同\n * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text\n * vout根据需求，输出值指的是每个单词的总个数，类型是int，对应序列化类型是IntWritable\n *\n */\npublic class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {\n    /**\n     *\n     * key相同的一组kv对，会调用一次reduce方法\n     * 如reduce task汇聚了众多的键值对，有key是hello的键值对，也有key是spark的键值对，如下\n     * (hello, 1)\n     * (hello, 1)\n     * (hello, 1)\n     * (hello, 1)\n     * ...\n     * (spark, 1)\n     * (spark, 1)\n     * (spark, 1)\n     *\n     * 其中，key是hello的键值对被分成一组；merge成[hello, Iterable(1,1,1,1)]，调用一次reduce方法\n     * 同样，key是spark的键值对被分成一组；merge成[spark, Iterable(1,1,1)]，再调用一次reduce方法\n     *\n     * @param key 当前组的key\n     * @param values 当前组中，所有value组成的可迭代集和\n     * @param context reduce上下文环境对象\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public void reduce(Text key, Iterable<IntWritable> values,\n                          Context context) throws IOException, InterruptedException {\n        //定义变量，用于累计当前单词出现的次数\n        int sum = 0;\n\n        for (IntWritable count : values) {\n            //从count中获得值，累加到sum中\n            sum += count.get();\n        }\n\n        //将单词、单词次数，分别作为键值对，输出\n        context.write(key, new IntWritable(sum));// 输出最终结果\n    };\n}\n```\n\n**2.4.3 Main程序入口**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport java.io.IOException;\n\n/**\n *\n * MapReduce程序入口\n * 注意：\n *  导包时，不要导错了；\n *  另外，map\\reduce相关的类，使用mapreduce包下的，是新API，如org.apache.hadoop.mapreduce.Job;；\n */\npublic class WordCountMain {\n    //若在IDEA中本地执行MR程序，需要将mapred-site.xml中的mapreduce.framework.name值修改成local\n    //参数 c:/test/README.txt c:/test/wc\n    public static void main(String[] args) throws IOException,\n            ClassNotFoundException, InterruptedException {\n\n        //判断一下，输入参数是否是两个，分别表示输入路径、输出路径\n       if (args.length != 2 || args == null) {\n            System.out.println(\"please input Path!\");\n            System.exit(0);\n        }\n\n        Configuration configuration = new Configuration();\n        //configuration.set(\"mapreduce.framework.name\",\"local\");\n\n\n        //告诉程序，要运行的jar包在哪\n        //configuration.set(\"mapreduce.job.jar\",\"/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n\n        //调用getInstance方法，生成job实例\n        Job job = Job.getInstance(configuration, WordCountMain.class.getSimpleName());\n\n        //设置job的jar包，如果参数指定的类包含在一个jar包中，则此jar包作为job的jar包； 参数class跟主类在一个工程即可；一般设置成主类\n//        job.setJarByClass(WordCountMain.class);\n        job.setJarByClass(WordCountMain.class);\n\n        //通过job设置输入/输出格式\n        //MR的默认输入格式是TextInputFormat，输出格式是TextOutputFormat；所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n        //设置输入/输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        //设置处理Map阶段的自定义的类\n        job.setMapperClass(WordCountMap.class);\n        //设置map combine类，减少网路传出量\n        job.setCombinerClass(WordCountReduce.class);\n        //设置处理Reduce阶段的自定义的类\n        job.setReducerClass(WordCountReduce.class);\n        //注意：如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n//        job.setMapOutputKeyClass(Text.class);\n//        job.setMapOutputValueClass(IntWritable.class);\n\n        //设置reduce task最终输出key/value的类型\n        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // 提交作业\n        job.waitForCompletion(true);\n\n    }\n}\n```\n\n> 程序运行有两种方式，分别是windows本地运行、集群运行，依次演示\n\n\n#### 2.5 集群运行\n\n- 用maven插件打jar包；①点击Maven，②双击package打包\n\n```shell\n[hadoop@node01 ~]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.wordcount.WordCountMain /README.txt /wordcount01\n```\n\n> 说明：\n>\n> com.kaikeba.hadoop-1.0-SNAPSHOT.jar是jar包名\n>\n> com.kaikeba.hadoop.wordcount.WordCountMain是包含main方法的类的全限定名\n>\n> /NOTICE.txt和/wordcount是main方法的两个参数，表示输入路径、输出路径\n\n![](assets/hadoop jar.gif)\n\n- 确认结果\n\n```shell\n[hadoop@node01 ~]$ hadoop fs -ls /wordcount01\n```\n\n![](assets/Image201908221620.png)\n\n#### 2.6 总结\n\n- MR分为两个阶段：map阶段、reduce阶段\n- MR输入的文件有几个block，就会生成几个map任务\n- MR的reduce任务的个数，由程序中编程指定：job.setNumReduceTasks(4)\n- map任务\n  - map任务中map()一次读取block的一行数据，以kv对的形式输入map()\n  - map()的输出作为reduce()的输入\n- reduce任务\n  - reduce任务通过网络将各执行完成的map任务输出结果中，属于自己的数据取过来\n  - key相同的键值对作为一组，调用一次reduce()\n  - reduce任务生成一个结果文件\n  - 文件写入HDFS\n\n\n\n### 3. WEB UI查看结果\n\n#### 3.1 Yarn\n\n> node01是resourcemanager所在节点主机名，根据自己的实际情况修改主机名\n\n浏览器访问url地址：http://node01:8088\n\n![](assets/Image201908221638.png)\n\n#### 3.2 HDFS结果\n\n浏览器输入URL：http://node01:50070\n\n①点击下拉框；②浏览文件系统；③输入根目录，查看hdfs根路径中的内容\n\n![](assets/Image201908221639.png)\n\n\n\n### 4. Shuffle\n\n- shuffle主要指的是map端的输出作为reduce端输入的过程\n\n#### 4.1 shuffle简图\n\n![](assets/Image201905231409.png)\n\n#### 4.2 shuffle细节图\n\n![](assets/Image201906280906.png)\n\n- 分区用到了分区器，默认分区器是HashPartitioner\n\n  源码：\n\n  ```java\n  public class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n  \n    public void configure(JobConf job) {}\n  \n    /** Use {@link Object#hashCode()} to partition. */\n    public int getPartition(K2 key, V2 value,\n                            int numReduceTasks) {\n      return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n  \n  }\n  ```\n\n#### 4.3 map端\n\n  - 每个map任务都有一个对应的环形内存缓冲区；输出是kv对，先写入到环形缓冲区（默认大小100M），当内容占据80%缓冲区空间后，由一个后台线程将缓冲区中的数据溢出写到一个磁盘文件\n  - 在溢出写的过程中，map任务可以继续向环形缓冲区写入数据；但是若写入速度大于溢出写的速度，最终造成100m占满后，map任务会暂停向环形缓冲区中写数据的过程；只执行溢出写的过程；直到环形缓冲区的数据全部溢出写到磁盘，才恢复向缓冲区写入\n  - 后台线程溢写磁盘过程，有以下几个步骤：\n    - 先对每个溢写的kv对做分区；分区的个数由MR程序的reduce任务数决定；默认使用HashPartitioner计算当前kv对属于哪个分区；计算公式：(key.hashCode() & Integer.MAX_VALUE) % numReduceTasks\n    - 每个分区中，根据kv对的key做内存中排序；\n    - 若设置了map端本地聚合combiner，则对每个分区中，排好序的数据做combine操作；\n    - 若设置了对map输出压缩的功能，会对溢写数据压缩\n  - 随着不断的向环形缓冲区中写入数据，会多次触发溢写（每当环形缓冲区写满100m），本地磁盘最终会生成多个溢出文件\n  - 合并溢写文件：在map task完成之前，所有溢出文件会被合并成一个大的溢出文件；且是已分区、已排序的输出文件\n  - 小细节：\n    - 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作；\n    - 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销）\n\n#### 4.4 reduce端\n\n- reduce task会在每个map task运行完成后，通过HTTP获得map task输出中，属于自己的分区数据（许多kv对）\n\n- 如果map输出数据比较小，先保存在reduce的jvm内存中，否则直接写入reduce磁盘\n\n- 一旦内存缓冲区达到阈值（默认0.66）或map输出数的阈值（默认1000），则触发**归并merge**，结果写到本地磁盘\n\n- 若MR编程指定了combine，在归并过程中会执行combine操作\n\n- 随着溢出写的文件的增多，后台线程会将它们合并大的、排好序的文件\n\n- reduce task将所有map task复制完后，将合并磁盘上所有的溢出文件\n\n- 默认一次合并10个\n\n- 最后一批合并，部分数据来自内存，部分来自磁盘上的文件\n\n- 进入“归并、排序、分组阶段”\n\n- 每组数据调用一次reduce方法\n\n- 参考文件《**reduce端merge 排序 分组.txt**》\n\n\n\n#### 4.5 总结\n\n- map端\n  - map()输出结果先写入环形缓冲区\n  - 缓冲区100M；写满80M后，开始溢出写磁盘文件\n  - 此过程中，会进行分区、排序、combine（可选）、压缩（可选）\n  - map任务完成前，会将多个小的溢出文件，合并成一个大的溢出文件（已分区、排序）\n- reduce端\n  - 拷贝阶段：reduce任务通过http将map任务属于自己的分区数据拉取过来\n  - 开始merge及溢出写磁盘文件\n  - 所有map任务的分区全部拷贝过来后，进行阶段合并、排序、分组阶段\n  - 每组数据调用一次reduce()\n  - 结果写入HDFS\n","tags":["hadoop","MapReduce"]},{"title":"HDFS文件系统","url":"/2019/10/14/hadoop/HDFS文件系统/","content":"\n#  HDFS分布式文件系统\n\n### 1. HDFS读写流程\n\n#### 1.1 数据写流程\n\n![1557999856839](img/1557999856839.png)\n\n![HDFS写入文件流程](img/HDFS写入文件流程.png)\n\n**1.1 详细流程**\n\n- 创建文件：\t\n\n  - HDFS客户端向HDFS写数据，先调用DistributedFileSystem.create()方法，在HDFS创建新的空文件\n  - RPC（ClientProtocol.create()）远程过程调用NameNode（NameNodeRpcServer）的create()，首先在HDFS目录树指定路径添加新文件\n  - 然后将创建新文件的操作记录在editslog中\n  - NameNode.create方法执行完后，DistributedFileSystem.create()返回FSDataOutputStream，它本质是封装了一个DFSOutputStream对象\n\n- 建立数据流管道：\n\n  - 客户端调用DFSOutputStream.write()写数据\n  - DFSOutputStream调用ClientProtocol.addBlock()，首先向NameNode申请一个空的数据块\n  - addBlock()返回LocatedBlock对象，对象包含当前数据块的所有datanode的位置信息\n  - 根据位置信息，建立数据流管道\n\n- 向数据流管道pipeline中写当前块的数据：\n\n  - 客户端向流管道中写数据，先将数据写入一个检验块chunk中，大小512Byte，写满后，计算chunk的检验和checksum值（4Byte）\n  - 然后将chunk数据本身加上checksum，形成一个带checksum值的chunk（516Byte）\n  - 保存到一个更大一些的结构**packet数据包**中，packet为64kB大小\n- packet写满后，先被写入一个**dataQueue**队列中\n  - packet被从队列中取出，向pipeline中写入，先写入datanode1，再从datanoe1传到datanode2，再从datanode2传到datanode3中\n- 一个packet数据取完后，后被放入到**ackQueue**中等待pipeline关于该packet的ack的反馈\n  - 每个packet都会有ack确认包，逆pipeline（dn3 -> dn2 -> dn1）传回输出流\n- 若packet的ack是SUCCESS成功的，则从ackQueue中，将packet删除；否则，将packet从ackQueue中取出，重新放入dataQueue，重新发送\n  - 如果当前块写完后，文件还有其它块要写，那么再调用addBlock方法（**流程同上**）\n- 文件最后一个block块数据写完后，会再发送一个空的packet，表示当前block写完了，然后关闭pipeline\n  - 所有块写完，close()关闭流\n- ClientProtocol.complete()通知namenode当前文件所有块写完了\n\n**6.1.2 容错**\n\n- 在写的过程中，pipeline中的datanode出现故障（如网络不通），输出流如何恢复\n  - 输出流中ackQueue缓存的所有packet会被重新加入dataQueue\n  - 输出流调用ClientProtocol.updateBlockForPipeline()，为block申请一个新的时间戳，namenode会记录新时间戳\n  - 确保故障datanode即使恢复，但由于其上的block时间戳与namenode记录的新的时间戳不一致，故障datanode上的block进而被删除\n  - 故障的datanode从pipeline中删除\n  - 输出流调用ClientProtocol.getAdditionalDatanode()通知namenode分配新的datanode到数据流pipeline中，并使用新的时间戳建立pipeline\n  - 新添加到pipeline中的datanode，目前还没有存储这个新的block，HDFS客户端通过DataTransferProtocol通知pipeline中的一个datanode复制这个block到新的datanode中\n  - pipeline重建后，输出流调用ClientProtocol.updatePipeline()，更新namenode中的元数据\n  - 故障恢复完毕，完成后续的写入流程\n\n#### 1.2 数据读流程\n\n**1.2.1 基本流程**\n\n![HDFS文件读取流程](img/HDFS文件读取流程.png)\n\n- 1、client端读取HDFS文件，client调用文件系统对象DistributedFileSystem的open方法\n- 2、返回FSDataInputStream对象（对DFSInputStream的包装）\n- 3、构造DFSInputStream对象时，调用namenode的getBlockLocations方法，获得file的开始若干block（如blk1, blk2, blk3, blk4）的存储datanode（以下简称dn）列表；针对每个block的dn列表，会根据网络拓扑做排序，离client近的排在前；\n- 4、调用DFSInputStream的read方法，先读取blk1的数据，与client最近的datanode建立连接，读取数据\n- 5、读取完后，关闭与dn建立的流\n- 6、读取下一个block，如blk2的数据（重复步骤4、5、6）\n- 7、这一批block读取完后，再读取下一批block的数据（重复3、4、5、6、7）\n- 8、完成文件数据读取后，调用FSDataInputStream的close方法\n\n**1.2.2 容错**\n\n- 情况一：读取block过程中，client与datanode通信中断\n\n  - client与存储此block的第二个datandoe建立连接，读取数据\n  - 记录此有问题的datanode，不会再从它上读取数据\n\n- 情况二：client读取block，发现block数据有问题\n  -  client读取block数据时，同时会读取到block的校验和，若client针对读取过来的block数据，计算检验和，其值与读取过来的校验和不一样，说明block数据损坏\n  -  client从存储此block副本的其它datanode上读取block数据（也会计算校验和）\n  -  同时，client会告知namenode此情况；\n\n\n\n\n\n### 2. Hadoop HA高可用\n\n#### 2.1 HDFS高可用原理\n\n![](img/Image201905211519.png)\n\n- 对于HDFS ，NN存储元数据在内存中，并负责管理文件系统的命名空间和客户端对HDFS的读写请求。但是，如果只存在一个NN，一旦发生“单点故障”，会使整个系统失效。\n- 虽然有个SNN，但是它并不是NN的热备份\n- 因为SNN无法提供“热备份”功能，在NN故障时，无法立即切换到SNN对外提供服务，即HDFS处于停服状态。\n- HDFS2.x采用了HA（High Availability高可用）架构。\n  - 在HA集群中，可设置两个NN，一个处于“活跃（Active）”状态，另一个处于“待命（Standby）”状态。\n  - 由zookeeper确保一主一备（讲zookeeper时具体展开）\n  - 处于Active状态的NN负责响应所有客户端的请求，处于Standby状态的NN作为热备份节点，保证与active的NN的元数据同步\n  - Active节点发生故障时，zookeeper集群会发现此情况，通知Standby节点立即切换到活跃状态对外提供服务\n  - 确保集群一直处于可用状态\n- 如何热备份元数据：\n  - Standby NN是Active NN的“热备份”，因此Active NN的状态信息必须实时同步到StandbyNN。\n  - 可借助一个共享存储系统来实现状态同步，如NFS(NetworkFile System)、QJM(Quorum Journal Manager)或者Zookeeper。\n  - Active NN将更新数据写入到共享存储系统，Standby NN一直监听该系统，一旦发现有新的数据写入，就立即从公共存储系统中读取这些数据并加载到Standby NN自己内存中，从而保证元数据与Active NN状态一致。\n- 块报告：\n  - NN保存了数据块到实际存储位置的映射信息，为了实现故障时的快速切换，必须保证StandbyNN中也包含最新的块映射信息\n  - 因此需要给所有DN配置Active和Standby两个NN的地址，把块的位置和心跳信息同时发送到两个NN上。\n\n### 3. Hadoop联邦\n\n#### 3.1 为什么需要联邦\n\n- 虽然HDFS HA解决了“单点故障”问题，但HDFS在扩展性、整体性能和隔离性方面仍有问题\n  - 系统扩展性方面，元数据存储在NN内存中，受限于内存上限（每个文件、目录、block占用约150字节）\n  - 整体性能方面，吞吐量受单个NN的影响\n  - 隔离性方面，一个程序可能会影响其他程序的运行，如果一个程序消耗过多资源会导致其他程序无法顺利运行\n  - HDFS HA本质上还是单名称节点\n\n#### 3.2 联邦\n\n![](img/Image201909041239.png)\n\n\n- HDFS联邦可以解决以上三个问题\n  - HDFS联邦中，设计了多个命名空间；每个命名空间有一个NN或一主一备两个NN，使得HDFS的命名服务能够水平扩展\n  - 这些NN分别进行各自命名空间namespace和块的管理，相互独立，不需要彼此协调\n  - 每个DN要向集群中所有的NN注册，并周期性的向所有NN发送心跳信息和块信息，报告自己的状态\n  - HDFS联邦每个相互独立的NN对应一个独立的命名空间\n  - 每一个命名空间管理属于自己的一组块，这些属于同一命名空间的块对应一个“块池”的概念。\n  - 每个DN会为所有块池提供块的存储，块池中的各个块实际上是存储在不同DN中的\n\n#### 3.3 扩展\n\n[联邦-官网](<https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/Federation.html>)\n\n\n\n\n\n### 4. 文件压缩\n\n#### 4.1 压缩算法\n\n- 文件压缩好处：\n\n  - 减少数据所占用的磁盘空间\n  - 加快数据在磁盘、网络上的IO\n\n- 常用压缩格式\n\n  | 压缩格式 | UNIX工具 | 算      法 | 文件扩展名 | 可分割 |\n  | -------- | -------- | ---------- | ---------- | ------ |\n  | DEFLATE  | 无       | DEFLATE    | .deflate   | No     |\n  | gzip     | gzip     | DEFLATE    | .gz        | No     |\n  | zip      | zip      | DEFLATE    | .zip       | YES    |\n  | bzip     | bzip2    | bzip2      | .bz2       | YES    |\n  | LZO      | lzop     | LZO        | .lzo       | No     |\n  | Snappy   | 无       | Snappy     | .snappy    | No     |\n\n- Hadoop的压缩实现类；均实现CompressionCodec接口\n\n  | 压缩格式 | 对应的编码/解码器                          |\n  | -------- | ------------------------------------------ |\n  | DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |\n  | gzip     | org.apache.hadoop.io.compress.GzipCodec    |\n  | bzip2    | org.apache.hadoop.io.compress.BZip2Codec   |\n  | LZO      | com.hadoop.compression.lzo.LzopCodec       |\n  | Snappy   | org.apache.hadoop.io.compress.SnappyCodec  |\n\n- 查看集群是否支持本地压缩（所有节点都要确认）\n\n  ```\n  [hadoop@node01 ~]$ hadoop checknative\n  ```\n\n  ![](img/Image201910111114.png)\n\n#### 4.2 编程实践\n\n- 编程：上传压缩过的文件到HDFS\n\n\n```java\n\n    /**\n     * 上传压缩文件到服务器\n     *  传递参数\n     *  args[0] 本地文件路径\n     *  args[1] hdoop文件系统 路径\n     */\n    public static void uploadFileZipToFileSystem(String source,String targetUrl){\n        System.out.println(\"文件地址：\" + source);\n        System.out.println(\"目标服务器：\" + targetUrl);\n        InputStream inputStreamSourceFile = null;\n\n        try {\n            // 获取文件输入流\n            inputStreamSourceFile = new BufferedInputStream(new FileInputStream(source));\n            // HDFS 读写配置文件\n            Configuration configuration = new Configuration();\n            // 压缩类型\n            BZip2Codec codec = new BZip2Codec();\n            codec.setConf(configuration);\n            // 通过url 返回文件系统实例\n            FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);\n            //调用Filesystem的create方法返回的是FSDataOutputStream对象\n            //该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n            // 获取文件系用的输出流\n            OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n            // 对输出流进行压缩\n            CompressionOutputStream compressionOut = codec.createOutputStream(outputStreamTarget);\n            // 将文件输入流，写入输入流\n            IOUtils.copyBytes(inputStreamSourceFile,compressionOut,4069,true);\n            System.out.println(\"上传成功\");\n        } catch (FileNotFoundException e) {\n            System.err.println(e.getMessage());\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n```\n\n- 扩展阅读\n  - 《Hadoop权威指南》 5.2章节 压缩\n  - [HDFS文件压缩](<https://blog.csdn.net/qq_38262266/article/details/79171524>)\n\n\n\n\n\n### 5. 小文件治理\n\n#### 5.1 有没有问题\n\n- NameNode存储着文件系统的元数据，每个文件、目录、块大概有150字节的元数据；\n- 因此文件数量的限制也由NN内存大小决定，如果小文件过多则会造成NN的压力过大\n- 且HDFS能存储的数据总量也会变小\n\n#### 5.2 HAR文件方案（10分钟）\n\n- 本质启动mr程序，所以需要启动yarn\n\n![1558004541101](img/1558004541101.png)\n\n用法：\n\n```sh\narchive -archiveName <NAME>.har -p <parent path> [-r <replication factor>]<src>* <dest>\n```\n\n![](img/Image201909041408.png)\n\n![alt](img/Image201906210960.png)\n\n```shell\n# 创建archive文件；/testhar有两个子目录th1、th2；两个子目录中有若干文件\nhadoop archive -archiveName test.har -p /testhar -r 3 th1 th2 /outhar # 原文件还存在，需手动删除\n\n# 查看archive文件\nhdfs dfs -ls -R har:///outhar/test.har\n\n# 解压archive文件\n# 方式一\nhdfs dfs -cp har:///outhar/test.har/th1 hdfs:/unarchivef # 顺序\nhadoop fs -ls /unarchivef\t\n# 方式二\nhadoop distcp har:///outhar/test.har/th1 hdfs:/unarchivef2 # 并行，启动MR\n```\n\n#### 5.3 Sequence Files方案（*）\n\n- SequenceFile文件，主要由一条条record记录组成；每个record是键值对形式的\n- SequenceFile文件可以作为小文件的存储容器；\n  - 每条record保存一个小文件的内容\n  - 小文件名作为当前record的键；\n  - 小文件的内容作为当前record的值；\n  - 如10000个100KB的小文件，可以编写程序将这些文件放到一个SequenceFile文件。\n- 一个SequenceFile是**可分割**的，所以MapReduce可将文件切分成块，每一块独立操作。\n- 具体结构（如下图）：\n  - 一个SequenceFile首先有一个4字节的header（文件版本号）\n  - 接着是若干record记录\n  - 记录间会随机的插入一些同步点sync marker，用于方便定位到记录边界\n- 不像HAR，SequenceFile**支持压缩**。记录的结构取决于是否启动压缩\n  - 支持两类压缩：\n    - 不压缩NONE，如下图\n    - 压缩RECORD，如下图\n    - 压缩BLOCK，①一次性压缩多条记录；②每一个新块Block开始处都需要插入同步点；如下图\n  - 在大多数情况下，以block（注意：指的是SequenceFile中的block）为单位进行压缩是最好的选择\n  - 因为一个block包含多条记录，利用record间的相似性进行压缩，压缩效率更高\n  - 把已有的数据转存为SequenceFile比较慢。比起先写小文件，再将小文件写入SequenceFile，一个更好的选择是直接将数据写入一个SequenceFile文件，省去小文件作为中间媒介.\n\n![](img/Image201907101934.png)\n\n\n\n![](img/Image201907101935.png)\n\n- 向SequenceFile写入数据\n\n```java\npackage com.kaikeba.hadoop.sequencefile;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.SequenceFile;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.BZip2Codec;\n\nimport java.io.IOException;\nimport java.net.URI;\n\npublic class SequenceFileWriteNewVersion {\n\n    //模拟数据源\n    private static final String[] DATA = {\n            \"The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.\",\n            \"It is designed to scale up from single servers to thousands of machines, each offering local computation and storage.\",\n            \"Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer\",\n            \"o delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.\",\n            \"Hadoop Common: The common utilities that support the other Hadoop modules.\"\n    };\n\n    public static void main(String[] args) throws IOException {\n        //输出路径：要生成的SequenceFile文件名\n        String uri = \"hdfs://node01:9000/writeSequenceFile\";\n\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(URI.create(uri), conf);\n        //向HDFS上的此SequenceFile文件写数据\n        Path path = new Path(uri);\n\n        //因为SequenceFile每个record是键值对的\n        //指定key类型\n        IntWritable key = new IntWritable();\n        //指定value类型\n        Text value = new Text();\n//\n//            FileContext fileContext = FileContext.getFileContext(URI.create(uri));\n//            Class<?> codecClass = Class.forName(\"org.apache.hadoop.io.compress.SnappyCodec\");\n//            CompressionCodec SnappyCodec = (CompressionCodec)ReflectionUtils.newInstance(codecClass, conf);\n//            SequenceFile.Metadata metadata = new SequenceFile.Metadata();\n//            //writer = SequenceFile.createWriter(fs, conf, path, key.getClass(), value.getClass());\n//            writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(path), SequenceFile.Writer.keyClass(IntWritable.class),\n//                                        SequenceFile.Writer.valueClass(Text.class));\n\n        //创建向SequenceFile文件写入数据时的一些选项\n        //要写入的SequenceFile的路径\n        SequenceFile.Writer.Option pathOption       = SequenceFile.Writer.file(path);\n        //record的key类型选项\n        SequenceFile.Writer.Option keyOption        = SequenceFile.Writer.keyClass(IntWritable.class);\n        //record的value类型选项\n        SequenceFile.Writer.Option valueOption      = SequenceFile.Writer.valueClass(Text.class);\n        //SequenceFile压缩方式：NONE | RECORD | BLOCK三选一\n        //方案一：RECORD、不指定压缩算法\n        SequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD);\n        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);\n\n\n        //方案二：BLOCK、不指定压缩算法\n//        SequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.BLOCK);\n//        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);\n\n\n\n        //方案三：使用BLOCK、压缩算法BZip2Codec；压缩耗时间\n        //再加压缩算法\n//        BZip2Codec codec = new BZip2Codec();\n//        codec.setConf(conf);\n//        SequenceFile.Writer.Option compressAlgorithm = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD, codec);\n//        //创建写数据的Writer实例\n//        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressAlgorithm);\n\n\n\n        for (int i = 0; i < 100000; i++) {\n            //分别设置key、value值\n            key.set(100 - i);\n            value.set(DATA[i % DATA.length]);\n            System.out.printf(\"[%s]\\t%s\\t%s\\n\", writer.getLength(), key, value);\n            //在SequenceFile末尾追加内容\n            writer.append(key, value);\n        }\n        //关闭流\n        IOUtils.closeStream(writer);\n    }\n}\n```\n\n- 命令查看SequenceFile内容\n\n```shell\n hadoop fs -text /writeSequenceFile\n```\n\n- 读取SequenceFile文件\n\n```java\npackage com.kaikeba.hadoop.sequencefile;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.SequenceFile;\nimport org.apache.hadoop.io.Writable;\nimport org.apache.hadoop.util.ReflectionUtils;\n\nimport java.io.IOException;\n\npublic class SequenceFileReadNewVersion {\n\n    public static void main(String[] args) throws IOException {\n        //要读的SequenceFile\n        String uri = \"hdfs://node01:9000/writeSequenceFile\";\n        Configuration conf = new Configuration();\n        Path path = new Path(uri);\n\n        //Reader对象\n        SequenceFile.Reader reader = null;\n        try {\n            //读取SequenceFile的Reader的路径选项\n            SequenceFile.Reader.Option pathOption = SequenceFile.Reader.file(path);\n\n            //实例化Reader对象\n            reader = new SequenceFile.Reader(conf, pathOption);\n\n            //根据反射，求出key类型\n            Writable key = (Writable)\n                    ReflectionUtils.newInstance(reader.getKeyClass(), conf);\n            //根据反射，求出value类型\n            Writable value = (Writable)\n                    ReflectionUtils.newInstance(reader.getValueClass(), conf);\n\n            long position = reader.getPosition();\n            System.out.println(position);\n\n            while (reader.next(key, value)) {\n                String syncSeen = reader.syncSeen() ? \"*\" : \"\";\n                System.out.printf(\"[%s%s]\\t%s\\t%s\\n\", position, syncSeen, key, value);\n                position = reader.getPosition(); // beginning of next record\n            }\n        } finally {\n            IOUtils.closeStream(reader);\n        }\n    }\n}\n```\n\n\n\n###  6. 文件快照\n\n####  6.1 什么是快照\n\n- 快照比较常见的应用场景是数据备份，以防一些用户错误或灾难恢复\n- 快照snapshots是HDFS文件系统的，只读的、某时间点的拷贝\n- 可以针对某个目录，或者整个文件系统做快照\n- 创建快照时，block块并不会被拷贝。快照文件中只是记录了block列表和文件大小，**不会做任何数据拷贝**\n\n####  6.2 快照操作\n\n- 允许快照\n\n  允许一个快照目录被创建。如果这个操作成功完成，这个目录就变成snapshottable\n\n  用法：hdfs dfsadmin -allowSnapshot <snapshotDir>\n\n  ```shell\n  hdfs dfsadmin -allowSnapshot /wordcount\n  ```\n\n- 禁用快照\n\n  用法：hdfs dfsadmin -disallowSnapshot <snapshotDir>\n\n  ```shell\n  hdfs dfsadmin -disallowSnapshot /wordcount\n  ```\n\n- 创建快照\n\n  用法：hdfs dfs -createSnapshot <snapshotDir> [<snapshotName>]\n\n  ```shell\n  #注意：先将/wordcount目录变成允许快照的\n  hdfs dfs -createSnapshot /wordcount wcSnapshot\n  ```\n\n- 查看快照\n\n  ```shell\n  hdfs dfs -ls /wordcount/.snapshot\n  \n  ```\n\n  ![](img/Image201909041346.png)\n\n- 重命名快照\n\n  这个操作需要拥有snapshottabl目录所有者权限\n\n  用法：hdfs dfs -renameSnapshot <snapshotDir> <oldName> <newName>\n\n  ```shell\n  hdfs dfs -renameSnapshot /wordcount wcSnapshot newWCSnapshot\n  \n  ```\n\n- 用快照恢复误删除数据\n\n  HFDS的/wordcount目录，文件列表如下\n\n  ![](img/Image201909041356.png)\n\n  误删除/wordcount/edit.xml文件\n\n  ```shell\n  hadoop fs -rm /wordcount/edit.xml\n  \n  ```\n\n  ![](img/Image201909041400.png)\n\n  恢复数据\n\n  ```shell\n  hadoop fs -cp /wordcount/.snapshot/newWCSnapshot/edit.xml /wordcount\n  \n  ```\n\n- 删除快照\n\n  这个操作需要拥有snapshottabl目录所有者权限\n\n  用法：hdfs dfs -deleteSnapshot <snapshotDir> <snapshotName>\n\n  ```shell\n  hdfs dfs -deleteSnapshot /wordcount newWCSnapshot\n  \n  ```\n\n\n\n\n\n\n##  7、拓展点、未来计划、行业趋势\n\n1. HDFS存储地位\n\n2. **block块为什么设置的比较大**\n\n- [磁盘基础知识](<https://www.cnblogs.com/jswang/p/9071847.html>)\t\n\n  - 盘片platter、磁头head、磁道track、扇区sector、柱面cylinder\n  - 为了最小化寻址开销；从磁盘传输数据的时间明显大于定位这个块开始位置所需的时间\n\n- 问：块的大小是不是设置的越大越好呢？\n\n  1、 不是，寻址的时间大概是 100ms，设计一般设置为寻址时间占用十分之一，也就是一秒。 硬盘的传输速录大概是100m/s 一秒大概为100M，最接近100的大小为128M。 \n\n![](img/Image201906211143.png)\n","tags":["hadoop","hdfs"]},{"title":"数据结构与算法（一）","url":"/2018/11/18/datastructure/数据结构与算法（一）/","content":"\n## 1. 数据结构基础概念\n\n### 1.1 数据结构定义\n- 数据结构是指相互之间存在着一种或多种关系的数据元素的集合和该集合中数据元素之间的关系组成。记为:Data_Structure=(D,R) \n  - D： 是数据元素的集合\n  - R： 是该集合中所有元素之间的关系的有限集合\n\n-  广义:：数据结构是指数据的存储结构 \n\n- 狭义：在使用数据的过程中，定义的各种特性及限制的抽象出的逻辑结。\n\n  - 数据结构服务于算法，算法依赖于数据结构。\n\n  - 数据结构是静态，是定理，而算法依赖于其相关计算，才构成了我们绚烂多 彩的程序帝国 。\n\n### 1.2 数据结构的组成\n\n<img src=\"assets/image-20191118232701597.png\" alt=\"image-20191118232701597\" style=\"zoom: 40%;\" />\n\n#### 1.2.1 存储结构\n\n- 存储结构:又名物理结构，是数据存储到计算机上的真实结构。 \n- 包含顺序/链式/哈希/索引等 \n\n<img src=\"assets/image-20191118232932344.png\" alt=\"image-20191118232932344\" style=\"zoom: 50%;\" />\n\n#### 1.2.2 逻辑结构\n\n- 逻辑结构: 逻辑结构是数据组合的规则与关系的抽象描 述，一般我们讨论的数据结构通常是指逻辑结构。 \n- 包含树/图等非线性结构与数组/队列/串等线性结构。 \n\n#### 1.2.3  数据算法\n\n- 算法: 算法是针对逻辑结构一系列操作的方法。 \n- 算法在考虑性能的时候，受到存储结构的制约。\n- 核心理念：用最省力最省空间的方法，去促成数据呈现的结果。\n\n### 1.3 算法的复杂度分析\n","tags":["data structure"]},{"title":"大数据概论-HDFS理论基础","url":"/2018/10/10/hadoop/Java大数据基础概论/","content":"## 大数据概论\n\n> 概念： 大数据（big data）是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产\n\n| 数据单位 | B    | KB   | MB   | GB   | PE   | PB   | EB   | ZB   | YB   |\n| -------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| 基数     |      | 2    | 2    | 2    | 2    | 2    | 2    | 10   | 10   |\n| 次方     | 0    | 10   | 20   | 30   | 40   | 50   | 60   | 21   | 24   |\n\n### 一、大数据特性\n\n1. 数据量大（Volume） \n2. 类型繁多（Variety） \n3. 价值密度低（Value） \n4. 速度快时效高（Velocity）\n\n### 二、大数据的挑战\n\n1. 存储： 每天几TB、GB的数据增量，并且还在持续的增长中。\n2. 分析： 如何从巨大的数据中挖掘出隐藏的商业价值。\n3. 管理： 如何快速构建并且保证系统的安全、简单可用。\n\n### 三、传统的大数据项目流程\n\n```flow\nst=>start: 开始\ndataCollect=>operation: 数据收集 ： Flume、Kafaka、Scribe\ndataStore=>operation: 数据存储 ： HDFS、HBase、Cassadra\ndataCaculate=>operation: 数据计算 : Mapreduce、Strom、Impala、Spark、Spark Streaming...\n数据计算三大类：\n1、离线处理平台： Spark、Spark Core\n2、交互式处理平台： Spark SQL、Hive 、Impala\n3、流处理平台 ： Strom、Spring Stoeaming 、 Flink\ndataAnalyse=>operation: 分析与挖掘 ： Mahour、R语言、Hive、Pig\ndataEtl=>operation: ETL ： sqoop、DataX\ndataView=>operation: 可视化 ： Echarts.js 、 E3.js、 数据报表系统\ndataActual=>operation: 项目实战\ne=>end: 结束\n\nst->dataCollect->dataStore->dataCaculate->dataAnalyse->dataEtl->dataView->dataActual->e\n\n```\n\n## 分布式文件系统\n\n### 一、Hadoop简介\n\n1. Hadoop架构\n\n   ![Image201906191834](img/Image201906191834.png)\n\n2. Hadoop历史\n\n   ![Image201906202055](img/Image201906202055.png)\n\n### 二、HDFS\n\n- HDFS是Hadoop中的一个存储子模块\n- HDFS (全称Hadoop Distributed File System)，即hadoop的分布式文件系统\n- File System**文件系统**：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读出、修改、转储、删除文件等\n- 当数据集大小超出一台计算机的存储能力时，就有必要将它拆分成若干部分，然后分散到不同的计算机中存储。管理网络中跨多台计算机存储的文件系统称之为**分布式文件系统**（distributed filesystem）\n\n#### 2.1 HDFS特点\n\n**2.1.1 优点：**\n\n- 适合存储大文件，能用来存储管理PB级的数据；不适合存储小文件\n- 处理非结构化数据\n- 流式的访问数据，一次写入、多次读写\n- 运行于廉价的商用机器集群上，成本低\n- 高容错：故障时能继续运行且不让用户察觉到明显的中断\n- 可扩展\n\n![](img/Image201907081216.png)\n\n**2.1.2 局限性**\n\n- 不适合处理低延迟数据访问\n  - HDFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的\n  - 对于低延时的访问需求，HBase是更好的选择\n- 无法高效存储大量的小文件\n  - 小文件会给Hadoop的扩展性和性能带来严重问题\n  - 利用SequenceFile、MapFile等方式归档小文件\n- 不支持多用户写入及任意修改文件\n  - 文件有一个写入者，只能执行追加操作\n  - 不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改\n\n#### 2.2 HDFS常用命令\n\n> HDFS两种命令风格，两种命令效果等同\n>\n> hadoop fs / hdfs dfs\n\n![image-20191010155353956](/Users/dingchuangshi/Library/Application Support/typora-user-images/image-20191010155353956.png)\n\n\n\n1. 如何查看hdfs或hadoop子命令的**帮助信息**，如ls子命令\n\n   ```shell\n   hdfs dfs -help ls\n   hadoop fs -help ls\t#两个命令等价\n   ```\n\n2. **查看**hdfs文件系统中已经存在的文件。对比linux命令ls\n\n   ```shell\n   hdfs dfs -ls /\n   hadoop fs -ls /\n   ```\n\n3. 在hdfs文件系统中创建文件\n\n   ```shell\n   hdfs dfs -touchz /edits.txt\n   ```\n\n4. 向HDFS文件中追加内容\n\n    ```shell\n    hadoop fs -appendToFile edit1.xml /edits.txt #将本地磁盘当前目录的edit1.xml内容追加到HDFS根目录 的edits.txt文件\n    ```\n\n5. 查看HDFS文件内容\n\n    ```shell\n    hdfs dfs -cat /edits.txt\n    ```\n\n6. **从本地路径上传文件至HDFS**\n\n    ````` shell\n    #用法：hdfs dfs -put /本地路径 /hdfs路径\n    hdfs dfs -put hadoop-2.7.3.tar.gz /\n    hdfs dfs -copyFromLocal hadoop-2.7.3.tar.gz /  #根put作用一样\n    hdfs dfs -moveFromLocal hadoop-2.7.3.tar.gz /  #根put作用一样，只不过，源文件被拷贝成功后，会被删除\n    `````\n\n7. **在hdfs文件系统中下载文件**\n\n     ```shell\n     hdfs dfs -get /hdfs路径 /本地路径\n     hdfs dfs -copyToLocal /hdfs路径 /本地路径  #根get作用一样\n     ```\n\n8. 在hdfs文件系统中**创建目录**\n\n     ```shell\n     hdfs dfs -mkdir /shell\n     ```\n\n9. 在hdfs文件系统中**删除**文件\n\n     ```shell\n     hdfs dfs -rm /edits.txt\n     hdfs dfs -rm -r /shell\n     ```\n\n10. 在hdfs文件系统中**修改文件名称**（也可以用来**移动**文件到目录）\n\n     ```shell\n     hdfs dfs -mv /xcall.sh /call.sh\n     hdfs dfs -mv /call.sh /shell\n     ```\n\n11. 在hdfs中拷贝文件到目录\n\n      ```shell\n      hdfs dfs -cp /xrsync.sh /shell\n      ```\n\n12. 递归删除目录\n\n      ```shell\n      hdfs dfs -rmr /shell\n      ```\n\n13. 列出本地文件的内容（默认是hdfs文件系统）\n\n      ```shell\n      hdfs dfs -ls file:///home/bruce/\n      ```\n\n14. 查找文件\n\n      ```shell\n      # linux find命令\n      find . -name 'edit*'\n      \n      # HDFS find命令\n      hadoop fs -find / -name part-r-00000 # 在HDFS根目录中，查找part-r-00000文件\n      ```\n\n\n> 还有许多其他命令，大家可以自己探索一下   \n\n##### 2.2.1 命令行小结\n\n- 输入hadoop fs 或hdfs dfs，回车，查看所有的HDFS命令\n\n- 许多命令与linux命令有很大的相似性，学会举一反三\n- 有用的help，如查看ls命令的使用说明：hadoop fs -help ls\n\n##### 2.2.2 hdfs与getconf结合使用\n\n1. 获取NameNode的节点名称（可能有多个）\n\n      ``````shell\n      hdfs getconf -namenodes\n      ``````\n\n2. 获取hdfs最小块信息\n\n      ``````shell\n      hdfs getconf -confKey dfs.namenode.fs-limits.min-block-size\n      ``````\n\n3. 查找hdfs的NameNode的RPC地址\n\n\t``````shell\n\thdfs getconf -nnRpcAddresses\n\t``````\n\t\n\t\n\n##### 2.2.3 hdfs与dfsadmin结合使用\n\n1. 同样要学会借助帮助信息\n\n      ```shell\n      hdfs dfsadmin -help safemode\n      ```\n\n2. 查看hdfs dfsadmin的帮助信息\n\n      ``````shell\n      hdfs dfsadmin\n      ``````\n\n3. 查看当前的模式\n\n      ``````shell\n      hdfs dfsadmin -safemode get\n      ``````\n\n4. 进入安全模式\n\n  ``````shell\n  hdfs dfsadmin -safemode enter\n  ``````\n\n  \n\n##### 2.2.4 hdfs与fsck结合使用\n\n1. fsck指令**显示HDFS块信息**\n\n\t``````shell\n\thdfs fsck /02-041-0029.mp4 -files -blocks -locations # 查看文件02-041-0029.mp4的块信息\n\t``````\n\t\n\t\n\n##### 2.2.5 其他命令\n\n1. 检查压缩库本地安装情况\n\n      ``````shell\n      hadoop checknative\n      ``````\n\n2. 格式化名称节点（**慎用**，一般只在初次搭建集群，使用一次；格式化成功后，不要再使用）\n\n      ``````shell\n      hadoop namenode -format\n      ``````\n\n3. 执行自定义jar包\n\n   ``````shell\n   hadoop jar YinzhengjieMapReduce-1.0-SNAPSHOT.jar com.kaikeba.hadoop.WordCount /world.txt /out\n   ``````\n\n#### 2.3 HDFS编程\n\n\n- 1.向hdfs中,上传一个文本文件\n\n  ```java\n   /**\n       * 上传文件到服务器\n       *  传递参数\n       *  args[0] 本地文件路径\n       *  args[1] hdoop文件系统 路径\n       */\n      public static void uploadFileToFileSystem(String source,String targetUrl){\n          System.out.println(\"文件地址：\" + source);\n          System.out.println(\"目标服务器：\" + targetUrl);\n          InputStream inputStreamSourceFile = null;\n          try {\n              // 获取文件输入流\n              inputStreamSourceFile = new BufferedInputStream(new FileInputStream(source));\n              // HDFS 读写配置文件\n              Configuration configuration = new Configuration();\n              // 通过url 返回文件系统实例\n              FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);\n              //调用Filesystem的create方法返回的是FSDataOutputStream对象\n              //该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n              // 获取文件系用的输出流\n              OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n              // 将文件输入流，写入输入流\n              IOUtils.copyBytes(inputStreamSourceFile,outputStreamTarget,4069,true);\n              System.out.println(\"上传成功\");\n          } catch (FileNotFoundException e) {\n              System.err.println(e.getMessage());\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n      }\n  \n  ```\n\n- 2.读取hdfs上的文件\n\n```java\n\n    /**\n     * 从文件系统中读取文件\n     * @param source 需要读取的文件\n     * @return 读取文件内容\n     */\n    public static String readFileFromFileSystem(String source){\n        String result = null;\n        try {\n            // HDFS 读写文件配置\n            Configuration configuration = new Configuration();\n            // HDFS文件系统\n            FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n            // 文件输入流，用于读取文件\n            InputStream inputStream = fileSystem.open(new Path(source));\n            BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream));\n            result = readBufferReader(bufferedReader).toString();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return result;\n    }\n    \n    \n    /**\n     * 获取内容\n     * @param br\n     * @return\n     */\n    private static StringBuffer readBufferReader(BufferedReader br) throws IOException {\n        StringBuffer stringBuffer = new StringBuffer();\n        String temp = \"\";\n        while ((temp = br.readLine()) != null){\n            stringBuffer.append(temp);\n        }\n        return stringBuffer;\n    }\n```\n\n\n\n- 3.列出某一个文件夹下的所有文件\n\n```java\n\n    /**\n     * 列出当前目录下所有字文件名称\n     * @param source\n     * @return\n     */\n    public static String listAllFileChildren(String source){\n        StringBuffer stringBuffer = new StringBuffer();\n        try {\n            // HDFS 读写文件配置\n            Configuration configuration = new Configuration();\n            // HDFS文件系统\n            FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n            // recursive 继续深入遍历\n            RemoteIterator<LocatedFileStatus> iterator = fileSystem.listFiles(new Path(source),true);\n            while (iterator.hasNext()){\n                LocatedFileStatus fileStatus = iterator.next();\n                stringBuffer.append(fileStatus.getPath() + \"\\n\");\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return stringBuffer.toString();\n    }\n\n```\n\n\n\n- 4.列出多级目录名称和目录下的文件名称\n\n  ```java\n  \n      /**\n       * 递归列出当前目录下所有目录和文件名称\n       * @param source\n       * @return\n       */\n      public static String listAllChildren(String source){\n          StringBuffer stringBuffer = new StringBuffer();\n          try {\n              // HDFS 读写文件配置\n              Configuration configuration = new Configuration();\n              // HDFS文件系统\n              FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n              list(stringBuffer,fileSystem,new Path(source));\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n          return stringBuffer.toString();\n      }\n  \n      /**\n       * 递归目录和文件\n       * @param stringBuffer  文件目录名称集合\n       * @param fileSystem  hdfs 文件系统\n       * @param source path 路径\n       * @throws IOException\n       */\n      private static void list(StringBuffer stringBuffer,FileSystem fileSystem, Path source) throws IOException {\n          FileStatus[] iterator = fileSystem.listStatus(source);\n          for (FileStatus status:iterator) {\n              stringBuffer.append(status.getPath() + \"\\n\");\n              if(status.isDirectory()){\n                  list(stringBuffer,fileSystem,status.getPath());\n              }\n  \n          }\n      }\n  \n  \n  ```\n\n  \n\n#### 2.4  HDFS架构\n\n![](img/1558073557041.png)\n\n- 大多数分布式框架都是主从架构\n- HDFS也是主从架构Master|Slave或称为管理节点|工作节点\n\n##### 1、NameNode\n\n**1.1 文件系统**\n\n- file system文件系统：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读取、修改、转储、删除文件等\n- 读文件 =>>找到文件 =>> 在哪 + 叫啥？\n- 元数据\n  - 关于文件或目录的描述信息，如文件所在路径、文件名称、文件类型等等，这些信息称为文件的元数据metadata\n- 命名空间\n  - 文件系统中，为了便于管理存储介质上的，给每个目录、目录中的文件、子目录都起了名字，这样形成的层级结构，称之为命名空间\n  - 同一个目录中，不能有同名的文件或目录\n  - 这样通过目录+文件名称的方式能够唯一的定位一个文件\n\n![](img/Image201906211418.png)\n\n**5.1.2 HDFS-NameNode**\n\n- HDFS本质上也是文件系统filesystem，所以它也有元数据metadata；\n- 元数据metadata保存在NameNode**内存**中\n- NameNode作用\n  - HDFS的主节点，负责管理文件系统的命名空间，将HDFS的元数据存储在NameNode节点的内存中\n  - 负责响应客户端对文件的读写请求\n- HDFS元数据\n  - 文件目录树、所有的文件（目录）名称、文件属性（生成时间、副本、权限）、每个文件的块列表、每个block块所在的datanode列表\n\n![](img/Image201909031504.png)\n\n  - 每个文件、目录、block占用大概**150Byte字节的元数据**；所以HDFS适合存储大文件，不适合存储小文件\n\n  - HDFS元数据信息以两种形式保存：①编辑日志**edits log**②命名空间镜像文件**fsimage**\n    - edits log：HDFS编辑日志文件 ，保存客户端对HDFS的所有更改记录，如增、删、重命名文件（目录），这些操作会修改HDFS目录树；\u0010NameNode会在编辑日志edit日志中记录下来；\n    - fsimage：HDFS元数据镜像文件 ，即将namenode内存中的数据落入磁盘生成的文件；保存了文件系统目录树信息以及文件、块、datanode的映射关系，如下图\n\n\n![](img/Image201910091133.png)\n\n> 说明：\n>\n> ①为hdfs-site.xml中属性dfs.namenode.edits.dir的值决定；用于namenode保存edits.log文件\n>\n> ②为hdfs-site.xml中属性dfs.namenode.name.dir的值决定；用于namenode保存fsimage文件\n\n##### 2、DataNode\n\n- DataNode数据节点的作用\n  - 存储block以及block元数据到datanode本地磁盘；此处的元数据包括数据块的长度、块数据的校验和、时间戳\n\n##### 3 SeconddaryNameNode   \n\n- 为什么引入SecondaryNameNode\n\n  - 为什么元数据存储在NameNode在内存中？\n\n  - 这样做有什么问题？如何解决？\n\n  - HDFS编辑日志文件 editlog：在NameNode节点中的编辑日志editlog中，记录下来客户端对HDFS的所有更改的记录，如增、删、重命名文件（目录）；\n\n  - 作用：一旦系统出故障，可以从editlog进行恢复；\n\n  - 但editlog日志大小会随着时间变在越来越大，导致系统重启根据日志恢复的时候会越来越长；\n\n  - 为了避免这种情况，引入**检查点机制checkpoint**，命名空间镜像fsimage就是HDFS元数据的持久性检查点，即将内存中的元数据落磁盘生成的文件；\n\n  - 此时，namenode如果重启，可以将磁盘中的fsimage文件读入内容，将元数据恢复到某一个检查点，然后再执行检查点之后记录的编辑日志，最后完全恢复元数据。\n\n  - 但是依然，随着时间的推移，editlog记录的日志会变多，那么当namenode重启，恢复元数据过程中，会花越来越长的时间执行editlog中的每一个日志；而在namenode元数据恢复期间，HDFS不可用。\n\n  - 为了解决此问题，引入secondarynamenode辅助namenode，用来合并fsimage及editlog\n\n\n\n![](img/Image201906211525.png)\n\n- SecondaryNameNode定期做checkpoint检查点操作\n\n  - 创建检查点checkpoint的两大条件：\n    - SecondaryNameNode每隔1小时创建一个检查点\n    - 另外，Secondary NameNode每1分钟检查一次，从上一检查点开始，edits日志文件中是否已包括100万个事务，如果是，也会创建检查点\n  - Secondary NameNode首先请求原NameNode进行edits的滚动，这样新的编辑操作就能够进入新的文件中\n  - Secondary NameNode通过HTTP GET方式读取原NameNode中的fsimage及edits\n  - Secondary NameNode读取fsimage到内存中，然后执行edits中的每个操作，并创建一个新的统一的fsimage文件\n  - Secondary NameNode通过HTTP PUT方式将新的fsimage发送到原NameNode\n  - 原NameNode用新的fsimage替换旧的fsimage，同时系统会更新fsimage文件到记录检查点的时间。 \n  - 这个过程结束后，NameNode就有了最新的fsimage文件和更小的edits文件\n\n- SecondaryNameNode一般部署在另外一台节点上\n\n  - 因为它需要占用大量的CPU时间\n  - 并需要与namenode一样多的内存，来执行合并操作\n\n- 如何查看edits日志文件\n\n  ```shell\n  hdfs oev -i edits_0000000000000000256-0000000000000000363 -o /home/hadoop/edit1.xml\n  ```\n\n- 如何查看fsimage文件\n\n  ```shell\n  hdfs oiv -p XML -i fsimage_0000000000000092691 -o fsimage.xml  \n  ```\n\n- checkpoint相关属性\n\n  ```properties\n  # 3600秒(即1小时) 每隔1小时创建一个检查点\n  #The number of seconds between two periodic checkpoints\ndfs.namenode.checkpoint.period = 3600\n  \n  # edits日志文件中是否已包括100万个事务，如果是，也会创建检查点\n  # The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless of whether 'dfs.namenode.checkpoint.period' has expired.\n  dfs.namenode.checkpoint.txns = 1000000 \n  \n  # 60(1分钟)  SecondaryNameNode每1分钟检查一次\n  #  The SecondaryNameNode and CheckpointNode will poll the NameNode every 'dfs.namenode.checkpoint.check.period' seconds to query the number of uncheckpointed transactions.\n  dfs.namenode.checkpoint.check.period = 60\n  ```\n  \n  \n\n##### 4 心跳机制\n\n![](img/Image201906211518.png)\n\n**工作原理：**\n\n1. NameNode启动的时候，会开一个ipc server在那里\n2. DataNode启动后向NameNode注册，每隔**3秒钟**向NameNode发送一个“**心跳heartbeat**”\n3. 心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一DataNode，或删除某个数据块\n4. 如果超过**10分钟**NameNode没有收到某个DataNode 的心跳，则认为该DataNode节点不可用\n5. DataNode周期性（**6小时**）的向NameNode上报当前DataNode上的块状态报告BlockReport；块状态报告包含了一个该 Datanode上所有数据块的列表\n\n**心跳的作用：**\n\n1. 通过周期心跳，NameNode可以向DataNode返回指令\n\n2. 可以判断DataNode是否在线\n\n3. 通过BlockReport，NameNode能够知道各DataNode的存储情况，如磁盘利用率、块列表；跟**负载均衡**有关\n\n4. **hadoop集群刚开始启动时，99.9%的block没有达到最小副本数(dfs.namenode.replication.min默认值为1)，集群处于安全模式，涉及BlockReport；**\n\n**心跳相关配置**\n\n- [hdfs-default.xml](<https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml>)\n- 心跳间隔\n\n| 属性                   | 值   | 解释                                               |\n| ---------------------- | ---- | -------------------------------------------------- |\n| dfs.heartbeat.interval | 3    | Determines datanode heartbeat interval in seconds. |\n\n- **block report**\n\n| More Actions属性             | 值               | 解释                                                 |\n| ---------------------------- | ---------------- | ---------------------------------------------------- |\n| dfs.blockreport.intervalMsec | 21600000 (6小时) | Determines block reporting interval in milliseconds. |\n\n- 查看hdfs-default.xml默认配置文件\n\n![](img/Image201907311730.png)\n\n##### 5 负载均衡\n\n- 什么原因会有可能造成不均衡？\n  - 机器与机器之间磁盘利用率不平衡是HDFS集群非常容易出现的情况\n  - 尤其是在DataNode节点出现故障或在现有的集群上增添新的DataNode的时候\n\n- 为什么需要均衡？\n  - 提升集群存储资源利用率\n  - 从存储与计算两方面提高集群性能\n\n- 如何手动负载均衡？\n\n```shell\n$HADOOP_HOME/sbin/start-balancer.sh -t 5%\t# 磁盘利用率最高的节点若比最少的节点，大于5%，触发均衡\n```\n\n##### 6 小结\n\n- NameNode负责存储元数据，存在内存中\n- DataNode负责存储block块及块的元数据\n- SecondaryNameNode主要负责对HDFS元数据做checkpoint操作\n- 集群的心跳机制，让集群中各节点形成一个整体；主节点知道从节点的死活\n- 节点的上下线，导致存储的不均衡，可以手动触发负载均衡\n","tags":["hadoop"]}]