[{"title":"Spark调优","url":"/2020/01/05/it/spark/Spark调优/","content":"\n# Spark调优\n\n### 1. 分配更多的资源\n\n~~~\n分配更多的资源：\n\t它是性能优化调优的王道，就是增加和分配更多的资源，这对于性能和速度上的提升是显而易见的，\n\t基本上，在一定范围之内，增加资源与性能的提升，是成正比的；写完了一个复杂的spark作业之后，进行性能调优的时候，首先第一步，就是要来调节最优的资源配置；\n\t在这个基础之上，如果说你的spark作业，能够分配的资源达到了你的能力范围的顶端之后，无法再分配更多的资源了，公司资源有限；那么才是考虑去做后面的这些性能调优的点。\n\n相关问题：\n（1）分配哪些资源？\n（2）在哪里可以设置这些资源？\n（3）剖析为什么分配这些资源之后，性能可以得到提升？\n~~~\n\n#### 1.1 分配哪些资源\n\n~~~\nexecutor-memory、executor-cores、driver-memory\n~~~\n\n#### 1.2 在哪里可以设置这些资源\n\n~~~\n在实际的生产环境中，提交spark任务时，使用spark-submit shell脚本，在里面调整对应的参数。\n \t\n 提交任务的脚本:\n spark-submit \\\n --master spark://node1:7077 \\\n --class cn.itcast.WordCount \\\n --num-executors 3 \\    配置executor的数量\n --driver-memory 1g \\   配置driver的内存（影响不大）\n --executor-memory 1g \\ 配置每一个executor的内存大小\n --executor-cores 3 \\   配置每一个executor的cpu个数\n /export/servers/wordcount.jar\n \n~~~\n\n\n\n#### 1.3 参数调节到多大，算是最大\n\n* ==Standalone模式==\n\n~~~\n \t先计算出公司spark集群上的所有资源 每台节点的内存大小和cpu核数，\n \t比如：一共有20台worker节点，每台节点8g内存，10个cpu。\n \t实际任务在给定资源的时候，可以给20个executor、每个executor的内存8g、每个executor的使用的cpu个数10。\n~~~\n\n* ==Yarn模式==\n\n~~~\n \t先计算出yarn集群的所有大小，比如一共500g内存，100个cpu；\n \t这个时候可以分配的最大资源，比如给定50个executor、每个executor的内存大小10g,每个executor使用的cpu个数为2。\n~~~\n\n* 使用原则\n\n~~~\n在资源比较充足的情况下，尽可能的使用更多的计算资源，尽量去调节到最大的大小\n~~~\n\n#### 1.4 为什么调大资源以后性能可以提升\n\n![spark性能优化--分配资源](http://kfly.top/picture/kfly-top/Spark调优/assets/spark性能优化--分配资源.png)\n\n\n\n### 2. 提高并行度\n\n#### 2.1 Spark的并行度指的是什么\n\n~~~\n spark作业中，各个stage的task的数量，也就代表了spark作业在各个阶段stage的并行度！\n    当分配完所能分配的最大资源了，然后对应资源去调节程序的并行度，如果并行度没有与资源相匹配，那么导致你分配下去的资源都浪费掉了。同时并行运行，还可以让每个task要处理的数量变少（很简单的原理。合理设置并行度，可以充分利用集群资源，减少每个task处理数据量，而增加性能加快运行速度。）\n~~~\n\n#### 2.2 如何提高并行度\n\n##### 2.2.1 可以设置task的数量\n\n~~~\n\t至少设置成与spark Application 的总cpu core 数量相同。\n\t最理想情况，150个core，分配150task，一起运行，差不多同一时间运行完毕\n\t官方推荐，task数量，设置成spark Application 总cpu core数量的2~3倍 。\n\t\n\t\n\t比如150个cpu core ，基本设置task数量为300~500. 与理想情况不同的，有些task会运行快一点，比如50s就完了，有些task 可能会慢一点，要一分半才运行完，所以如果你的task数量，刚好设置的跟cpu core 数量相同，可能会导致资源的浪费。\n\t因为比如150个task中10个先运行完了，剩余140个还在运行，但是这个时候，就有10个cpu core空闲出来了，导致浪费。如果设置2~3倍，那么一个task运行完以后，另外一个task马上补上来，尽量让cpu core不要空闲。同时尽量提升spark运行效率和速度。提升性能。\n~~~\n\n\n\n##### 2.2.2 如何设置task数量来提高并行度\n\n~~~\n设置参数spark.defalut.parallelism  \n   默认是没有值的，如果设置了值为10，它会在shuffle的过程才会起作用。\n   比如 val rdd2 = rdd1.reduceByKey(_+_) \n   此时rdd2的分区数就是10\n   \n可以通过在构建SparkConf对象的时候设置，例如：\n   new SparkConf().set(\"spark.defalut.parallelism\",\"500\")\n~~~\n\n\n\n##### 2.2.3 给RDD重新设置partition的数量\n\n~~~\n使用rdd.repartition 来重新分区，该方法会生成一个新的rdd，使其分区数变大。\n此时由于一个partition对应一个task，那么对应的task个数越多，通过这种方式也可以提高并行度。\n~~~\n\n\n\n##### 2.2.4 提高sparksql运行的task数量\n\n~~~\n通过设置参数 spark.sql.shuffle.partitions=500  默认为200；\n可以适当增大，来提高并行度。 比如设置为 spark.sql.shuffle.partitions=500\n~~~\n\n\n\n### 3. RDD的重用和持久化\n\n#### 3.1 实际开发遇到的情况说明\n\n![rdd重用1](http://kfly.top/picture/kfly-top/Spark调优/assets/rdd重用1.png)\n\n~~~\n如上图所示的计算逻辑：\n（1）当第一次使用rdd2做相应的算子操作得到rdd3的时候，就会从rdd1开始计算，先读取HDFS上的文件，然后对rdd1做对应的算子操作得到rdd2,再由rdd2计算之后得到rdd3。同样为了计算得到rdd4，前面的逻辑会被重新计算。\n\n（3）默认情况下多次对一个rdd执行算子操作，去获取不同的rdd，都会对这个rdd及之前的父rdd全部重新计算一次。\n这种情况在实际开发代码的时候会经常遇到，但是我们一定要避免一个rdd重复计算多次，否则会导致性能急剧降低。\n\n总结：可以把多次使用到的rdd，也就是公共rdd进行持久化，避免后续需要，再次重新计算，提升效率。\n~~~\n\n![rdd重用2](http://kfly.top/picture/kfly-top/Spark调优/assets/rdd重用2.png)\n\n\n\n#### 3.2 如何对rdd进行持久化\n\n* 可以调用rdd的cache或者persist方法。\n\n~~~\n（1）cache方法默认是把数据持久化到内存中 ，例如：rdd.cache ，其本质还是调用了persist方法\n（2）persist方法中有丰富的缓存级别，这些缓存级别都定义在StorageLevel这个object中，可以结合实际的应用场景合理的设置缓存级别。例如： rdd.persist(StorageLevel.MEMORY_ONLY),这是cache方法的实现。\n~~~\n\n#### 3.3 rdd持久化的时可以采用序列化\n\n~~~\n（1）如果正常将数据持久化在内存中，那么可能会导致内存的占用过大，这样的话，也许会导致OOM内存溢出。\n（2）当纯内存无法支撑公共RDD数据完全存放的时候，就优先考虑使用序列化的方式在纯内存中存储。将RDD的每个partition的数据，序列化成一个字节数组；序列化后，大大减少内存的空间占用。\n（3）序列化的方式，唯一的缺点就是，在获取数据的时候，需要反序列化。但是可以减少占用的空间和便于网络传输\n（4）如果序列化纯内存方式，还是导致OOM，内存溢出；就只能考虑磁盘的方式，内存+磁盘的普通方式（无序列化）。\n（5）为了数据的高可靠性，而且内存充足，可以使用双副本机制，进行持久化\n\t持久化的双副本机制，持久化后的一个副本，因为机器宕机了，副本丢了，就还是得重新计算一次；\n\t持久化的每个数据单元，存储一份副本，放在其他节点上面，从而进行容错；\n\t一个副本丢了，不用重新计算，还可以使用另外一份副本。这种方式，仅仅针对你的内存资源极度充足。\n\t 比如: StorageLevel.MEMORY_ONLY_2\n~~~\n\n\n\n### 4.  广播变量的使用\n\n#### 4.1 场景描述\n\n~~~\n\t在实际工作中可能会遇到这样的情况，由于要处理的数据量非常大，这个时候可能会在一个stage中出现大量的task，比如有1000个task，这些task都需要一份相同的数据来处理业务，这份数据的大小为100M，该数据会拷贝1000份副本，通过网络传输到各个task中去，给task使用。这里会涉及大量的网络传输开销，同时至少需要的内存为1000*100M=100G，这个内存开销是非常大的。不必要的内存的消耗和占用，就导致了你在进行RDD持久化到内存，也许就没法完全在内存中放下；就只能写入磁盘，最后导致后续的操作在磁盘IO上消耗性能；这对于spark任务处理来说就是一场灾难。\n\n    由于内存开销比较大，task在创建对象的时候，可能会出现堆内存放不下所有对象，就会导致频繁的垃圾回收器的回收GC。GC的时候一定是会导致工作线程停止，也就是导致Spark暂停工作那么一点时间。频繁GC的话，对Spark作业的运行的速度会有相当可观的影响。\n\n~~~\n\n![task共享数据](http://kfly.top/picture/kfly-top/Spark调优/assets/task共享数据.png)\n\n\n\n#### 4.2 广播变量引入\n\n~~~\n\tSpark中分布式执行的代码需要传递到各个executor的task上运行。对于一些只读、固定的数据,每次都需要Driver广播到各个Task上，这样效率低下。广播变量允许将变量只广播给各个executor。该executor上的各个task再从所在节点的BlockManager(负责管理某个executor对应的内存和磁盘上的数据)获取变量，而不是从Driver获取变量，从而提升了效率。\n~~~\n\n![广播变量](http://kfly.top/picture/kfly-top/Spark调优/assets/广播变量.png)\n\n~~~\n广播变量，初始的时候，就在Drvier上有一份副本。通过在Driver把共享数据转换成广播变量。\n\n\ttask在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中，尝试获取变量副本；如果本地没有，那么就从Driver远程拉取广播变量副本，并保存在本地的BlockManager中；\n\t\n\t此后这个executor上的task，都会直接使用本地的BlockManager中的副本。那么这个时候所有该executor中的task都会使用这个广播变量的副本。也就是说一个executor只需要在第一个task启动时，获得一份广播变量数据，之后的task都从本节点的BlockManager中获取相关数据。\n\n\texecutor的BlockManager除了从driver上拉取，也可能从其他节点的BlockManager上拉取变量副本，网络距离越近越好。\n~~~\n\n#### 4.3 使用广播变量后的性能分析\n\n~~~\n比如一个任务需要50个executor，1000个task，共享数据为100M。\n(1)在不使用广播变量的情况下，1000个task，就需要该共享数据的1000个副本，也就是说有1000份数需要大量的网络传输和内存开销存储。耗费的内存大小1000*100=100G.\n\n(2)使用了广播变量后，50个executor就只需要50个副本数据，而且不一定都是从Driver传输到每个节点，还可能是就近从最近的节点的executor的blockmanager上拉取广播变量副本，网络传输速度大大增加；内存开销 50*100M=5G\n\n总结：\n\t不使用广播变量的内存开销为100G，使用后的内存开销5G，这里就相差了20倍左右的网络传输性能损耗和内存开销，使用广播变量后对于性能的提升和影响，还是很可观的。\n\t\n\t广播变量的使用不一定会对性能产生决定性的作用。比如运行30分钟的spark作业，可能做了广播变量以后，速度快了2分钟，或者5分钟。但是一点一滴的调优，积少成多。最后还是会有效果的。\n~~~\n\n\n\n#### 4.4 广播变量使用注意事项\n\n~~~\n（1）能不能将一个RDD使用广播变量广播出去？\n\n       不能，因为RDD是不存储数据的。可以将RDD的结果广播出去。\n\n（2）广播变量只能在Driver端定义，不能在Executor端定义。\n\n（3）在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。\n\n（4）如果executor端用到了Driver的变量，如果不使用广播变量在Executor有多少task就有多少Driver端的变量副本。\n\n（5）如果Executor端用到了Driver的变量，如果使用广播变量在每个Executor中只有一份Driver端的变量副本。\n~~~\n\n\n\n#### 4.5 如何使用广播变量\n\n* 例如\n\n~~~\n(1) 通过sparkContext的broadcast方法把数据转换成广播变量，类型为Broadcast，\n\tval broadcastArray: Broadcast[Array[Int]] = sc.broadcast(Array(1,2,3,4,5,6))\n\t\n(2) 然后executor上的BlockManager就可以拉取该广播变量的副本获取具体的数据。\n\t\t获取广播变量中的值可以通过调用其value方法\n\t val array: Array[Int] = broadcastArray.value\n~~~\n\n\n\n### 5. 尽量避免使用shuffle类算子\n\n#### 5.1 shuffle描述\n\n~~~\n\tspark中的shuffle涉及到数据要进行大量的网络传输，下游阶段的task任务需要通过网络拉取上阶段task的输出数据，shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。\n\t\n\t如果有可能的话，要尽量避免使用shuffle类算子。\n\t因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。\n\t\n~~~\n\n#### 5.2 哪些算子操作会产生shuffle\n\n~~~\n\tspark程序在开发的过程中使用reduceByKey、join、distinct、repartition等算子操作，这里都会产生shuffle，由于shuffle这一块是非常耗费性能的，实际开发中尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。\n~~~\n\n\n\n#### 5.3 如何避免产生shuffle\n\n* 小案例\n\n~~~scala\n//错误的做法：\n// 传统的join操作会导致shuffle操作。\n// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。\nval rdd3 = rdd1.join(rdd2)\n    \n//正确的做法：\n// Broadcast+map的join操作，不会导致shuffle操作。\n// 使用Broadcast将一个数据量较小的RDD作为广播变量。\nval rdd2Data = rdd2.collect()\nval rdd2DataBroadcast = sc.broadcast(rdd2Data)\n\n// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。\n// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。\n// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。\nval rdd3 = rdd1.map(rdd2DataBroadcast...)\n\n// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。\n// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。\n~~~\n\n\n\n#### 5.4 使用map-side预聚合的shuffle操作\n\n* map-side预聚合\n\n~~~\n\t如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。\n\n\t所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。\n\tmap-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。\n\t通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。\n\t而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。\n\t\n\t比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。\n~~~\n\n* ==groupByKey进行单词计数原理==\n\n![1577080609633](http://kfly.top/picture/kfly-top/Spark调优/assets/groupByKey.png)\n\n\n\n* ==reduceByKey单词计数原理==\n\n![1577080686083](http://kfly.top/picture/kfly-top/Spark调优/assets/reduceByKey.png)\n\n\n\n\n\n### 6. 使用高性能的算子\n\n\n\n#### 6.1 使用reduceByKey/aggregateByKey替代groupByKey\n\n* reduceByKey/aggregateByKey 可以进行预聚合操作，减少数据的传输量，提升性能\n* groupByKey 不会进行预聚合操作，进行数据的全量拉取，性能比较低\n\n\n\n#### 6.2 使用mapPartitions替代普通map\n\n~~~\n\tmapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。\n\t但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！\n~~~\n\n\n\n#### 6.3 使用foreachPartitions替代foreach\n\n~~~\n\t原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。\n\t在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；\t但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。\n~~~\n\n\n\n#### 6.4 使用filter之后进行coalesce操作\n\n~~~\n\t通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。\n\t因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。\n\t因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。\n~~~\n\n\n\n#### 6.5 使用repartitionAndSortWithinPartitions替代repartition与sort类操作\n\n~~~\n\trepartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。\n\t因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。\n~~~\n\n\n\n### 7. 使用Kryo优化序列化性能\n\n#### 7.1 spark序列化介绍\n\n~~~\n\tSpark在进行任务计算的时候，会涉及到数据跨进程的网络传输、数据的持久化，这个时候就需要对数据进行序列化。Spark默认采用Java的序列化器。默认java序列化的优缺点如下:\n其好处：\n\t处理起来方便，不需要我们手动做其他操作，只是在使用一个对象和变量的时候，需要实现Serializble接口。\n其缺点：\n\t默认的序列化机制的效率不高，序列化的速度比较慢；序列化以后的数据，占用的内存空间相对还是比较大。\n\nSpark支持使用Kryo序列化机制。Kryo序列化机制，比默认的Java序列化机制，速度要快，序列化后的数据要更小，大概是Java序列化机制的1/10。所以Kryo序列化优化以后，可以让网络传输的数据变少；在集群中耗费的内存资源大大减少。\n\n~~~\n\n#### 7.2 Kryo序列化启用后生效的地方\n\n~~~\nKryo序列化机制，一旦启用以后，会生效的几个地方：\n（1）算子函数中使用到的外部变量\n\t算子中的外部变量可能来着与driver需要涉及到网络传输，就需要用到序列化。\n\t    最终可以优化网络传输的性能，优化集群中内存的占用和消耗\n\t\t\n（2）持久化RDD时进行序列化，StorageLevel.MEMORY_ONLY_SER\n\t将rdd持久化时，对应的存储级别里，需要用到序列化。\n\t    最终可以优化内存的占用和消耗；持久化RDD占用的内存越少，task执行的时候，创建的对象，就不至于频繁的占满内存，频繁发生GC。\n\t\t\n（3）\t产生shuffle的地方，也就是宽依赖\n\t下游的stage中的task，拉取上游stage中的task产生的结果数据，跨网络传输，需要用到序列化。最终可以优化网络传输的性能\n\t\n\t\n~~~\n\n\n\n#### 7.3 如何开启Kryo序列化机制\n\n~~~scala\n// 创建SparkConf对象。\nval conf = new SparkConf().setMaster(...).setAppName(...)\n// 设置序列化器为KryoSerializer。\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n\n// 注册要序列化的自定义类型。\nconf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))\n~~~\n\n\n\n### 8. 使用fastutil优化数据格式\n\n#### 8.1 fastutil介绍\n\n```\nfastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue；\n\nfastutil能够提供更小的内存占用，更快的存取速度；我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set.\n```\n\n#### 8.2 fastutil好处\n\n```\nfastutil集合类，可以减小内存的占用，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度；\n```\n\n#### 8.3 Spark中应用fastutil的场景和使用\n\n##### 8.3.1 算子函数使用了外部变量\n\n```\n（1）你可以使用Broadcast广播变量优化；\n\n（2）可以使用Kryo序列化类库，提升序列化性能和效率；\n\n（3）如果外部变量是某种比较大的集合，那么可以考虑使用fastutil改写外部变量；\n\n首先从源头上就减少内存的占用(fastutil)，通过广播变量进一步减少内存占用，再通过Kryo序列化类库进一步减少内存占用。\n```\n\n##### 8.3.2 算子函数里使用了比较大的集合Map/List\n\n```\n在你的算子函数里，也就是task要执行的计算逻辑里面，如果有逻辑中，出现，要创建比较大的Map、List等集合，\n可能会占用较大的内存空间，而且可能涉及到消耗性能的遍历、存取等集合操作； \n那么此时，可以考虑将这些集合类型使用fastutil类库重写，\n\n使用了fastutil集合类以后，就可以在一定程度上，减少task创建出来的集合类型的内存占用。 \n避免executor内存频繁占满，频繁唤起GC，导致性能下降。\n```\n\n\n\n##### 8.3.3 fastutil的使用\n\n```\n第一步：在pom.xml中引用fastutil的包\n    <dependency>\n      <groupId>fastutil</groupId>\n      <artifactId>fastutil</artifactId>\n      <version>5.0.9</version>\n    </dependency>\n    \n第二步：平时使用List （Integer）的替换成IntList即可。 \n\tList<Integer>的list对应的到fastutil就是IntList类型\n\t\n\t\n使用说明：\n基本都是类似于IntList的格式，前缀就是集合的元素类型； \n特殊的就是Map，Int2IntMap，代表了key-value映射的元素类型。\n```\n\n\n\n### 9. 调节数据本地化等待时长\n\n~~~\n\tSpark在Driver上对Application的每一个stage的task进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先会希望每个task正好分配到它要计算的数据所在的节点，这样的话就不用在网络间传输数据；\n\n\t但是通常来说，有时事与愿违，可能task没有机会分配到它的数据所在的节点，为什么呢，可能那个节点的计算资源和计算能力都满了；所以这种时候，通常来说，Spark会等待一段时间，默认情况下是3秒（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后实在是等待不了了，就会选择一个比较差的本地化级别，比如说将task分配到距离要计算的数据所在节点比较近的一个节点，然后进行计算。\n\n~~~\n\n\n\n#### 9.1 本地化级别\n\n```\n（1）PROCESS_LOCAL：进程本地化\n\t代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好\n（2）NODE_LOCAL：节点本地化\n\t代码和数据在同一个节点中；比如说数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是数据和task在一个节点上的不同executor中；数据需要在进程间进行传输；性能其次\n（3）RACK_LOCAL：机架本地化\t\n\t数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输； 性能比较差\n（4）\tANY：无限制\n\t数据和task可能在集群中的任何地方，而且不在一个机架中；性能最差\n\t\n```\n\n#### 9.2 数据本地化等待时长\n\n```\nspark.locality.wait，默认是3s\n首先采用最佳的方式，等待3s后降级,还是不行，继续降级...,最后还是不行，只能够采用最差的。\n\n```\n\n#### 9.3 如何调节参数并且测试\n\n```\n修改spark.locality.wait参数，默认是3s，可以增加\n\n下面是每个数据本地化级别的等待时间，默认都是跟spark.locality.wait时间相同，\n默认都是3s(可查看spark官网对应参数说明，如下图所示)\nspark.locality.wait.node\nspark.locality.wait.process\nspark.locality.wait.rack\n\n```\n\n![data-local-spark](/Users/dingchuangshi/Documents/hexo-kfly-blog/source/_posts/it/spark/http://kfly.top/picture/kfly-top/Spark调优/assets/data-local-spark.png)\n\n\n\n```\n在代码中设置：\nnew SparkConf().set(\"spark.locality.wait\",\"10\")\n\n然后把程序提交到spark集群中运行，注意观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。 \n日志里面会显示，starting task .... PROCESS LOCAL、NODE LOCAL.....\n例如：\nStarting task 0.0 in stage 1.0 (TID 2, 192.168.200.102, partition 0, NODE_LOCAL, 5254 bytes)\n\n观察大部分task的数据本地化级别 \n如果大多都是PROCESS_LOCAL，那就不用调节了。如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长。应该是要反复调节，每次调节完以后，再来运行，观察日志 \n看看大部分的task的本地化级别有没有提升；看看整个spark作业的运行时间有没有缩短。\n\n注意注意：\n在调节参数、运行任务的时候，别本末倒置，本地化级别倒是提升了， 但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了。\n```\n\n\n\n### 10. 基于Spark内存模型调优\n\n#### 10.1 spark中executor内存划分\n\n* Executor的内存主要分为三块\n  * 第一块是让task执行我们自己编写的代码时使用；\n  * 第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用\n  * 第三块是让RDD缓存时使用\n\n#### 10.2 spark的内存模型\n\n~~~\n\t在spark1.6版本以前 spark的executor使用的静态内存模型，但是在spark1.6开始，多增加了一个统一内存模型。\n\t通过spark.memory.useLegacyMode 这个参数去配置\n\t\t\t默认这个值是false，代表用的是新的动态内存模型；\n\t\t\t如果想用以前的静态内存模型，那么就要把这个值改为true。\n~~~\n\n\n\n##### 10.2.1 静态内存模型\n\n![1570604272790](http://kfly.top/picture/kfly-top/Spark调优/assets/1570604272790.png)\n\n~~~\n实际上就是把我们的一个executor分成了三部分，\n\t一部分是Storage内存区域，\n\t一部分是execution区域，\n\t还有一部分是其他区域。如果使用的静态内存模型，那么用这几个参数去控制：\n\t\nspark.storage.memoryFraction：默认0.6\nspark.shuffle.memoryFraction：默认0.2  \n所以第三部分就是0.2\n\n如果我们cache数据量比较大，或者是我们的广播变量比较大，\n\t那我们就把spark.storage.memoryFraction这个值调大一点。\n\t但是如果我们代码里面没有广播变量，也没有cache，shuffle又比较多，那我们要把spark.shuffle.memoryFraction 这值调大。\n~~~\n\n* 静态内存模型的缺点\n\n~~~\n我们配置好了Storage内存区域和execution区域后，我们的一个任务假设execution内存不够用了，但是它的Storage内存区域是空闲的，两个之间不能互相借用，不够灵活，所以才出来我们新的统一内存模型。\n~~~\n\n\n\n##### 10.2.2 统一内存模型\n\n![img](http://kfly.top/picture/kfly-top/Spark调优/assets/image2018-11-1_16-39-33.png)\n\n~~~\n\t动态内存模型先是预留了300m内存，防止内存溢出。动态内存模型把整体内存分成了两部分，\n由这个参数表示spark.memory.fraction 这个指的默认值是0.6 代表另外的一部分是0.4,\n\n然后spark.memory.fraction 这部分又划分成为两个小部分。这两小部分共占整体内存的0.6 .这两部分其实就是：Storage内存和execution内存。由spark.memory.storageFraction 这个参数去调配，因为两个共占0.6。如果spark.memory.storageFraction这个值配的是0.5,那说明这0.6里面 storage占了0.5，也就是execution占了0.3 。\n~~~\n\n\n\n* 统一内存模型有什么特点呢?\n\n  ~~~\n  Storage内存和execution内存 可以相互借用。不用像静态内存模型那样死板，但是是有规则的\n  ~~~\n\n\n\n  * ==场景一==\n\n    * Execution使用的时候发现内存不够了，然后就会把storage的内存里的数据驱逐到磁盘上。\n\n      ![1570604662552](http://kfly.top/picture/kfly-top/Spark调优/assets/1570604662552.png)\n\n\n  * ==场景二==\n    * 一开始execution的内存使用得不多，但是storage使用的内存多，所以storage就借用了execution的内存，但是后来execution也要需要内存了，这个时候就会把storage的内存里的数据写到磁盘上，腾出内存空间。\n\n![1570604675176](http://kfly.top/picture/kfly-top/Spark调优/assets/1570604675176.png)\n\n\n\n~~~\n为什么受伤的都是storage呢？\n\n是因为execution里面的数据是马上就要用的，而storage里的数据不一定马上就要用。\t\n~~~\n\n\n\n##### 10.2.3 任务提交脚本参考\n\n* 以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节\n\n```shell\n./bin/spark-submit \\\n  --master yarn-cluster \\\n  --num-executors 100 \\\n  --executor-memory 6G \\\n  --executor-cores 4 \\\n  --driver-memory 1G \\\n  --conf spark.default.parallelism=1000 \\\n  --conf spark.storage.memoryFraction=0.5 \\\n  --conf spark.shuffle.memoryFraction=0.3 \\\n```\n\n##### 10.2.4 个人经验\n\n~~~shell\njava.lang.OutOfMemoryError\nExecutorLostFailure\nExecutor exit code 为143\nexecutor lost\nhearbeat time out\nshuffle file lost\n\n# 如果遇到以上问题，很有可能就是内存除了问题，可以先尝试增加内存。如果还是解决不了，那么请听下一次数据倾斜调优的课。\n~~~\n\n### 11.  数据倾斜原理和现象分析\n\n#### 11.1 数据倾斜概述\n\n~~~\n\t有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。\n\t数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。\n~~~\n\n\n\n#### 11.2 数据倾斜发生时的现象\n\n* （1）绝大多数task执行得都非常快，但个别task执行极慢\n\n  ~~~\n  \t你的大部分的task，都执行的特别快，很快就执行完了，剩下几个task，执行的特别特别慢，\n  前面的task，一般10s可以执行完5个；最后发现某个task，要执行1个小时，2个小时才能执行完一个task。\n  \t\n  \t这个时候就出现数据倾斜了。\n  这种方式还算好的，因为虽然老牛拉破车一样，非常慢，但是至少还能跑。\n  ~~~\n\n* （2）绝大数task执行很快，有的task直接报OOM (Jvm Out Of Memory) 异常\n\n  ~~~\n  \t运行的时候，其他task都很快执行完了，也没什么特别的问题；但是有的task，就是会突然间报了一个OOM，JVM Out Of Memory，内存溢出了，task failed，task lost，resubmitting task等日志异常信息。反复执行几次都到了某个task就是跑不通，最后就挂掉。\n  \n  \t某个task就直接OOM，那么基本上也是因为数据倾斜了，task分配的数量实在是太大了！！！所以内存放不下，然后你的task每处理一条数据，还要创建大量的对象。内存爆掉了。\n  ~~~\n\n\n#### 11.3 数据倾斜发生的原理\n\n![数据倾斜](http://kfly.top/picture/kfly-top/Spark调优/assets/数据倾斜.png)\n\n~~~\n如上图所示：\n\t在进行任务计算shuffle操作的时候，第一个task和第二个task各分配到了1万条数据；需要5分钟计算完毕；第一个和第二个task，可能同时在5分钟内都运行完了；第三个task要98万条数据，98 * 5 = 490分钟 = 8个小时；\n\t本来另外两个task很快就运行完毕了（5分钟），第三个task数据量比较大，要8个小时才能运行完，就导致整个spark作业，也得8个小时才能运行完。最终导致整个spark任务计算特别慢。\n~~~\n\n\n\n#### 11.4 数据倾斜如何定位原因\n\n* 主要是根据log日志信息去定位\n\n  ~~~\n  \t数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。\n  \t出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。因为某个或者某些key对应的数据，远远的高于其他的key。\n  ~~~\n\n* 分析定位逻辑\n\n  ~~~\n  \t由于代码中有大量的shuffle操作，一个job会划分成很多个stage，首先要看的，就是数据倾斜发生在第几个stage中。\n  \t可以在任务运行的过程中，观察任务的UI界面，可以观察到每一个stage中运行的task的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。\n  \t比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完;而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。\n  \t此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。\n  ~~~\n\n![20170308091203159](http://kfly.top/picture/kfly-top/Spark调优/assets/20170308091203159.png)\n\n**某个task莫名其妙内存溢出的情况**\n\n~~~\n\t这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。\n\t但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。\n~~~\n\n\n\n**查看导致数据倾斜的key的数据分布情况**\n\n~~~\n\t知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。\n此时根据你执行操作的情况不同，可以有很多种查看key分布的方式：\n\t如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。\n\t如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。\n\t举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。\n~~~\n\n~~~scala\nval sampledPairs = pairs.sample(false, 0.1)\nval sampledWordCounts = sampledPairs.countByKey()\nsampledWordCounts.foreach(println(_))\n\n//sample算子时用来抽样用的，其有3个参数\n\n//withReplacement：表示抽出样本后是否在放回去，true表示会放回去，这也就意味着抽出的样本可能有重复\n\n//fraction ：抽出多少，这是一个double类型的参数,0-1之间，eg:0.3表示抽出30%\n\n//seed：表示一个种子，根据这个seed随机抽取，一般情况下只用前两个参数就可以，那么这个参数是干嘛的呢，这个参数一般用于调试，有时候不知道是程序出问题还是数据出了问题，就可以将这个参数设置为定值\n~~~\n\n\n\n#### 11.5 数据倾斜原因总结\n\n* 数据本身问题\n\n  ~~~\n  （1）、key本身分布不均衡（包括大量的key为空）\n  （2）、key的设置不合理\n  ~~~\n\n* spark使用不当的问题\n\n  ~~~\n  （1）、shuffle时的并发度不够\n  （2）、计算方式有误\t\n  ~~~\n\n\n#### 11.6 数据倾斜的后果\n\n~~~\n（1）spark中的stage的执行时间受限于最后那个执行完成的task,因此运行缓慢的任务会拖垮整个程序的运行速度（分布式程序运行的速度是由最慢的那个task决定的）。\n\n（2）过多的数据在同一个task中运行，将会把executor内存撑爆，导致OOM内存溢出。\n~~~\n\n### 12. spark中数据倾斜的解决方案\n\n#### 解决方案一：使用Hive ETL预处理数据\n\n<font color=red>方案适用场景</font>：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀(比如某个key对应了100万数据，其他key才对应了10条数据)，而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。\n<font color=red>方案实现思路</font>：此时可以评估一下，是否可以通过Hive来进行数据预处理(即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join)，然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。\n<font color=red>方案实现原理</font>：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。\n<font color=red>方案优点</font>：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。\n<font color=red>方案缺点</font>：治标不治本，Hive ETL中还是会发生数据倾斜。\n<font color=red>方案实践经验</font>：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。\n<font color=red>项目实践经验</font>：有一个交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。\n\n![交互式用户行为分析系统](http://kfly.top/picture/kfly-top/Spark调优/assets/交互式用户行为分析系统.png)\n\n\n\n#### 解决方案二：过滤少数导致倾斜的key\n\n　　<font color=red>方案适用场景</font>：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。\n　　<font color=red>方案实现思路</font>：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。\n　　<font color=red>方案实现原理</font>：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。\n　　<font color=red>方案优点</font>：实现简单，而且效果也很好，可以完全规避掉数据倾斜。\n　　<font color=red>方案缺点</font>：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。\n　　<font color=red>方案实践经验</font>：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。\n\n\n\n#### 解决方案三：提高shuffle操作的并行度(效果差)\n\n　　　<font color=red>方案适用场景</font>：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。\n　　　<font color=red>方案实现思路</font>：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。\n　　　<font color=red>方案实现原理</font>：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。\n　　　<font color=red>方案优点</font>：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。\n　　　<font color=red>方案缺点</font>：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。\n　　　<font color=red>方案实践经验</font>：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。\n\n![1570609831990](http://kfly.top/picture/kfly-top/Spark调优/assets/1570609831990.png)\n\n\n\n#### 解决方案四：两阶段聚合（局部聚合+全局聚合）\n\n　　<font color=red>方案适用场景</font>：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。\n　　<font color=red>方案实现思路</font>：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。\n　　<font color=red>方案实现原理</font>：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。\n　　<font color=red>方案优点</font>：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。\n　　<font color=red>方案缺点</font>：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。\n\n~~~scala\n//案例\n//  如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作流程如下:\n//    (1) 将原始的 key 转化为  随机值 + key  (随机值 = Random.nextInt)\n//    (2) 对数据进行 reduceByKey(func)\n//    (3) 将 key + 随机值转成 key\n//    (4) 再对数据进行 reduceByKey(func)\n\nobject WordCountAggTest {\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"WordCount\")\n    val sc = new SparkContext(conf)\n    val array = Array(\"you you\",\"you you\",\"you you\",\n      \"you you\",\n      \"you you\",\n      \"you you\",\n      \"you you\",\n      \"jump jump\")\n    val rdd = sc.parallelize(array,8)\n    rdd.flatMap( line => line.split(\" \"))\n      .map(word =>{\n        val prefix = (new util.Random).nextInt(3)\n        (prefix+\"_\"+word,1)\n      }).reduceByKey(_+_)\n       .map( wc =>{\n         val newWord=wc._1.split(\"_\")(1)\n         val count=wc._2\n         (newWord,count)\n       }).reduceByKey(_+_)\n      .foreach( wc =>{\n        println(\"单词：\"+wc._1 + \" 次数：\"+wc._2)\n      })\n\n  }\n}\n注：我们这儿使用的是reduceByKey天然的有调优的效果，如果这儿是groupBykey那么发生数据倾斜的概率就会更大，更严重。\n~~~\n\n\n\n#### 解决方案五：将reduce join转为map join\n\n　　<font color=red>方案适用场景</font>：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。\n　　<font color=red>方案实现思路</font>：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。\n　　<font color=red>方案实现原理</font>：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。\n　　<font color=red>方案优点</font>：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。\n　　<font color=red>方案缺点</font>：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。\n\n![reduce joinz转换为map join ](http://kfly.top/picture/kfly-top/Spark调优/assets/reduce joinz转换为map join .png)\n\n~~~scala\nobject MapJoinTest {\n \n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"WordCount\")\n    val sc = new SparkContext(conf)\n    val lista=Array(\n      Tuple2(\"001\",\"令狐冲\"),\n      Tuple2(\"002\",\"任盈盈\")\n    )\n     //数据量小一点\n    val listb=Array(\n      Tuple2(\"001\",\"一班\"),\n      Tuple2(\"002\",\"二班\")\n    )\n    val listaRDD = sc.parallelize(lista)\n    val listbRDD = sc.parallelize(listb)\n    //val result: RDD[(String, (String, String))] = listaRDD.join(listbRDD)\n     //设置广播变量\n    val listbBoradcast = sc.broadcast(listbRDD.collect())\n    listaRDD.map(  tuple =>{\n      val key = tuple._1\n      val name = tuple._2\n      val map = listbBoradcast.value.toMap\n      val className = map.get(key)\n      (key,(name,className))\n    }).foreach( tuple =>{\n      println(\"班级号\"+tuple._1 + \" 姓名：\"+tuple._2._1 + \" 班级名：\"+tuple._2._2.get)\n    })\n  }\n}\n~~~\n\n\n\n#### 解决方案六：采样倾斜key并分拆join操作\n\n　　<font color=red>方案适用场景</font>：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。\n　　<font color=red>方案实现思路</font>：\n　　1、对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。\n　　2、然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。\n　　3、接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。\n　　4、再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，==此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。==\n　　5、而另外两个普通的RDD就照常join即可。\n　　6、最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。\n　　<font color=red>方案实现原理</font>：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。\n　　<font color=red>方案优点</font>：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。\n　　<font color=red>方案缺点</font>：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。\n\n![随机前缀和扩容RDD](http://kfly.top/picture/kfly-top/Spark调优/assets/随机前缀和扩容RDD.png)\n\n\n\n#### 解决方案七：使用随机前缀和扩容RDD进行join\n\n　　<font color=red>方案适用场景</font>：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用这一种方案来解决问题了。\n　　<font color=red>方案实现思路</font>：\n　　1、该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。\n　　2、然后将该RDD的每条数据都打上一个n以内的随机前缀。\n　　3、同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。\n　　4、最后将两个处理后的RDD进行join即可。\n　　方案实现原理：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。\n　　<font color=red>方案优点</font>：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。\n　　<font color=red>方案缺点</font>：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。\n　　<font color=red>方案实践经验</font>：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。\n\n\n\n#### 解决方案八：把上面的几种数据倾斜的解决方案综合的灵活运行\n\n\n\n### 13. Shuffle调优\n\n#### 13.1 什么时候发生shuffle\n\n![1567231926304](http://kfly.top/picture/kfly-top/Spark调优/assets/1567231926304.png)\n\n![1567232029615](http://kfly.top/picture/kfly-top/Spark调优/assets/1567232029615.png)\n\n#### 13.2 Shuffle的核心组件\n\n碰到ShuffleDenpendency就进行stage的划分，ShuffleMapStage: 为shuffle提供数据的中间stage，ResultStage: 为一个action操作计算结果的stage。\n\n![1567232245246](http://kfly.top/picture/kfly-top/Spark调优/assets/1567232245246.png)\n\n#### 13.3 Shuffle组件调度\n\n![1567232408746](http://kfly.top/picture/kfly-top/Spark调优/assets/1567232408746.png)\n\n\n\n\n\n#### 13.4 Shuffle原理剖析\n\n##### 13.4.1 MapOutputTracker\n\n解决的一个问题是resut task如何知道从哪个Executor去拉取Shuffle data\n\n![1567232849493](http://kfly.top/picture/kfly-top/Spark调优/assets/1567232849493.png)\n\n##### 13.4.2 ShuffleWriter\n\n**（1）HashShuffleWriter**\n\n![1567232923386](http://kfly.top/picture/kfly-top/Spark调优/assets/1567232923386.png)\n\n特点：根据Hash分区，分区数是m * n 个。\n\n~~~scala\nval counts: RDD[(String, Int)] \n\t= wordCount.reduceByKey(new HashPartitioner(2), (x, y) => x + y)\n\n~~~\n\n![1567233073556](http://kfly.top/picture/kfly-top/Spark调优/assets/1567233073556.png)\n\n**（2）SortShuffleWriter**\n\n![1567233308867](http://kfly.top/picture/kfly-top/Spark调优/assets/1567233308867.png)\n\n特点：\n\na、文件数量为m\n\nb、如果需要排序或者需要combine，那么每一个partition数据排序要自己实现。（SortShuffleWriter里的sort指的是对partition的分区号进行排序）\n\nc、数据先放在内存,内存不够则写到磁盘中,最后再全部写到磁盘。\n\n**（3）BypassMergeSortShuffleWriter**\n\n![1567234608013](http://kfly.top/picture/kfly-top/Spark调优/assets/1567234608013.png)\n\n\n\n这种模式同时具有HashShuffleWriter和SortShuffleter的特点。因为其实HashShufflerWriter的性能不错，但是如果task数太多的话，性能话下降，所以Spark在task数较少的时候自动使用了这种模式，一开始还是像HashShufflerWriter那种生成多个文件，但是最后会把多个文件合并成一个文件。然后下游来读取文件。默认map的分区需要小于spark.shuffle.sort.bypassMergeThreshold(默认是200),因为如何分区数太多，产生的小文件就会很多性能就会下降。\n\n\n\n##### 13.4.3 ShuffleReader\n\n![1567235022607](http://kfly.top/picture/kfly-top/Spark调优/assets/1567235022607.png)\n\n##### 3.4.4 Spark Shuffle参数调优\n\n==spark.shuffle.file.buffer==\n\n- 默认值：32k\n- 参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。\n- 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。\n\n==spark.reducer.maxSizeInFlight==\n\n- 默认值：48m\n- 参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。\n- 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。\n\n==spark.shuffle.io.maxRetries==\n\n- 默认值：3\n- 参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。\n- 调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。\n\n==spark.shuffle.io.retryWait==\n\n- 默认值：5s\n- 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。\n- 调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。\n\n==spark.shuffle.memoryFraction==（Spark1.6是这个参数，1.6以后参数变了，具体参考上一讲Spark内存模型知识）\n\n- 默认值：0.2\n- 参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。\n- 调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。\n\n==spark.shuffle.manager==\n\n- 默认值：sort\n- 参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。Spark1.6以后把hash方式给移除了，tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。\n- 调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。\n\n==spark.shuffle.sort.bypassMergeThreshold==\n\n- 默认值：200\n- 参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。\n- 调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。\n\n","tags":["调优","spark"]},{"title":"SparkCore/sql知识梳理","url":"/2019/12/27/it/spark/SparkCore:SQL知识梳理/","content":"\n# 一、Spark Code\n\n## Spark简介\n\n### 1. spark是什么\n\n* **Apache Spark™** is a unified analytics engine for large-scale data processing.\n\n* spark是针对于大规模数据处理的统一分析引擎\n\n  ~~~\n  \tspark是在Hadoop基础上的改进，是UC Berkeley AMP lab所开源的类Hadoop MapReduce的通用的并行计算框架，Spark基于map reduce算法实现的分布式计算，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是Job中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的map reduce的算法。\n  \t\n  \tspark是基于内存计算框架，计算速度非常之快，但是它仅仅只是涉及到计算，并没有涉及到数据的存储，后期需要使用spark对接外部的数据源，比如hdfs。\n  ~~~\n\n### 2. spark的四大特性\n\n#### 2.1 速度快\n\n* 运行速度提高100倍\n\n  * Apache Spark使用最先进的DAG调度程序，查询优化程序和物理执行引擎，实现批量和流式数据的高性能。\n\n* spark比mapreduce快的2个主要原因\n\n  * 1、==基于内存==\n\n    ~~~\n    （1）mapreduce任务后期再计算的时候，每一个job的输出结果会落地到磁盘，后续有其他的job需要依赖于前面job的输出结果，这个时候就需要进行大量的磁盘io操作。性能就比较低。\n    \n    \n    （2）spark任务后期再计算的时候，job的输出结果可以保存在内存中，后续有其他的job需要依赖于前面job的输出结果，这个时候就直接从内存中获取得到，避免了磁盘io操作，性能比较高\n    \n    ~~~\n\n  * 2、==进程与线程==\n\n    ~~~\n    （1）mapreduce任务以进程的方式运行在yarn集群中，比如程序中有100个MapTask，一个task就需要一个进程，这些task要运行就需要开启100个进程。\n    \n    （2）spark任务以线程的方式运行在进程中，比如程序中有100个MapTask，后期一个task就对应一个线程，这里就不在是进程，这些task需要运行，这里可以极端一点：\n    只需要开启1个进程，在这个进程中启动100个线程就可以了。\n    进程中可以启动很多个线程，而开启一个进程与开启一个线程需要的时间和调度代价是不一样。 开启一个进程需要的时间远远大于开启一个线程。\n    ~~~\n\n#### 2.2 易用性\n\n- 可以快速去编写spark程序通过 java/scala/python/R/SQL等不同语言\n\n#### 2.3 通用性\n\n* spark框架不在是一个简单的框架，可以把spark理解成一个==**生态系统**==，它内部是包含了很多模块，基于不同的应用场景可以选择对应的模块去使用\n  * ==**sparksql**==\n    * 通过sql去开发spark程序做一些离线分析\n  * ==**sparkStreaming**==\n    * 主要是用来解决公司有实时计算的这种场景\n  * ==**Mlib**==\n    * 它封装了一些机器学习的算法库\n  * ==**Graphx**==\n    * 图计算\n\n#### 2.4 兼容性\n\nspark程序就是一个计算逻辑程序，这个任务要运行就需要计算资源（内存、cpu、磁盘），哪里可以给当前这个任务提供计算资源，就可以把spark程序提交到哪里去运行\n\n* ==**standAlone**==\n  * 它是spark自带的独立运行模式，整个任务的资源分配由spark集群的老大Master负责\n* ==**yarn**==\n  * 可以把spark程序提交到yarn中运行，整个任务的资源分配由yarn中的老大ResourceManager负责\n* **mesos**\n  * 它也是apache开源的一个类似于yarn的资源调度平台\n\n### 3. spark集群架构\n\n![spark](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/spark.png)\n\n- **==Driver==**\n  - 它会执行客户端写好的main方法，它会构建一个名叫SparkContext对象\n    - 该对象是所有spark程序的执行入口\n\n- **==Application==**\n  - 就是一个spark的应用程序，它是包含了客户端的代码和任务运行的资源信息\n\n- **==ClusterManager==**\n  - 它是给程序提供计算资源的外部服务\n    - **standAlone**\n      - 它是spark自带的集群模式，整个任务的资源分配由spark集群的老大Master负责\n    - **yarn**\n      - 可以把spark程序提交到yarn中运行，整个任务的资源分配由yarn中的老大ResourceManager负责\n    - **mesos**\n      - 它也是apache开源的一个类似于yarn的资源调度平台。\n\n\n- **==Master==**\n  - 它是整个spark集群的主节点，负责任务资源的分配\n\n- **==Worker==**\n  - 它是整个spark集群的从节点，负责任务计算的节点\n\n- **==Executor==**\n  - 它是一个进程，它会在worker节点启动该进程（计算资源）\n\n- **==Task==**\n  - spark任务是以task线程的方式运行在worker节点对应的executor进程中\n\n### 4. Spark启动停止\n\n```reStructuredText\n(1) 如何恢复到上一次活着master挂掉之前的状态?\n\t在高可用模式下，整个spark集群就有很多个master，其中只有一个master被zk选举成活着的master，其他的多个master都处于standby，同时把整个spark集群的元数据信息通过zk中节点进行保存。\n\n\t后期如果活着的master挂掉。首先zk会感知到活着的master挂掉，下面开始在多个处于standby中的master进行选举，再次产生一个活着的master，这个活着的master会读取保存在zk节点中的spark集群元数据信息，恢复到上一次master的状态。整个过程在恢复的时候经历过了很多个不同的阶段，每个阶段都需要一定时间，最终恢复到上个活着的master的转态，整个恢复过程一般需要1-2分钟。\n\n(2) 在master的恢复阶段对任务的影响?\n   a）对已经运行的任务是没有任何影响\n   \t  由于该任务正在运行，说明它已经拿到了计算资源，这个时候就不需要master。\n   b) 对即将要提交的任务是有影响\n   \t  由于该任务需要有计算资源，这个时候会找活着的master去申请计算资源，由于没有一个活着的master,该任务是获取不到计算资源，也就是任务无法运行。\n```\n\n### 5. 初识spark程序\n\n```shell\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://node01:7077 \\\n--executor-memory 1G \\\n--total-executor-cores 2 \\\nexamples/jars/spark-examples_2.11-2.3.3.jar \\\n10\n\n####参数说明\n--class：指定包含main方法的主类\n--master：指定spark集群master地址\n--executor-memory：指定任务在运行的时候需要的每一个executor内存大小\n--total-executor-cores： 指定任务在运行的时候需要总的cpu核数\n\n### 注意\nspark集群中有很多个master，并不知道哪一个master是活着的master，即使你知道哪一个master是活着的master，它也有可能下一秒就挂掉，这里就可以把所有master都罗列出来\n--master spark://node01:7077,node02:7077,node03:7077\n\n后期程序会轮训整个master列表，最终找到活着的master，然后向它申请计算资源，最后运行程序。\n```\n\n# 二、SparkRDD\n\n### 1. RDD是什么\n\n* RDD（Resilient Distributed Dataset）叫做==弹性分布式数据集==，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合.\n  * **Dataset**:          就是一个集合，存储很多数据.\n  * **Distributed**：它内部的元素进行了分布式存储，方便于后期进行分布式计算.\n  * **Resilient**：     表示弹性，rdd的数据是可以保存在内存或者是磁盘中.\n\n* （1）A list of partitions\n  * ==一个分区（Partition）列表，数据集的基本组成单位。==\n\n~~~\n\t这里表示一个rdd有很多分区，每一个分区内部是包含了该rdd的部分数据，\nspark中任务是以task线程的方式运行， 一个分区就对应一个task线程。\n\n\t用户可以在创建RDD时指定RDD的分区个数，如果没有指定，那么就会采用默认值。\n    val rdd=sparkContext.textFile(\"/words.txt\")\n    如果该文件的block块个数小于等于2，这里生产的RDD分区数就为2\n    如果该文件的block块个数大于2，这里生产的RDD分区数就与block块个数保持一致\n~~~\n\n* （2）A function for computing each split\n  * ==一个计算每个分区的函数==\n\n~~~\n\tSpark中RDD的计算是以分区为单位的，每个RDD都会实现compute计算函数以达到这个目的.\n~~~\n\n* （3）A list of dependencies on other RDDs\n  * ==一个rdd会依赖于其他多个rdd==\n\n~~~\n  这里就涉及到rdd与rdd之间的依赖关系，spark任务的容错机制就是根据这个特性（血统）而来。\n~~~\n\n* （4）Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)\n  * ==一个Partitioner，即RDD的分区函数（可选项）==\n\n~~~\n当前Spark中实现了两种类型的分区函数，\n一个是基于哈希的HashPartitioner，(key.hashcode % 分区数= 分区号)\n另外一个是基于范围的RangePartitioner。\n只有对于key-value的RDD,并且产生shuffle，才会有Partitioner，\n\n非key-value的RDD的Parititioner的值是None。\n~~~\n\n* （5）Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)\n  * ==一个列表，存储每个Partition的优先位置(可选项)==\n\n~~~\n这里涉及到数据的本地性，数据块位置最优。\nspark任务在调度的时候会优先考虑存有数据的节点开启计算任务，减少数据的网络传输，提升计算效率。\n~~~\n\n* 流程分析\n\n![rdd的五大属性](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/rdd的五大属性.png)\n\n### 2. RDD的算子分类\n\n* 1、==transformation（转换）==\n  - 根据已经存在的rdd转换生成一个新的rdd,  它是延迟加载，它不会立即执行\n  - 例如\n    - map / flatMap / reduceByKey 等\n* 2、==action (动作)==\n  - 它会真正触发任务的运行\n    - 将rdd的计算的结果数据返回给Driver端，或者是保存结果数据到外部存储介质中\n  - 例如\n    - collect / saveAsTextFile 等\n\n### 4. RDD的依赖关系\n\n![rdd-dependencies](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/rdd-dependencies.png)\n\n- RDD和它依赖的父RDD的关系有两种不同的类型\n\n- 窄依赖（narrow dependency）和宽依赖（wide dependency）\n\n  - ==窄依赖==\n\n    - 窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用\n\n      - 总结：窄依赖我们形象的比喻为独生子女\n\n      ```\n      哪些算子操作是窄依赖：\n      \tmap/flatMap/filter/union等等\n      \t\n      \t所有的窄依赖不会产生shuffle\n      ```\n\n  - ==宽依赖==\n\n    - 宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition\n\n      - 总结：宽依赖我们形象的比喻为超生 \n\n      ```\n      哪些算子操作是宽依赖：\n      \treduceByKey/sortByKey/groupBy/groupByKey/join等等\n      \t\n      \t所有的宽依赖会产生shuffle\n      ```\n\n  - 补充说明\n\n    ~~~\n    由上图可知，join分为宽依赖和窄依赖，如果RDD有相同的partitioner，那么将不会引起shuffle，这种join是窄依赖，反之就是宽依赖\n    ~~~\n\n\n\n\n### 5. lineage（血统）\n\n* RDD只支持粗粒度转换\n  * 即只记录单个块上执行的单个操作。\n* 将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区\n* ==RDD的Lineage会记录RDD的元数据信息和转换行为，lineage保存了RDD的依赖关系，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区==。\n\n\n\n### 6. RDD的缓存机制\n\n#### 6.1 什么是rdd的缓存\n\n~~~\n\t可以把一个rdd的数据缓存起来，后续有其他的job需要用到该rdd的结果数据，可以直接从缓存中获取得到，避免了重复计算。缓存是加快后续对该数据的访问操作。\n~~~\n\n\n\n#### 6.2 如何对rdd设置缓存\n\n* RDD通过==persist方法==或==cache方法==可以将前面的计算结果缓存。\n  * 但是并不是这两个方法被调用时立即缓存，而是==触发后面的action==时，该RDD将会被缓存在计算节点的内存中，并供后面重用。\n\n![1569036662039](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569036662039.png)\n\n* 通过查看源码发现==cache最终也是调用了persist方法==，默认的存储级别都是==仅在内存存储一份==，Spark的存储级别还有好多种，存储级别在==object StorageLevel==中定义的。\n\n![1569036703460](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569036703460.png)\n\n* 使用演示\n\n~~~scala\nval rdd1=sc.textFile(\"/words.txt\")\nval rdd2=rdd1.flatMap(_.split(\" \"))\nval rdd3=rdd2.cache\nrdd3.collect\n\nval rdd4=rdd3.map((_,1))\nval rdd5=rdd4.persist(缓存级别)\nrdd5.collect\n~~~\n\n\n\n#### 6.3 cache和persist区别\n\n* ==面试经常被问到==\n\n  * 例如\n    * ==简述下如何对RDD设置缓存，以及它们的区别是什么？==\n\n  ~~~\n  \t对RDD设置缓存成可以调用rdd的2个方法： 一个是cache，一个是persist\n  调用上面2个方法都可以对rdd的数据设置缓存，但不是立即就触发缓存执行，后面需要有action，才会触发缓存的执行。\n  \n  cache方法和persist方法区别：\n      cache:   默认是把数据缓存在内存中，其本质就是调用persist方法；\n      persist：可以把数据缓存在内存或者是磁盘，有丰富的缓存级别，这些缓存级别都被定义在StorageLevel这个object中。\n  ~~~\n\n\n\n#### 6.4 什么时候设置缓存\n\n* ==1、某个rdd的数据后期被使用了多次==\n\n![1569037915592](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569037915592.png)\n\n~~~\n如上图所示的计算逻辑： \n（1）当第一次使用rdd2做相应的算子操作得到rdd3的时候，就会从rdd1开始计算，先读取HDFS上的文件，然后对rdd1 做对应的算子操作得到rdd2,再由rdd2计算之后得到rdd3。同样为了计算得到rdd4，前面的逻辑会被重新计算。\n\n（2）默认情况下多次对一个rdd执行算子操作， rdd都会对这个rdd及之前的父rdd全部重新计算一次。 这种情况在实际开发代码的时候会经常遇到，但是我们一定要避免一个rdd重复计算多次，否则会导致性能急剧降低。   \n\n总结：\n可以把多次使用到的rdd，也就是公共rdd进行持久化，避免后续需要，再次重新计算，提升效率。\n~~~\n\n![1569045749546](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569045749546.png)\n\n* 2、为了获取得到一个rdd的结果数据，经过了大量的算子操作或者是计算逻辑比较复杂\n  * 总之某个rdd的数据来之不易\n\n~~~\nval rdd2=rdd1.flatMap(函数).map(函数).reduceByKey(函数).xxx.xxx.xxx.xxx.xxx\n~~~\n\n\n\n#### 6.5  清除缓存数据\n\n* 1、==自动清除==\n\n  ~~~\n  一个application应用程序结束之后，对应的缓存数据也就自动清除\n  ~~~\n\n* 2、==手动清除==\n\n  ~~~\n  调用rdd的unpersist方法\n  ~~~\n\n\n\n### 7. RDD的checkpoint机制\n\n#### 7.1 checkpoint概念\n\n* 我们可以对rdd的数据进行缓存，保存在内存或者是磁盘中。\n\n  * 后续就可以直接从内存或者磁盘中获取得到，但是它们不是特别安全。\n\n  * **cache**\n\n    ~~~\n    它是直接把数据保存在内存中，后续操作起来速度比较快，直接从内存中获取得到。但这种方式很不安全，由于服务器挂掉或者是进程终止，会导致数据的丢失。\n    ~~~\n\n  * **persist**\n\n    ~~~\n    它可以把数据保存在本地磁盘中，后续可以从磁盘中获取得到该数据，但它也不是特别安全，由于系统管理员一些误操作删除了，或者是磁盘损坏，也有可能导致数据的丢失。\n    ~~~\n\n* ==**checkpoint**（检查点）==\n\n  ~~~\n  它是提供了一种相对而言更加可靠的数据持久化方式。它是把数据保存在分布式文件系统，\n  比如HDFS上。这里就是利用了HDFS高可用性，高容错性（多副本）来最大程度保证数据的安全性。\n  ~~~\n\n\n\n#### 7.2 如何设置checkpoint\n\n* 1、在hdfs上设置一个checkpoint目录\n\n  ~~~scala\n  sc.setCheckpointDir(\"hdfs://node01:8020/checkpoint\") \n  ~~~\n\n* 2、对需要做checkpoint操作的rdd调用checkpoint方法\n\n  ~~~scala\n  val rdd1=sc.textFile(\"/words.txt\")\n  rdd1.checkpoint\n  val rdd2=rdd1.flatMap(_.split(\" \")) \n  ~~~\n\n* 3、最后需要有一个action操作去触发任务的运行\n\n  ~~~scala\n  rdd2.collect\n  ~~~\n\n#### 7.3 cache、persist、checkpoint三者区别\n\n* ==cache和persist==\n  * cache默认数据缓存在内存中\n  * persist可以把数据保存在内存或者磁盘中\n  * 后续要触发 cache 和 persist 持久化操作，需要有一个action操作\n  * 它不会开启其他新的任务，一个action操作就对应一个job \n  * 它不会改变rdd的依赖关系，程序运行完成后对应的缓存数据就自动消失\n\n* ==checkpoint==\n  * 可以把数据持久化写入到hdfs上\n  * 后续要触发checkpoint持久化操作，需要有一个action操作，后续会开启新的job执行checkpoint操作\n  * 它会改变rdd的依赖关系，后续数据丢失了不能够在通过血统进行数据的恢复。\n  * 程序运行完成后对应的checkpoint数据就不会消失\n\n~~~scala\n   sc.setCheckpointDir(\"/checkpoint\")\n   val rdd1=sc.textFile(\"/words.txt\")\n   val rdd2=rdd1.cache\n   rdd2.checkpoint\n   val rdd3=rdd2.flatMap(_.split(\" \"))\n   rdd3.collect\n   \n   checkpoint操作要执行需要有一个action操作，一个action操作对应后续的一个job。该job执行完成之后，它会再次单独开启另外一个job来执行 rdd1.checkpoint操作。\n   \n   对checkpoint在使用的时候进行优化，在调用checkpoint操作之前，可以先来做一个cache操作，缓存对应rdd的结果数据，后续就可以直接从cache中获取到rdd的数据写入到指定checkpoint目录中\n~~~\n\n### 8. DAG有向无环图生成\n\n#### 8.1 DAG是什么\n\n* ==DAG(Directed Acyclic Graph)== 叫做有向无环图（有方向,无闭环,代表着数据的流向），原始的RDD通过一系列的转换就形成了DAG。\n\n* 下图是基于单词统计逻辑得到的DAG有向无环图\n\n![1569047954944](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/1569047954944.png)\n\n### 9. DAG划分stage\n\n#### 9.1 stage是什么\n\n* ==一个Job会被拆分为多组Task，每组任务被称为一个stage==\n* stage表示不同的调度阶段，一个spark job会对应产生很多个stage\n  * stage类型一共有2种\n    * ==ShuffleMapStage==\n      * 最后一个shuffle之前的所有变换的Stage叫ShuffleMapStage\n        * 它对应的task是shuffleMapTask\n    * ==ResultStage==\n      * 最后一个shuffle之后操作的Stage叫ResultStage，它是最后一个Stage。\n        * 它对应的task是ResultTask\n\n\n\n#### 9.2 为什么要划分stage\n\n~~~\n根据RDD之间依赖关系的不同将DAG划分成不同的Stage(调度阶段)\n对于窄依赖，partition的转换处理在一个Stage中完成计算\n对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，\n\n由于划分完stage之后，在同一个stage中只有窄依赖，没有宽依赖，可以实现流水线计算，\nstage中的每一个分区对应一个task，在同一个stage中就有很多可以并行运行的task。\n~~~\n\n#### 9.3 如何划分stage\n\n* ==划分stage的依据就是宽依赖==\n\n~~~\n(1) 首先根据rdd的算子操作顺序生成DAG有向无环图，接下里从最后一个rdd往前推，创建一个新的stage，把该rdd加入到该stage中，它是最后一个stage。\n\n(2) 在往前推的过程中运行遇到了窄依赖就把该rdd加入到本stage中，如果遇到了宽依赖，就从宽依赖切开，那么最后一个stage也就结束了。\n\n(3) 重新创建一个新的stage，按照第二个步骤继续往前推，一直到最开始的rdd，整个划分stage也就结束了\n~~~\n\n![划分stage](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/划分stage.png)\n\n\n\n#### 9.4 stage与stage之间的关系\n\n~~~\n\t划分完stage之后，每一个stage中有很多可以并行运行的task，后期把每一个stage中的task封装在一个taskSet集合中，最后把一个一个的taskSet集合提交到worker节点上的executor进程中运行。\n\nrdd与rdd之间存在依赖关系，stage与stage之前也存在依赖关系，前面stage中的task先运行，运行完成了再运行后面stage中的task，也就是说后面stage中的task输入数据是前面stage中task的输出结果数据。\n~~~\n\n![stage](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/stage.png)\n\n# 三、Spark任务调度\n\n### 1. spark的任务调度\n\n![spark任务调度](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/spark任务调度.png)\n\n\n\n~~~\n(1) Driver端运行客户端的main方法，构建SparkContext对象，在SparkContext对象内部依次构建DAGScheduler和TaskScheduler\n\n(2) 按照rdd的一系列操作顺序，来生成DAG有向无环图\n\n(3) DAGScheduler拿到DAG有向无环图之后，按照宽依赖进行stage的划分。每一个stage内部有很多可以并行运行的task，最后封装在一个一个的taskSet集合中，然后把taskSet发送给TaskScheduler\n\n（4）TaskScheduler得到taskSet集合之后，依次遍历取出每一个task提交到worker节点上的executor进程中运行。\n\n（5）所有task运行完成，整个任务也就结束了\n~~~\n\n\n\n### 2. spark的运行架构\n\n![spark](/Users/dingchuangshi/Documents/hexo-kfly-blog/source/_posts/it/spark/https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/spark-7527224.png)\n\n~~~\n(1) Driver端向资源管理器Master发送注册和申请计算资源的请求\n\n(2) Master通知对应的worker节点启动executor进程(计算资源)\n\n(3) executor进程向Driver端发送注册并且申请task请求\n\n(4) Driver端运行客户端的main方法，构建SparkContext对象，在SparkContext对象内部依次构建DAGScheduler和TaskScheduler\n\n(5) 按照客户端代码洪rdd的一系列操作顺序，生成DAG有向无环图\n\n(6) DAGScheduler拿到DAG有向无环图之后，按照宽依赖进行stage的划分。每一个stage内部有很多可以并行运行的task，最后封装在一个一个的taskSet集合中，然后把taskSet发送给TaskScheduler\n\n(7) TaskScheduler得到taskSet集合之后，依次遍历取出每一个task提交到worker节点上的executor进程中运行\n\n(8) 所有task运行完成，Driver端向Master发送注销请求，Master通知Worker关闭executor进程，Worker上的计算资源得到释放，最后整个任务也就结束了。\n~~~\n\n### 3.  基于wordcount程序剖析spark任务的提交、划分、调度流程\n\n![job-scheduler-running](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/job-scheduler-running.png)\n\n\n\n### 4. spark自定义分区\n\n#### 4.1 自定义分区说明\n\n- 在对RDD数据进行分区时，默认使用的是==HashPartitioner==\n\n- 该函数对key进行哈希，然后对分区总数取模，取模结果相同的就会被分到同一个partition中\n\n  ```\n  HashPartitioner分区逻辑：\n  \tkey.hashcode % 分区总数 = 分区号\n  ```\n\n- 如果嫌HashPartitioner功能单一，可以自定义partitioner\n\n\n\n#### 4.2 自定义partitioner\n\n- 实现自定义partitioner大致分为3个步骤\n  - 1、继承==org.apache.spark.Partitioner==\n  - 2、重写==numPartitions==方法\n  - 3、重写==getPartition==方法\n\n```scala\n//1、对应上面的rdd数据进行自定义分区\n val result: RDD[(String, Int)] = wordLengthRDD.partitionBy(new MyPartitioner(3))\n\n//2、自定义分区\nclass MyPartitioner(num:Int) extends Partitioner{\n  //指定rdd的总的分区数\n  override def numPartitions: Int = {\n    num\n  }\n  //消息按照key的某种规则进入到指定的分区号中\n  override def getPartition(key: Any): Int ={\n    //这里的key就是单词\n    val length: Int = key.toString.length\n    length match {\n      case 4 =>0\n      case 5 =>1\n    }\n  }\n}\n```\n\n### 5. spark的共享变量\n\n#### 5.1 spark的广播变量(broadcast variable)\n\n- ​\tSpark中分布式执行的代码需要==传递到各个Executor的Task上运行==。对于一些只读、固定的数据(比如从DB中读出的数据),每次都需要Driver广播到各个Task上，这样效率低下。\n- ​      广播变量允许==将变量只广播给各个Executor==。该Executor上的各个Task再从所在节点的BlockManager获取变量，而不是从Driver获取变量，以减少通信的成本，减少内存的占用，从而提升了效率。\n\n##### 5.1.1 广播变量原理\n\n![广播变量](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/广播变量.png)\n\n##### 5.1.2 广播变量使用\n\n```\n(1) 通过对一个类型T的对象调用 SparkContext.broadcast创建出一个Broadcast[T]对象。\n    任何可序列化的类型都可以这么实现\n(2) 通过 value 属性访问该对象的值\n(3) 变量只会被发到各个节点一次，应作为只读值处理（修改这个值不会影响到别的节点）\n```\n\n- 使用广播变量代码示例\n\n```scala\nval word=\"spark\"\nval rddData = rdd.collect\n//通过调用sparkContext对象的broadcast方法把数据广播出去\nval broadCast = sc.broadcast(word)\nval broadRddData = sc.broadcast(rddData)\n\n//在executor中通过调用广播变量的value属性获取广播变量的值,分布式环境下广播变量通过网络传输需要序列化\nval rdd2=rdd1.flatMap(_.split(\" \")).filter(x=>x.equals(broadCast.value))\n```\n\n##### 5.1.3 广播变量使用注意事项\n\n```\n1、不能将一个RDD使用广播变量广播出去\n\n2、广播变量只能在Driver端定义，不能在Executor端定义\n\n3、在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值\n\n4、如果executor端用到了Driver的变量，如果不使用广播变量在Executor有多少task就有多少Driver端的变量副本\n\n5、如果Executor端用到了Driver的变量，如果使用广播变量在每个Executor中只有一份Driver端的变量副本\n```\n\n#### 5.2 spark的累加器(accumulator)\n\n- 累加器（accumulator）是Spark中提供的一种分布式的变量机制，其原理类似于mapreduce，即分布式的改变，然后聚合这些改变\n- ==累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数。可以使用累加器来进行全局的计数\n\n```scala\nval accumulator = sc.accumulator(0); \n    val result = linesRDD.map(s => {\n      accumulator.add(1) //有一条数据就增加1\n    })\n```\n\n### 6. spark程序的序列化问题\n\n#### 6.1 transformation操作为什么需要序列化\n\n- spark是分布式执行引擎，其核心抽象是弹性分布式数据集RDD，其代表了分布在不同节点的数据。Spark的计算是在executor上分布式执行的，故用户开发的关于RDD的map，flatMap，reduceByKey等transformation 操作（闭包）有如下执行过程：\n  - （1）代码中对象在driver本地序列化\n  - （2）对象序列化后传输到远程executor节点\n  - （3）远程executor节点反序列化对象\n  - （4）最终远程节点执行\n- 故对象在执行中需要序列化通过网络传输，则必须经过序列化过程。\n\n#### 6.2 spark的任务序列化异常\n\n- 在编写spark程序中，由于在map，foreachPartition等算子==内部使用了外部定义的变量和函数==，从而引发Task未序列化问题。\n- 然而spark算子在计算过程中使用外部变量在许多情形下确实在所难免，比如在filter算子根据外部指定的条件进行过滤，map根据相应的配置进行变换。\n- 经常会出现“==org.apache.spark.SparkException: Task not serializable==”这个错误\n  - 其原因就在于这些算子使用了==外部的变量==，但是这个变量不能序列化。\n  - 当前类使用了“extends Serializable”声明支持序列化，但是由于某些字段==不支持序列化==，仍然会导致整个类序列化时出现问题，最终导致出现Task未序列化问题。\n\n#### 6.3 spark中解决序列化的办法\n\n- (1) 如果函数中使用了该类对象，该类要实现序列化\n  - ==类  extends  Serializable==\n- (2) 如果函数中使用了该类对象的成员变量，该类除了要实现序列化之外，所有的成员变量必须要实现序列化\n- (3) 对于不能序列化的成员变量使用==“@transient”==标注，告诉编译器不需要序列化\n- (4) 也可将依赖的变量独立放到一个小的class中，让这个class支持序列化，这样做可以减少网络传输量，提高效率。\n- (5) 可以把对象的创建直接在该函数中构建这样避免需要序列化\n\n### 7. application、job、stage、task之间的关系\n\n![application](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/application.png)\n\n- 一个application就是一个应用程序，包含了客户端所有的代码和计算资源\n- 一个action操作对应一个DAG有向无环图，即一个action操作就是一个job \n- 一个job中包含了大量的宽依赖，按照宽依赖进行stage划分，一个job产生了很多个stage\n- 一个stage中有很多分区，一个分区就是一个task，即一个stage中有很多个task\n- ==总结==\n  - 一个application包含了很多个job\n  - 一个job包含了很多个stage\n  - 一个stage包含了很多个task\n\n# 四、Spark内存计算框架\n\n### 1. 两种计算模型\n\n```shell\nspark-submit --class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n# cluster / client\n--deploy-mode cluster \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 1 \\\n/kfly/install/spark-2.3.3-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.3.jar \\\n10\n```\n\n- yarn-cluster模式\n  - spark程序的==Driver程序在YARN中运行==，运行结果不能在客户端显示，并且客户端可以在启动应用程序后消失应用的。\n\n  - 最好运行那些将结果最终保存在外部存储介质（如HDFS、Redis、Mysql），客户端的终端显示的仅是作为YARN的job的简单运行状况。\n\n<img src=\"https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/yarn-cluster.png\" alt=\"yarn-cluster\" style=\"zoom:33%;\" />\n\n- yarn-client模式\n  - spark程序的==Driver运行在Client上==，应用程序运行结果会在客户端显示，所有适合运行结果有输出的应用程序（如spark-shell）\n\n<img src=\"https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/yarn-client.png\" alt=\"yarn-client\" style=\"zoom: 33%;\" />\n\n```\n最大的区别就是Driver端的位置不一样。\n\nyarn-cluster: Driver端运行在yarn集群中，与ApplicationMaster进程在一起。\nyarn-client:  Driver端运行在提交任务的客户端,与ApplicationMaster进程没关系,经常用于进行测试\n```\n\n### 2. collect 算子操作剖析 \n\n- collect算子操作的作用\n\n  - 1、它是一个action操作，会触发任务的运行\n\n  - 2、它会把RDD的数据进行收集之后，以数组的形式返回给Driver端\n\n  - - ==默认Driver端的内存大小为1G，由参数 spark.driver.memory 设置==\n\n    - 如果某个rdd的数据量超过了Driver端默认的1G内存，对rdd调用collect操作，这里会出现Driver端的内存溢出，所有这个collect操作存在一定的风险，实际开发代码一般不会使用。\n\n    - ==实际企业中一般都会把该参数调大，比如5G/10G等==\n\n      - 可以在代码中修改该参数，如下\n\n        ```scala\n        new SparkConf().set(\"spark.driver.memory\",\"5G\")\n        ```\n\n  ~~~\n  比如说rdd的数据量达到了10G\n  \n  rdd.collect这个操作非常危险，很有可能出现driver端的内存不足\n  ~~~\n\n### 3. spark任务中资源参数剖析\n\n- ==--executor-memory==\n\n  - 表示每一个executor进程需要的内存大小，它决定了后期操作数据的速度\n\n  ```\n  比如说一个rdd的数据量大小为5g,这里给定的executor-memory为2g, 在这种情况下，内存是存储不下，它会把一部分数据保存在内存中，还有一部分数据保存在磁盘，后续需要用到该rdd的结果数据，可以从内存和磁盘中获取得到，这里就涉及到一定的磁盘io操作。\n  \n  ,这里给定的executor-memory为10g，这里数据就可以完全在内存中存储下，后续需要用到该rdd的数据，就可以直接从内存中获取，这样一来，避免了大量的磁盘io操作。性能得到提升。\n  \n  \n  在实际的工作，这里 --executor-memory 需要设置的大一点。\n  比如说10G/20G/30G等\n  ```\n\n- ==--total-executor-cores==\n\n  - 表示任务运行需要总的cpu核数，它决定了任务并行运行的粒度\n\n  ~~~\n  比如说要处理100个task，注意一个cpu在同一时间只能处理一个task线程。\n  \n  如果给定的总的cpu核数是5个，这里就需要100/5=20个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行20分钟。\n  \n  如果给定的总的cpu核数是20个，这里就需要100/20=5个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行5分钟。\n  \n  如果如果给定的总的cpu核数是100个，这里就需要100/100=1个批次才可以把这100个task运行完成，如果平均每个task运行1分钟，这里最后一共运行1分钟。\n  \n  \n  在实际的生产环境中，--total-executor-cores 这个参数一般也会设置的大一点，\n  比如说 30个/50个/100个\n  ~~~\n\n\n- ==总结==\n\n  ```\n  \t后期对于spark程序的优化，可以从这2个参数入手，无论你把哪一个参数调大，对程序运行的效率来说都会达到一定程度的提升\n      加大计算资源它是最直接、最有效果的优化手段。\n      在计算资源有限的情况下，可以考虑其他方面，比如说代码层面，JVM层面等\n  ```\n\n### 4. spark任务的调度模式\n\n* Spark中的调度模式主要有两种：==FIFO 和 FAIR==\n  * ==FIFO（先进先出）==\n    * 默认情况下Spark的调度模式是FIFO，谁先提交谁先执行，后面的任务需要等待前面的任务执行。\n  * ==FAIR（公平调度）==\n    * 支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。避免大任务运行时间长，占用了大量的资源，后面小任务无法提交运行。\n\n### 5. spark任务的分配资源策略\n\n* 给application分配资源选择worker（executor），现在有两种策略\n  * ==尽量的打散==，即一个Application尽可能多的分配到不同的节点。这个可以通过设置spark.deploy.spreadOut来实现。默认值为true，即尽量的打散（默认）\n    * 可以充分的发挥数据的本地性，提升执行效率\n\n  * ==尽量的集中==，即一个Application尽量分配到尽可能少的节点。\n  \n  ```sh\n  # 假如集群有两个节点，worker1,worker2。各 cores 4 memory 128G，需要分配 4cores。 32g\n  # 1. 尽量的集中(尽可能分配更少的节点，worker1 4 32g)\n  # 2. 尽量打散（尽可能多的分配，worker1 worker2按照顺序依次分配，不够再次循环）\n  ```\n\n\n### 6. spark的shuffle原理分析\n\n\n  #### 6.1 shuffle概述\n\n  ```\n  \tShuffle就是对数据进行重组，由于分布式计算的特性和要求，在实现细节上更加繁琐和复杂。\n  \t在MapReduce框架，Shuffle是连接Map和Reduce之间的桥梁，Map阶段通过shuffle读取数据并输出到对应的Reduce；而Reduce阶段负责从Map端拉取数据并进行计算。在整个shuffle过程中，往往伴随着大量的磁盘和网络I/O。所以shuffle性能的高低也直接决定了整个程序的性能高低。Spark也会有自己的shuffle实现过程。 \n  ```\n\n  #### 6.2 spark中的shuffle介绍\n\n  ```\n  \t在DAG调度的过程中，Stage阶段的划分是根据是否有shuffle过程，也就是存在wide Dependency宽依赖的时候,需要进行shuffle,这时候会将作业job划分成多个Stage，每一个stage内部有很多可以并行运行的task。\n  \t\n  \tstage与stage之间的过程就是shuffle阶段，在Spark的中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。ShuffleManager随着Spark的发展有两种实现的方式，分别为HashShuffleManager和SortShuffleManager，因此spark的Shuffle有Hash Shuffle和Sort Shuffle两种。\n  ```\n\n![img](https://kfly.top/2019/10/20/it/hadoop/MapReduce%E7%BC%96%E7%A8%8B%EF%BC%88%E4%BA%8C%EF%BC%89/https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/Image201906272145.png)\n\n  #### 6.3 HashShuffle机制\n\n  ##### 6.3.1 HashShuffle概述\n\n  ```\n  \t在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。\n  \t该ShuffleManager-HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。\n  \tSortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并(merge)成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。\n  ```\n\n  - ==Hash shuffle==\n    - HashShuffleManager的运行机制主要分成两种\n      - 一种是==普通运行机制==\n      - 另一种是==合并的运行机制==。\n    - ==合并机制主要是通过复用buffer来优化Shuffle过程中产生的小文件的数量。==\n    - ==Hash shuffle是不具有排序的Shuffle。==\n\n  ##### 6.3.2 普通机制的Hash shuffle\n\n![未优化的HashShuffle机制](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/未优化的HashShuffle机制.png)\n\n  - ==图解==\n\n  ```\n     这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。\n  \n      图中有3个ReduceTask，从ShuffleMapTask 开始那边各自把自己进行 Hash 计算(分区器：hash/numreduce取模)，分类出3个不同的类别，每个 ShuffleMapTask 都分成3种类别的数据，想把不同的数据汇聚然后计算出最终的结果，所以ReduceTask 会在属于自己类别的数据收集过来，汇聚成一个同类别的大集合，每1个 ShuffleMapTask 输出3份本地文件，这里有4个 ShuffleMapTask，所以总共输出了4 x 3个分类文件 = 12个本地小文件。\n  ```\n\n  - ==shuffle Write阶段==\n\n  ```\n  \t主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子(比如reduceByKey，groupByKey)，而将每个task处理的数据按key进行“分区”。所谓“分区”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于reduce端的stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。\n  \n       那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢? 很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。\n  ```\n\n  - ==shuffle Read阶段==\n\n  ```\n  \tshuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给Reduce端的stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。\n  \n    shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。\n  ```\n\n  - ==注意==\n\n  ```\n  （1）buffer起到的是缓存作用，缓存能够加速写磁盘，提高计算的效率,buffer的默认大小32k。\n  \n  （2）分区器：根据hash/numRedcue取模决定数据由几个Reduce处理，也决定了写入几个buffer中\n  \n  （3）block file：磁盘小文件，从图中我们可以知道磁盘小文件的个数计算公式：\n                   block file=M*R\n  \n   (4) M为map task的数量，R为Reduce的数量，一般Reduce的数量等于buffer的数量，都是由分区器决定的\n  ```\n\n  - ==Hash shuffle普通机制的问题==\n\n  ```\n  （1).Shuffle阶段在磁盘上会产生海量的小文件，建立通信和拉取数据的次数变多,此时会产生大量耗时低效的 IO 操作 (因为产生过多的小文件)\n  \n  （2).可能导致OOM，大量耗时低效的 IO 操作 ，导致写磁盘时的对象过多，读磁盘时候的对象也过多，这些对象存储在堆内存中，会导致堆内存不足，相应会导致频繁的GC，GC会导致OOM。由于内存中需要保存海量文件操作句柄和临时信息，如果数据处理的规模比较庞大的话，内存不可承受，会出现 OOM 等问题\n  ```\n\n  ##### 6.3.3 合并机制的Hash shuffle\n\n  ```\n  \t合并机制就是复用buffer缓冲区，开启合并机制的配置是spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。\n  ```\n\n![优化后的Shuffle机制](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/优化后的Shuffle机制.png)\n\n\n\n  - ==图解==\n\n  ```\n  \t这里有6个这里有6个shuffleMapTask，数据类别还是分成3种类型，因为Hash算法会根据你的 Key 进行分类，在同一个进程中，无论是有多少过Task，都会把同样的Key放在同一个Buffer里，然后把Buffer中的数据写入以Core数量为单位的本地文件中，(一个Core只有一种类型的Key的数据)，每1个Task所在的进程中，分别写入共同进程中的3份本地文件，这里有6个shuffleMapTasks，所以总共输出是 2个Cores x 3个分类文件 = 6个本地小文件。\n  ```\n\n  - ==注意==\n\n  ```\n  （1).启动HashShuffle的合并机制ConsolidatedShuffle的配置\n     spark.shuffle.consolidateFiles=true\n  \n  （2).block file=Core*R\n  \tCore为CPU的核数，R为Reduce的数量\n  ```\n\n  - ==Hash shuffle合并机制的问题==\n\n  ```\n  \t如果 Reducer 端的并行任务或者是数据分片过多的话则 Core * Reducer Task 依旧过大，也会产生很多小文件。\n  ```\n\n\n\n  #### 6.4 Sort shuffle\n\n  - SortShuffleManager的运行机制主要分成两种，\n    - 一种是==普通运行机制==\n    - 另一种是==bypass运行机制==\n\n\n\n  ##### 6.4.1 Sort shuffle的普通机制\n\n![sortshuffle](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/sortshuffle.png)\n\n  - ==图解==\n\n  ```\n  \t在该模式下，数据会先写入一个数据结构，聚合算子写入Map，一边通过Map局部聚合，一边写入内存。Join算子写入ArrayList直接写入内存中。然后需要判断是否达到阈值（5M），如果达到就会将内存数据结构的数据写入到磁盘，清空内存数据结构。\n  \n  在溢写磁盘前，先根据key进行排序，排序过后的数据，会分批写入到磁盘文件中。默认批次为10000条，数据会以每批一万条写入到磁盘文件。写入磁盘文件通过缓冲区溢写的方式，每次溢写都会产生一个磁盘文件，也就是说一个task过程会产生多个临时文件\n  。\n  \n  最后在每个task中，将所有的临时文件合并，这就是merge过程，此过程将所有临时文件读取出来，一次写入到最终文件。意味着一个task的所有数据都在这一个文件中。同时单独写一份索引文件，标识下游各个task的数据在文件中的索引start offset和end offset。\n  \n  \t这样算来如果第一个stage 50个task，每个Executor执行一个task，那么无论下游有几个task，就需要50*2=100个磁盘文件。\n  ```\n\n  - ==好处==\n\n  ```\n  1. 小文件明显变少了，一个task只生成一个file文件\n  \n  2. file文件整体有序，加上索引文件的辅助，查找变快，虽然排序浪费一些性能，但是查找变快很多\n  ```\n\n\n\n  ##### 6.4.2 bypass模式的sortShuffle\n\n  - bypass机制运行条件\n    - shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值\n    - 不是聚合类的shuffle算子（比如reduceByKey）\n\n![bypasssortshuffle](https://kfly.top/picture/kfly-top/SparkCore:SQL%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/assets/bypasssortshuffle.png)\n\n  - ==好处==\n\n  ```\n      该机制与sortshuffle的普通机制相比，在shuffleMapTask不多的情况下，首先写的机制是不同，其次不会进行排序。这样就可以节约一部分性能开销。\n  ```\n\n  - ==总结==\n\n  ```\n在shuffleMapTask数量小于默认值200时，启用bypass模式的sortShuffle(原因是数据量本身比较少，没必要进行sort全排序，因为数据量少本身查询速度就快，正好省了sort的那部分性能开销。)\n \n 该机制与普通SortShuffleManager运行机制的不同在于：\n    第一: 磁盘写机制不同；\n    第二: 不会进行sort排序；\n  ```\n\n## 五、Spark SQL\n\n### 1.sparksql概述\n\n#### 1.1 sparksql的前世今生\n\n- ==Shark是专门针对于spark的构建大规模数据仓库系统的一个框架==\n- Shark与Hive兼容、同时也依赖于Spark版本\n- Hivesql底层把sql解析成了mapreduce程序，Shark是把sql语句解析成了Spark任务\n- 随着性能优化的上限，以及集成SQL的一些复杂的分析功能，发现Hive的MapReduce思想限制了Shark的发展。\n- 最后Databricks公司终止对Shark的开发\n  - 决定单独开发一个框架，不在依赖hive，把重点转移到了==sparksql==这个框架上。\n\n#### 1.2 什么是sparksql \n\n- Spark SQL is Apache Spark's module for working with structured data.\n- SparkSQL是apache Spark用来处理结构化数据的一个模块\n\n### 2. sparksql的四大特性\n\n- ==1、易整合(Integrated)==\n\n  - ```sql\n    results = spark.sql(\"SELECT * FROM people\")\n    ```\n\n- ==2、统一的数据源访问(Uniform Data Access)==\n\n  - ```scala\n    spark.read.json(\"s3n://...\")\n    spark.read.text(\"s3n://...\")\n    spark.read.parquet(\"s3n://...\")\n    ```\n\n- ==3、兼容hive(Hive Integration)==\n\n- ==4、支持标准的数据库连接(Standard Connectivity)==\n\n  - ```scala\n    spark.read.jdbc(---)\n    ```\n\n### 3. DataFrame概述\n\n#### 3.1 DataFrame发展\n\n- DataFrame前身是schemaRDD,这个schemaRDD是直接继承自RDD，它是RDD的一个实现类\n- 在spark1.3.0之后把schemaRDD改名为DataFrame,它不在继承自RDD，而是自己实现RDD上的一些功能\n- 也可以把dataFrame转换成一个rdd，调用rdd这个方法\n  - 例如 val rdd1=dataFrame.rdd\n\n#### 3.2 DataFrame是什么\n\n- 在Spark中，DataFrame是一种==以RDD为基础的分布式数据集==，类似于==传统数据库的二维表格==\n- DataFrame带有==Schema元信息==，即DataFrame所表示的二维表数据集的每一列都带有名称和类型，但底层做了更多的优化\n- DataFrame可以从很多数据源构建\n  - 比如：已经存在的RDD、结构化文件、外部数据库、Hive表。\n- RDD可以把它内部元素看成是一个java对象\n- DataFrame可以把内部是一个Row对象，它表示一行一行的数据\n\n#### 3.3 DataFrame和RDD的优缺点\n\n- ==1、RDD==\n\n  - ==优点==\n\n    - 1、编译时类型安全\n      - 开发会进行类型检查，在编译的时候及时发现错误\n    - 2、具有面向对象编程的风格\n\n  - ==缺点==\n\n    - 1、构建大量的java对象占用了大量heap堆空间，导致频繁的GC\n\n      ```\n      由于数据集RDD它的数据量比较大，后期都需要存储在heap堆中，这里有heap堆中的内存空间有限，出现频繁的垃圾回收（GC），程序在进行垃圾回收的过程中，所有的任务都是暂停。影响程序执行的效率\n      ```\n\n    - 2、数据的序列化和反序列性能开销很大\n\n      ```\n        在分布式程序中，对象(对象的内容和结构)是先进行序列化，发送到其他服务器，进行大量的网络传输，然后接受到这些序列化的数据之后，再进行反序列化来恢复该对象\n      ```\n\n- ==2、DataFrame==\n  - ==DataFrame引入了schema元信息和off-heap(堆外)==\n  - ==优点==\n    - 1、DataFrame引入off-heap，大量的对象构建直接使用操作系统层面上的内存，不在使用heap堆中的内存，这样一来heap堆中的内存空间就比较充足，不会导致频繁GC，程序的运行效率比较高，它是解决了RDD构建大量的java对象占用了大量heap堆空间，导致频繁的GC这个缺点。\n    - 2、DataFrame引入了schema元信息---就是数据结构的描述信息，后期spark程序中的大量对象在进行网络传输的时候，只需要把数据的内容本身进行序列化就可以，数据结构信息可以省略掉。这样一来数据网络传输的数据量是有所减少，数据的序列化和反序列性能开销就不是很大了。它是解决了RDD数据的序列化和反序列性能开销很大这个缺点\n    - ==缺点==\n      - DataFrame引入了schema元信息和off-heap(堆外)它是分别解决了RDD的缺点，同时它也丢失了RDD的优点\n        - 1、编译时类型不安全\n          - 编译时不会进行类型的检查，这里也就意味着前期是无法在编译的时候发现错误，只有在运行的时候才会发现\n        - 2、不在具有面向对象编程的风格\n\n### 4. DataFrame使用\n\n```SCALA\n// 1. 读取文本文件\nval personDF=spark.read.text(\"/person.txt\")\nval peopleDF=spark.read.json(\"/people.json\")\nval usersDF=spark.read.parquet(\"/users.parquet\")\n\n// 2. 加载数据\nval rdd1=sc.textFile(\"/person.txt\").map(x=>x.split(\" \"))\n    //定义一个样例类\n    case class Person(id:String,name:String,age:Int)\n    //把rdd与样例类进行关联\n    val personRDD=rdd1.map(x=>Person(x(0),x(1),x(2).toInt))\n    //把rdd转换成DataFrame\n    val personDF=personRDD.toDF\n// 3.语法风格\n// 3.1 DSL\n        personDF.select(\"name\").show\n        personDF.select($\"name\").show\n        personDF.select(col(\"name\").show\n        //实现age+1\n         personDF.select($\"name\",$\"age\",$\"age\"+1)).show   \n        //实现age大于30过滤\n         personDF.filter($\"age\" > 30).show\n         //按照age分组统计次数\n         personDF.groupBy(\"age\").count.show \n        //按照age分组统计次数降序\n         personDF.groupBy(\"age\").count().sort($\"count\".desc)show  \n// 3.2 sql\n        //DataFrame注册成表\n        personDF.createTempView(\"person\")\n\n        //使用SparkSession调用sql方法统计查询\n        spark.sql(\"select * from person\").show\n        spark.sql(\"select name from person\").show\n        spark.sql(\"select name,age from person\").show\n```\n\n### 5. DataSet概述\n\n#### 5.1 DataSet是什么\n\n- DataSet是分布式的数据集合，Dataset提供了强类型支持，也是在RDD的每行数据加了类型约束。\n- DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型和可以用强大lambda函数）以及使用了Spark SQL优化的执行引擎。\n\n#### 5.2 DataSet的区别\n\n```properties\n1. 假设RDD中的两行数据长这样\n        1,张三,23\n        2,李四,35\n2.那么DataFrame中的数据长这样\n\t\t\t\tID:String\tName:String\tAge:int\n        \t\t1\t\t\t\t张三\t\t\t\t23\n        \t\t2\t\t\t\t李四\t\t\t\t35\n3.Dataset中的数据长这样 \n\t\t\tvalue:String\n        1,张三,23\n        2,李四,35\n  或者\n  value:People(age:bigint,id:bigint,name:string)\n        People(id=1,name=\"张三\",age=23)\n        People(id=2,name=\"李四\",age=23)\n```\n\n```properties\nDataSet包含了DataFrame的功能，Spark2.0中两者统一，DataFrame表示为DataSet[Row]，即DataSet的子集。\n（1）DataSet可以在编译时检查类型\n（2）并且是面向对象的编程接口\n```\n\n5.3 DataFrame DataSet转换 构建dataset\n\n```scala\n// 把一个DataFrame转换成DataSet\nval dataSet=dataFrame.as[强类型]\n//  2、把一个DataSet转换成DataFrame\nval dataFrame=dataSet.toDF\n\n// 补充说明,可以从dataFrame和dataSet获取得到rdd\nval rdd1=dataFrame.rdd\nval rdd2=dataSet.rdd\n\n// 1、 通过sparkSession调用createDataset方法\n  val ds=spark.createDataset(1 to 10) //scala集合\n  val ds=spark.createDataset(sc.textFile(\"/person.txt\"))  //rdd\n\n// 2、使用scala集合和rdd调用toDS方法\n  sc.textFile(\"/person.txt\").toDS\n  List(1,2,3,4,5).toDS\n\n// 3、把一个DataFrame转换成DataSet\n  val dataSet=dataFrame.as[强类型]\n\n// 4、通过一个DataSet转换生成一个新的DataSet\n   List(1,2,3,4,5).toDS.map(x=>x*10)\n\n// 5、将rdd与Row对象进行关联\n    val rowRDD: RDD[Row] = data.map(x=>Row(x(0),x(1),x(2).toInt))\n    //指定dataFrame的schema信息   \n    //这里指定的字段个数和类型必须要跟Row对象保持一致\n    val schema=StructType(\n        StructField(\"id\",StringType)::\n        StructField(\"name\",StringType)::\n        StructField(\"age\",IntegerType)::Nil\n    )\n    val dataFrame: DataFrame = spark.createDataFrame(rowRDD,schema)\n```\n\n### 6. Spark IDEA开发\n\n```scala\n// EG\n // 1、构建SparkSession对象,开启hive支持\n    val spark: SparkSession = SparkSession.builder()\n      .appName(\"HiveSupport\")\n      .master(\"local[2]\")\n      .enableHiveSupport() //开启对hive的支持\n      .getOrCreate()\n\n// 2. 读取mysql数据\n\t\tval spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()\n        val url=\"jdbc:mysql://node03:3306/spark\"\n        val tableName=\"user\"\n        val properties = new Properties()\n      properties.setProperty(\"user\",\"root\")\n      properties.setProperty(\"password\",\"123456\")\n   val mysqlDF: DataFrame = spark.read.jdbc(url,tableName,properties)\n\n// 3. 保存数据到mysql表中\n     //mode:指定数据的插入模式\n        //overwrite: 表示覆盖，如果表不存在，事先帮我们创建\n        //append   :表示追加， 如果表不存在，事先帮我们创建\n        //ignore   :表示忽略，如果表事先存在，就不进行任何操作\n        //error    :如果表事先存在就报错（默认选项）\n    result.write.mode(\"append\").jdbc(url,\"kaikeba\",properties)\n```\n\n### 7 Spark自定义函数\n\n```scala\n//小写转大写\nsparkSession.udf.register(\"low2Up\",new UDF1[String,String]() {\n  override def call(t1: String): String = {\n    t1.toUpperCase\n  }\n},StringType)\n//大写转小写\nsparkSession.udf.register(\"up2low\",(x:String)=>x.toLowerCase)\n// 把数据文件中的单词统一转换成大小写\nsparkSession.sql(\"select  value from t_udf\").show()\nsparkSession.sql(\"select  low2Up(value) from t_udf\").show()\nsparkSession.sql(\"select  up2low(value) from t_udf\").show()\n```\n\n","tags":["spark sql","spark core"]},{"title":"大数据面试试题汇总","url":"/2019/12/08/it/meet/大数据面试试题汇总/","content":"\n# 1. Java基础篇\n\n- 语言基础\n- 锁\n- 多线程\n- 并发包中常用的并发容器（JUC）\n\n## 1.1 语言基础\n\n- Java面向对象\n\n- Java语言的三大特性： 封装、继承、多态\n\n- Java语言的数据类型\n\n  - 内置数据类型：\n\n    ```java\n    // 1. 在被创建时，在栈上给其划分一块内存，将数值直接存储在栈上。\n    int byte short float double char boolean\n    ```\n\n  - 引用数据类型\n\n     ```java\n    // 2. 在被创建时，首先要在栈上给其引用（句柄）分配一块内存，而对象的具体信息都存储在堆内存上，然后由栈上面的引用指向堆中对象的地址。\n    class interface array enum @interface\n    ```\n\n- Java的自动类型转换，强制类型转换\n\n- String的不可变性、虚拟机的常量池、String.intern() 的底层原理\n\n  - String.intern(): \n\n    ```java\n    // String.intern();\n    // return 一个字符串，内容与此字符串相同，但一定取自具有唯一字符串的池。\n    String a = \"a\";\n    String b = \"b\";\n    String ab = \"ab\";\n    String ab0 = \"a\" + \"b\";\n    String ab1 = a + b;\n    String ab2 = new String(\"ab\");\n    String ab3 = new String(\"a\") + new String(\"b\");\n    // 以下结果为。true\n    System.out.println(ab == ab0);\n    System.out.println(ab == ab1.intern());\n    System.out.println(ab == ab2.intern());\n    System.out.println(ab == ab3.intern());\n    System.out.println(ab2.intern() == ab3.intern());\n    ```\n\n    \n\n     \n\n- Java 语言中的关键字：**final**、**static**、**transient**、**instanceof**、**volatile**、**synchronized**的底层原理\n\n  -  [查看](https://blog.csdn.net/Tang_zhihong/article/details/88744343)\n\n- Java 中常用的集合类的实现原理： ArrayList/LinkedList/Vector、SynchronizedList/Vector、HashMap/HashTable/ConcurrentHashMap 互相的区别以及底层实现原理\n\n  -  [查看](https://crossoverjie.top/JCSprout/#/collections/ArrayList)\n\n- 动态代理的实现方式 \n\n  - jdk动态代理：interface 、 InvocationHandler\n  - CGLIB： MethodInterceptor\n\n## 1.2 锁\n\n- CAS、乐观锁与悲观锁、数据库相关锁机制、分布式锁、偏向锁、轻量级锁、重量级锁、monitor\n- 锁优化、锁消除、锁粗化、自旋锁、可重入锁、阻塞锁、死锁\n- 死锁的原因\n- 死锁的解决办法\n- CountDownLatch、CyclicBarrier 和 Semaphore 三个类的使用和原理\n\n## 1.3 多线程\n\n- 并发和并行的区别 [点](https://blog.csdn.net/weixin_30363263/article/details/80732156)\n- 线程与进程的区别   [点](https://blog.csdn.net/feiBlog/article/details/85397287)\n- 线程的实现、线程的状态、优先级、线程调度、创建线程的多种方式、守护线程 \n- 自己设计线程池、submit() 和 execute()、线程池原理\n- 为什么不允许使用 Executors 创建线程池 [点](https://blog.csdn.net/fly910905/article/details/81584675)\n- 死锁、死锁如何排查、线程安全和内存模型的关系\n- ThreadLocal 变量\n- Executor 创建线程池的几种方式：\n  - newFixedThreadPool(int nThreads)\n  - newCachedThreadPool()\n  - newSingleThreadExecutor()\n  - newScheduledThreadPool(int corePoolSize)\n  - newSingleThreadExecutor()\n- ThreadPoolExecutor 创建线程池、拒绝策略\n- 线程池关闭的方式\n\n## 1.4 并发容器（J.U.C）\n\n- JUC 包中 List 接口的实现类：CopyOnWriteArrayList\n- JUC 包中 Set 接口的实现类：CopyOnWriteArraySet、ConcurrentSkipListSet\n- JUC 包中 Map 接口的实现类：ConcurrentHashMap、ConcurrentSkipListMap\n- JUC包中Queue接口的实现类：ConcurrentLinkedQueue、ConcurrentLinkedDeque、ArrayBlockingQueue、LinkedBlockingQueue、LinkedBlockingDeque\n\n# 2. Java进阶篇\n\n## 2.1 JVM\n\n- JVM内存结构\n  \n  ```\n  class 文件格式、运行时数据区：堆、栈、方法区、直接内存、运行时常量池\n  ```\n  \n- 堆和栈区别\n\n  ```\n  Java 中的对象一定在堆上分配吗？ \n  ```\n\n- Java 内存模型\n\n  ```\n  计算机内存模型、缓存一致性、MESI 协议、可见性、原子性、顺序性、happens-before、内存屏障、synchronized、volatile、final、锁\n  ```\n\n- 垃圾回收\n\n  ```\n  GC 算法：标记清除、引用计数、复制、标记压缩、分代回收、增量式回收、GC 参数、对象存活的判定、垃圾收集器（CMS、G1、ZGC、Epsilon）\n  ```\n\n- JVM 参数及调优\n\n\t```\n-Xmx、-Xmn、-Xms、Xss、-XX:SurvivorRatio、-XX:PermSize、-XX:MaxPermSize、-XX:MaxTenuringThreshold\n\t```\n\n- Java 对象模型\n\n  ```\n  oop-klass、对象头\n  ```\n\n- HotSpot\n\n  ```\n即时编译器、编译优化\n\t```\n\n- 虚拟机性能监控与故障处理工具\n\n  ```\n  jps、jstack、jmap、jstat、jconsole、 jinfo、 jhat、javap、btrace、TProfiler、Arthas\n  ```\n\n- 类加载机制\n\n  ```\n  classLoader、类加载过程、双亲委派（破坏双亲委派）、模块化（jboss modules、osgi、jigsaw）\n  ```\n\n## 2.2 NIO\n\n- 用户空间以及内核空间\n- Linux 网络 I/O 模型：阻塞 I/O (Blocking I/O)、非阻塞 I/O (Non-Blocking I/O)、I/O 复用（I/O Multiplexing)、信号驱动的 I/O (Signal Driven I/O)、异步 I/O\n- 灵拷贝（ZeroCopy）\n- BIO 与 NIO 对比\n- 缓冲区 Buffer\n- 通道 Channel\n- 反应堆\n- 选择器\n- AIO\n\n## 2.3 RPC\n\n- RPC 的原理编程模型\n- 常用的 RPC 框架：Thrift、Dubbo、SpringCloud\n- RPC 的应用场景和与消息队列的差别\n- RPC 核心技术点：服务暴露、远程代理对象、通信、序列化\n\n# 3. Linux 基础\n\n- 了解 Linux 的常用命令\n- 远程登录\n- 上传下载\n- 系统目录\n- 文件和目录操作\n- Linux 下的权限体系\n- 压缩和打包\n- 用户和组\n- Shell 脚本的编写\n- 管道操作\n\n# 4.分布式理论篇\n\n- 分布式中的一些基本概念：集群（Cluster）、负载均衡（Load Balancer）等\n- 分布式系统理论基础： 一致性、2PC 和 3PC\n- 分布式系统理论基础：CAP\n- 分布式系统理论基础：时间、时钟和事件顺序\n- 分布式系统理论进阶：Paxos\n- 分布式系统理论进阶：Raft、Zab\n- 分布式系统理论进阶：选举、多数派和租约\n- 分布式锁的解决方案\n- 分布式事务的解决方案\n- 分布式 ID 生成器解决方案\n\n# 5. Netty\n\n- Netty 三层网络架构：Reactor 通信调度层、职责链 PipeLine、业务逻辑处理层\n\n  ```html\n  1. Reactor通信调度层: 该层的主要职责就是监听网络的连接和读写操作，负责将网络层的数据读取到内存缓冲区中，然后触发各种网络事件，例如连接创建、连接激活、读事件、写事件等，将这些事件触发到Pipeline中，再由Pipeline充当的职责链来进行后续的处理\n  2. 职责链Pipeline层。负责事件在职责链中有序的向前（后）传播，同时负责动态的编排职责链。Pipeline可以选择监听和处理自己关心的事件\n  3. 业务逻辑处理层，一般可分为两类：\n  \t\ta. 纯粹的业务逻辑处理，例如日志、订单处理。\n  \t\tb. 应用层协议管理，例如HTTP(S)协议、FTP协议等。\n  ```\n\n  \n\n- Netty 的线程调度模型\n\n- 序列化方式\n\n- 链路有效性检测\n\n- 流量整形\n\n- 优雅停机策略\n\n- Netty 对 SSL/TLS 的支持\n\n- Netty 的源码质量极高，推荐对部分的核心代码进行阅读：\n\n- Netty 的 Buffer\n\n- Netty 的 Reactor\n\n- Netty 的 Pipeline\n\n- Netty 的 Handler 综述\n\n- Netty 的 ChannelHandler\n\n- Netty 的 LoggingHandler\n\n- Netty 的 TimeoutHandler\n\n- Netty 的 CodecHandler\n\n- Netty 的 MessageToByteEncoder\n\n# 6. Hadoop\n\n## 6.1 MapReduce\n\n- 掌握 MapReduce 的工作原理\n- 能用 MapReduce 手写代码实现简单的 WordCount 或者 TopN 算法\n- 掌握 MapReduce Combiner 和 Partitioner的作用\n- 熟悉 Hadoop 集群的搭建过程，并且能解决常见的错误\n- 熟悉 Hadoop 集群的扩容过程和常见的坑\n- 如何解决 MapReduce 的数据倾斜\n- Shuffle 原理和减少 Shuffle 的方法\n\n## 6.2 HDFS\n\n- 十分熟悉 HDFS 的架构图和读写流程\n- 十分熟悉 HDFS 的配置\n- 熟悉 DataNode 和 NameNode 的作用\n- NameNode 的 HA 搭建和配置，Fsimage 和 EditJournal 的作用的场景\n- HDFS 操作文件的常用命令\n- HDFS 的安全模式\n\n## 6.3 Yarn：\n\n- Yarn 的产生背景和架构\n- Yarn 中的角色划分和各自的作用\n- Yarn 的配置和常用的资源调度策略\n- Yarn 进行一次任务资源调度的过程\n\n# 7. Hive\n\n> Hive 是一个数据仓库基础工具，在 Hadoop 中用来处理结构化数据。它架构在 Hadoop 之上，总归为大数据，并使得查询和分析方便。Hive 是应用最广泛的 OLAP 框架。Hive SQL 也是我们进行 SQL 开发用的最多的框架。\n\n- HiveSQL 的原理：我们都知道 HiveSQL 会被翻译成 MapReduce 任务执行，那么一条 SQL 是如何翻译成 MapReduce 的？\n- Hive 和普通关系型数据库有什么区别？\n- Hive 支持哪些数据格式\n- Hive 在底层是如何存储 NULL 的\n- HiveSQL 支持的几种排序各代表什么意思（Sort By/Order By/Cluster By/Distrbute By）\n- Hive 的动态分区\n- HQL 和 SQL 有哪些常见的区别\n- Hive 中的内部表和外部表的区别\n- Hive 表进行关联查询如何解决长尾和数据倾斜问题\n- HiveSQL 的优化（系统参数调整、SQL 语句优化）\n\n# 8. Hbase\n\n> HBase 本质上是一个数据模型，类似于谷歌的大表设计，可以提供快速随机访问海量结构化数据。它利用了 Hadoop 的文件系统（HDFS）提供的容错能力。\n>\n> 它是 Hadoop 的生态系统，提供对数据的随机实时读/写访问，是 Hadoop 文件系统的一部分。\n>\n> 我们可以直接或通过 HBase 的存储 HDFS 数据。使用 HBase 在 HDFS 读取消费/随机访问数据。 HBase 在 Hadoop 的文件系统之上，并提供了读写访问。\n>\n> HBase 是一个面向列的数据库，在表中它由行排序。表模式定义只能列族，也就是键值对。一个表有多个列族以及每一个列族可以有任意数量的列。后续列的值连续地存储在磁盘上。表中的每个单元格值都具有时间戳。总之，在一个 HBase：表是行的集合、行是列族的集合、列族是列的集合、列是键值对的集合。\n\n- Hbase 的架构和原理  \n- Hbase 的读写流程\n- Hbase 有没有并发问题？Hbase 如何实现自己的 MVVC 的？\n- Hbase 中几个重要的概念：HMaster、RegionServer、WAL 机制、MemStore\n- Hbase 在进行表设计过程中如何进行列族和 RowKey 的设计\n- Hbase 的数据热点问题发现和解决办法\n- 提高 Hbase 的读写性能的通用做法\n- HBase 中 RowFilter 和 BloomFilter 的原理\n- Hbase API 中常见的比较器\n- Hbase 的预分区\n- Hbase 的 Compaction\n- Hbase 集群中 HRegionServer 宕机如何解决\n\n# 9. Scala\n\n# 10. Spark\n\n> Spark 是专门为大数据处理设计的通用计算引擎，是一个实现快速通用的集群计算平台。它是由加州大学伯克利分校 AMP 实验室开发的通用内存并行计算框架，用来构建大型的、低延迟的数据分析应用程序。它扩展了广泛使用的 MapReduce 计算模型。高效的支撑更多计算模式，包括交互式查询和流处理。Spark 的一个主要特点是能够在内存中进行计算，即使依赖磁盘进行复杂的运算，Spark 依然比 MapReduce 更加高效。\n>\n> Spark 生态包含了：Spark Core、Spark Streaming、Spark SQL、Structured Streming 和机器学习相关的库等。\n\n## 10.1 Spark Core\n\n- Spark的集群搭建和集群架构（Spark 集群中的角色）👌\n- Spark Cluster 和 Client 模式的区别\n- Spark 的弹性分布式数据集 RDD 👌 \n  - spark 的rdd是什么，rdd的特性是什么，以及它的五大属性 👌\n  -  常见的算子操作map/mapPartitions 和 foreach 和 foreachPartitions是什么 👌\n- Spark DAG（有向无环图）\n- 掌握 Spark RDD 编程的算子 API（Transformation 和 Action 算子）\n- RDD 的依赖关系，什么是宽依赖和窄依赖\n- RDD 的血缘机制\n- Spark 核心的运算机制\n- Spark 的任务调度和资源调度\n- Spark 的 CheckPoint 和容错\n- Spark 的通信机制\n- Spark Shuffle 原理和过程\n\n## 10.2 Spark Streaming：\n\n- 原理剖析（源码级别）和运行机制\n- Spark Dstream 及其 API 操作\n- Spark Streaming 消费 Kafka 的两种方式\n- Spark 消费 Kafka 消息的 Offset 处理\n- 数据倾斜的处理方案\n- Spark Streaming 的算子调优\n- 并行度和广播变量\n- Shuffle 调优\n\n## 10.3 Spark SQL：\n\n- Spark SQL 的原理和运行机制\n- Catalyst 的整体架构\n- Spark SQL 的 DataFrame\n\n- Spark SQL 的优化策略：内存列式存储和内存缓存表、列存储压缩、逻辑查询优化、Join 的优化\n\n## 10.4 Structured Streaming\n\n> Spark 从 2.3.0 版本开始支持 Structured Streaming，它是一个建立在 Spark SQL 引擎之上可扩展且容错的流处理引擎，统一了批处理和流处理。正是 Structured Streaming 的加入使得 Spark 在统一流、批处理方面能和 Flink 分庭抗礼。\n\n- Structured Streaming 的模型\n- Structured Streaming 的结果输出模式\n- 事件时间（Event-time）和延迟数据（Late Data）\n- 窗口操作\n- 水印\n- 容错和数据恢复\n\n## 10.5 Spark Mlib：\n\n> 本部分是 Spark 对机器学习支持的部分，我们学有余力的同学可以了解一下 Spark 对常用的分类、回归、聚类、协同过滤、降维以及底层的优化原语等算法和工具。可以尝试自己使用 Spark Mlib 做一些简单的算法应用\n\n# 11. Kafka\n\n> Kafka 是最初由 Linkedin 公司开发，是一个分布式、支持分区的（partition）、多副本的（replica）的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于 Hadoop 的批处理系统、低延迟的实时系统、Spark 流式处理引擎，Nginx 日志、访问日志，消息服务等等，用 Scala 语言编写，Linkedin 于 2010 年贡献给了 Apache 基金会并成为顶级开源项目。\n>\n> Kafka 或者类似 Kafka 各个公司自己造的消息'轮子'已经是大数据领域消息中间件的事实标准。目前 Kafka 已经更新到了 2.x 版本，支持了类似 KafkaSQL 等功能，Kafka 不满足单纯的消息中间件，也正朝着平台化的方向演进。\n\n#### 11.1 如何提升生产者的吞吐量？\n\n1）buffer.memory：设置发送消息的缓冲区，默认值是33554432，就是32MB\n如果发送消息出去的速度小于写入消息进去的速度，就会导致缓冲区写满，此时生产消息就会阻塞住，所以说这里就应该多做一些压测，尽可能保证说这块缓冲区不会被写满导致生产行为被阻塞住\n\n```java\n\t  Long startTime=System.currentTime();\n\t  producer.send(record, new Callback() {\n\t  @Override\n\t\tpublic void onCompletion(RecordMetadata metadata, Exception exception) {\n\t\t\tif(exception == null) {\n\t\t\t\t// 消息发送成功\n\t\t\t\tSystem.out.println(\"消息发送成功\");  \n\t\t\t} else {\n\t\t\t\t// 消息发送失败，需要重新发送\n\t\t\t}\n\t\t}\n\t});\n    Long endTime=System.currentTime();\n    If(endTime - startTime > 100){//说明内存被压满了\n     说明有问题\n     }\t\t\n```\n\n2）compression.type，默认是none，不压缩，但是也可以使用lz4压缩，效率还是不错的，压缩之后可以减小数据量，提升吞吐量，但是会加大producer端的cpu开销。\n3）batch.size，设置meigebatch的大小，如果batch太小，会导致频繁网络请求，吞吐量下降；如果batch太大，会导致一条消息需要等待很久才能被发送出去，而且会让内存缓冲区有很大压力，过多数据缓冲在内存里\n默认值是：16384，就是16kb，也就是一个batch满了16kb就发送出去，一般在实际生产环境，这个batch的值可以增大一些来提升吞吐量，可以自己压测一下。\n4）linger.ms，这个值默认是0，意思就是消息必须立即被发送，但是这是不对的，一般设置一个100毫秒之类的，这样的话就是说，这个消息被发送出去后进入一个batch，如果100毫秒内，这个batch满了16kb，自然就会发送出去。但是如果100毫秒内，batch没满，那么也必须把消息发送出去了，不能让消息的发送延迟时间太长，也避免给内存造成过大的一个压力。\n\n#### 11.2 如何保证Kafka内部数据不丢失？\n\n如果要回答这个问题的话，要从三个角度去回答：Producer，consumer，broker。\n\n==**producer**==\n\n```html\nacks参数：\nacks = 0\n\t生产者发送消息之后 不需要等待服务端的任何响应，它不管消息有没有发送成功，如果发送过程中遇到了异常，导致broker端没有收到消息，消息也就丢失了。实际上它只是把消息发送到了socketBuffer(缓存)中，而socketBuffer什么时候被提交到broker端并不关心，它不担保broker端是否收到了消息，但是这样的配置对retry是不起作用的，因为producer端都不知道是否发生了错误，而且对于offset的获取永远都是-1，因为broker端可能还没有开始写数据。这样不保险的操作为什么还有这样的配置？kafka对于收集海量数据，如果在收集某一项日志时是允许数据量有一定丢失的话，是可以用这种配置来收集日志。\nacks = 1(默认值)\n\t生产者发送消息之后，只要分区的leader副本成功写入消息，那么它就会收到来自服务端的成功响应。其实就是消息只发给了leader leader收到消息后会返回ack到producer端。如果消息无法写入leader时(选举、宕机等情况时)，生产都会收到一个错误的响应，为了避免消息丢失，生产者可以选择重发消息，如果消息成功写入，在被其它副本同步数据时leader　　崩溃,那么此条数据还是会丢失，因为新选举的leader是没有收到这条消息，ack设置为1是消息可靠性和吞吐量折中的方案。\nacks = all (或-1)\n\t生产者在发送消息之后，需要等待ISR中所有的副本都成功写入消息之后才能够收到来自服务端的成功响应，在配置环境相同的情况下此种配置可以达到最强的可靠性。即：在发送消息时，需要leader 向fllow 同步完数据之后，也就是ISR队列中所有的broker全部保存完这条消息后，才会向ack发送消息，表示发送成功。\n\nretry参数：\n在kafka中错误分为2种，一种是可恢复的，另一种是不可恢复的。\n　　可恢复性的错误：\n　　　　　　如遇到在leader的选举、网络的抖动等这些异常时，如果我们在这个时候配置的retries大于0的，也就是可以进行重试操作，那么等到leader选举完成后、网络稳定后，这些异常就会消息，错误也就可以恢复，数据再次重发时就会正常发送到broker端。需要注意retries(重试)之间的时间间隔，以确保在重试时可恢复性错误都已恢复。\n　　不可恢复性的错误：\n　　　　　　如：超过了发送消息的最大值(max.request.size)时，这种错误是不可恢复的，如果不做处理，那么数据就会丢失，因此我们需要注意在发生异常时把这些消息写入到DB、缓存本地文件中等等，把这些不成功的数据记录下来，等错误修复后，再把这些数据发送到broker端。\n　　　　　　　　　　\n配置方案\n1.高可用型\n　　配置：acks = all，retries > 0 retry.backoff.ms=100(毫秒) (并根据实际情况设置retry可能恢复的间隔时间)\n　　优点：这样保证了producer端每发送一条消息都要成功，如果不成功并将消息缓存起来，等异常恢复后再次发送。\n　　缺点：这样保证了高可用，但是这会导致集群的吞吐量不是很高，因为数据发送到broker之后，leader要将数据同步到fllower上，如果网络带宽、不稳定等情况时，ack响应时间会更长\n　　\n2.折中型\n　　配置：acks = 1  retries > 0 retries 时间间隔设置 (并根据实际情况设置retries可能恢复的间隔时间)\n　　优点：保证了消息的可靠性和吞吐量，是个折中的方案\n　　缺点：性能处于2者中间\n　　\n3.高吞吐型 \n　　配置：acks = 0\n　　优点：可以相对容忍一些数据的丢失，吞吐量大，可以接收大量请求\n　　缺点：不知道发送的消息是 否成功\n```\n\n\n\n==**Consumer**==\n\n```html\ngroup.id:\n\tconsumer group分组的一个id，消费者隶属的消费组名称。在kafka中只允许消息只能被某个组里面的一个consumer端消费，如果为空，则会报异常。对于一个新的consumer加入到消费时，肯定会隶属于哪个组，只有这样才能消费数据\nauto.offset.reset = earliest(最早) /latest(最晚)\n\t从何处开始进行消费　　当一个新加入的consumer要进行消费数据，如果这个consumer是做数据分析工作的，是需要以前的历史数据那就需要从最早的位置消费数据，如果仅仅是查看消费情况，那可以从最晚位置开始消费数据\nenable.auto.commit = true/false(默认true)　\n\t是否开启自动提交消费位移的功能,默认开启.　　当设置为true时，意味着由kafka的consumer端自己间隔一定的时间会自动提交offset，如果设置成了fasle，也就是由客户端(自己写代码)来提交，那就还得控制提交的时间间隔auto.commit.interval.msauto.commit.interval.ms\n\t当enable.auto.commit设置为true时才生效，表示开启自动提交消费位移功能时自动提交消费位移的时间间隔。\n```\n\n配置方案\n\n```html\n在consumer消费阶段，对offset的处理，关系到是否丢失数据，是否重复消费数据，因此，我们把处理好offset就可以做到exactly-once && at-least-once(只消费一次)数据。当enable.auto.commit=true时　　　　表示由kafka的consumer端自动提交offset,当你在pull(拉取)30条数据，在处理到第20条时自动提交了offset,但是在处理21条的时候出现了异常，当你再次pull数据时，由于之前是自动提交的offset，所以是从30条之后开始拉取数据，这也就意味着21-30条的数据发生了丢失。\n\n当enable.auto.commit=false时，由于上面的情况可知自动提交offset时，如果处理数据失败就会发生数据丢失的情况。那我们设置成手动提交。当设置成false时，由于是手动提交的，可以处理一条提交一条，也可以处理一批，提交一批，由于consumer在消费数据时是按一个batch来的，当pull了30条数据时，如果我们处理一条，提交一个offset，这样会严重影响消费的能力，那就需要我们来按一批来处理，或者设置一个累加器，处理一条加1，如果在处理数据时发生了异常，那就把当前处理失败的offset进行提交(放在finally代码块中)注意一定要确保offset的正确性，当下次再次消费的时候就可以从提交的offset处进行再次消费。\n\n```\n\n==**Broker**==\n\n```html\n1.replication-factor >=2\n在创建topic时会通过replication-factor来创建副本的个数，它提高了kafka的高可用性，同时，它允许n-1台broker挂掉，设置好合理的副本因子对kafka整体性能是非常有帮助的，通常是3个，极限是5个，如果多了也会影响开销。\n\n2.min.insync.replicas = 2\n分区ISR队列集合中最少有多少个副本，默认值是1\n\n3.unclean.leander.election.enable = false 　　　　\n是否允许从ISR队列中选举leader副本，默认值是false,如果设置成true,则可能会造成数据丢失。\n```\n\n\n\n#### 11.3 积压了百万消息如何处理？\n\n据我了解，在使用消息队列遇到的问题中，消息积压这个问题，应该是最常遇到的问题了，并且，这个问题还不太好解决。我们都知道，消息积压的直接原因，一定是系统中的某个部分出现了性能问题，来不及处理上游发送的消息，才会导致消息积压。所以，我们先来分析下，在使用消息队列时，如何来优化代码的性能，避免出现消息积压。然后再来看看，如果你的线上系统出现了消息积压，该如何进行紧急处理，最大程度地避免消息积压对业务的影响。\n\n1. 最大程度避免消息积压\n\n   生产者\n\n   ​\t提升吞吐量\n\n   消费者\n\n   ​\t扩容，扩分区\n\n   ​\t增加consumer\n\n2. 如何处理消息积压\n\n   日常系统正常运转的时候，没有积压或者只有少量积压很快就消费掉了，但是某一个时刻，突然就开始积压消息并且积压持续上涨。这种情况下需要你在短时间内找到消息积压的原因，迅速解决问题才不至于影响业务。导致突然积压的原因肯定是多种多样的，不同的系统、不同的情况有不同的原因，不能一概而论。但是，我们排查消息积压原因，是有一些相对固定而且比较有效的方法的。能导致积压突然增加，最粗粒度的原因，只有两种：要么是发送变快了，要么是消费变慢了。大部分消息队列都内置了监控的功能，只要通过监控数据，很容易确定是哪种原因。如果是单位时间发送的消息增多，比如说是赶上大促或者抢购，短时间内不太可能优化消费端的代码来提升消费性能，唯一的方法是通过扩容消费端的实例数来提升总体的消费能力。如果短时间内没有足够的服务器资源进行扩容，没办法的办法是，将系统降级，通过关闭一些不重要的业务，减少发送方发送的数据量，最低限度让系统还能正常运转，服务一些重要业务。还有一种不太常见的情况，你通过监控发现，无论是发送消息的速度还是消费消息的速度和原来都没什么变化，这时候你需要检查一下你的消费端，是不是消费失败导致的一条消息反复消费这种情况比较多，这种情况也会拖慢整个系统的消费速度。如果监控到消费变慢了，你需要检查你的消费实例，分析一下是什么原因导致消费变慢。优先检查一下日志是否有大量的消费错误，如果没有错误的话，可以通过打印堆栈信息，看一下你的消费线程是不是卡在什么地方不动了，比如触发了死锁或者卡在等待某些资源上了。\n\n#### 11.4 生产者遇到了异常如何处理？\n\n1. 添加重试功能和重试时间间隔\n2. 对于重试也失败了任务进行特殊处理\n\n\n\n#### 11.5 说一下Kafka的HW，LEO的更新机制\n\n![HW和LEO的更新](https://kfly.top/picture/kfly-top/大数据面试试题汇总/assets/HW和LEO的更新-8199995.png)\n\n\n\n#### 11.6 Zookeeper对于Kafka的作用是什么？\n\nKafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。Controller的管理工作都是依赖于Zookeeper的\n\n注：参考Kafka核心原理的Kafka的集群管理机制\n\n####  11.7 讲一讲Kafka的ack的三种机制\n\nacks参数，其实是控制发送出去的消息的持久化机制的\n1）如果acks=0，那么producer根本不管写入broker的消息到底成功没有，发送一条消息出去，立马就可以发送下一条消息，这是吞吐量最高的方式，但是可能消息都丢失了，你也不知道的，但是说实话，你如果真是那种实时数据流分析的业务和场景，就是仅仅分析一些数据报表，丢几条数据影响不大的。会让你的发送吞吐量会提升很多，你发送弄一个batch出，不需要等待人家leader写成功，直接就可以发送下一个batch了，吞吐量很大的，哪怕是偶尔丢一点点数据，实时报表，折线图，饼图。\n\n2）acks=all，或者acks=-1：这个leader写入成功以后，必须等待其他ISR中的副本都写入成功，才可以返回响应说这条消息写入成功了，此时你会收到一个回调通知\n\n3）acks=1：只要leader写入成功，就认为消息成功了，默认给这个其实就比较合适的，还是可能会导致数据丢失的，如果刚写入leader，leader就挂了，此时数据必然丢了，其他的follower没收到数据副本，变成leader\n\n#### 11.8 Kafka如何不重复消费数据\n\n1. 保存并查询\n\n   给每个消息都设置一个独一无二的key,消费的时候把这些key记录下来，然后每次消费的时候都查询一下，看这个key是否消费过，如果没有消费过才消费。\n\n2. 幂等\n\n   幂等（Idempotence） 本来是一个数学上的概念，它是这样定义的：如果一个函数 f(x) 满足：f(f(x)) = f(x)，则函数 f(x) 满足幂等性。这个概念被拓展到计算机领域，被用来描述一个操作、方法或者服务。一个幂等操作的特点是，其任意多次执行所产生的影响均与一次执行的影响相同。一个幂等的方法，使用同样的参数，对它进行多次调用和一次调用，对系统产生的影响是一样的。所以，对于幂等的方法，不用担心重复执行会对系统造成任何改变。我们举个例子来说明一下。在不考虑并发的情况下，“将账户 X 的余额设置为 100 元”，执行一次后对系统的影响是，账户 X 的余额变成了 100 元。只要提供的参数 100 元不变，那即使再执行多少次，账户 X 的余额始终都是 100 元，不会变化，这个操作就是一个幂等的操作。再举一个例子，“将账户 X 的余额加 100 元”，这个操作它就不是幂等的，每执行一次，账户余额就会增加 100 元，执行多次和执行一次对系统的影响（也就是账户的余额）是不一样的。如果我们系统消费消息的业务逻辑具备幂等性，那就不用担心消息重复的问题了，因为同一条消息，消费一次和消费多次对系统的影响是完全一样的。也就可以认为，消费多次等于消费一次。从对系统的影响结果来说：At least once + 幂等消费 = Exactly once\n\n   \n\n   那么如何实现幂等操作呢？最好的方式就是，从业务逻辑设计上入手，将消费的业务逻辑设计成具备幂等性的操作。但是，不是所有的业务都能设计成天然幂等的，这里就需要一些方法和技巧来实现幂等。下面我给你介绍几种常用的设计幂等操作的方法：1. 利用数据库的唯一约束实现幂等例如我们刚刚提到的那个不具备幂等特性的转账的例子：将账户 X 的余额加 100 元。在这个例子中，我们可以通过改造业务逻辑，让它具备幂等性。首先，我们可以限定，对于每个转账单每个账户只可以执行一次变更操作，在分布式系统中，这个限制实现的方法非常多，最简单的是我们在数据库中建一张转账流水表，这个表有三个字段：转账单 ID、账户 ID 和变更金额，然后给转账单 ID 和账户 ID 这两个字段联合起来创建一个唯一约束，这样对于相同的转账单 ID 和账户 ID，表里至多只能存在一条记录。这样，我们消费消息的逻辑可以变为：“在转账流水表中增加一条转账记录，然后再根据转账记录，异步操作更新用户余额即可。”在转账流水表增加一条转账记录这个操作中，由于我们在这个表中预先定义了“账户 ID 转账单 ID”的唯一约束，对于同一个转账单同一个账户只能插入一条记录，后续重复的插入操作都会失败，这样就实现了一个幂等的操作。我们只要写一个 SQL，正确地实现它就可以了。基于这个思路，不光是可以使用关系型数据库，只要是支持类似“INSERT IF NOT EXIST”语义的存储类系统都可以用于实现幂等，比如，你可以用 Redis 的 SETNX 命令来替代数据库中的唯一约束，来实现幂等消费。\n\n   3. 为更新的数据设置前置条件\n\n   为更新的数据设置前置条件另外一种实现幂等的思路是，给数据变更设置一个前置条件，如果满足条件就更新数据，否则拒绝更新数据，在更新数据的时候，同时变更前置条件中需要判断的数据。这样，重复执行这个操作时，由于第一次更新数据的时候已经变更了前置条件中需要判断的数据，不满足前置条件，则不会重复执行更新数据操作。比如，刚刚我们说过，“将账户 X 的余额增加 100 元”这个操作并不满足幂等性，我们可以把这个操作加上一个前置条件，变为：“如果账户 X 当前的余额为 500 元，将余额加 100 元”，这个操作就具备了幂等性。对应到消息队列中的使用时，可以在发消息时在消息体中带上当前的余额，在消费的时候进行判断数据库中，当前余额是否与消息中的余额相等，只有相等才执行变更操作。但是，如果我们要更新的数据不是数值，或者我们要做一个比较复杂的更新操作怎么办？用什么作为前置判断条件呢？更加通用的方法是，给你的数据增加一个版本号属性，每次更数据前，比较当前数据的版本号是否和消息中的版本号一致，如果不一致就拒绝更新数据，更新数据的同时将版本号 +1，一样可以实现幂等更新\n\n#### 11.9 如何保证同一分区一定有序\n\n两种方案：\n方案一，kafka topic 只设置一个partition分区  \n方案二，producer将消息发送到指定partition分区\n解析：\n方案一：kafka默认保证同一个partition分区内的消息是有序的，则可以设置topic只使用一个分区，这样消息就是全局有序，缺点是只能被consumer group里的一个消费者消费，降低了性能，不适用高并发的情况\n方案二：既然kafka默认保证同一个partition分区内的消息是有序的，则producer可以在发送消息时可以指定需要保证顺序的几条消息发送到同一个分区，这样消费者消费时，消息就是有序。\n\n但是个时候还有个问题就是消息重试的时候会让消息顺序打乱，所以还需要设置这个参数：\nmax.in.flight.requests.per.connection 默认值5，设置为1\n\n- Kafka 的特性和使用场景\n- Kafka 中的一些概念：Leader、Broker、Producer、Consumer、Topic、Group、Offset、Partition、ISR\n- Kafka 的整体架构\n- Kafka 选举策略\n- Kafka 读取和写入消息过程中都发生了什么\n- Kakfa 如何进行数据同步（ISR）\n- Kafka 实现分区消息顺序性的原理\n- 消费者和消费组的关系\n- 消费 Kafka 消息的 Best Practice（最佳实践）是怎样的\n- Kafka 如何保证消息投递的可靠性和幂等性\n- Kafka 消息的事务性是如何实现的\n- 如何管理 Kafka 消息的 Offset\n- Kafka 的文件存储机制\n- Kafka 是如何支持 Exactly-once 语义的\n- 通常 Kafka 还会要求和 RocketMQ 等消息中间件进行比较\n\n\n\n# 12. Flink\n\n> Apache Flink（以下简称 Flink）项目是大数据处理领域最近冉冉升起的一颗新星，其不同于其他大数据项目的诸多特性吸引了越来越多人的关注。尤其是 2019 年初 Blink 开源将 Flink 的关注度提升到了前所未有的程度。\n\n- Flink 集群的搭建\n- Flink 的架构原理\n- Flink 的编程模型\n- Flink 集群的 HA 配置\n- Flink DataSet 和 DataSteam API\n- 序列化\n- Flink 累加器\n- 状态 State 的管理和恢复\n- 窗口和时间\n- 并行度\n- Flink 和消息中间件 Kafka 的结合\n- Flink Table 和 SQL 的原理和用法\n\n# 13. 大数据算法\n\n- 两个超大文件找共同出现的单词\n- 海量数据求 TopN\n- 海量数据找出不重复的数据\n- 布隆过滤器\n- bit-map\n- 堆\n- 字典树\n- 倒排索引","tags":["bigdata","meet"]},{"title":"实现高效工作","url":"/2019/12/08/work/生与活的统一-方法/","content":"\n# 1. 行为认知的误区\n\n## 1.1. 破除认知的误差\n\n### 1.1.1 行为分析的误差\n\n- 三大类**认知偏差**\n  - 知识的诅咒：指一旦你知道了某件事情，就很难去想象不知道这件事会是什么样子。\n  - 证实偏见：指如果你已经开始对某件事情有了偏见，那么你就会主动寻找能够增强这种偏见的信息，而不顾及其他因素。\n  - 刻板印象：指人类对于某些特定类型人活着事物的概括看法\n  \n- 个体行为的**三大影响因素**\n\n  | 福格行为模型 | 简单概括       | 在系统层面的对应                               |\n  | ------------ | -------------- | ---------------------------------------------- |\n  | 动机         | 愿（行为意愿） | 动力系统：对方了解要做什么之后，是否有意愿去做 |\n  | 能力         | 能（行为能力） | 能力系统：对方具备了意愿，是否有足够能力来完成 |\n  | 触发条件     | 知（角色认知） | 解释系统：对方是否清楚自身的角色定位           |\n\n  \n\n# 2. 高效工作\n\n## 2.1 结果导向、极致专注\n\n- 将 “以终为始” 付诸实践的两大步骤\n\n  - 明确我们的目标：\n    - 从职位本身出发：基于该职位的职责、做这件事情饿目的是什么？\n    - 从公司的近期目标来看：做这件事情的目的是什么？\n    - 从任务本身出发：这件事本身的目的是什么？\n  - 从目的出发、运用“二、八法则 ” 筛选 “正确的事”\n    - 列出所有的待办事项、分为 “可以做、但不做也无妨” 和 “必须做、不做会出大麻烦”两种\n    - 可以以天、周、月、年为基本单位\n\n  ","tags":["work"]},{"title":"scala学习入门","url":"/2019/12/07/it/scala/scala学习入门/","content":"\n### 1. scala简介\n\n* scala是运行在 JVM 上的多范式编程语言，同时支持==面向对象==和==面向函数编程==\n* 早期scala刚出现的时候，并没有怎么引起重视，随着==Spark==和==Kafka==这样基于scala的大数据框架的兴起，scala逐步进入大数据开发者的眼帘。scala的主要优势是它的表达性。\n* 官网地址\n  * http://www.scala-lang.org\n\n\n\n### 2. 为什么要使用scala\n\n* 开发大数据应用程序（Spark程序、Flink程序）\n* 表达能力强，一行代码抵得上Java多行，开发速度快\n* 兼容Java，可以访问庞大的Java类库\n\n![1568095858219](https://kfly.top/picture/kfly-top/scala学习入门/assets/1568095858219.png)\n\n\n\n### 3. 开发环境安装\n\n* 学习如何编写scala代码之前，需要先安装scala编译器以及开发工具\n\n  * Java程序编译执行流程\n\n    ![1556551819121](https://kfly.top/picture/kfly-top/scala学习入门/assets/1556551819121.png)\n\n  * Scala程序编译执行流程\n\n    ![1556551904384](https://kfly.top/picture/kfly-top/scala学习入门/assets/1556551904384.png)\n\n* scala程序运行需要依赖于Java类库，必须要有**==Java运行环境==**，scala才能正确执行\n\n  * **要编译运行scala程序需要**\n    * ==jdk  ( jvm )==\n    * ==scala编译器（scala SDK）==\n\n### 4. scala中声明变量\n\n* **1、语法格式**\n\n~~~scala\nval/var 变量名称:变量类型 = 初始值\n~~~\n\n* 其中\n\n  * `val`定义的是**不可重新赋值**的变量(值不可修改)\n  * `var`定义的是**可重新赋值**的变量(值可以修改)\n\n* ps\n\n  * scala中声明变量是变量名称在前，变量类型在后，跟java是正好相反\n  * scala的语句最后不需要添加分号\n\n* **2、演示**\n\n  ~~~scala\n  #使用val声明变量,相当于java中的final修饰,不能在指向其他的数据了\n   val  a:Int = 10\n  #使用var声明变量,后期可以被修改重新赋值\n   var  b:Int = 20\t \n   b=100\n  #scala中的变量的类型可以显式的声明,也可以不声明,如果不显式的声明这会根据变量的值来推断出来变量的类型(scala支持类型推断)\n   val c = 20\n  ~~~\n\n![1568103524126](https://kfly.top/picture/kfly-top/scala学习入门/assets/1568103524126.png)\n\n\n\n* 3、**惰性变量**\n\n  * Scala中使用==关键字lazy==来定义惰性变量，实现延迟加载(懒加载)。 \n  * 惰性变量只能是不可变变量，并且只有在调用惰性变量时，才会去实例化这个变量。\n  * 语法格式\n\n  ~~~scala\n  lazy val 变量名 = 表达式\n  ~~~\n\n  ![1568104205830](https://kfly.top/picture/kfly-top/scala学习入门/assets/1568104205830.png)\n\n​         \n\n### 5. scala中数据类型\n\n* scala中的类型绝大多数和Java一样\n* 数据类型\n\n| 基础类型 | 类型说明                 |\n| -------- | ------------------------ |\n| Byte     | 8位带符号整数            |\n| Short    | 16位带符号整数           |\n| **Int**  | 32位带符号整数           |\n| Long     | 64位带符号整数           |\n| Char     | 16位无符号Unicode字符    |\n| String   | Char类型的序列（字符串） |\n| Float    | 32位单精度浮点数         |\n| Double   | 64位双精度浮点数         |\n| Boolean  | true或false              |\n\n* =**=注意下 scala类型与Java的区别**==\n\n~~~html\n1. scala中所有的类型都使用大写字母开头\n2. 整形使用Int而不是Integer\n3. scala中定义变量可以不写类型，让scala编译器自动推断\n~~~\n\n\n\n* scala类型层次结构\n\n![1556592270468](https://kfly.top/picture/kfly-top/scala学习入门/assets/1556592270468.png)\n\n\n\n| 类型    | 说明                                                         |\n| ------- | ------------------------------------------------------------ |\n| Any     | **所有类型**的父类，,它有两个子类AnyRef与AnyVal              |\n| AnyVal  | **所有数值类型**的父类                                       |\n| AnyRef  | 所有对象类型（引用类型）的父类                               |\n| Unit    | 表示空，Unit是AnyVal的子类，它只有一个的实例（），它类似于Java中的void，但scala要比Java更加面向对象 |\n| Null    | Null是AnyRef的子类，也就是说它是所有引用类型的子类。它的实例是null,   可以将null赋值给任何对象类型 |\n| Nothing | 所有类型的**子类**不能直接创建该类型实例，某个方法抛出异常时，返回的就是Nothing类型，因为Nothing是所有类的子类，那么它可以赋值为任何类型 |\n\n~~~~\n\n~~~~\n\n\n\n### 6.  scala中的条件表达式\n\n* 条件表达式就是if表达式，if表达式可以根据给定的条件是否满足，根据条件的结果（真或假）决定执行对应的操作。scala条件表达式的语法和Java一样。\n\n~~~scala\n//定义变量x\nscala> val x =1\nx: Int = 1\n\n//if表达式\nscala> val y =if(x>0) 1 else -1\ny: Int = 1\n\n//支持混合类型表达式\nscala> val z=if(x>1) 1 else \"error\"\nz: Any = error\n\n//缺失else 相当于 if(x>2) 1 else ()\nscala> val m=if(x>2) 1\nm: AnyVal = ()\n\n//scala中有个Unit类，用作不返回任何结果的方法的结果类型,相当于Java中的void，Unit只有一个实例值，写成()\nscala> val n=if(x>2) 1 else ()\nn: AnyVal = ()\n\n//if(xx) else if(xx) else \nscala> val k=if(x<0) -1 else if (x==0) 0 else 1\nk: Int = 1\n~~~\n\n![1568107951316](https://kfly.top/picture/kfly-top/scala学习入门/assets/1568107951316.png)\n\n\n\n### 7. scala中的块表达式\n\n* 定义变量时用 {} 包含一系列表达式，其中块的最后一个表达式的值就是块的值。\n\n~~~scala\nval x=0 \nval result={\n  val y=x+10\n  val z=y+\"-hello\"  \n  val m=z+\"-kaikeba\"\n    \"over\"\n}\n//result的值就是块表达式的结果    \n//后期一个方法的返回值不需要加上return,把要返回的结果放在方法的最后一行就可以了 \n~~~\n\n![1568108241418](https://kfly.top/picture/kfly-top/scala学习入门/assets/1568108241418.png)\n\n* 在scala解释器中先输入 ==:paste== ,然后写多行代码, 之后按===ctrl+d==结束输入\n\n\n\n### 8. 循环\n\n~~~html\n在scala中，可以使用for和while，但一般推荐使用for表达式，因为for表达式语法更简洁\n~~~\n\n#### 8.1 for循环\n\n* 1、语法结构\n\n  ~~~scala\n  for (i <- 表达式/数组/集合){\n      //表达式\n  }\n  ~~~\n\n* 2、演示\n\n  * 简单的for循环\n\n  ~~~scala\n  //简单的for循环\n  scala> val nums= 1 to 10\n  nums: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n  \n  scala> for(i <- nums) println(i)\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  10\n  \n  ~~~\n\n  * 双重for循环\n\n  ~~~scala\n  //双重for循环\n  scala>  for(i <- 1 to 3; j <- 1 to 3) println(i*10+j)\n  11\n  12\n  13\n  21\n  22\n  23\n  31\n  32\n  33\n  \n  //双重for循环打印99乘法表\n  for(i <- 1 to 9; j <- 1 to i){\n      print(i+\"*\"+j+\"=\"+i*j+\"\\t\")\n       if(i==j){\n         println()\n      }    \n  } \n  \n  1*1=1\n  2*1=2   2*2=4\n  3*1=3   3*2=6   3*3=9\n  4*1=4   4*2=8   4*3=12  4*4=16\n  5*1=5   5*2=10  5*3=15  5*4=20  5*5=25\n  6*1=6   6*2=12  6*3=18  6*4=24  6*5=30  6*6=36\n  7*1=7   7*2=14  7*3=21  7*4=28  7*5=35  7*6=42  7*7=49\n  8*1=8   8*2=16  8*3=24  8*4=32  8*5=40  8*6=48  8*7=56  8*8=64\n  9*1=9   9*2=18  9*3=27  9*4=36  9*5=45  9*6=54  9*7=63  9*8=72  9*9=81\n  ~~~\n\n  * 守卫\n    * 在for表达式中可以添加if判断语句，这个if判断就称为守卫\n\n  ~~~scala\n  //语法结构\n  for(i <- 表达式/数组/集合 if 表达式) {\n      // 表达式\n  }\n  \n  scala> for(i <- 1 to 10 if i >5) println(i)\n  6\n  7\n  8\n  9\n  10\n  \n  ~~~\n\n  * for推导式\n    * 在for循环体中，可以使用yield表达式构建出一个集合，我们把使用yield的for表达式称之为推导式\n\n  ~~~scala\n  // for推导式：for表达式中以yield开始，该for表达式会构建出一个集合\n  \n  val v = for(i <- 1 to 5) yield i * 10\n  ~~~\n\n\n#### 8.2 while循环\n\n* scala中while循环和Java中是一致的\n* 语法结构\n\n~~~scala\nwhile(返回值为布尔类型的表达式){\n    //表达式\n}\n~~~\n\n* 演示\n\n~~~scala\nscala> var x = 10\nx: Int = 10\n\nscala> while(x >5){\n     | println(x)\n     | x -= 1\n     | }\n10\n9\n8\n7\n6\n~~~\n\n### 10. 方法和函数\n\n#### 10.1 方法\n\n* 语法\n\n~~~scala\ndef methodName (参数名:参数类型, 参数名:参数类型) : [return type] = {\n    // 方法体：一系列的代码\n}\n~~~\n\n![1568110629253](https://kfly.top/picture/kfly-top/scala学习入门/assets/1568110629253.png)\n\n* 说明\n\n  ~~~html\n  - 参数列表的参数类型不能省略\n  - 返回值类型可以省略，由scala编译器自动推断\n  - 返回值可以不写return，默认就是{}块表达式的值\n  \n  ~~~\n\n* 演示\n\n  ~~~scala\n  scala> def add(a:Int,b:Int) = a+b\n  add: (a: Int, b: Int)Int\n  \n  scala> add(1,2)\n  res8: Int = 3\n  \n  scala>\n  \n  ~~~\n\n* 注意\n\n  * 如果定义递归方法，不能省略返回值类型\n  * 示例：\n    * 定义递归方法（求阶乘）\n      * 10 * 9 * 8 * 7 * 6 * ... * 1\n\n  ~~~scala\n  scala> def m1(x:Int)={\n       | if(x==1) 1\n       | else x * m1(x-1)\n       | }\n  <console>:14: error: recursive method m1 needs result type\n         else x * m1(x-1)\n                  ^\n  \n  scala> def m1(x:Int):Int={\n       | if(x==1) 1\n       | else x * m1(x-1)\n       | }\n  m1: (x: Int)Int\n  \n  scala> m1(10)\n  res9: Int = 3628800\n  \n  ~~~\n\n\n\n* 方法的参数\n\n  * 1、默认参数\n\n    * 在定义方法时可以给参数定义一个默认值。\n\n    * 示例\n\n      ~~~scala\n      //1. 定义一个计算两个值相加的方法，这两个值默认为0\n      //2. 调用该方法\n      \n      scala> def add(x:Int = 0, y:Int = 0) = x + y\n      add: (x: Int, y: Int)Int\n      \n      scala> add(10)\n      res14: Int = 10\n      \n      scala> add(10,20)\n      res15: Int = 30\n      \n      ~~~\n\n  * 2、带名参数\n\n    * 在调用方法时，可以指定参数的名称来进行调用。\n\n    * 示例\n\n    ~~~scala\n    scala> def add(x:Int = 0, y:Int = 0) = x + y\n    add: (x: Int, y: Int)Int\n    \n    scala> add(x=1)\n    res16: Int = 1\n    \n    ~~~\n\n  * 3、变长参数\n\n    * 如果方法的参数是不固定的，可以定义一个方法的参数是变长参数。\n\n    * 语法格式：\n\n      ~~~scala\n      def 方法名(参数名:参数类型*):返回值类型 = {\n          方法体\n      }\n      \n      //在参数类型后面加一个*号，表示参数可以是0个或者多个\n      ~~~\n\n    * 示例\n\n      ~~~scala\n      scala> def add(num:Int*) = num.sum\n      add: (num: Int*)Int\n      \n      scala> add(1,2,3,4,5)\n      res17: Int = 15\n      ~~~\n\n\n#### 10.2 函数\n\n* scala支持函数式编程，将来编写Spark/Flink程序中，会大量使用到函数\n* 语法\n\n~~~scala\nval 函数变量名 = (参数名:参数类型, 参数名:参数类型....) => 函数体\n~~~\n\n![1568111630788](https://kfly.top/picture/kfly-top/scala学习入门/assets/1568111630788.png)\n\n* 注意\n\n~~~html\n- 函数是一个对象（变量）\n- 类似于方法，函数也有输入参数和返回值\n- 函数定义不需要使用def定义\n- 无需指定返回值类型\n~~~\n\n* 演示\n\n~~~scala\nscala> val add = (x:Int, y:Int) => x + y\nadd: (Int, Int) => Int = <function2>\n\nscala> add(1,2)\nres3: Int = 3\n\n\n//一个函数没有赋予一个变量，则称为匿名函数，\n//后期再实际开发代码的时候，基本上都是使用匿名函数\n(x:Int,y:Int)=>x+y\n~~~\n\n\n\n#### 10.3 方法和函数的区别\n\n* 方法是隶属于类或者对象的，在运行时，它是加载到JVM的方法区中\n* 可以将函数对象赋值给一个变量，在运行时，它是加载到JVM的堆内存中\n* ==函数是一个对象，继承自FunctionN==，函数对象有apply，curried，toString，tupled这些方法，而方法则没有\n\n\n\n#### 10.4 方法转换为函数\n\n* 有时候需要将方法转换为函数，作为变量传递，就需要将方法转换为函数\n\n* 使用`_`即可将方法转换为函数\n\n* 示例\n\n  ~~~scala\n  scala> def add(x:Int,y:Int)=x+y\n  add: (x: Int, y: Int)Int\n  \n  scala> val a = add _\n  a: (Int, Int) => Int = <function2>\n  ~~~\n\n\n### 11. 数组\n\n* scala中数组的概念是和Java类似，可以用数组来存放一组数据\n* scala中，有两种数组，一种是**定长数组**，另一种是**变长数组**\n\n\n\n#### 11.1 定长数组\n\n* 定长数组指的是数组的**长度**是**不允许改变**的\n\n* 数组的**元素**是**可以改变**的\n\n* 语法\n\n  ~~~scala\n  // 通过指定长度定义数组\n  val/var 变量名 = new Array[元素类型](数组长度)\n  \n  // 用元素直接初始化数组\n  val/var 变量名 = Array(元素1, 元素2, 元素3...)\n  \n  ~~~\n\n* 注意\n\n  ~~~html\n  在scala中，数组的泛型使用[]来指定\n  使用()来获取元素\n  \n  ~~~\n\n* 演示\n\n  ~~~scala\n  scala> val a=new Array[Int](10)\n  a: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n  \n  scala> a(0)\n  res19: Int = 0\n  \n  scala> a(0)=10\n  \n  scala> a\n  res21: Array[Int] = Array(10, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n  \n  //////////////////////////////////////////////////////////////////\n  scala> val b =Array(\"hadoop\",\"spark\",\"hive\")\n  b: Array[String] = Array(hadoop, spark, hive)\n  \n  scala> b(0)\n  res24: String = hadoop\n  \n  scala> b.length\n  res25: Int = 3\n  ~~~\n\n\n\n#### 11.2 变长数组\n\n* 变长数组指的是数组的长度是可变的，可以往数组中添加、删除元素\n\n* 创建变长数组，需要提前导入ArrayBuffer类\n\n  ~~~scala\n  import scala.collection.mutable.ArrayBuffer\n  ~~~\n\n* 语法\n\n  * 创建空的ArrayBuffer变长数组\n\n  ~~~scala\n  val/var a = ArrayBuffer[元素类型]()\n  ~~~\n\n  * 创建带有初始元素的ArrayBuffer\n\n  ~~~scala\n  val/var a = ArrayBuffer(元素1，元素2，元素3....)\n  ~~~\n\n* 演示\n\n  ~~~scala\n  //导入ArrayBuffer类型\n  scala> import scala.collection.mutable.ArrayBuffer\n  import scala.collection.mutable.ArrayBuffer\n  \n  //定义一个长度为0的整型变长数组\n  scala> val a=ArrayBuffer[Int]()\n  a: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer()\n  \n  //定义一个有初始元素的变长数组\n  scala> val b = ArrayBuffer(\"hadoop\", \"storm\", \"spark\")\n  b: scala.collection.mutable.ArrayBuffer[String] = ArrayBuffer(hadoop, storm, spark)\n  ~~~\n\n* 变长数组的增删改操作\n\n  - 使用`+=`添加元素\n  - 使用`-=`删除元素\n  - 使用`++=`追加一个数组到变长数组\n\n* 示例\n\n  ~~~scala\n  // 定义变长数组\n  scala> val a = ArrayBuffer(\"hadoop\", \"spark\", \"flink\")\n  a: scala.collection.mutable.ArrayBuffer[String] = ArrayBuffer(hadoop, spark, flink)\n  \n  // 追加一个元素\n  scala> a += \"flume\"\n  res10: a.type = ArrayBuffer(hadoop, spark, flink, flume)\n  \n  // 删除一个元素\n  scala> a -= \"hadoop\"\n  res11: a.type = ArrayBuffer(spark, flink, flume)\n  \n  // 追加一个数组\n  scala> a ++= Array(\"hive\", \"sqoop\")\n  res12: a.type = ArrayBuffer(spark, flink, flume, hive, sqoop)\n  \n  ~~~\n\n\n\n\n\n#### 11.3 遍历数组\n\n* 可以使用以下两种方式来遍历数组：\n  * 使用==for表达式== 直接遍历数组中的元素\n  * 使用 ==索引== 遍历数组中的元素\n\n* 示例\n\n~~~scala\nscala> for(i <- a)println(i)\nhadoop\nhive\nflume\nspark\n\nscala> for(i <- 0 to a.length -1 )println(a(i))\nhadoop\nhive\nflume\nspark\n\nscala> for(i <- 0 until a.length) println(a(i))\nhadoop\nhive\nflume\nspark\n\n\n//0 until n ——生成一系列的数字，包含0，不包含n\n//0 to n    ——包含0，也包含n\n\n~~~\n\n\n\n#### 11.4 数组常用操作\n\n* scala中的数组封装了丰富的计算操作，将来在对数据处理的时候，不需要我们自己再重新实现。\n  * 求和——sum方法\n  * 求最大值——max方法 \n  * 求最小值——min方法 \n  * 排序——sorted方法\n* 示例\n\n~~~scala\nscala> val array=Array(1,3,4,2,5)\narray: Array[Int] = Array(1, 3, 4, 2, 5)\n\n//求和\nscala> array.sum\nres10: Int = 15\n\n//求最大值\nscala> array.max\nres11: Int = 5\n\n//求最小值\nscala> array.min\nres12: Int = 1\n\n//升序\nscala> array.sorted\nres13: Array[Int] = Array(1, 2, 3, 4, 5)\n\n//降序    reverse 反转\nscala> array.sorted.reverse\nres14: Array[Int] = Array(5, 4, 3, 2, 1)\n\n~~~\n\n\n\n### 12. 元组\n\n* 元组可以用来包含一组不同类型的值。例如：姓名，年龄，性别，出生年月。元组的元素是不可变 的。\n\n#### 12.1 定义元组\n\n* 语法\n\n  * 使用括号来定义元组\n\n    ~~~scala\n    val/var 元组变量名称 = (元素1, 元素2, 元素3....)\n    ~~~\n\n  * 使用箭头来定义元素（元组只有两个元素）\n\n    ~~~scala\n    val/var 元组 = 元素1->元素2\n    ~~~\n\n#### 12.2 示例\n\n~~~scala\n// 可以直接使用括号来定义一个元组 \nscala> val a = (1, \"张三\", 20, \"北京市\") \na: (Int, String, Int, String) = (1,张三,20,北京市)\n\n//使用箭头来定义元素\nscala> val b = 1->2 \nb: (Int, Int) = (1,2)\n\n~~~\n\n\n\n#### 12.3 访问元组\n\n* 使用\n\n  ```html\n   _1、_2、_3....\n  \n  ```\n\n  来访问元组中的元素，_1表示访问第一个元素，依次类推\n\n* 示例\n\n~~~scala\nscala> val a = (1, \"张三\", 20, \"北京市\")\na: (Int, String, Int, String) = (1,张三,20,北京市)\n\n//获取元组中的第一个元素\nscala> a._1\nres18: Int = 1\n\n//获取元组中的第二个元素\nscala> a._2\nres19: String = 张三\n\n//获取元组中的第三个元素\nscala> a._3\nres20: Int = 20\n\n//获取元组中的第四个元素\nscala> a._4\nres21: String = 北京市\n\n//不能修改元组中的值\nscala> a._4=\"上海\"\n<console>:12: error: reassignment to val\n       a._4=\"上海\"\n           ^\n\n\n~~~\n\n\n\n### 13. 映射Map\n\n* Map可以称之为映射。它是由键值对组成的集合。scala当中的Map集合与java当中的Map类似，也是key，value对形式的。\n* 在scala中，Map也分为不可变Map和可变 Map。\n\n\n\n#### 13.1 不可变Map\n\n* 定义语法\n\n~~~scala\nval/var map = Map(键->值, 键->值, 键->值...)    // 推荐，可读性更好 \nval/var map = Map((键, 值), (键, 值), (键, 值), (键, 值)...)\n~~~\n\n* 演示\n\n~~~scala\nscala> val map1 = Map(\"zhangsan\"->30, \"lisi\"->40) \nmap: scala.collection.immutable.Map[String,Int] = Map(zhangsan -> 30, lisi -> 40)\n\nscala> val map2 = Map((\"zhangsan\", 30), (\"lisi\", 30)) \nmap: scala.collection.immutable.Map[String,Int] = Map(zhangsan -> 30, lisi -> 30)\n// 根据key获取value \nscala> map1(\"zhangsan\") \nres10: Int = 30\n~~~\n\n\n\n#### 13.2 可变Map\n\n* 可变Map需要手动导入==import scala.collection.mutable.Map==, 定义语法与不可变Map一致。\n\n* 演示\n\n~~~scala\n//导包\nscala> import scala.collection.mutable.Map\nimport scala.collection.mutable.Map\n\n//定义可变的map\nscala> val map3 = Map(\"zhangsan\"->30, \"lisi\"->40)\nmap3: scala.collection.mutable.Map[String,Int] = Map(lisi -> 40, zhangsan -> 30)\n\n//获取zhangsan这个key对应的value\nscala> map3(\"zhangsan\")\nres26: Int = 30\n\n//给zhangsan这个key重新赋值value\nscala> map3(\"zhangsan\")=50\n\n//显示map3\nscala> map3\nres28: scala.collection.mutable.Map[String,Int] = Map(lisi -> 40, zhangsan -> 50)\n\n\n~~~\n\n\n\n#### 13.3 Map基本操作\n\n* 创建一个可变的map\n\n~~~scala\n//导包\nscala> import scala.collection.mutable.Map\nimport scala.collection.mutable.Map\n\nscala> val map = Map(\"zhangsan\"->30, \"lisi\"->40) \nmap: scala.collection.mutable.Map[String,Int] = Map(lisi -> 40, zhangsan -> 30)\n\n~~~\n\n* 按照key获取value\n\n~~~scala\n// 获取zhagnsan的年龄 \nscala> map(\"zhangsan\")\nres10: Int = 30\n\n// 获取wangwu的年龄，如果wangwu不存在，则返回-1 比较友好，避免遇到不存在的key而报错\nscala> map.getOrElse(\"wangwu\", -1) \nres11: Int = -1\n\n\n~~~\n\n* 修改key对应的value\n\n~~~scala\nscala> map(\"lisi\")=50\n\n~~~\n\n\n\n* 添加key-value键值对\n\n~~~scala\nscala> map+=(\"wangwu\" ->35)\nres12: map.type = Map(lisi -> 50, zhangsan -> 30, wangwu -> 35)\n\n\n\n~~~\n\n\n\n* 删除key-value键值对\n\n~~~scala\nscala> map -=\"wangwu\"\nres13: map.type = Map(lisi -> 50, zhangsan -> 30)\n~~~\n\n\n\n* 获取所有的key和所有的value\n\n~~~scala\n//获取所有的key\nscala> map.keys\nres36: Iterable[String] = Set(lisi, zhangsan)\n\n//获取所有的key\nscala> map.keySet\nres37: scala.collection.Set[String] = Set(lisi, zhangsan)\n\n//获取所有的value\nscala> map.values\nres38: Iterable[Int] = HashMap(50, 30)\n~~~\n\n\n\n* 遍历map\n\n~~~scala\n//第一种遍历\nscala> for(k <- map.keys) println(k+\" -> \" +map(k))\nlisi -> 50\nzhangsan -> 30\n\n\n//第二种遍历\nscala> for((k,v) <- map) println(k+\" -> \"+v)\nlisi -> 50\nzhangsan -> 30\n~~~\n\n\n\n### 14. Set集合\n\n* Set是代表没有重复元素的集合。\n* Set具备以下性质：\n  * 1、元素不重复 \n  * 2、不保证插入顺序\n* scala中的set集合也分为两种，一种是不可变集合，另一种是可变集合。\n\n\n\n#### 14.1 不可变Set集合\n\n* 语法\n\n~~~scala\n//创建一个空的不可变集\nval/var 变量名 = Set[类型]()\n\n//给定元素来创建一个不可变集\nval/var 变量名 = Set[类型](元素1, 元素2, 元素3...)\n\n~~~\n\n* 演示\n\n~~~scala\n// 创建set集合 \nscala> val a = Set(1,1,2,3,4,5) \na: scala.collection.immutable.Set[Int] = Set(5, 1, 2, 3, 4)\n\n// 获取集合的大小 \nscala> a.size \nres0: Int = 5\n\n// 遍历集合\nscala> for(i <- a) println(i)\n\n//添加元素生成新的集合\nscala> a + 6\nres1: scala.collection.immutable.Set[Int] = Set(5, 1, 6, 2, 3, 4)\n\n// 删除一个元素 \nscala> a - 1 \nres2: scala.collection.immutable.Set[Int] = Set(5, 2, 3, 4)\n\n// 删除set集合中存在的元素 \nscala> a -- Set(2,3) \nres3: scala.collection.immutable.Set[Int] = Set(5, 1, 4)\n\n// 拼接两个集合 \nscala> a ++ Set(6,7,8) \nres4: scala.collection.immutable.Set[Int] = Set(5, 1, 6, 2, 7, 3, 8, 4)\n\n//求2个Set集合的交集\nscala> a & Set(3,4,5,6)\nres5: scala.collection.immutable.Set[Int] = Set(5, 3, 4)\n\n\n\n//注意：这里对不可变的set集合进行添加删除等操作，对于该集合来说是没有发生任何变化，这里是生成了新的集合，新的集合相比于原来的集合来说发生了变化。\n~~~\n\n\n\n#### 14.2 可变Set集合\n\n* 要使用可变集，必须要手动导入： ==import scala.collection.mutable.Set==\n* 演示\n\n~~~scala\n//导包\nscala> import scala.collection.mutable.Set\nimport scala.collection.mutable.Set\n\n//定义可变的set集合\nscala> val set=Set(1,2,3,4,5)\nset: scala.collection.mutable.Set[Int] = Set(1, 5, 2, 3, 4)\n\n//添加单个元素\nscala> set +=6\nres10: set.type = Set(1, 5, 2, 6, 3, 4)\n\n//添加多个元素\nscala> set +=(6,7,8,9)\nres11: set.type = Set(9, 1, 5, 2, 6, 3, 7, 4, 8)\n\n//添加一个set集合中的元素\nscala> set ++=Set(10,11)\nres12: set.type = Set(9, 1, 5, 2, 6, 3, 10, 7, 4, 11, 8)\n\n//删除一个元素\nscala> set -=11\nres13: set.type = Set(9, 1, 5, 2, 6, 3, 10, 7, 4, 8)\n\n//删除多个元素\nscala> set -=(9,10)\nres15: set.type = Set(1, 5, 2, 6, 3, 7, 4, 8)\n\n//删除一个set子集\nscala> set --=Set(7,8)\nres19: set.type = Set(1,5, 2, 6, 3, 4)\n\nscala> set.remove(1)\nres17: Boolean = true\n\nscala> set\nres18: scala.collection.mutable.Set[Int] = Set(5, 2, 6, 3, 4)\n\n~~~\n\n\n\n### 15. 列表 List\n\n* List是scala中最重要的、也是最常用的数据结构。\n* List具备以下性质：\n  * 1、可以保存重复的值 \n  * 2、有先后顺序\n\n* 在scala中，也有两种列表，一种是不可变列表、另一种是可变列表\n\n\n\n#### 15.1 不可变列表\n\n* 不可变列表就是列表的元素、长度都是不可变的\n* 语法\n  * 使用 List(元素1, 元素2, 元素3, ...) 来创建一个不可变列表，语法格式\n\n~~~scala\nval/var 变量名 = List(元素1, 元素2, 元素3...)\n\n//使用 Nil 创建一个不可变的空列表\nval/var 变量名 = Nil\n\n//使用 :: 方法创建一个不可变列表\nval/var 变量名 = 元素1 :: 元素2 :: Nil\n\n~~~\n\n* 演示\n\n~~~scala\n//创建一个不可变列表，存放以下几个元素（1,2,3,4）\nscala> val  list1=List(1,2,3,4)\nlist1: List[Int] = List(1, 2, 3, 4)\n\n//使用Nil创建一个不可变的空列表\nscala> val  list2=Nil\nlist2: scala.collection.immutable.Nil.type = List()\n\n//使用 :: 方法创建列表，包含1、2、3三个元素\nscala> val list3=1::2::3::Nil\nlist3: List[Int] = List(1, 2, 3)\n~~~\n\n\n\n#### 15.2 可变列表\n\n* 可变列表就是列表的元素、长度都是可变的。\n\n* 要使用可变列表，先要导入 ==import scala.collection.mutable.ListBuffer==\n\n* 语法\n\n  * 使用ListBuffer[元素类型]() 创建空的可变列表，语法结构\n\n  ~~~scala\n  val/var 变量名 = ListBuffer[Int]()\n  ~~~\n\n  * 使用ListBuffer(元素1, 元素2, 元素3...)创建可变列表，语法结构\n\n  ~~~scala\n  val/var 变量名 = ListBuffer(元素1，元素2，元素3...)\n  ~~~\n\n* 演示\n\n  ~~~scala\n  //导包\n  scala> import scala.collection.mutable.ListBuffer\n  import scala.collection.mutable.ListBuffer\n  \n  //定义一个空的可变列表\n  scala> val a=ListBuffer[Int]()\n  a: scala.collection.mutable.ListBuffer[Int] = ListBuffer()\n  \n  //定义一个有初始元素的可变列表\n  scala> val b=ListBuffer(1,2,3,4)\n  b: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2, 3, 4)\n  ~~~\n\n\n\n#### 15.3 列表操作\n\n~~~scala\n//导包\nscala> import scala.collection.mutable.ListBuffer\nimport scala.collection.mutable.ListBuffer\n\n//定义一个可变的列表\nscala> val list=ListBuffer(1,2,3,4)\nlist: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2, 3, 4)\n\n//获取第一个元素\nscala> list(0)\nres4: Int = 1\n//获取第一个元素\nscala> list.head\nres5: Int = 1\n\n//获取除了第一个元素外其他元素组成的列表\nscala> list.tail\nres6: scala.collection.mutable.ListBuffer[Int] = ListBuffer(2, 3, 4)\n\n//添加单个元素\nscala> list +=5\nres7: list.type = ListBuffer(1, 2, 3, 4, 5)\n\n//添加一个不可变的列表\nscala> list ++=List(6,7)\nres8: list.type = ListBuffer(1, 2, 3, 4, 5, 6, 7)\n\n//添加一个可变的列表\nscala> list ++=ListBuffer(8,9)\nres9: list.type = ListBuffer(1, 2, 3, 4, 5, 6, 7, 8, 9)\n\n//删除单个元素\nscala> list -=9\nres10: list.type = ListBuffer(1, 2, 3, 4, 5, 6, 7, 8)\n\n//删除一个不可变的列表存在的元素\nscala> list --=List(7,8)\nres11: list.type = ListBuffer(1, 2, 3, 4, 5, 6)\n\n//删除一个可变的列表存在的元素\nscala> list --=ListBuffer(5,6)\nres12: list.type = ListBuffer(1, 2, 3, 4)\n\n//可变的列表转为不可变列表\nscala> list.toList\nres13: List[Int] = List(1, 2, 3, 4)\n\n//可变的列表转为不可变数组\nscala> list.toArray\nres14: Array[Int] = Array(1, 2, 3, 4)\n~~~\n\n\n\n### 16. 函数式编程\n\n* 我们将来使用Spark/Flink的大量业务代码都会使用到函数式编程。\n* 下面的这些操作是学习的重点，先来感受下如何进行函数式编程以及它的强大\n\n\n\n#### 16.1 遍历 - foreach\n\n* 方法描述\n\n  ~~~scala\n  foreach(f: (A) ⇒ Unit): Unit\n  ~~~\n\n* 方法说明\n\n  | foreach | API           | 说明                                                         |\n  | ------- | ------------- | ------------------------------------------------------------ |\n  | 参数    | f: (A) ⇒ Unit | 接收一个函数对象<br />函数的输入参数为集合的元素<br />返回值为空 |\n  | 返回值  | Unit          | 空                                                           |\n\n* 方法实操\n\n~~~scala\nscala> val list=List(1,2,3,4)\nlist: List[Int] = List(1, 2, 3, 4)\n\n//定义一个匿名函数传入到foreach方法中\nscala> list.foreach((x:Int)=>println(x))\n1\n2\n3\n4\n\n//匿名函数的输入参数类型可以省略，由编译器自动推断\nscala> list.foreach(x=>println(x))\n1\n2\n3\n4\n\n//当函数参数，只在函数体中出现一次，而且函数体没有嵌套调用时，可以使用下划线来简化函数定 义\nscala> list.foreach(println(_))\n1\n2\n3\n4\n\n//最简写，直接给定println\nscala> list.foreach(println)\n1\n2\n3\n4\n\n//很神奇的语法，别害怕，盘它就可以了，后期通过scala语言开发spark、Flink程序非常简洁方便\n~~~\n\n\n\n#### 16.2 映射 - map\n\n* 集合的映射操作是将来在编写Spark/Flink用得最多的操作，是我们必须要掌握的掌握。\n\n* 方法描述\n\n~~~scala\ndef map[B](f: (A) ⇒ B): TraversableOnce[B]\n~~~\n\n* 方法说明\n\n| map方法 | API                | 说明                                                         |\n| ------- | ------------------ | ------------------------------------------------------------ |\n| 泛型    | [B]                | 指定map方法最终返回的集合泛型                                |\n| 参数    | f: (A) ⇒ B         | 传入一个函数对象<br />该函数接收一个类型A（要转换的列表元素）<br />返回值为类型B |\n| 返回值  | TraversableOnce[B] | B类型的集合                                                  |\n\n* 方法实操\n\n~~~scala\n//定义一个list集合，实现把内部每一个元素做乘以10，生成一个新的list集合\nscala> val list=List(1,2,3,4)\nlist: List[Int] = List(1, 2, 3, 4)\n\n//定义一个匿名函数\nscala> list.map((x:Int)=>x*10)\nres21: List[Int] = List(10, 20, 30, 40)\n\n//省略匿名函数参数类型\nscala> list.map(x=>x*10)\nres22: List[Int] = List(10, 20, 30, 40)\n\n//最简写   用下划线\nscala> list.map(_*10)\nres23: List[Int] = List(10, 20, 30, 40)\n~~~\n\n\n\n#### 16.3 扁平化映射 - flatmap\n\n* 映射扁平化也是将来用得非常多的操作，也是必须要掌握的。\n* 方法描述\n\n~~~scala\ndef flatMap[B](f: (A) ⇒ GenTraversableOnce[B]): TraversableOnce[B]\n~~~\n\n* 方法说明\n\n| flatmap方法 | API                            | 说明                                                         |\n| ----------- | ------------------------------ | ------------------------------------------------------------ |\n| 泛型        | [B]                            | 最终要转换的集合元素类型                                     |\n| 参数        | f: (A) ⇒ GenTraversableOnce[B] | 传入一个函数对象<br />函数的参数是集合的元素<br />函数的返回值是一个集合 |\n| 返回值      | TraversableOnce[B]             | B类型的集合                                                  |\n\n* 方法实操\n\n~~~scala\n//定义一个List集合,每一个元素中就是一行数据，有很多个单词\nscala>  val list = List(\"hadoop hive spark flink\", \"hbase spark\")\nlist: List[String] = List(hadoop hive spark flink, hbase spark)\n\n//使用flatMap进行偏平化处理，获取得到所有的单词\nscala> list.flatMap(x => x.split(\" \"))\nres24: List[String] = List(hadoop, hive, spark, flink, hbase, spark)\n\n//简写\nscala> list.flatMap(_.split(\" \"))\nres25: List[String] = List(hadoop, hive, spark, flink, hbase, spark)\n\n// flatMap该方法其本质是先进行了map 然后又调用了flatten\nscala> list.map(_.split(\" \")).flatten\nres26: List[String] = List(hadoop, hive, spark, flink, hbase, spark)\n~~~\n\n\n\n#### 16.4 过滤 - filter\n\n* 过滤符合一定条件的元素\n\n- 方法描述\n\n```scala\ndef filter(p: (A) ⇒ Boolean): TraversableOnce[A]\n```\n\n- 方法说明\n\n| filter方法 | API                | 说明                                                         |\n| ---------- | ------------------ | ------------------------------------------------------------ |\n| 参数       | p: (A) ⇒ Boolean   | 传入一个函数对象<br />接收一个集合类型的参数<br />返回布尔类型，满足条件返回true, 不满足返回false |\n| 返回值     | TraversableOnce[A] | 列表                                                         |\n\n* 方法实操\n\n~~~scala\n//定义一个list集合\nscala> val list=List(1,2,3,4,5,6,7,8,9,10)\nlist: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n//过滤出集合中大于5的元素\nscala> list.filter(x => x >5)\nres27: List[Int] = List(6, 7, 8, 9, 10)\n\n//把集合中大于5的元素取出来乘以10生成一个新的list集合\nscala> list.filter(_ > 5).map(_ * 10)\nres29: List[Int] = List(60, 70, 80, 90, 100)\n\n\n//通过这个案例，应该是可以感受到scala比java的强大了...\n~~~\n\n\n\n#### 16.5 排序 - sort\n\n* 在scala集合中，可以使用以下几种方式来进行排序\n  * sorted默认排序 \n  * sortBy指定字段排序 \n  * sortWith自定义排序\n* ==sorted默认排序== \n\n~~~~scala\n//定义一个List集合\nscala> val list=List(5,1,2,4,3)\nlist: List[Int] = List(5, 1, 2, 4, 3)\n\n//默认就是升序\nscala> list.sorted\nres30: List[Int] = List(1, 2, 3, 4, 5)\n~~~~\n\n* ==sortBy指定字段排序==\n\n  * 根据传入的函数转换后，再进行排序\n  * 方法描述\n\n  ~~~scala\n  def sortBy[B](f: (A) ⇒ B): List[A]\n  ~~~\n\n  * 方法说明\n\n  | sortBy方法 | API        | 说明                                                         |\n  | ---------- | ---------- | ------------------------------------------------------------ |\n  | 泛型       | [B]        | 按照什么类型来进行排序                                       |\n  | 参数       | f: (A) ⇒ B | 传入函数对象<br />接收一个集合类型的元素参数<br />返回B类型的元素进行排序 |\n  | 返回值     | List[A]    | 返回排序后的列表                                             |\n\n  * 方法实操\n\n  ~~~scala\n  //定义一个List集合\n  scala> val list=List(\"1 hadoop\",\"2 spark\",\"3 flink\")\n  list: List[String] = List(1 hadoop, 2 spark, 3 flink)\n  \n  //按照单词的首字母进行排序\n  scala> list.sortBy(x=>x.split(\" \")(1))\n  res33: List[String] = List(3 flink, 1 hadoop, 2 spark)\n  \n  ~~~\n\n* ==sortWith自定义排序==\n\n  * 自定义排序，根据一个函数来进行自定义排序\n  * 方法描述\n\n  ~~~scala\n  def sortWith(lt: (A, A) ⇒ Boolean): List[A]\n  ~~~\n\n  * 方法说明\n\n  | sortWith方法 | API                  | 说明                                                         |\n  | ------------ | -------------------- | ------------------------------------------------------------ |\n  | 参数         | lt: (A, A) ⇒ Boolean | 传入一个比较大小的函数对象<br />接收两个集合类型的元素参数<br />返回两个元素大小，小于返回true，大于返回false |\n  | 返回值       | List[A]              | 返回排序后的列表                                             |\n\n  * 方法实操\n\n  ~~~scala\n  scala> val list = List(2,3,1,6,4,5)\n  a: List[Int] = List(2, 3, 1, 6, 4, 5)\n  \n  //降序\n  scala> list.sortWith((x,y)=>x>y)\n  res35: List[Int] = List(6, 5, 4, 3, 2, 1)\n  \n  //升序\n  scala> list.sortWith((x,y)=>x<y)\n  res36: List[Int] = List(1, 2, 3, 4, 5, 6)\n  ~~~\n\n\n\n#### 16.6 分组 - groupBy\n\n* 我们如果要将数据按照分组来进行统计分析，就需要使用到分组方法\n* groupBy表示按照函数将列表分成不同的组\n* 方法描述\n\n```scala\ndef groupBy[K](f: (A) ⇒ K): Map[K, List[A]]\n```\n\n- 方法说明\n\n| groupBy方法 | API             | 说明                                                         |\n| ----------- | --------------- | ------------------------------------------------------------ |\n| 泛型        | [K]             | 分组字段的类型                                               |\n| 参数        | f: (A) ⇒ K      | 传入一个函数对象<br />接收集合元素类型的参数<br />返回一个K类型的key，这个key会用来进行分组，相同的key放在一组中 |\n| 返回值      | Map[K, List[A]] | 返回一个映射，K为分组字段，List为这个分组字段对应的一组数据  |\n\n* 方法实操\n\n~~~scala\nscala> val a = List(\"张三\"->\"男\", \"李四\"->\"女\", \"王五\"->\"男\")\na: List[(String, String)] = List((张三,男), (李四,女), (王五,男))\n\n// 按照性别分组\nscala> a.groupBy(_._2)\nres0: scala.collection.immutable.Map[String,List[(String, String)]] = Map(男 -> List((张三,男), (王五,男)),\n女 -> List((李四,女)))\n\n// 将分组后的映射转换为性别/人数元组列表\nscala> res0.map(x => x._1 -> x._2.size)\nres3: scala.collection.immutable.Map[String,Int] = Map(男 -> 2, 女 -> 1)\n~~~\n\n\n\n#### 16.7 聚合 - reduce\n\n* reduce表示将列表，传入一个函数进行聚合计算\n* 方法描述\n\n~~~scala\ndef reduce[A1 >: A](op: (A1, A1) ⇒ A1): A1\n~~~\n\n* 方法说明\n\n| reduce方法 | API               | 说明                                                         |\n| ---------- | ----------------- | ------------------------------------------------------------ |\n| 泛型       | [A1 >: A]         | （下界）A1必须是集合元素类型的子类                           |\n| 参数       | op: (A1, A1) ⇒ A1 | 传入函数对象，用来不断进行聚合操作<br />第一个A1类型参数为：当前聚合后的变量<br />第二个A1类型参数为：当前要进行聚合的元素 |\n| 返回值     | A1                | 列表最终聚合为一个元素                                       |\n\n* 方法实操\n\n~~~scala\nscala> val a = List(1,2,3,4,5,6,7,8,9,10)\na: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\nscala> a.reduce((x,y) => x + y)\nres5: Int = 55\n\n// 第一个下划线表示第一个参数，就是历史的聚合数据结果\n// 第二个下划线表示第二个参数，就是当前要聚合的数据元素\nscala> a.reduce(_ + _)\nres53: Int = 55\n\n// 与reduce一样，从左往右计算\nscala> a.reduceLeft(_ + _)\nres0: Int = 55\n\n// 从右往左聚合计算\nscala> a.reduceRight(_ + _)\nres1: Int = 55\n~~~\n\n\n\n#### 16.8 折叠 - fold\n\n* fold与reduce很像，但是多了一个指定初始值参数\n* 方法描述\n\n```scala\ndef fold[A1 >: A](z: A1)(op: (A1, A1) ⇒ A1): A1\n```\n\n- 方法说明\n\n| reduce方法 | API               | 说明                                                         |\n| ---------- | ----------------- | ------------------------------------------------------------ |\n| 泛型       | [A1 >: A]         | （下界）A1必须是集合元素类型的子类                           |\n| 参数1      | z: A1             | 初始值                                                       |\n| 参数2      | op: (A1, A1) ⇒ A1 | 传入函数对象，用来不断进行折叠操作<br />第一个A1类型参数为：当前折叠后的变量<br />第二个A1类型参数为：当前要进行折叠的元素 |\n| 返回值     | A1                | 列表最终折叠为一个元素                                       |\n\n* 方法实操\n\n~~~scala\n//定义一个List集合\nscala> val a = List(1,2,3,4,5,6,7,8,9,10)\na: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n//求和\nscala> a.sum\nres41: Int = 55\n\n//给定一个初始值，，折叠求和\nscala> a.fold(0)(_+_)\nres42: Int = 55\n\nscala> a.fold(10)(_+_)\nres43: Int = 65\n\n//从左往右\nscala> a.foldLeft(10)(_+_)\nres44: Int = 65\n\n//从右往左\nscala> a.foldRight(10)(_+_)\nres45: Int = 65\n\n\n//fold和foldLet效果一致，表示从左往右计算\n//foldRight表示从右往左计算\n\n~~~\n\n\n\n### 17. 高阶函数\n\n* 使用函数值作为参数，或者返回值为函数值的“函数”和“方法”，均称之为“高阶函数”。\n\n\n\n#### 17.1 函数值作为参数\n\n~~~scala\n//定义一个数组\nscala> val array=Array(1,2,3,4,5)\narray: Array[Int] = Array(1, 2, 3, 4, 5)\n\n//定义一个函数\nscala> val func=(x:Int)=>x*10\nfunc: Int => Int = <function1>\n\n//函数作为参数传递到方法中\nscala> array.map(func)\nres0: Array[Int] = Array(10, 20, 30, 40, 50)\n~~~\n\n#### 17.2 匿名函数\n\n~~~scala\n//定义一个数组\nscala> val array=Array(1,2,3,4,5)\narray: Array[Int] = Array(1, 2, 3, 4, 5)\n\n//定义一个没有名称的函数----匿名函数\nscala> array.map(x=>x*10)\nres1: Array[Int] = Array(10, 20, 30, 40, 50)\n\n~~~\n\n#### 17.3 柯里化\n\n* 方法可以定义多个参数列表，当使用较少的参数列表调用多参数列表的方法时，会产生一个新的函数，该函数接收剩余的参数列表作为其参数。这被称为柯里化。\n\n~~~scala\ndef getAddress(a:String):(String,String)=>String={\n    (b:String,c:String)=>a+\"-\"+b+\"-\"+c\n}\n\nscala> val f1=getAddress(\"china\")\nf1: (String, String) => String = <function2>\n\nscala> f1(\"beijing\",\"tiananmen\")\nres5: String = china-beijing-tiananmen\n\n\n\n//这里就可以这样去定义方法\ndef getAddress(a:String)(b:String,c:String):String={ \n  \t\ta+\"-\"+b+\"-\"+c \n}\n//调用\nscala> getAddress(\"china\")(\"beijing\",\"tiananmen\")\nres0: String = china-beijing-tiananmen\n\n//之前学习使用的下面这些操作就是使用到了柯里化\nList(1,2,3,4).fold(0)(_+_)\nList(1,2,3,4).foldLeft(0)(_+_)\nList(1,2,3,4).foldRight(0)(_+_)\n\n~~~\n\n\n\n#### 17.4 闭包\n\n* 函数里面引用外面类成员变量叫作闭包\n\n~~~scala\nvar factor=10\n\nval f1=(x:Int) => x*factor\n\n\n//定义的函数f1，它的返回值是依赖于不在函数作用域的一个变量\n//后期必须要要获取到这个变量才能执行\n//spark和flink程序的开发中大量的使用到函数，函数的返回值依赖的变量可能都需要进行大量的网络传输获取得到。这里就需要这些变量实现序列化进行网络传输。\n\n~~~\n\n\n\n### 18. scala面向对象编程之类\n\n#### 18.1 类的定义\n\n* scala是支持面向对象的，也有类和对象的概念。\n  * 定义一个Customer类，并添加成员变量/成员方法\n  * 添加一个main方法，并创建Customer类的对象，并给对象赋值，打印对象中的成员，调用成员方法\n\n~~~scala\nclass Customer {\n  var name:String = _\n  var sex:String = _\n  val registerDate:Date = new Date\n\n  def sayHi(msg:String) = {\n    println(msg)\n  }\n}\n\nobject Main {\n  def main(args: Array[String]): Unit = {\n    val customer = new Customer\n    //给对象的成员变量赋值\n    customer.name = \"张三\"\n    customer.sex = \"男\"\n\n    println(s\"姓名: ${customer.name}, 性别：${customer.sex}, 注册时间: ${customer.registerDate}\")\n    //对象调用方法  \n    customer.sayHi(\"你好!\")\n  }\n}\n\n~~~\n\n* 说明\n\n~~~html\n(1). var name:String = _，  _表示使用默认值进行初始化\n   例如：String类型默认值是null，Int类型默认值是0，Boolean类型默认值是false...\n(2). val变量不能使用_来进行初始化，因为val是不可变的，所以必须手动指定一个默认值\n(3). main方法必须要放在一个scala的object（单例对象）中才能执行\n\n~~~\n\n\n\n#### 18.2 类的构造器\n\n* 主构造器\n\n  * 主构造器是指在类名的后面跟上一系列参数，例如\n\n  ~~~scala\n  class 类名(var/val 参数名:类型 = 默认值, var/val 参数名:类型 = 默认值){\n      // 构造代码块\n  }\n  \n  ~~~\n\n* 辅助构造器\n\n  * 在类中使用this来定义，例如\n\n  ~~~scala\n  def this(参数名:类型, 参数名:类型) {\n      ...\n  }\n  ~~~\n\n* 演示\n\n  ~~~scala\n  class Student(val name:String, val age:Int) {\n      \n     val address:String=\"beijing\" \n    // 定义一个参数的辅助构造器\n    def this(name:String) {\n      // 第一行必须调用主构造器、其他辅助构造器或者super父类的构造器\n      this(name, 20)\n    }\n  \n    def this(age:Int) {\n      this(\"某某某\", age)\n    }\n  }\n  \n  ~~~\n\n\n\n### 19.scala面向对象编程之对象\n\n#### 19.1 scala中的object\n\n* scala中是没有Java中的静态成员的。如果将来我们需要用到static变量、static方法，就要用到scala中的单例对象object\n\n* 定义object\n  * 定义单例对象和定义类很像，就是把class换成object\n* 演示\n  * 定义一个工具类，用来格式化日期时间\n\n~~~scala\nobject DateUtils {\n\n  // 在object中定义的成员变量，相当于Java中定义一个静态变量\n  // 定义一个SimpleDateFormat日期时间格式化对象\n  val simpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm\")\n\n  // 构造代码\n  println(\"构造代码\")\n    \n  // 相当于Java中定义一个静态方法\n  def format(date:Date) = simpleDateFormat.format(date)\n\n  // main是一个静态方法，所以必须要写在object中\n  def main(args: Array[String]): Unit = {\n      \n    println { DateUtils.format(new Date()) };\n  }\n}\n\n~~~\n\n* 说明\n\n~~~html\n(1). 使用object 单例对象名定义一个单例对象，可以用object作为工具类或者存放常量\n(2). 在单例对象中定义的变量，类似于Java中的static成员变量\n(3). 在单例对象中定义的方法，类似于Java中的static方法\n(4). object单例对象的构造代码可以直接写在花括号中\n(5). 调用单例对象的方法，直接使用单例对象名.方法名，访问单例对象的成员变量也是使用单例对象名.变量名\n(6). 单例对象只能有一个无参的主构造器，不能添加其他参数\n~~~\n\n#### 19.2 scala中的伴生对象\n\n* 在==同一个scala文件，有一个class和object具有同样的名字===，那么就称这个object是class的伴生对象，class是object的伴生类；\n* 伴生类和伴生对象的最大特点是，可以相互访问；\n\n* 演示\n\n~~~scala\nclass ClassObject {\n  val id = 1\n  private var name = \"itcast\"\n  def printName(): Unit ={\n    //在Dog类中可以访问伴生对象Dog的私有属性\n    println(ClassObject.CONSTANT + name )\n  }\n\n\n}\n\nobject ClassObject{\n  //伴生对象中的私有属性\n  private val CONSTANT = \"汪汪汪 : \"\n  def main(args: Array[String]) {\n    val p = new ClassObject\n    //访问私有的字段name\n    p.name = \"123\"\n    p.printName()\n  }\n}\n~~~\n\n* 说明\n\n~~~~html\n(1). 伴生类和伴生对象的名字必须是一样的\n(2). 伴生类和伴生对象需要在一个scala源文件中\n(3). 伴生类和伴生对象可以互相访问private的属性\n\n~~~~\n\n\n\n#### 19.3 scala中object的apply方法\n\n\n\n* 我们之前使用过这种方式来创建一个Array对象。\n\n~~~scala\n// 创建一个Array对象\nval a = Array(1,2,3,4)\n\n~~~\n\n* 这种写法非常简便，不需要再写一个new，然后敲一个空格，再写类名。如何直接使用类名来创建对象呢？\n* 查看scala源代码：\n\n![1568196769539](https://kfly.top/picture/kfly-top/scala学习入门/assets/1568196769539.png)\n\n\n\n* 答案就是：**==实现伴生对象的apply方法==**\n* 伴生对象的apply方法用来快速地创建一个伴生类的对象。\n\n* 演示\n\n~~~scala\nclass Person(var name:String, var age:Int) {\n\n  override def toString = s\"Person($name, $age)\"\n}\n\nobject Person {\n  // 实现apply方法\n  // 返回的是伴生类的对象\n  def apply(name:String, age:Int): Person = new Person(name, age)\n\n  // apply方法支持重载\n  def apply(name:String):Person = new Person(name, 20)\n\n  def apply(age:Int):Person = new Person(\"某某某\", age)\n\n  def apply():Person = new Person(\"某某某\", 20)\n}\n\nobject Main2 {\n  def main(args: Array[String]): Unit = {\n    val p1 = Person(\"张三\", 20)\n    val p2 = Person(\"李四\")\n    val p3 = Person(100)\n    val p4 = Person()\n\n    println(p1)\n    println(p2)\n    println(p3)\n    println(p4)\n  }\n}\n~~~\n\n* 说明\n\n~~~html\n（1）. 当遇到类名(参数1, 参数2...)会自动调用apply方法，在apply方法中来创建对象\n（2）. 定义apply时，如果参数列表是空，也不能省略括号()，否则引用的是伴生对象\n\n~~~\n\n\n\n#### 19.4 scala中object的main方法\n\n* scala和Java一样，如果要运行一个程序，必须有一个main方法。\n\n* 而在Java中main方法是静态的，而在scala中没有静态方法。\n\n* ==在scala中，这个main方法必须放在一个object中==\n\n  * 演示1\n\n    ~~~scala\n    object Main1{\n      def main(args:Array[String]) = {\n        println(\"hello, scala\")\n      }\n    }\n    \n    ~~~\n\n* ==也可以继承自App Trait（特质==），然后将需要编写在main方法中的代码，写在object的构造方法体内。其本质是调用了Trait这个特质中的main方法。\n\n  * 演示2\n\n    ~~~scala\n    object Main2 extends App {\n      println(\"hello, scala\")\n    }\n    \n    ~~~\n\n\n\n### 20. scala面向对象编程之继承\n\n#### 20.1 继承extends\n\n* scala和Java一样，使用**extends**关键字来实现继承。可以在子类中定义父类中没有的字段和方法，或者重写父类的方法。\n* ==示例1：实现简单继承==\n\n~~~scala\nclass Person1 {\n  var name = \"super\"\n\n  def getName = this.name\n}\n\nclass Student1 extends Person1\n\nobject Main1 {\n  def main(args: Array[String]): Unit = {\n    val p1 = new Person1()\n    val p2 = new Student1()\n\n    p2.name = \"张三\"\n\n    println(p2.getName)\n  }\n}\n\n~~~\n\n\n\n* ==示例2：单例对象实现继承==\n\n~~~scala\nclass Person2 {\n  var name = \"super\"\n\n  def getName = this.name\n}\n\nobject Student2 extends Person2\n\nobject Main2 {\n  def main(args: Array[String]): Unit = {\n    println(Student2.getName)\n  }\n}\n\n~~~\n\n\n\n#### 20.2 override和super\n\n* 如果子类要覆盖父类中的一个非抽象方法，必须要使用override关键字\n* 可以使用override关键字来重写一个val字段\n* 可以使用super关键字来访问父类的成员\n* ==示例1：class继承class==\n\n~~~scala\nclass Person3 {\n  val name = \"super\"\n\n  def getName = name\n}\n\nclass Student3 extends Person3 {\n  // 重写val字段\n  override val name: String = \"child\"\n\n  // 重写getName方法\n  override def getName: String = \"hello, \" + super.getName\n}\n\nobject Main3 {\n  def main(args: Array[String]): Unit = {\n    println(new Student3().getName)\n  }\n}\n~~~\n\n\n\n#### 20.3 isInstanceOf和asInstanceOf\n\n* 我们经常要在代码中进行类型的判断和类型的转换。在Java中，我们可以使用instanceof关键字、以及(类型)object来实现，在scala中如何实现呢？\n* scala中对象提供==isInstanceOf ==和 ==asInstanceOf==方法。\n  * isInstanceOf判断对象是否为指定类的对象\n  * asInstanceOf将对象转换为指定类型\n\n\n\n\n|                        | Java             | Scala               |\n| ---------------------- | ---------------- | ------------------- |\n| 判断对象是否是C类型    | obj instanceof C | obj.isInstanceof[C] |\n| 将对象强转成C类型      | (C ) obj         | obj.asInstanceof[C] |\n| 获取类型为T的class对象 | C.class          | classOf[C]          |\n\n* ==示例==\n\n~~~scala\nclass Person4\nclass Student4 extends Person4\n\nobject Main4 {\n  def main(args: Array[String]): Unit = {\n    val s1:Person4 = new Student4\n\n    // 判断s1是否为Student4类型\n    if(s1.isInstanceOf[Student4]) {\n      // 将s1转换为Student3类型\n      val s2 =  s1.asInstanceOf[Student4]\n      println(s2)\n    }\n\n  }\n}\n~~~\n\n\n\n#### 20.4 getClass和classOf\n\n* isInstanceOf 只能判断出对象是否为指定类以及其子类的对象，而不能精确的判断出，对象就是指定类的对象。如果要求精确地判断出对象就是指定类的对象，那么就只能使用 getClass 和 classOf 。\n\n  * 对象.getClass可以精确获取对象的类型\n\n  * classOf[x]可以精确获取类型\n  * 使用==操作符就可以直接比较\n\n* ==示例==\n\n~~~scala\nclass Person5\nclass Student5 extends Person5\n\nobject Student5{\n  def main(args: Array[String]) {\n    val p:Person5=new Student5\n    //判断p是否为Person5类的实例\n    println(p.isInstanceOf[Person5])//true\n\n    //判断p的类型是否为Person5类\n    println(p.getClass == classOf[Person5])//false\n\n    //判断p的类型是否为Student5类\n    println(p.getClass == classOf[Student5])//true\n  }\n}\n~~~\n\n\n\n#### 20.5 访问修饰符\n\n* Java中的访问控制，同样适用于scala，可以在成员前面添加private/protected关键字来控制成员的可见性。但在scala中，==**没有public关键字**，任何没有被标为private或protected的成员都是公共的==。\n\n  * ==**private[this]修饰符**==\n\n    * 被修饰的成员只能在当前类中被访问。或者可以理解为：`只能通过this.来访问`（在当前类中访问成员会自动添加this.）。\n\n    * ==示例==\n\n      ~~~scala\n      class Person6 {\n        // 只有在当前对象中能够访问\n        private[this] var name = \"super\"\n      \n        def getName = this.name\t// 正确！\n      \n        def sayHelloTo(p:Person6) = {\n          println(\"hello\" + p.name)     // 报错!无法访问\n        }\n      }\n      \n      object Person6 {\n        def showName(p:Person6) = println(p.name)  // 报错!无法访问\n      }\n      \n      ~~~\n\n  * ==**protected[this]修饰符**==\n\n    * ==被修饰的成员只能在当前类和当前子类中被访问==。也可以理解为：当前类通过**this.**访问或者子类通过**this.**访问\n\n    * 示例\n\n      ~~~scala\n      class Person7 {\n        // 只有在当前类以及继承该类的当前对象中能够访问\n        protected[this] var name = \"super\"\n        \n        def getName = {\n          // 正确！\n          this.name\n        }\n      \n        def sayHelloTo1(p:Person7) = {\n          // 编译错误！无法访问\n          println(p.name)\n        }\n      }\n      \n      object Person7 {\n        def sayHelloTo3(p:Person7) = {\n          // 编译错误！无法访问\n          println(p.name)\n        }\n      }\n      \n      class Student7 extends Person7 {\n        def showName = {\n          // 正确！\n          println(name)\n        }\n      \n        def sayHelloTo2(p:Person7) = {\n          // 编译错误！无法访问\n          println(p.name)\n        }\n      }\n      \n      ~~~\n\n\n\n#### 20.6 调用父类的constructor\n\n* ==实例化子类对象，必须要调用父类的构造器==，在scala中，只能在子类的`主构造器`中调用父类的构造器\n* 示例\n\n~~~scala\nclass Person8(var name:String){\n    println(\"name:\"+name)\n}\n\n// 直接在父类的类名后面调用父类构造器\nclass Student8(name:String, var clazz:String) extends Person8(name)\n\nobject Main8 {\n  def main(args: Array[String]): Unit = {\n    val s1 = new Student8(\"张三\", \"三年二班\")\n    println(s\"${s1.name} - ${s1.clazz}\")\n  }\n}\n\n~~~\n\n\n\n#### 20.7 抽象类\n\n* 如果类的某个成员在当前类中的定义是不包含完整的，它就是一个**抽象类**\n* 不完整定义有两种情况：\n  * 1.方法没有方法体\n  * 2.变量没有初始化\n* 没有方法体的方法称为**抽象方法**，没有初始化的变量称为**抽象字段**。定义抽象类和Java一样，在类前面加上**abstract**关键字就可以了\n\n* ==示例==\n\n~~~scala\nabstract class Person9(val name:String) {\n  //抽象方法\n  def sayHello:String\n  def sayBye:String\n  //抽象字段  \n  val address:String  \n}\nclass Student9(name:String) extends Person9(name){\n  //重写抽象方法\n  def sayHello: String = \"Hello,\"+name\n  def sayBye: String =\"Bye,\"+name\n  //重写抽象字段\n  override val address:String =\"beijing \"\n}\nobject Main9{\n  def main(args: Array[String]) {\n    val s = new Student9(\"tom\")\n    println(s.sayHello)\n    println(s.sayBye)\n    println(s.address)\n  }\n}\n\n~~~\n\n#### 20.8 匿名内部类\n\n* 匿名内部类是没有名称的子类，直接用来创建实例对象。Spark的源代码中有大量使用到匿名内部类。\n\n* ==示例==\n\n~~~scala\nabstract class Person10 {\n  //抽象方法  \n  def sayHello:Unit\n}\n\nobject Main10 {\n  def main(args: Array[String]): Unit = {\n    // 直接用new来创建一个匿名内部类对象\n    val p1 = new Person10 {\n      override def sayHello: Unit = println(\"我是一个匿名内部类\")\n    }\n    p1.sayHello\n  }\n}\n\n~~~\n\n\n\n\n\n### 21. scala面向对象编程之trait特质\n\n* 特质是scala中代码复用的基础单元\n* 它可以将方法和字段定义封装起来，然后添加到类中\n* 与类继承不一样的是，类继承要求每个类都只能继承`一个`超类，而一个类可以添加`任意数量`的特质。\n* 特质的定义和抽象类的定义很像，但它是使用`trait`关键字\n\n\n\n#### 21.1 作为接口使用\n\n* 使用`extends`来继承trait（scala不论是类还是特质，都是使用extends关键字）\n\n* 如果要继承多个trait，则使用`with`关键字\n\n* ==**示例一：继承单个trait**==\n\n  ~~~scala\n  trait Logger1 {\n    // 抽象方法\n    def log(msg:String)\n  }\n  \n  class ConsoleLogger1 extends Logger1 {\n    override def log(msg: String): Unit = println(msg)\n  }\n  \n  object LoggerTrait1 {\n    def main(args: Array[String]): Unit = {\n      val logger = new ConsoleLogger1\n      logger.log(\"控制台日志: 这是一条Log\")\n    }\n  }\n  ~~~\n\n\n* ==**示例二：继承多个trait**==\n\n  ~~~scala\n  trait Logger2 {\n    // 抽象方法\n    def log(msg:String)\n  }\n  \n  trait MessageSender {\n    def send(msg:String)\n  }\n  \n  class ConsoleLogger2 extends Logger2 with MessageSender {\n    \n    override def log(msg: String): Unit = println(msg)\n  \n    override def send(msg: String): Unit = println(s\"发送消息:${msg}\")\n  }\n  \n  object LoggerTrait2 {\n    def main(args: Array[String]): Unit = {\n      val logger = new ConsoleLogger2\n      logger.log(\"控制台日志: 这是一条Log\")\n      logger.send(\"你好!\")\n    }\n  }\n  ~~~\n\n\n\n#### 21.2 定义具体的方法\n\n* 和类一样，trait中还可以定义具体的方法。\n\n* ==示例==\n\n  ~~~scala\n  trait LoggerDetail {\n    // 在trait中定义具体方法\n    def log(msg:String) = println(msg)\n  }\n  \n  class PersonService extends LoggerDetail {\n    def add() = log(\"添加用户\")\n  }\n  \n  object MethodInTrait {\n    def main(args: Array[String]): Unit = {\n      val personService = new PersonService\n      personService.add()\n    }\n  }\n  \n  ~~~\n\n\n\n#### 21.3 定义具体方法和抽象方法\n\n* 在trait中，可以混合使用具体方法和抽象方法\n\n* 使用具体方法依赖于抽象方法，而抽象方法可以放到继承trait的子类中实现，这种设计方式也称为**模板模式**\n\n* ==示例==\n\n  ~~~scala\n  trait Logger3 {\n    // 抽象方法\n    def log(msg:String)\n    // 具体方法（该方法依赖于抽象方法log\n    def info(msg:String) = log(\"INFO:\" + msg)\n    def warn(msg:String) = log(\"WARN:\" + msg)\n    def error(msg:String) = log(\"ERROR:\" + msg)\n  }\n  \n  class ConsoleLogger3 extends Logger3 {\n    override def log(msg: String): Unit = println(msg)\n  }\n  \n  object LoggerTrait3 {\n    def main(args: Array[String]): Unit = {\n      val logger3 = new ConsoleLogger3\n  \n      logger3.info(\"这是一条普通信息\")\n      logger3.warn(\"这是一条警告信息\")\n      logger3.error(\"这是一条错误信息\")\n    }\n  }\n  ~~~\n\n\n\n#### 21.4 定义具体字段和抽象字段\n\n* 在trait中可以定义具体字段和抽象字段\n\n* 继承trait的子类自动拥有trait中定义的字段\n\n* 字段直接被添加到子类中\n\n* ==示例==\n\n  ~~~scala\n  trait LoggerEx {\n    // 具体字段\n    val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm\")\n    val INFO = \"信息:\" + sdf.format(new Date)\n    // 抽象字段\n    val TYPE:String\n  \n    // 抽象方法\n    def log(msg:String)\n  }\n  \n  class ConsoleLoggerEx extends LoggerEx {\n    // 实现抽象字段\n    override val TYPE: String = \"控制台\"\n    // 实现抽象方法\n    override def log(msg:String): Unit = print(s\"$TYPE$INFO $msg\")\n  }\n  \n  object FieldInTrait {\n    def main(args: Array[String]): Unit = {\n      val logger = new ConsoleLoggerEx\n  \n      logger.log(\"这是一条消息\")\n    }\n  }\n  \n  \n  ~~~\n\n\n\n#### 21.5 实例对象混入trait\n\n* trait还可以混入到`实例对象`中，给对象实例添加额外的行为\n\n* 只有混入了trait的对象才具有trait中的方法，其他的类对象不具有trait中的行为\n\n* 使用with将trait混入到实例对象中\n\n* ==示例==\n\n  ~~~scala\n  trait LoggerMix {\n    def log(msg:String) = println(msg)\n  }\n  \n  class UserService\n  \n  object FixedInClass {\n    def main(args: Array[String]): Unit = {\n      // 使用with关键字直接将特质混入到对象中\n      val userService = new UserService with LoggerMix\n  \n      userService.log(\"你好\")\n    }\n  }\n  \n  ~~~\n\n\n\n### 22. 模式匹配和样例类\n\n* scala有一个十分强大的模式匹配机制，可以应用到很多场合。\n  * switch语句\n  * 类型查询\n  * 以及快速获取数据\n* 并且scala还提供了样例类，对模式匹配进行了优化，可以快速进行匹配。\n\n#### 22.1 匹配字符串\n\n~~~scala\n//todo:匹配字符串\nobject CaseDemo01 extends App{\n  //定义一个数组\n  val arr=Array(\"hadoop\",\"zookeeper\",\"spark\",\"storm\")\n\n  //随机取数组中的一位，使用Random.nextInt\n  val name = arr(Random.nextInt(arr.length))\n  println(name)\n\n  name match {\n    case \"hadoop\"     => println(\"大数据分布式存储和计算框架...\")\n    case \"zookeeper\"  => println(\"大数据分布式协调服务框架...\")\n    case \"spark\"      => println(\"大数据分布式内存计算框架...\")\n      //表示以上情况都不满足才会走最后一个\n    case _            => println(\"我不认识你\")\n  }\n\n}\n\n~~~\n\n\n\n#### 22.2 匹配类型\n\n~~~scala\n//todo:匹配类型\nobject CaseDemo02 extends App{\n  //定义一个数组\n  val arr=Array(\"hello\",1,-2.0,CaseDemo02)\n\n  //随机获取数组中的元素\n  val value=arr(Random.nextInt(arr.length))\n  println(value)\n\n    \n  value match {\n    case x:Int                => println(\"Int=>\"+x)\n    case y:Double if(y>=0)    => println(\"Double=>\"+y)\n    case z:String             => println(\"String=>\"+z)\n    case _                    => throw new Exception(\"not match exception\")\n  }\n\n}\n\n~~~\n\n\n\n#### 22.3 匹配数组\n\n~~~scala\n//匹配数组\nobject CaseDemo03 extends App{\n\n  //匹配数组\n  val  arr=Array(1,3,5)\n  arr match{\n    case Array(1,x,y) =>println(x+\"---\"+y)\n    case Array(1,_*)  =>println(\"1...\")\n    case Array(0)     =>println(\"only 0\")\n    case _            =>println(\"something else\")\n      \n  }\n}\n\n~~~\n\n\n\n#### 22.4 匹配集合\n\n~~~scala\n//匹配集合\nobject CaseDemo04 extends App{\n\n  val list=List(0,3,6)\n  list match {\n    case 0::Nil        => println(\"only 0\")\n    case 0::tail       => println(\"0....\")\n    case x::y::z::Nil  => println(s\"x:$x y:$y z:$z\")\n    case _             => println(\"something else\")\n  }\n}    \n\n\n~~~\n\n\n\n#### 22.5 匹配元组\n\n~~~scala\n//匹配元组\nobject CaseDemo05 extends App{\n  \n  val tuple=(1,3,5)\n  tuple match{\n    case (1,x,y)    => println(s\"1,$x,$y\")\n    case (2,x,y)    => println(s\"$x,$y\")\n    case _          => println(\"others...\")\n  }\n}\n\n~~~\n\n\n\n#### 22.6 样例类\n\n* 样例类是一种特殊类，它可以用来快速定义一个用于**保存数据**的类（类似于Java POJO类），==而且它会自动生成apply方法，允许我们快速地创建样例类实例对象==。后面在并发编程和spark、flink这些框架也都会经常使用它。\n\n* 定义样例类\n\n  * 语法结构\n\n    ~~~scala\n    case class 样例类名(成员变量名1:类型1, 成员变量名2:类型2 ...)\n    \n    ~~~\n\n* ==示例==\n\n  ~~~scala\n  // 定义一个样例类\n  // 样例类有两个成员name、age\n  case class CasePerson(name:String, age:Int)\n  \n  // 使用var指定成员变量是可变的\n  case class CaseStudent(var name:String, var age:Int)\n  \n  object CaseClassDemo {\n    def main(args: Array[String]): Unit = {\n      // 1. 使用new创建实例\n      val zhagnsan = new CasePerson(\"张三\", 20)\n      println(zhagnsan)\n  \n      // 2. 使用类名直接创建实例\n      val lisi = CasePerson(\"李四\", 21)\n      println(lisi)\n  \n      // 3. 样例类默认的成员变量都是val的，除非手动指定变量为var类型\n      //lisi.age = 22  // 编译错误！age默认为val类型\n  \n      val xiaohong = CaseStudent(\"小红\", 23)\n      xiaohong.age = 24\n      println(xiaohong)\n    }\n  }\n  ~~~\n\n\n* 样例对象\n\n  * 使用case object可以创建样例对象。样例对象是单例的，而且它**没有主构造器**。样例对象是可序列化的。格式：\n\n    ~~~scala\n    case object 样例对象名\n    ~~~\n\n  * ==示例==\n\n    ~~~scala\n    case class SendMessage(text:String)\n    \n    // 消息如果没有任何参数，就可以定义为样例对象\n    case object startTask\n    case object PauseTask\n    case object StopTask\n    \n    ~~~\n\n* 样例类和样例对象结合模式使用\n\n  * ==示例==\n\n    ~~~scala\n    case class SubmitTask(id: String, name: String)\n    case class HeartBeat(time: Long)\n    case object CheckTimeOutTask\n    \n    object CaseDemo06 extends App{\n    \n      val arr = Array(CheckTimeOutTask,\n                      HeartBeat(10000), \n                      SubmitTask(\"0001\", \"task-0001\"))\n    \n      arr(Random.nextInt(arr.length)) match {\n          \n           case SubmitTask(id, name) => println(s\"id=$id, name=$name\")\n           case HeartBeat(time) => println(s\"time=$time\")\n           case CheckTimeOutTask => println(\"检查超时\")\n    \n      }\n    }\n    \n    ~~~\n\n\n#### 22.7 Option类型\n\n* 在Scala中Option类型用样例类来表示可能存在或也可能不存在的值\n\n* Option类型有2个子类\n\n  * 一个是Some\n\n    * Some包装了某个值\n\n    ![1568271621212](https://kfly.top/picture/kfly-top/scala学习入门/assets/1568271621212.png)\n\n  * 一个是None\n\n    * None表示没有值\n\n    ![1568271671144](https://kfly.top/picture/kfly-top/scala学习入门/assets/1568271671144.png)\n\n* 示例\n\n  ~~~scala\n  object TestOption {\n    def main(args: Array[String]) {\n      val map = Map(\"a\" -> 1, \"b\" -> 2)\n        \n      val value: Option[Int] = map.get(\"b\")\n      val v1 =value match {\n        case Some(i) => i\n        case None => 0\n      }\n      println(v1)\n  \n      //更好的方式\n      val v2 = map.getOrElse(\"c\", 0)\n      println(v2)\n    }\n  }\n  \n  \n  ~~~\n\n\n\n#### 22.8 偏函数\n\n* 被包在花括号内==没有match的一组case语句==是一个偏函数\n\n* 它是PartialFunction[A, B]的一个实例，\n\n  * A代表输入参数类型\n  * B代表返回结果类型\n  * 可以理解为：偏函数是一个参数和一个返回值的函数。\n\n* ==示例==\n\n  ~~~scala\n  object TestPartialFunction {\n    // func1是一个输入参数为Int类型，返回值为String类型的偏函数\n    val func1: PartialFunction[Int, String] = {\n      case 1 => \"一\"\n      case 2 => \"二\"\n      case 3 => \"三\"\n      case _ => \"其他\"\n    }\n  \n    def main(args: Array[String]): Unit = {\n      println(func1(1))\n      \n      val list=List(1,2,3,4,5,6)\n  \n      //使用偏函数操作\n      val result=list.filter{\n        case x if x >3 => true\n        case _ => false\n      }\n      println(result)\n    }\n  \n  }\n  \n  \n  ~~~\n\n\n\n### 23. 异常处理\n\n#### 23.1 异常场景\n\n* 来看看下面一段代码\n\n~~~scala\n  def main(args: Array[String]): Unit = {\n   val i = 10 / 0\n    \n    println(\"你好！\")\n  }\n\nException in thread \"main\" java.lang.ArithmeticException: / by zero\n\tat ForDemo$.main(ForDemo.scala:3)\n\tat ForDemo.main(ForDemo.scala)\n\n~~~\n\n* 执行程序，可以看到scala抛出了异常，而且没有打印出来\"你好\"。说明程序出现错误后就终止了。那怎么解决该问题呢？\n\n\n\n#### 23.2 捕获异常\n\n* 在scala中，可以使用异常处理来解决这个问题。\n\n  * 在Scala里，借用了==模式匹配的思想来做异常的匹配==\n  * 以下为scala中try...catch异常处理的语法格式：\n\n  ~~~scala\n  try {\n      // 代码\n  }\n  catch {\n      case ex:异常类型1 => // 代码\n      case ex:异常类型2 => // 代码\n  }\n  finally {\n      // 代码\n  }\n  \n  \n  ~~~\n\n  * try中的代码是我们编写的业务处理代码\n  * 在catch中表示当出现某个异常时，需要执行的代码\n  * 在finally中，是不管是否出现异常都会执行的代码\n\n* ==示例==\n\n  ~~~scala\n  try {\n      val i = 10 / 0\n  \n  } catch {\n      case ex: Exception => println(ex.getMessage)\n  } finally {\n      println(\"我始终都会执行!\")\n  }\n  ~~~\n\n\n#### 23.3 抛出异常\n\n* 我们也可以在一个方法中，抛出异常。语法格式和Java类似，使用throw new Exception...\n\n* ==示例==\n\n  ~~~scala\n    def main(args: Array[String]): Unit = {\n      throw new Exception(\"这是一个异常\")\n    }\n  \n  Exception in thread \"main\" java.lang.Exception: 这是一个异常\n  \tat ForDemo$.main(ForDemo.scala:3)\n  \tat ForDemo.main(ForDemo.scala)\n  ~~~\n\n\n\n### 24. 提取器(Extractor)\n\n* ==提取器是从传递给它的对象中提取出构造该对象的参数==。(回想样例类进行模式匹配提取参数)\n\n* scala 提取器是一个带有unapply方法的对象。\n  * ==unapply方法算是apply方法的反向操作==\n    * unapply接受一个对象，然后从对象中提取值，提取的值通常是用来构造该对象的值。\n\n![1552639637165](https://kfly.top/picture/kfly-top/scala学习入门/assets/1552639637165.png)\n\n![1552639674932](https://kfly.top/picture/kfly-top/scala学习入门/assets/1552639674932.png)\n\n* ==示例==\n\n~~~scala\nclass Student {\n  var name:String = _   // 姓名\n  var age:Int = _       // 年龄\n  \n  // 实现一个辅助构造器\n  def this(name:String, age:Int) = {\n    this()\n    \n    this.name = name\n    this.age = age\n  }\n}\n\nobject Student {\n  def apply(name:String, age:Int): Student = new Student(name, age)\n\n  // 实现一个解构器\n  def unapply(arg: Student): Option[(String, Int)] = Some(arg.name, arg.age))\n}\n\nobject extractor_DEMO {\n  def main(args: Array[String]): Unit = {\n    val zhangsan = Student(\"张三\", 20)\n\n    zhangsan match {\n      case Student(name, age) => println(s\"姓名：$name 年龄：$age\")\n      case _ => println(\"未匹配\")\n    }\n  }\n}\n~~~\n\n\n\n### 25. 泛型\n\n\n\n* scala和Java一样，类和特质、方法都可以支持泛型。我们在学习集合的时候，一般都会涉及到泛型。\n\n```scala\nscala> val list1:List[String] = List(\"1\", \"2\", \"3\")\nlist1: List[String] = List(1, 2, 3)\n\n```\n\n* 在scala中，使用方括号来定义类型参数。\n\n\n\n#### 25.1 定义一个泛型方法\n\n* 不考虑泛型的支持\n\n  ~~~scala\n    def getMiddle(arr:Array[Int]) = arr(arr.length / 2)\n  \n    def main(args: Array[String]): Unit = {\n      val arr1 = Array(1,2,3,4,5)\n  \n      println(getMiddle(arr1))\n    }\n  \n  ~~~\n\n* 考虑泛型的支持\n\n  ~~~scala\n    def getMiddle[A](arr:Array[A]) = arr(arr.length / 2)\n  \n    def main(args: Array[String]): Unit = {\n      val arr1 = Array(1,2,3,4,5)\n      val arr2 = Array(\"a\", \"b\", \"c\", \"d\", \"f\")\n  \n      println(getMiddle[Int](arr1))\n      println(getMiddle[String](arr2))\n  \n      // 简写方式\n      println(getMiddle(arr1))\n      println(getMiddle(arr2))\n    }\n  \n  ~~~\n\n\n\n#### 25.2 定义一个泛型类\n\n* 定义一个Pair类包含2个类型不固定的泛型\n* ==示例==\n\n ~~~scala\n// 类名后面的方括号，就表示这个类可以使用两个类型、分别是T和S\n// 这个名字可以任意取\nclass Pair[T, S](val first: T, val second: S)\n\ncase class People(var name:String, val age:Int)\n\nobject Pair {\n  def main(args: Array[String]): Unit = {\n\n  val p1 = new Pair[String, Int](\"张三\", 10)\n  val p2 = new Pair[String, String](\"张三\", \"1988-02-19\")\n  val p3 = new Pair[People, People](People(\"张三\", 20), People(\"李四\", 30))\n  }\n}\n ~~~\n\n\n\n### 26. 上下界\n\n* ==在指定泛型类型时，有时需要界定泛型类型的范围，而不是接收任意类型==。比如，要求某个泛型类型，必须是某个类的子类，这样在程序中就可以放心的调用父类的方法，程序才能正常的使用与运行.\n* scala的上下边界特性允许泛型类型是某个类的子类，或者是某个类的父类\n  * 1、 ==U >: T==\n    * 这是类型==下界==的定义，也就是U必须是类型T的父类或者是自己本身。\n  * 2、 ==U <: T==\n    - 这是类型==上界==的定义，也就是U必须是类型T的子类或者是自己本身。\n* ==示例一==\n\n~~~scala\n// 类名后面的指定泛型的范围 ----上界\nclass Pair1[T <: Person, S <:Person](val first: T, val second: S) {\n  def chat(msg:String) = println(s\"${first.name}对${second.name}说: $msg\")\n}\n\nclass Person(var name:String, val age:Int)\n\nobject Pair1 {\n  def main(args: Array[String]): Unit = {\n\n  val p3 = new Pair1[Person,Person](new Person(\"张三\", 20), new Person(\"李四\", 30))\n  p3.chat(\"你好啊！\")\n  }\n}\n\n~~~\n\n* ==示例二==\n\n  ![1552657709922](https://kfly.top/picture/kfly-top/scala学习入门/assets/1552657709922.png)\n\n~~~scala\n//要控制Person只能和Person、Policeman聊天，但是不能和Superman聊天。此时，还需要给泛型添加一个下界。\n\n//上下界\nclass Pair[T <: Person, S >: Policeman <:Person](val first: T, val second: S) {\n  def chat(msg:String) = println(s\"${first.name}对${second.name}说: $msg\")\n}\n\nclass Person(var name:String, val age:Int)\nclass Policeman(name:String, age:Int) extends Person(name, age)\nclass Superman(name:String) extends Policeman(name, -1)\n\nobject Pair {\n  def main(args: Array[String]): Unit = {\n\t// 编译错误：第二个参数必须是Person的子类（包括本身）、Policeman的父类（包括本身）\n   val p3 = new Pair[Person,Superman](new Person(\"张三\", 20), new Superman(\"李四\"))\n   p3.chat(\"你好啊！\")\n  }\n}\n~~~\n\n\n\n\n\n### 27. 协变、逆变、非变\n\n* 来一个类型转换的问题\n\n~~~scala\nclass Pair[T](a:T)\n\nobject Pair {\n  def main(args: Array[String]): Unit = {\n    val p1 = new Pair(\"hello\")\n    // 编译报错，无法将p1转换为p2\n    val p2:Pair[AnyRef] = p1\n\n    println(p2)\n  }\n}\n~~~\n\n* ==**协变**==\n\n  ~~~html\n  class Pair[+T]，这种情况是协变。类型B是A的子类型，Pair[B]可以认为是Pair[A]的子类型。这种情况，参数化类型的方向和类型的方向是一致的。\n  \n  ~~~\n\n* ==**逆变**==\n\n  ~~~html\n  class Pair[-T]，这种情况是逆变。类型B是A的子类型，Pair[A]反过来可以认为是Pair[B]的子类型。这种情况，参数化类型的方向和类型的方向是相反的。\n  \n  ~~~\n\n* ==**非变**==\n\n  ~~~html\n  class Pair[T]{}，这种情况就是非变（默认），类型B是A的子类型，Pair[A]和Pair[B]没有任何从属关系，这种情况和Java是一样的。\n  \n  ~~~\n\n  ![1558064807949](https://kfly.top/picture/kfly-top/scala学习入门/assets/1558064807949.png)\n\n* ==示例==\n\n  ~~~scala\n  class Super\n  class Sub extends Super\n  \n  //非变\n  class Temp1[A](title: String)\n  //协变\n  class Temp2[+A](title: String)\n  //逆变\n  class Temp3[-A](title: String)\n  \n  object Covariance_demo {\n    def main(args: Array[String]): Unit = {\n      val a = new Sub()\n      // 没有问题，Sub是Super的子类\n      val b:Super = a\n  \n      // 非变\n      val t1:Temp1[Sub] = new Temp1[Sub](\"测试\")\n      // 报错！默认不允许转换\n      // val t2:Temp1[Super] = t1\n  \n      // 协变\n      val t3:Temp2[Sub] = new Temp2[Sub](\"测试\")\n      val t4:Temp2[Super] = t3\n      \n      // 逆变\n      val t5:Temp3[Super] = new Temp3[Super](\"测试\")\n      val t6:Temp3[Sub] = t5\n    }\n  }\n  \n  ~~~\n\n* ==总结==\n\n  ~~~html\n  C[+T]：如果A是B的子类，那么C[A]是C[B]的子类。\n  C[-T]：如果A是B的子类，那么C[B]是C[A]的子类。\n  C[T]： 无论A和B是什么关系，C[A]和C[B]没有从属关系。\n  \n  ~~~\n\n\n### 28. 隐式转换和隐式参数\n\n#### 28.1 隐式转换\n\n~~~html\n\tScala提供的隐式转换和隐式参数功能，是非常有特色的功能。是Java等编程语言所没有的功能。它可以允许你手动指定，将某种类型的对象转换成其他类型的对象或者是给一个类增加方法。通过这些功能，可以实现非常强大、特殊的功能。\n\n~~~\n\n* 隐式转换其核心就是定义一个使用 ==implicit== 关键字修饰的方法 实现把一个原始类转换成目标类，进而可以调用目标类中的方法\n\n\n\n#### 28.2 隐式参数\n\n~~~html\n\t所谓的隐式参数，指的是在函数或者方法中，定义一个用implicit修饰的参数，\n此时Scala会尝试找到一个指定类型的用implicit修饰的参数，即隐式值，并注入参数。\n\n~~~\n\n\n\n* ==所有的隐式转换和隐式参数必须定义在一个object中==\n\n\n\n#### 28.3 案例演示\n\n* ==案例一==\n\n  * **让File类具备RichFile类中的read方法**\n\n  ~~~scala\n  package com.kaikeba.implic_demo\n  \n  import java.io.File\n  \n  import scala.io.Source\n  \n  //todo:隐式转换案例一:让File类具备RichFile类中的read方法\n  \n  object MyPredef{\n    //定义一个隐式转换的方法，实现把File转换成RichFile\n    implicit  def file2RichFile(file:File)=new RichFile(file)\n  \n  }\n  \n  class RichFile(val file:File){\n       //读取数据文件的方法\n      def read():String={\n         Source.fromFile(file).mkString\n      }\n  }\n  \n  object RichFile{\n    def main(args: Array[String]): Unit = {\n       //1、构建一个File对象\n            val file = new File(\"E:\\\\aa.txt\")\n  \n       //2、手动导入隐式转换\n        import MyPredef.file2RichFile\n  \n         val data: String = file.read\n          println(data)\n    }\n  }\n  \n  ~~~\n\n\n\n* ==案例二==\n\n  * **超人变身**\n\n  ~~~scala\n  package com.kaikeba.implic_demo\n  \n  //todo:隐式转换案例二:超人变身\n  class Man(val name:String)\n  \n  class SuperMan(val name: String) {\n    def heat=print(\"超人打怪兽\")\n  \n  }\n  \n  object SuperMan{\n    //隐式转换方法\n    implicit def man2SuperMan(man:Man)=new SuperMan(man.name)\n  \n    def main(args: Array[String]) {\n        val hero=new Man(\"hero\")\n        //Man具备了SuperMan的方法\n        hero.heat\n    }\n  \n  }\n  \n  ~~~\n\n\n\n* ==案例三==\n\n  * **一个类隐式转换成具有相同方法的多个类**\n\n  ~~~~scala\n  package com.kaikeba.implic_demo\n  \n  //todo:隐式转换案例三（一个类隐式转换成具有相同方法的多个类）\n  \n  class C\n  class A(c:C) {\n      def readBook(): Unit ={\n        println(\"A说：好书好书...\")\n      }\n  }\n  \n  class B(c:C){\n    def readBook(): Unit ={\n      println(\"B说：看不懂...\")\n    }\n    def writeBook(): Unit ={\n      println(\"B说：不会写...\")\n    }\n  }\n  \n  object AB{\n  \n    //创建一个类转换为2个类的隐式转换\n    implicit def C2A(c:C)=new A(c)\n    implicit def C2B(c:C)=new B(c)\n  }\n  \n  object B{\n    def main(args: Array[String]) {\n      //导包\n      //1. import AB._ 会将AB类下的所有隐式转换导进来\n      //2. import AB.C2A 只导入C类到A类的的隐式转换方法\n      //3. import AB.C2B 只导入C类到B类的的隐式转换方法\n      import AB._\n      val c=new C\n  \n      //由于A类与B类中都有readBook()，只能导入其中一个，否则调用共同方法时代码报错\n       //c.readBook()\n  \n      //C类可以执行B类中的writeBook()\n      c.writeBook()\n  \n    }\n  }\n  \n  ~~~~\n\n\n\n* ==案例四==\n\n  - **员工领取薪水**\n\n  ~~~scala\n  package cn.itcast.implic_demo\n  \n  //todo:隐式参数案例四:员工领取薪水\n  \n  object Company{\n    //在object中定义隐式值    注意：同一类型的隐式值只允许出现一次，否则会报错\n    implicit  val xxx=\"zhangsan\"\n    implicit  val yyy=10000.00\n  \n    //implicit  val zzz=\"lisi\"\n  \n  }\n  \n  class Boss {\n    //定义一个用implicit修饰的参数 类型为String\n    //注意参数匹配的类型   它需要的是String类型的隐式值\n    def callName(implicit name:String):String={\n      name+\" is coming !\"\n    }\n  \n    //定义一个用implicit修饰的参数，类型为Double\n    //注意参数匹配的类型    它需要的是Double类型的隐式值\n    def getMoney(implicit money:Double):String={\n      \" 当月薪水：\"+money\n    }\n  \n  \n  }\n  match\n  object Boss extends App{\n    //使用import导入定义好的隐式值，注意：必须先加载否则会报错\n    import Company.xxx\n    import Company.yyy\n  \n    val boss =new Boss\n    println(boss.callName+boss.getMoney)\n  \n  }\n  \n  ~~~\n","tags":["-scala"]},{"title":"数仓商城项目之-系统业务数仓","url":"/2019/12/04/it/project/数仓商城项目之-系统业务数仓/","content":"\n# 电商数仓（系统业务数据仓库）\n\n## 1. 电商业务与数据结构简介\n\n### 1.1 电商业务流程  \n\n![image-20191204124410155](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204124410155.png)\n\n### 1.2  电商常识（SKU、SPU）  \n\n1. SKU=Stock Keeping Unit（库存量基本单位）。现在已经被引申为产品统一编号的简称，每种产品均对应有唯一的SKU号。\n\n2. SPU(Standard Product Unit)：是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息集合。\n\n### 1.3 电商表结构\n\n![image-20191204130052893](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204130052893.png)\n\n#### 1.3.1 订单表（order_info）\n\n 标签         | 含义       \n ------------ | ---------- \n id           | 订单编号   \n total_amount | 订单金额   \n order_status | 订单状态   \n user_id      | 用户id     \n payment_way  | 支付方式   \n out_trade_no | 支付流水号 \n create_time  | 创建时间   \n operate_time | 操作时间   \n\n#### 1.3.2 订单详情表（order_detail）\n\n| 标签        | 含义     |      |\n| ----------- | -------- | ---- |\n| id          | 订单编号 |      |\n| order_id    | 订单号   |      |\n| user_id     | 用户id   |      |\n| sku_id      | 商品id   |      |\n| sku_name    | 商品名称 |      |\n| order_price | 商品价格 |      |\n| sku_num     | 商品数量 |      |\n| create_time | 创建时间 |      |\n|             |          |      |\n\n#### 1.3.3 商品表\n\n| 标签         | 含义     |      |\n| ------------ | -------- | ---- |\n| id           | skuId    |      |\n| spu_id       | spuid    |      |\n| price        | 价格     |      |\n| sku_name     | 商品名称 |      |\n| sku_desc     | 商品描述 |      |\n| weight       | 重量     |      |\n| tm_id        | 品牌id   |      |\n| category3_id | 品类id   |      |\n| create_time  | 创建时间 |      |\n|              |          |      |\n\n#### 1.3.4 用户表\n\n| 标签        | 含义     |      |\n| ----------- | -------- | ---- |\n| id          | 用户id   |      |\n| name        | 姓名     |      |\n| birthday    | 生日     |      |\n| gender      | 性别     |      |\n| email       | 邮箱     |      |\n| user_level  | 用户等级 |      |\n| create_time | 创建时间 |      |\n|             |          |      |\n\n#### 1.3.5 商品一级分类表\n\n| 标签 | 含义 |      |\n| ---- | ---- | ---- |\n| id   | id   |      |\n| name | 名称 |      |\n|      |      |      |\n\n#### 1.3.6 商品二级分类表\n\n| 标签         | 含义       |      |\n| ------------ | ---------- | ---- |\n| id           | id         |      |\n| name         | 名称       |      |\n| category1_id | 一级品类id |      |\n|              |            |      |\n\n#### 1.3.7 商品三级分类表\n\n| 标签         | 含义       |      |\n| ------------ | ---------- | ---- |\n| id           | id         |      |\n| name         | 名称       |      |\n| Category2_id | 二级品类id |      |\n|              |            |      |\n\n#### 1.3.8 支付流水表\n\n| 标签            | 含义               |\n| --------------- | ------------------ |\n| id              | 编号               |\n| out_trade_no    | 对外业务编号       |\n| order_id        | 订单编号           |\n| user_id         | 用户编号           |\n| alipay_trade_no | 支付宝交易流水编号 |\n| total_amount    | 支付金额           |\n| subject         | 交易内容           |\n| payment_type    | 支付类型           |\n| payment_time    | 支付时间           |\n\n## 2. 数仓理论\n\n### 2.1 表的分类\n\n#### 2.1.1 实体表\n\n- **实体表**，一般是指一个现实存在的业务对象，比如用户，商品，商家，销售员等等。\n\n用户表：\n\n| 用户id | 姓名 | 生日       | 性别 | 邮箱       | 用户等级 | 创建时间   |\n| ------ | ---- | ---------- | ---- | ---------- | -------- | ---------- |\n| 1      | 张三 | 2011-11-11 | 男   | zs@163.com | 2        | 2018-11-11 |\n| 2      | 李四 | 2011-11-11 | 女   | ls@163.com | 3        | 2018-11-11 |\n| 3      | 王五 | 2011-11-11 | 中性 | ww@163.com | 1        | 2018-11-11 |\n| …      | …    | …          | …    | …          | …        | …          |\n\n#### 2.1.2 维度表\n\n- **维度表**，一般是指对应一些业务状态，编号的解释表。也可以称之为码表。\n\n比如地区表，订单状态，支付方式，审批状态，商品分类等等。\n\n订单状态表：\n\n| 订单状态编号 | 订单状态名称 |\n| ------------ | ------------ |\n| 1            | 未支付       |\n| 2            | 支付         |\n| 3            | 发货中       |\n| 4            | 已发货       |\n| 5            | 已完成       |\n\n   商品分类表：\n\n| 商品分类编号 | 分类名称 |\n| ------------ | -------- |\n| 1            | 服装     |\n| 2            | 保健     |\n| 3            | 电器     |\n| 4            | 图书     |\n\n#### 2.1.3 事务型事实表\n\n- **事务型事实表**，一般指随着业务发生不断产生的数据。特点是一旦发生不会再变化。\n\n一般比如，交易流水，操作日志，出库入库记录等等。\n\n交易流水表：\n\n| 编号   | 对外业务编号 | 订单编号 | 用户编号 | 支付宝交易流水编号 | 支付金额 | 交易内容    | 支付类型  | 支付时间            |\n| ------ | ------------ | -------- | -------- | ------------------ | -------- | ----------- | --------- | ------------------- |\n| 1      | 7577697945   | 1        | 111      | QEyF-63000323      | 223.00   | 海狗人参丸1 | alipay    | 2019-02-10 00:50:02 |\n| 2      | 0170099522   | 2        | 222      | qdwV-25111279      | 589.00   | 海狗人参丸2 | wechatpay | 2019-02-10 00:50:02 |\n| 3      | 1840931679   | 3        | 666      | hSUS-65716585      | 485.00   | 海狗人参丸3 | unionpay  | 2019-02-10 00:50:02 |\n| 。。。 | 。。。       | 。。。   | 。。。   | 。。。             | 。。。   | 。。。      | 。。。    | 。。。              |\n\n- **周期型事实表**，一般指随着业务发生不断产生的数据。\n\n与事务型不同的是，数据会随着业务周期性的推进而变化。\n\n 比如订单，其中订单状态会周期性变化。再比如，请假、贷款申请，随着批复状态在周期性变化。\n\n订单表：\n\n| 订单编号 | 订单金额 | 订单状态 | 用户id | 支付方式  | 支付流水号    | 创建时间            | 操作时间            |\n| -------- | -------- | -------- | ------ | --------- | ------------- | ------------------- | ------------------- |\n| 1        | 223.00   | 2        | 111    | alipay    | QEyF-63000323 | 2019-02-10 00:01:29 | 2019-02-10 00:01:29 |\n| 2        | 589.00   | 2        | 222    | wechatpay | qdwV-25111279 | 2019-02-10 00:05:02 | 2019-02-10 00:05:02 |\n| 3        | 485.00   | 1        | 666    | unionpay  | hSUS-65716585 | 2019-02-10 00:50:02 | 2019-02-10 00:50:02 |\n| 。。。   | 。。。   | 。。。   | 。。。 | 。。。    | 。。。        | 。。。              | 。。。              |\n\n### 2.2 同步策略\n\n数据同步策略的类型包括：全量表、增量表、新增及变化表、拉链表\n\n-  全量表：存储完整的数据。\n\n\n-  增量表：存储新增加的数据。\n\n\n-  新增及变化表：存储新增加的数据和变化的数据。\n\n\n-  拉链表：对新增及变化表做定期合并。\n\n\n#### 2.2.1 实体表同步策略\n\n- 实体表：比如用户，商品，商家，销售员等\n\n- 实体表数据量比较小：通常可以做每日全量，就是每天存一份完整数据。即每日全量。\n\n\n#### 2.2.2 维度表同步策略\n\n- 维度表：比如订单状态，审批状态，商品分类\n\n- 维度表数据量比较小：通常可以做每日全量，就是每天存一份完整数据。即每日全量。\n\n- 说明：\n  - 针对可能会有变化的状态数据可以存储每日全量。\n  - 没变化的客观世界的维度（比如性别，地区，民族，政治成分，鞋子尺码）可以只存一份固定值。\n\n#### 2.2.3 事务型事实表同步策略\n\n- 事务型事实表：比如，交易流水，操作日志，出库入库记录等。\n\n- 因为数据不会变化，而且数据量巨大，所以每天只同步新增数据即可，所以可以做成每日增量表，即每日创建一个分区存储。\n\n\n#### 2.2.4 周期型事实表同步策略\n\n- 周期型事实表：比如，订单、请假、贷款申请等\n\n- 这类表从数据量的角度，存每日全量的话，数据量太大，冗余也太大。如果用每日增量的话无法反应数据变化。\n\n-  每日新增及变化量，包括了当日的新增和修改。一般来说这个表，足够计算大部分当日数据的。但是这种依然无法解决能够得到某一个历史时间点（时间切片）的切片数据。 \n\n-  所以要用利用每日新增和变化表，制作一张拉链表，以方便的取到某个时间切片的快照数据。所以我们需要得到每日新增及变化量。\n\n\n- 拉链表：\n\n\n| name姓名 | start新名字创建时间 | end名字更改时间 |\n| -------- | ------------------- | --------------- |\n| 张三     | 1990/1/1            | 2018/12/31      |\n| 张小三   | 2019/1/1            | 2019/4/30       |\n| 张大三   | 2019/5/1            | 9999-99-99      |\n| 。。。   | 。。。              | 。。。          |\n\n### 2.3 范式理论\n\n#### 2.3.1 范式概念\n\n- ​\t关系型数据库设计时，遵照一定的规范要求，目的在于降低数据的冗余性，目前业界范式有：第一范式(1NF)、第二范式(2NF)、第三范式(3NF)、巴斯-科德范式(BCNF)、第四范式(4NF)、第五范式(5NF)。\n\n范式可以理解为设计一张数据表的表结构，符合的标准级别。\n\n-  使用范式的根本目的是：\n  -  减少数据冗余，尽量让每个数据只出现一次。\n  -  保证数据一致性\n\n-  缺点是获取数据时，需要通过拼接出最后的数据。\n\n#### 2.3.2 函数依赖\n\n![image-20191204143818657](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204143818657.png)\n\n#### 2.3.3 范式区分\n\n- 1NF：属性不可分割\n- 2NF：只含有部分依赖\n- 3NF：不包含传递依赖\n\n### 2.4 关系建模和纬度建模\n\n- 关系模型：主要用于OLTP系统，为了保证数据的一致性以及避免冗余，所以大部分业务系统的表都是遵循第三范式的。  \n- 维度模型主要应用于OLAP系统中，因为关系模型虽然冗余少，但是在大规模数据，跨表分析统计查询过程中，会造成多表关联，这会大大降低执行效率。\n\n#### 2.5 雪花模型、星型模型和星座模型\n\n![image-20191204145843067](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204145843067.png)\n\n![image-20191204145910488](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204145910488.png)\n\n## 3. 数仓搭建\n\n### 3.1 数据生成\n\n#### 3.1.1 生成业务数据\n\n1）生成业务数据函数说明\n\n     ```sql\n init_data ( do_date_string VARCHAR(20) , order_incr_num INT, user_incr_num INT , sku_num INT , if_truncate BOOLEAN )：\n\n-- 参数一：do_date_string生成数据日期\n--     参数二：order_incr_num订单id个数\n--     参数三：user_incr_num用户id个数\n--     参数四：sku_num商品sku个数\n--     参数五：if_truncate是否删除数据\n\n-- 需求：生成日期2019年2月10日数据、订单1000个、用户200个、商品sku300个、删除原始数据。\nCALL init_data('2019-02-10',1000,200,300,TRUE);\n     ```\n\n### 3.2 业务数据导入数仓\n\n#### 3.2.1 分析表的同步策略\n\n![image-20191204160350046](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204160350046.png)\n\n#### 3.2.2 Sqoop定时导入脚本\n\n```sh\n#!/bin/bash\n\ndb_date=$2\necho $db_date\ndb_name=gmall\n\nimport_data() {\n/kfly/install/sqoop-1.4.6-cdh5.14.2/bin/sqoop import \\\n--connect jdbc:mysql://node02:3306/$db_name?useSSL=false \\\n--username root \\\n--password 123456 \\\n--target-dir /origin_data/$db_name/db/$1/$db_date \\\n--delete-target-dir \\\n--num-mappers 1 \\\n--fields-terminated-by \"\\t\" \\\n--query \"$2\"' and $CONDITIONS;'\n}\n\nimport_sku_info(){\n  import_data \"sku_info\" \"select \nid, spu_id, price, sku_name, sku_desc, weight, tm_id,\ncategory3_id, create_time\n  from sku_info where 1=1\"\n}\n\nimport_user_info(){\n  import_data \"user_info\" \"select \nid, name, birthday, gender, email, user_level, \ncreate_time \nfrom user_info where 1=1\"\n}\n\nimport_base_category1(){\n  import_data \"base_category1\" \"select \nid, name from base_category1 where 1=1\"\n}\n\nimport_base_category2(){\n  import_data \"base_category2\" \"select \nid, name, category1_id from base_category2 where 1=1\"\n}\n\nimport_base_category3(){\n  import_data \"base_category3\" \"select id, name, category2_id from base_category3 where 1=1\"\n}\n\nimport_order_detail(){\n  import_data   \"order_detail\"   \"select \n    od.id, \n    order_id, \n    user_id, \n    sku_id, \n    sku_name, \n    order_price, \n    sku_num, \n    o.create_time  \n  from order_info o, order_detail od\n  where o.id=od.order_id\n  and DATE_FORMAT(create_time,'%Y-%m-%d')='$db_date'\"\n}\n\nimport_payment_info(){\n  import_data \"payment_info\"   \"select \n    id,  \n    out_trade_no, \n    order_id, \n    user_id, \n    alipay_trade_no, \n    total_amount,  \n    subject, \n    payment_type, \n    payment_time \n  from payment_info \n  where DATE_FORMAT(payment_time,'%Y-%m-%d')='$db_date'\"\n}\n\nimport_order_info(){\n  import_data   \"order_info\"   \"select \n    id, \n    total_amount, \n    order_status, \n    user_id, \n    payment_way, \n    out_trade_no, \n    create_time, \n    operate_time  \n  from order_info \n  where (DATE_FORMAT(create_time,'%Y-%m-%d')='$db_date' or DATE_FORMAT(operate_time,'%Y-%m-%d')='$db_date')\"\n}\n\ncase $1 in\n  \"base_category1\")\n     import_base_category1\n;;\n  \"base_category2\")\n     import_base_category2\n;;\n  \"base_category3\")\n     import_base_category3\n;;\n  \"order_info\")\n     import_order_info\n;;\n  \"order_detail\")\n     import_order_detail\n;;\n  \"sku_info\")\n     import_sku_info\n;;\n  \"user_info\")\n     import_user_info\n;;\n  \"payment_info\")\n     import_payment_info\n;;\n   \"all\")\n   import_base_category1\n   import_base_category2\n   import_base_category3\n   import_order_info\n   import_order_detail\n   import_sku_info\n   import_user_info\n   import_payment_info\n;;\nesac\n```\n\n### 3.3 建表\n\n#### 3.3.1 创建订单表\n\n```sql\ndrop table if exists gmall.ods_order_info;\ncreate external table gmall.ods_order_info (\n `id` string COMMENT '订单编号',\n `total_amount` decimal(10,2) COMMENT '订单金额',\n `order_status` string COMMENT '订单状态',\n `user_id` string COMMENT '用户id',\n `payment_way` string COMMENT '支付方式',\n `out_trade_no` string COMMENT '支付流水号',\n `create_time` string COMMENT '创建时间',\n `operate_time` string COMMENT '操作时间'\n) COMMENT '订单表'\nPARTITIONED BY (`dt` string)\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ods/ods_order_info/'\n;\n```\n\n#### 3.3.2 创建订单详情表\n\n```sql\ndrop table if exists gmall.ods_order_detail;\ncreate external table gmall.ods_order_detail( \n `id` string COMMENT '订单编号',\n `order_id` string  COMMENT '订单号', \n `user_id` string COMMENT '用户id',\n `sku_id` string COMMENT '商品id',\n `sku_name` string COMMENT '商品名称',\n `order_price` string COMMENT '商品价格',\n `sku_num` string COMMENT '商品数量',\n `create_time` string COMMENT '创建时间'\n) COMMENT '订单明细表'\nPARTITIONED BY (`dt` string)\nrow format delimited fields terminated by '\\t' \nlocation '/warehouse/gmall/ods/ods_order_detail/'\n;\n```\n\n####  3.3.3 创建商品表\n\n```sql\ndrop table if exists gmall.ods_sku_info;\ncreate external table gmall.ods_sku_info( \n `id` string COMMENT 'skuId',\n `spu_id` string COMMENT 'spuid', \n `price` decimal(10,2) COMMENT '价格',\n `sku_name` string COMMENT '商品名称',\n `sku_desc` string COMMENT '商品描述',\n `weight` string COMMENT '重量',\n `tm_id` string COMMENT '品牌id',\n `category3_id` string COMMENT '品类id',\n `create_time` string COMMENT '创建时间'\n) COMMENT '商品表'\nPARTITIONED BY (`dt` string)\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ods/ods_sku_info/'\n;\n\n```\n\n#### 3.3.4 创建用户表\n\n```\ndrop table if exists gmall.ods_user_info;\ncreate external table gmall.ods_user_info( \n `id` string COMMENT '用户id',\n `name` string COMMENT '姓名',\n `birthday` string COMMENT '生日',\n `gender` string COMMENT '性别',\n `email` string COMMENT '邮箱',\n `user_level` string COMMENT '用户等级',\n `create_time` string COMMENT '创建时间'\n) COMMENT '用户信息'\nPARTITIONED BY (`dt` string)\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ods/ods_user_info/'\n\n```\n\n#### 3.3.5 创建分类表\n\n```sql\ndrop table if exists gmall.ods_user_info;\ncreate external table gmall.ods_user_info( \n `id` string COMMENT '用户id',\n `name` string COMMENT '姓名',\n `birthday` string COMMENT '生日',\n `gender` string COMMENT '性别',\n `email` string COMMENT '邮箱',\n `user_level` string COMMENT '用户等级',\n `create_time` string COMMENT '创建时间'\n) COMMENT '用户信息'\nPARTITIONED BY (`dt` string)\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ods/ods_user_info/';\n\ndrop table if exists gmall.ods_base_category2;\ncreate external table gmall.ods_base_category2( \n `id` string COMMENT ' id',\n `name` string COMMENT '名称',\n category1_id string COMMENT '一级品类id'\n) COMMENT '商品二级分类'\nPARTITIONED BY (`dt` string)\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ods/ods_base_category2/'\n;\n\ndrop table if exists gmall.ods_base_category3;\ncreate external table gmall.ods_base_category3(\n `id` string COMMENT ' id',\n `name`  string COMMENT '名称',\n category2_id string COMMENT '二级品类id'\n) COMMENT '商品三级分类'\nPARTITIONED BY (`dt` string)\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ods/ods_base_category3/'\n;\n\n```\n\n#### 3.3.6 支付流水表\n\n```sql\ndrop table if exists gmall.ods_payment_info;\ncreate external table gmall.ods_payment_info(\n `id` bigint COMMENT '编号',\n `out_trade_no` string COMMENT '对外业务编号',\n `order_id` string COMMENT '订单编号',\n `user_id` string COMMENT '用户编号',\n `alipay_trade_no` string COMMENT '支付宝交易流水编号',\n `total_amount` decimal(16,2) COMMENT '支付金额',\n `subject` string COMMENT '交易内容',\n `payment_type` string COMMENT '支付类型',\n `payment_time` string COMMENT '支付时间'\n)COMMENT '支付流水表' \nPARTITIONED BY (`dt` string)\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ods/ods_payment_info/'\n;\n\n```\n\n### 3.4 数据导入脚本\n\n```sh\n#!/bin/bash\n\n   APP=gmall\n   hive=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive\n\n# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天\nif [ -n \"$1\" ] ;then\n\tdo_date=$1\nelse \n\tdo_date=`date -d \"-1 day\" +%F`\nfi\n\nsql=\" \nload data inpath '/origin_data/$APP/db/order_info/$do_date' OVERWRITE into table \"$APP\".ods_order_info partition(dt='$do_date');\n\nload data inpath '/origin_data/$APP/db/order_detail/$do_date' OVERWRITE into table \"$APP\".ods_order_detail partition(dt='$do_date');\n\nload data inpath '/origin_data/$APP/db/sku_info/$do_date' OVERWRITE into table \"$APP\".ods_sku_info partition(dt='$do_date');\n\nload data inpath '/origin_data/$APP/db/user_info/$do_date' OVERWRITE into table \"$APP\".ods_user_info partition(dt='$do_date');\n\nload data inpath '/origin_data/$APP/db/payment_info/$do_date' OVERWRITE into table \"$APP\".ods_payment_info partition(dt='$do_date');\n\nload data inpath '/origin_data/$APP/db/base_category1/$do_date' OVERWRITE into table \"$APP\".ods_base_category1 partition(dt='$do_date');\n\nload data inpath '/origin_data/$APP/db/base_category2/$do_date' OVERWRITE into table \"$APP\".ods_base_category2 partition(dt='$do_date');\n\nload data inpath '/origin_data/$APP/db/base_category3/$do_date' OVERWRITE into table \"$APP\".ods_base_category3 partition(dt='$do_date'); \n\"\n$hive -e \"$sql\"\n```\n\n## 4. DWD层\n\n- 对ODS层数据进行判空过滤。对商品分类表进行维度退化（降维）。\n\n### 4.1 创建表\n\n#### 4.1.1 创建订单表\n\n```sql\ndrop table if exists gmall.dwd_order_info;\ncreate external table gmall.dwd_order_info (\n `id` string COMMENT '',\n `total_amount` decimal(10,2) COMMENT '',\n `order_status` string COMMENT ' 1 2 3 4 5',\n `user_id` string COMMENT 'id',\n `payment_way` string COMMENT '',\n `out_trade_no` string COMMENT '',\n `create_time` string COMMENT '',\n `operate_time` string COMMENT ''\n) \nPARTITIONED BY (`dt` string)\nstored as parquet \nlocation '/warehouse/gmall/dwd/dwd_order_info/' \ntblproperties (\"parquet.compression\"=\"snappy\") \n;\n```\n\n#### 4.1.2 订单详情表\n\n```sql\ndrop table if exists gmall.dwd_order_detail;\ncreate external table gmall.dwd_order_detail( \n `id` string COMMENT '',\n `order_id` decimal(10,2) COMMENT '', \n `user_id` string COMMENT 'id',\n `sku_id` string COMMENT 'id',\n `sku_name` string COMMENT '',\n `order_price` string COMMENT '',\n `sku_num` string COMMENT '',\n `create_time` string COMMENT ''\n)\nPARTITIONED BY (`dt` string) \nstored as parquet\nlocation '/warehouse/gmall/dwd/dwd_order_detail/'\ntblproperties (\"parquet.compression\"=\"snappy\")\n;\n```\n\n#### 4.1.3 用户表\n\n```sql\ndrop table if exists gmall.dwd_user_info;\ncreate external table gmall.dwd_user_info( \n `id` string COMMENT 'id',\n `name` string COMMENT '', \n `birthday` string COMMENT '',\n `gender` string COMMENT '',\n `email` string COMMENT '',\n `user_level` string COMMENT '',\n `create_time` string COMMENT ''\n) \nPARTITIONED BY (`dt` string)\nstored as parquet\nlocation '/warehouse/gmall/dwd/dwd_user_info/'\ntblproperties (\"parquet.compression\"=\"snappy\")\n;\n```\n\n#### 4.1.4 支付流水表\n\n```sql\ndrop table if exists gmall.dwd_payment_info;\ncreate external table gmall.dwd_payment_info(\n `id` bigint COMMENT '',\n `out_trade_no` string COMMENT '',\n `order_id` string COMMENT '',\n `user_id` string COMMENT '',\n `alipay_trade_no` string COMMENT '',\n `total_amount` decimal(16,2) COMMENT '',\n `subject` string COMMENT '',\n `payment_type` string COMMENT '',\n `payment_time` string COMMENT ''\n)  \nPARTITIONED BY (`dt` string)\nstored as parquet\nlocation '/warehouse/gmall/dwd/dwd_payment_info/'\ntblproperties (\"parquet.compression\"=\"snappy\")\n;\n```\n\n#### 4.1.5 商品表\n\n![image-20191204164800632](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204164800632.png)\n\n```sql\ndrop table if exists gmall.dwd_sku_info;\ncreate external table gmall.dwd_sku_info(\n `id` string COMMENT 'skuId',\n `spu_id` string COMMENT 'spuid',\n `price` decimal(10,2) COMMENT '',\n `sku_name` string COMMENT '',\n `sku_desc` string COMMENT '',\n `weight` string COMMENT '',\n `tm_id` string COMMENT 'id',\n `category3_id` string COMMENT '1id',\n `category2_id` string COMMENT '2id',\n `category1_id` string COMMENT '3id',\n `category3_name` string COMMENT '3',\n `category2_name` string COMMENT '2',\n `category1_name` string COMMENT '1',\n `create_time` string COMMENT ''\n) \nPARTITIONED BY (`dt` string)\nstored as parquet\nlocation '/warehouse/gmall/dwd/dwd_sku_info/'\ntblproperties (\"parquet.compression\"=\"snappy\")\n;\n```\n\n### 4.2   DWD层数据导入脚本  \n\n```sh\n#!/bin/bash\n\n# 定义变量方便修改\nAPP=gmall\nhive=/kfly/install/hive-1.1.0-cdh5.14.2/bin/hive\n\n# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天\nif [ -n \"$1\" ] ;then\n\tdo_date=$1\nelse \n\tdo_date=`date -d \"-1 day\" +%F`  \nfi \n\nsql=\"\n\nset hive.exec.dynamic.partition.mode=nonstrict;\n\ninsert overwrite table \"$APP\".dwd_order_info partition(dt)\nselect * from \"$APP\".ods_order_info \nwhere dt='$do_date' and id is not null;\n \ninsert overwrite table \"$APP\".dwd_order_detail partition(dt)\nselect * from \"$APP\".ods_order_detail \nwhere dt='$do_date'   and id is not null;\n\ninsert overwrite table \"$APP\".dwd_user_info partition(dt)\nselect * from \"$APP\".ods_user_info\nwhere dt='$do_date' and id is not null;\n \ninsert overwrite table \"$APP\".dwd_payment_info partition(dt)\nselect * from \"$APP\".ods_payment_info\nwhere dt='$do_date' and id is not null;\n\ninsert overwrite table \"$APP\".dwd_sku_info partition(dt)\nselect  \n    sku.id,\n    sku.spu_id,\n    sku.price,\n    sku.sku_name,\n    sku.sku_desc,\n    sku.weight,\n    sku.tm_id,\n    sku.category3_id,\n    c2.id category2_id,\n    c1.id category1_id,\n    c3.name category3_name,\n    c2.name category2_name,\n    c1.name category1_name,\n    sku.create_time,\n    sku.dt\nfrom\n    \"$APP\".ods_sku_info sku\njoin \"$APP\".ods_base_category3 c3 on sku.category3_id=c3.id \n    join \"$APP\".ods_base_category2 c2 on c3.category2_id=c2.id \n    join \"$APP\".ods_base_category1 c1 on c2.category1_id=c1.id \nwhere sku.dt='$do_date'  and c2.dt='$do_date'\nand c3.dt='$do_date' and c1.dt='$do_date'\nand sku.id is not null;\n\"\n\n$hive -e \"$sql\"\n```\n\n#### 4.2.1 总结 \n\n- 维度退化要付出什么代价？ \n  - 如果被退化的维度，还有其他业务表使用，退化后处理起来就麻烦些。\n\n- 想想在实际业务中还有那些维度表可以退化\n  - 城市的三级分类（省、市、县）等\n\n## 5 DWS 用户行为宽表\n\n- 为什么要建宽表\n  - 需求目标，把每个用户单日的行为聚合起来组成一张多列宽表，以便之后关联用户维度信息后进行，不同角度的统计分析。\n\n![image-20191204165240937](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204165240937.png)\n\n```sql\ndrop table if exists gmall.dws_user_action;\ncreate external table gmall.dws_user_action \n(   \n    user_id          string      comment '用户 id',\n    order_count     bigint      comment '下单次数 ',\n    order_amount    decimal(16,2)  comment '下单金额 ',\n    payment_count   bigint      comment '支付次数',\n    payment_amount  decimal(16,2) comment '支付金额 ',\n    comment_count   bigint      comment '评论次数'\n) COMMENT '每日用户行为宽表'\nPARTITIONED BY (`dt` string)\nstored as parquet\nlocation '/warehouse/gmall/dws/dws_user_action/'\ntblproperties (\"parquet.compression\"=\"snappy\");\n\n```\n\n```sql\nwith  \ntmp_order as\n(\n    select \n        user_id, \ncount(*)  order_count,\n        sum(oi.total_amount) order_amount\n    from gmall.dwd_order_info oi\n    where date_format(oi.create_time,'yyyy-MM-dd')='2019-02-10'\n    group by user_id\n) ,\ntmp_payment as\n(\n    select\n        user_id, \n        sum(pi.total_amount) payment_amount, \n        count(*) payment_count \n    from gmall.dwd_payment_info pi \n    where date_format(pi.payment_time,'yyyy-MM-dd')='2019-02-10'\n    group by user_id\n),\ntmp_comment as\n(\n    select\n        user_id,\n        count(*) comment_count\n    from gmall.dwd_comment_log c\n    where date_format(c.dt,'yyyy-MM-dd')='2019-02-10'\n    group by user_id \n) \n\ninsert overwrite table gmall.dws_user_action partition(dt='2019-02-10') \nselect\n    user_actions.user_id,\n    sum(user_actions.order_count),\n    sum(user_actions.order_amount),\n    sum(user_actions.payment_count),\n    sum(user_actions.payment_amount),\n    sum(user_actions.comment_count)\nfrom \n(\n    select\n        user_id,\n        order_count,\n        order_amount,\n        0 payment_count,\n        0 payment_amount,\n        0 comment_count\n    from tmp_order\n\n    union all\n    select\n        user_id,\n        0 order_count,\n        0 order_amount,\n        payment_count,\n        payment_amount,\n        0 comment_count\n    from tmp_payment  \n\n    union all\n    select\n        user_id,\n        0 order_count,\n        0 order_amount,\n        0 payment_count,\n        0 payment_amount,\n        comment_count\n    from tmp_comment\n ) user_actions\ngroup by user_id;\n```\n\n```sh\n#!/bin/bash\n\n# 定义变量方便修改\nAPP=gmall\nhive=/kfly/install/hive-1.1.0-cdh5.14.2/bin/hive\n\n# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天\nif [ -n \"$1\" ] ;then\n\tdo_date=$1\nelse \n\tdo_date=`date -d \"-1 day\" +%F`  \nfi \n\nsql=\"\n\nwith  \ntmp_order as\n(\n    select \n        user_id, \n        sum(oi.total_amount) order_amount, \n        count(*)  order_count\n    from \"$APP\".dwd_order_info  oi\n    where date_format(oi.create_time,'yyyy-MM-dd')='$do_date'\n    group by user_id\n)  ,\ntmp_payment as\n(\n    select \n        user_id, \n        sum(pi.total_amount) payment_amount, \n        count(*) payment_count \n    from \"$APP\".dwd_payment_info pi \n    where date_format(pi.payment_time,'yyyy-MM-dd')='$do_date'\n    group by user_id\n),\ntmp_comment as\n(  \n    select  \n        user_id, \n        count(*) comment_count\n    from \"$APP\".dwd_comment_log c\n    where date_format(c.dt,'yyyy-MM-dd')='$do_date'\n    group by user_id \n)\n\nInsert overwrite table \"$APP\".dws_user_action partition(dt='$do_date')\nselect \n    user_actions.user_id, \n    sum(user_actions.order_count), \n    sum(user_actions.order_amount),\n    sum(user_actions.payment_count), \n    sum(user_actions.payment_amount),\n    sum(user_actions.comment_count) \nfrom\n(\n    select\n        user_id,\n        order_count,\n        order_amount,\n        0 payment_count,\n        0 payment_amount,\n        0 comment_count\n    from tmp_order\n\n    union all\n    select\n        user_id,\n        0 order_count,\n        0 order_amount,\n        payment_count,\n        payment_amount,\n        0 comment_count\n    from tmp_payment\n\n    union all\n    select\n        user_id,\n        0 order_count,\n        0 order_amount,\n        0 payment_count,\n        0 payment_amount,\n        comment_count \n    from tmp_comment\n ) user_actions\ngroup by user_id;\n\"\n\n$hive -e \"$sql\"\n\n```\n\n## 6. GWV 成交总额\n\n![image-20191204170348604](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204170348604.png)\n\n```sql\ndrop table if exists gmall.ads_gmv_sum_day;\ncreate external table gmall.ads_gmv_sum_day(\n `dt` string COMMENT '统计日期',\n `gmv_count` bigint COMMENT '当日gmv订单个数',\n `gmv_amount` decimal(16,2) COMMENT '当日gmv订单总金额',\n `gmv_payment` decimal(16,2) COMMENT '当日支付金额'\n) COMMENT 'GMV'\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ads/ads_gmv_sum_day/'\n;\n```\n\n```sql\ninsert into table gmall.ads_gmv_sum_day\nselect \n'2019-02-10' dt,\n    sum(order_count) gmv_count,\n    sum(order_amount) gmv_amount,\n    sum(payment_amount) payment_amount \nfrom gmall.dws_user_action\nwhere dt ='2019-02-10'\ngroup by dt\n;\n\n```\n\n```sh\n#!/bin/bash\n\n# 定义变量方便修改\nAPP=gmall\nhive=/kfly/install/hive-1.1.0-cdh5.14.2/bin/hive\n\n# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天\nif [ -n \"$1\" ] ;then\n\tdo_date=$1\nelse \n\tdo_date=`date -d \"-1 day\" +%F`\nfi \n\nsql=\"\ninsert into table \"$APP\".ads_gmv_sum_day \nselect \n    '$do_date' dt,\n    sum(order_count)  gmv_count,\n    sum(order_amount) gmv_amount,\n    sum(payment_amount) payment_amount \nfrom \"$APP\".dws_user_action \nwhere dt ='$do_date'\ngroup by dt;\n\"\n\n$hive -e \"$sql\"\n\n```\n\n## 7. 转化率之用户新鲜度及漏斗分析  \n\n![image-20191204170729077](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204170729077.png)\n\n### 7.1  ADS层之新增用户占日活跃用户比率（用户新鲜度）  \n\n![image-20191204171248249](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204171248249.png)\n\n```sql\ndrop table if exists gmall.ads_user_convert_day;\ncreate external table gmall.ads_user_convert_day( \n    `dt` string COMMENT '统计日期',\n    `uv_m_count`  bigint COMMENT '当日活跃设备',\n    `new_m_count`  bigint COMMENT '当日新增设备',\n    `new_m_ratio`   decimal(10,2) COMMENT '当日新增占日活的比率'\n) COMMENT '转化率'\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ads/ads_user_convert_day/'\n;\n\n\ninsert into table gmall.ads_user_convert_day\nselect\n    '2019-02-10',\n    sum(uc.dc) sum_dc,\n    sum(uc.nmc) sum_nmc,\n    cast(sum( uc.nmc)/sum( uc.dc)*100 as decimal(10,2))  new_m_ratio\nfrom \n(\n    select\n        day_count dc,\n        0 nmc\n    from gmall.ads_uv_count\nwhere dt='2019-02-10'\n\n    union all\n    select\n        0 dc,\n        new_mid_count nmc\n    from gmall.ads_new_mid_count\n    where create_date='2019-02-10'\n)uc;\n\n```\n\n### 7.2  ADS层之用户行为漏斗分析  \n\n![image-20191204171353833](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204171353833.png)\n\n![image-20191204171418617](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204171418617.png)\n\n```sql\ndrop table if exists gmall.ads_user_action_convert_day;\ncreate external  table gmall.ads_user_action_convert_day(\n    `dt` string COMMENT '统计日期',\n    `total_visitor_m_count`  bigint COMMENT '总访问人数',\n    `order_u_count` bigint     COMMENT '下单人数',\n    `visitor2order_convert_ratio`  decimal(10,2) COMMENT '访问到下单转化率',\n    `payment_u_count` bigint     COMMENT '支付人数',\n    `order2payment_convert_ratio` decimal(10,2) COMMENT '下单到支付的转化率'\n ) COMMENT '用户行为漏斗分析'\nrow format delimited  fields terminated by '\\t'\nlocation '/warehouse/gmall/ads/ads_user_action_convert_day/'\n;\n\n\ninsert into table gmall.ads_user_action_convert_day\nselect \n    '2019-02-10',\n    uv.day_count,\n    ua.order_count,\n    cast(ua.order_count/uv.day_count as  decimal(10,2)) visitor2order_convert_ratio,\n    ua.payment_count,\n    cast(ua.payment_count/ua.order_count as  decimal(10,2)) order2payment_convert_ratio\nfrom  \n(\nselect \n    dt,\n        sum(if(order_count>0,1,0)) order_count,\n        sum(if(payment_count>0,1,0)) payment_count\n    from gmall.dws_user_action\nwhere dt='2019-02-10'\ngroup by dt\n)ua join gmall.ads_uv_count  uv on uv.dt=ua.dt\n;\n\n```\n\n## 8. 品牌复购率\n\n### 8.1 复购率分析\n\n![image-20191204171732545](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204171732545.png)\n\n### 8.2 DWS层\n\n#### 8.2.1 用户购买商品明细表（宽表）\n\n![image-20191204171651319](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204171651319.png)\n\n```sql\ndrop table if exists gmall.dws_sale_detail_daycount;\ncreate external table gmall.dws_sale_detail_daycount\n(   \n    user_id   string  comment '用户 id',\n    sku_id    string comment '商品 Id',\n    user_gender  string comment '用户性别',\n    user_age string  comment '用户年龄',\n    user_level string comment '用户等级',\n    order_price decimal(10,2) comment '商品价格',\n    sku_name string   comment '商品名称',\n    sku_tm_id string   comment '品牌id',\n    sku_category3_id string comment '商品三级品类id',\n    sku_category2_id string comment '商品二级品类id',\n    sku_category1_id string comment '商品一级品类id',\n    sku_category3_name string comment '商品三级品类名称',\n    sku_category2_name string comment '商品二级品类名称',\n    sku_category1_name string comment '商品一级品类名称',\n    spu_id  string comment '商品 spu',\n    sku_num  int comment '购买个数',\n    order_count string comment '当日下单单数',\n    order_amount string comment '当日下单金额'\n) COMMENT '用户购买商品明细表'\nPARTITIONED BY (`dt` string)\nstored as parquet\nlocation '/warehouse/gmall/dws/dws_user_sale_detail_daycount/'\ntblproperties (\"parquet.compression\"=\"snappy\");\n\n\nwith\ntmp_detail as\n(\n    select\n        user_id,\n        sku_id, \n        sum(sku_num) sku_num,   \n        count(*) order_count, \n        sum(od.order_price*sku_num) order_amount\n    from gmall.dwd_order_detail od\n    where od.dt='2019-02-10'\n    group by user_id, sku_id\n)  \ninsert overwrite table gmall.dws_sale_detail_daycount partition(dt='2019-02-10')\nselect \n    tmp_detail.user_id,\n    tmp_detail.sku_id,\n    u.gender,\n    months_between('2019-02-10', u.birthday)/12  age, \n    u.user_level,\n    price,\n    sku_name,\n    tm_id,\n    category3_id,\n    category2_id,\n    category1_id,\n    category3_name,\n    category2_name,\n    category1_name,\n    spu_id,\n    tmp_detail.sku_num,\n    tmp_detail.order_count,\n    tmp_detail.order_amount \nfrom tmp_detail \nleft join gmall.dwd_user_info u on tmp_detail.user_id =u.id and u.dt='2019-02-10'\nleft join gmall.dwd_sku_info s on tmp_detail.sku_id =s.id and s.dt='2019-02-10'\n;\n```\n\n\n\n### 8.3 ADS层品牌复购率\n\n![image-20191204172657826](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204172657826.png)\n\n```sql\ndrop table if exists gmall.ads_sale_tm_category1_stat_mn;\ncreate external table gmall.ads_sale_tm_category1_stat_mn\n(   \n    tm_id string comment '品牌id',\n    category1_id string comment '1级品类id ',\n    category1_name string comment '1级品类名称 ',\n    buycount   bigint comment  '购买人数',\n    buy_twice_last bigint  comment '两次以上购买人数',\n    buy_twice_last_ratio decimal(10,2)  comment  '单次复购率',\n    buy_3times_last   bigint comment   '三次以上购买人数',\n    buy_3times_last_ratio decimal(10,2)  comment  '多次复购率',\n    stat_mn string comment '统计月份',\n    stat_date string comment '统计日期' \n)   COMMENT '复购率统计'\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ads/ads_sale_tm_category1_stat_mn/'\n;\n\n\ninsert into table gmall.ads_sale_tm_category1_stat_mn\nselect   \n    mn.sku_tm_id,\n    mn.sku_category1_id,\n    mn.sku_category1_name,\n    sum(if(mn.order_count>=1,1,0)) buycount,\n    sum(if(mn.order_count>=2,1,0)) buyTwiceLast,\n    sum(if(mn.order_count>=2,1,0))/sum( if(mn.order_count>=1,1,0)) buyTwiceLastRatio,\n    sum(if(mn.order_count>=3,1,0))  buy3timeLast  ,\n    sum(if(mn.order_count>=3,1,0))/sum( if(mn.order_count>=1,1,0)) buy3timeLastRatio ,\n    date_format('2019-02-10' ,'yyyy-MM') stat_mn,\n    '2019-02-10' stat_date\nfrom \n(\nselect \n        user_id, \nsd.sku_tm_id,\n        sd.sku_category1_id,\n        sd.sku_category1_name,\n        sum(order_count) order_count\n    from gmall.dws_sale_detail_daycount sd \n    where date_format(dt,'yyyy-MM')=date_format('2019-02-10' ,'yyyy-MM')\n    group by user_id, sd.sku_tm_id, sd.sku_category1_id, sd.sku_category1_name\n) mn\ngroup by mn.sku_tm_id, mn.sku_category1_id, mn.sku_category1_name\n\n```\n\n## 9. 数据可视化表\n\n```sql\n-- 2. 每日活跃统计\nDROP TABLE IF EXISTS `ads_uv_count`;\nCREATE TABLE `ads_uv_count`  (\n  `dt` varchar(255) DEFAULT NULL COMMENT '统计日期',\n  `day_count` bigint(200) DEFAULT NULL COMMENT '当日用户数量',\n  `wk_count` bigint(200) DEFAULT NULL COMMENT '当周用户数量',\n  `mn_count` bigint(200) DEFAULT NULL COMMENT '当月用户数量',\n  `is_weekend` varchar(200) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT 'Y,N是否是周末,用于得到本周最终结果',\n  `is_monthend` varchar(200) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT 'Y,N是否是月末,用于得到本月最终结果'\n) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '每日活跃用户数量' ROW_FORMAT = Dynamic;\n\n-- 2. 留存率统计\nDROP TABLE IF EXISTS `ads_user_retention_day_rate`;\nCREATE TABLE `ads_user_retention_day_rate`  (\n  `stat_date` varchar(255)  DEFAULT NULL COMMENT '统计日期',\n  `create_date` varchar(255) DEFAULT NULL COMMENT '设备新增日期',\n  `retention_day` bigint(200) DEFAULT NULL COMMENT '截止当前日期留存天数',\n  `retention_count` bigint(200) DEFAULT NULL COMMENT '留存数量',\n  `new_mid_count` bigint(200) DEFAULT NULL COMMENT '当日设备新增数量',\n  `retention_ratio` decimal(10, 2) DEFAULT NULL COMMENT '留存率'\n) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '每日用户留存情况' ROW_FORMAT = Dynamic;\n\n\n-- 3. 漏斗分析\nDROP TABLE IF EXISTS `ads_user_action_convert_day`;\nCREATE TABLE `ads_user_action_convert_day`  (\n  `dt` varchar(200) DEFAULT NULL COMMENT '统计日期',\n  `total_visitor_m_count` bigint(20) DEFAULT NULL COMMENT '总访问人数',\n  `order_u_count` bigint(20) DEFAULT NULL COMMENT '下单人数',\n  `visitor2order_convert_ratio` decimal(10, 2) DEFAULT NULL COMMENT '购物车到下单转化率',\n  `payment_u_count` bigint(20) DEFAULT NULL COMMENT '支付人数',\n  `order2payment_convert_ratio` decimal(10, 2) DEFAULT NULL COMMENT '下单到支付的转化率'\n) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '每日用户行为转化率统计' ROW_FORMAT = Dynamic;\n\n\n-- 4. gmv统计\nDROP TABLE IF EXISTS ads_gmv_sum_day;\nCREATE TABLE ads_gmv_sum_day(\n  `dt` varchar(200) DEFAULT NULL COMMENT '统计日期',\n  `gmv_count` bigint(20) DEFAULT NULL COMMENT '当日gmv订单个数',\n  `gmv_amount` decimal(16, 2) DEFAULT NULL COMMENT '当日gmv订单总金额',\n  `gmv_payment` decimal(16, 2) DEFAULT NULL COMMENT '当日支付金额'\n) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '每日活跃用户数量' ROW_FORMAT = Dynamic;\n\n-- 5. 全国商品销售\nDROP TABLE IF EXISTS `ads_gmv_sum_province`;\nCREATE TABLE `ads_gmv_sum_province`  (\n  `province` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,\n  `gmv` bigint(255) DEFAULT NULL,\n  `remark` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL\n) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;\n\n\n```\n\n## 10.sqoop导出脚本\n\n```sh\n#!/bin/bash\n\ndb_name=gmall\nexport_data(){\n/kkb/install/sqoop-1.4.6-cdh5.14.2/bin/sqoop export \\\n--connect \"jdbc:mysql://node03:3306/${db_name}?useUnicode=true&characterEncoding=utf-8&useSSL=false\" \\\n--username root \\\n--password 123456 \\\n--table $1 \\\n--m 1 \\\n--export-dir /warehouse/$db_name/ads/$1 \\\n--input-fields-terminated-by \"\\t\" \\\n--update-mode allowinsert \\\n--update-key  \"gmv_count,gmv_amount,gmv_payment\" \\\n--input-null-string  '\\\\N' \\\n--input-null-non-string  '\\\\N' \n}\n\ncase $1 in \n  \"ads_uv_count\")\n   export_data \"ads_uv_count\"\n;;\n \"ads_user_action_convert_day\")\n   export_data \"ads_user_action_convert_day\"\n;;\n \"ads_gmv_sum_day\")\n   export_data \"ads_gmv_sum_day\"\n;;\n \"all\")\n   export_data \"ads_uv_count\" \n   export_data \"ads_user_action_convert_day\"\n   export_data \"ads_gmv_sum_day\"\n;;\nesac\n\n```\n\n-  --update-mode：\n  - updateonly  只更新，无法插入新数据\n  - allowinsert  允许新增 \n\n-  --update-key：允许更新的情况下，指定哪些字段匹配视为同一条数据，进行更新而不增加。多个字段用逗号分隔。\n\n-  --input-null-string和--input-null-non-string：\n\n分别表示，将字符串列和非字符串列的空串和“null”转换成'\\\\N'。\n\nHive中的在底层是以“”来存储，而中的在底层就是，--input-null-string--input-null-non-string--null-string--null-non-string\n\n![image-20191204174225224](https://kfly.top/picture/kfly-top/数仓商城项目之-系统业务数仓/assets//image-20191204174225224.png)","tags":["数仓项目"]},{"title":"数仓商城项目-用户行为数仓","url":"/2019/11/29/it/project/数仓商城项目-用户行为数仓/","content":"\n# 电商数仓项目\n\n## 1. 数据仓库的概念\n\n![image-20191129162305823](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191129162305823.png)\n\n## 2. 项目需求和架构设计\n\n### 2.1 项目需求分析\n\n![image-20191129162437944](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191129162437944.png)\n\n###  2.2 项目框架\n\n#### 2.2.1 项目技术选型\n\n<img src=\"http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191129162531041.png\" alt=\"image-20191129162531041\" style=\"zoom:150%;\" />\n\n#### 2.2.2 系统数据流程设计\n\n![image-20191129162635970](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191129162635970.png)\n\n#### 2.2.3 框架版本选型\n\n<img src=\"http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191129162705449.png\" alt=\"image-20191129162705449\" style=\"zoom: 150%;\" />\n\n<img src=\"http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191129162732861.png\" alt=\"image-20191129162732861\" style=\"zoom:150%;\" />\n\n#### 2.2.4 服务器选型\n\n<img src=\"http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191129162835568.png\" alt=\"image-20191129162835568\" style=\"zoom:150%;\" />\n\n#### 2.2.5 集群资源规划\n\n1. 如何确认集群规模 ？（假设每台服务器8T磁盘空间，128G内存）\n   （1）每天日活跃用户100w，每人一天平均100条：100w * 100条 =10000w条\n   （2）每条日志1KB左右，每天1亿条：100000000 / 1024 /1024= 约100G\n   （3）半年内不扩容服务器来算： 100G* 180天= 约18T\n   （4）保存3份副本：18T * 3 =54T\n   （5）预留20%-30%Buf = 54T/0.7= 77T\n   （6）算到这： 约  8T * 10 台服务器\n\n2. 如果考虑数仓分层？\n\n     每一层会生成大量的中间结果表，服务器将近再扩容1-2 倍\n\n#### 2.2.6 测试集群服务器规划\n\n| 服务名称              | 子服务           | 服务器 Node01 | 服务器 Node02 | 服务器 Node03 |\n| --------------------- | ---------------- | ------------- | ------------- | ------------- |\n| HDFS                  | NameNode         | √             |               |               |\n| DataNode              | √                | √             | √             |               |\n| SecondaryNameNode     | √                |               |               |               |\n| Yarn                  | NodeManager      | √             | √             | √             |\n| Resourcemanager       | √                |               |               |               |\n| Zookeeper             | Zookeeper Server | √             | √             | √             |\n| Flume(采集日志)       | Flume            | √             | √             | √             |\n| Hive                  | Hive             |               |               | √             |\n| MySQL                 | MySQL            |               |               | √             |\n| Sqoop                 | Sqoop            |               |               | √             |\n| Azkaban               | AzkabanWebServer |               |               | √             |\n| AzkabanExecutorServer |                  |               | √             |               |\n| 服务数总计            |                  | 7             | 4             | 9             |\n\n## 3. 数据生成模块\n\n### 3.1 埋点数据的基本格式\n\n- 公共字段：基本所有安卓手机都包含的字段\n- 业务字段：埋点上报的字段，有具体的业务类型\n\n- 下面就是一个示例，表示业务字段的上传。\n\n```json\n {\n    \"ap\":\"xxxxx\",//项目数据来源 app pc\n    \"cm\": {  //公共字段\n            \"mid\": \"\",  // (String) 设备唯一标识\n            \"uid\": \"\",  // (String) 用户标识\n            \"vc\": \"1\",  // (String) versionCode，程序版本号\n            \"vn\": \"1.0\",  // (String) versionName，程序版本名\n            \"l\": \"zh\",  // (String) 系统语言\n            \"sr\": \"\",  // (String) 渠道号，应用从哪个渠道来的。\n            \"os\": \"7.1.1\",  // (String) Android系统版本\n            \"ar\": \"CN\",  // (String) 区域\n            \"md\": \"BBB100-1\",  // (String) 手机型号\n            \"ba\": \"blackberry\",  // (String) 手机品牌\n            \"sv\": \"V2.2.1\",  // (String) sdkVersion\n            \"g\": \"\",  // (String) gmail\n            \"hw\": \"1620x1080\",  // (String) heightXwidth，屏幕宽高\n            \"t\": \"1506047606608\",  // (String) 客户端日志产生时的时间\n            \"nw\": \"WIFI\",  // (String) 网络模式\n            \"ln\": 0,  // (double) lng经度\n            \"la\": 0  // (double) lat 纬度\n        },\n    \"et\":  [  //事件\n                {\n                    \"ett\": \"1506047605364\",  //客户端事件产生时间\n                    \"en\": \"display\",  //事件名称\n                    \"kv\": {  //事件结果，以key-value形式自行定义\n                        \"goodsid\": \"236\",\n                        \"action\": \"1\",\n                        \"extend1\": \"1\",\n                        \"place\": \"2\",\n                        \"category\": \"75\"\n                    }\n                }\n            ]\n    }\n```\n\n- 示例日志（服务器时间戳 | 日志）\n\n```json\n1540934156385|{\n    \"ap\": \"gmall\", \n    \"cm\": {\n        \"uid\": \"1234\", \n        \"vc\": \"2\", \n        \"vn\": \"1.0\", \n        \"la\": \"EN\", \n        \"sr\": \"\", \n        \"os\": \"7.1.1\", \n        \"ar\": \"CN\", \n        \"md\": \"BBB100-1\", \n        \"ba\": \"blackberry\", \n        \"sv\": \"V2.2.1\", \n        \"g\": \"abc@gmail.com\", \n        \"hw\": \"1620x1080\", \n        \"t\": \"1506047606608\", \n        \"nw\": \"WIFI\", \n        \"ln\": 0\n    }, \n        \"et\": [\n            {\n                \"ett\": \"1506047605364\",  //客户端事件产生时间\n                \"en\": \"display\",  //事件名称\n                \"kv\": {  //事件结果，以key-value形式自行定义\n                    \"goodsid\": \"236\",\n                    \"action\": \"1\",\n                    \"extend1\": \"1\",\n                    \"place\": \"2\",\n                    \"category\": \"75\"\n                }\n            },{\n                \"ett\": \"1552352626835\",\n                \"en\": \"active_background\",\n                \"kv\": {\n                     \"active_source\": \"1\"\n                }\n            }\n        ]\n    }\n}\n```\n\n### 3.2 事件日志数据\n\n#### 3.2.1 商品列表页（loading）\n\n| 标签         | 含义                                                         |\n| ------------ | ------------------------------------------------------------ |\n| action       | 动作：开始加载=1，加载成功=2，加载失败=3                     |\n| loading_time | **加载时长：计算下拉开始到接口返回数据的时间，（开始加载报****0****，加载成功或加载失败才上报时间）** |\n| loading_way  | 加载类型：1-读取缓存，2-从接口拉新数据  （加载成功才上报加载类型） |\n| extend1      | **扩展字段** **Extend1**                                     |\n| extend2      | 扩展字段 Extend2                                             |\n| type         | **加载类型：自动加载****=1****，用户下拽加载****=2****，底部加载****=3****（底部条触发点击底部提示条****/****点击返回顶部加载）** |\n| type1        | 加载失败码：把加载失败状态码报回来（报空为加载成功，没有失败） |\n\n#### 3.2.2 商品点击（display）\n\n- 事件标签：display\n\n| 标签     | 含义                                               |\n| -------- | -------------------------------------------------- |\n| action   | 动作：曝光商品=1，点击商品=2，                     |\n| goodsid  | 商品ID（服务端下发的ID）                           |\n| place    | 顺序（第几条商品，第一条为0，第二条为1，如此类推） |\n| extend1  | 曝光类型：1 - 首次曝光 2-重复曝光                  |\n| category | 分类ID（服务端定义的分类ID）                       |\n\n#### 3.2.3 商品详情页（NewsDetail）\n\n- 事件标签：newsdetail\n\n| 标签          | 含义                                                         |\n| ------------- | ------------------------------------------------------------ |\n| entry         | 页面入口来源：应用首页=1、push=2、详情页相关推荐=3           |\n| action        | 动作：开始加载=1，加载成功=2（pv），加载失败=3, 退出页面=4   |\n| goodsid       | 商品ID（服务端下发的ID）                                     |\n| show_style    | 商品样式：0、无图、1、一张大图、2、两张图、3、三张小图、4、一张小图、5、一张大图两张小图 |\n| news_staytime | 页面停留时长：从商品开始加载时开始计算，到用户关闭页面所用的时间。若中途用跳转到其它页面了，则暂停计时，待回到详情页时恢复计时。或中途划出的时间超过10分钟，则本次计时作废，不上报本次数据。如未加载成功退出，则报空。 |\n| loading_time  | 加载时长：计算页面开始加载到接口返回数据的时间 （开始加载报0，加载成功或加载失败才上报时间） |\n| type1         | 加载失败码：把加载失败状态码报回来（报空为加载成功，没有失败） |\n| category      | 分类ID（服务端定义的分类ID）                                 |\n\n#### 3.2.4 广告（Ad）\n\n- 事件名称：ad\n\n| 标签       | 含义                                                         |\n| ---------- | ------------------------------------------------------------ |\n| entry      | 入口：商品列表页=1 应用首页=2 商品详情页=3                   |\n| action     | 动作：请求广告=1 取缓存广告=2 广告位展示=3 广告展示=4 广告点击=5 |\n| content    | 状态：成功=1 失败=2                                          |\n| detail     | 失败码（没有则上报空）                                       |\n| source     | 广告来源:admob=1 facebook=2 ADX（百度）=3 VK（俄罗斯）=4     |\n| behavior   | 用户行为：  主动获取广告=1   被动获取广告=2                  |\n| newstype   | Type: 1- 图文 2-图集 3-段子 4-GIF 5-视频 6-调查 7-纯文 8-视频+图文 9-GIF+图文 0-其他 |\n| show_style | 内容样式：无图(纯文字)=6 一张大图=1 三站小图+文=4 一张小图=2 一张大图两张小图+文=3 图集+文 = 5   一张大图+文=11  GIF大图+文=12 视频(大图)+文 = 13  来源于详情页相关推荐的商品，上报样式都为0（因为都是左文右图） |\n\n#### 3.2.5 消息通知（notification）\n\n- 事件标签：notification\n\n| 标签    | 含义                                                         |\n| ------- | ------------------------------------------------------------ |\n| action  | 动作：通知产生=1，通知弹出=2，通知点击=3，常驻通知展示（不重复上报，一天之内只报一次）=4 |\n| type    | 通知id：预警通知=1，天气预报（早=2，晚=3），常驻=4           |\n| ap_time | 客户端弹出时间                                               |\n| content | 备用字段                                                     |\n\n#### 3.2.6 用户前台活跃(active_foreground)\n\n- 事件标签: active_foreground\n\n| 标签    | 含义                                         |\n| ------- | -------------------------------------------- |\n| push_id | 推送的消息的id，如果不是从推送消息打开，传空 |\n| access  | 1.push 2.icon 3.其他                         |\n\n#### 3.2.7 用户后台活跃(active_background)\n\n- 事件标签: active_background\n\n| 标签          | 含义                                        |\n| ------------- | ------------------------------------------- |\n| active_source | 1=upgrade,2=download(下载),3=plugin_upgrade |\n\n#### 3.2.8 评论（comment）\n\n- 描述： 评论表\n\n| **序号** | **字段名称** | **字段描述**                              | **字段类型** | **长度** | **允许空** | **缺省值** |\n| -------- | ------------ | ----------------------------------------- | ------------ | -------- | ---------- | ---------- |\n| 1        | comment_id   | 评论表                                    | int          | 10,0     |            |            |\n| 2        | userid       | 用户id                                    | int          | 10,0     | √          | 0          |\n| 3        | p_comment_id | 父级评论id(为0则是一级评论,不为0则是回复) | int          | 10,0     | √          |            |\n| 4        | content      | 评论内容                                  | string       | 1000     | √          |            |\n| 5        | addtime      | 创建时间                                  | string       |          | √          |            |\n| 6        | other_id     | 评论的相关id                              | int          | 10,0     | √          |            |\n| 7        | praise_count | 点赞数量                                  | int          | 10,0     | √          | 0          |\n| 8        | reply_count  | 回复数量                                  | int          | 10,0     | √          | 0          |\n\n#### 3.2.9 收藏（favorites）\n\n- 描述：收藏\n\n| **序号** | **字段名称** | **字段描述** | **字段类型** | **长度** | **允许空** | **缺省值** |\n| -------- | ------------ | ------------ | ------------ | -------- | ---------- | ---------- |\n| 1        | id           | 主键         | int          | 10,0     |            |            |\n| 2        | course_id    | 商品id       | int          | 10,0     | √          | 0          |\n| 3        | userid       | 用户ID       | int          | 10,0     | √          | 0          |\n| 4        | add_time     | 创建时间     | string       |          | √          |            |\n\n#### 3.2.10 点赞（praise）\n\n- 描述：所有的点赞表\n\n| **序号** | **字段名称** | **字段描述**                                            | **字段类型** | **长度** | **允许空** | **缺省值** |\n| -------- | ------------ | ------------------------------------------------------- | ------------ | -------- | ---------- | ---------- |\n| 1        | id           | 主键id                                                  | int          | 10,0     |            |            |\n| 2        | userid       | 用户id                                                  | int          | 10,0     | √          |            |\n| 3        | target_id    | 点赞的对象id                                            | int          | 10,0     | √          |            |\n| 4        | type         | 点赞类型 1问答点赞 2问答评论点赞 3 文章点赞数4 评论点赞 | int          | 10,0     | √          |            |\n| 5        | add_time     | 添加时间                                                | string       |          | √          |            |\n\n#### 3.2.11错误日志(error)\n\n| errorBrief  | 错误摘要 |\n| ----------- | -------- |\n| errorDetail | 错误详情 |\n\n#### 3.2.12 **启动日志数据**(startlog)\n\n- 事件标签: start action=1可以算成前台活跃\n\n| 标签         | 含义                                                         |\n| ------------ | ------------------------------------------------------------ |\n| entry        | 入口： push=1，widget=2，icon=3，notification=4, lockscreen_widget =5 |\n| open_ad_type | 开屏广告类型: 开屏原生广告=1, 开屏插屏广告=2                 |\n| action       | 状态：成功=1 失败=2                                          |\n| loading_time | 加载时长：计算下拉开始到接口返回数据的时间，（开始加载报0，加载成功或加载失败才上报时间） |\n| detail       | 失败码（没有则上报空）                                       |\n| extend1      | 失败的message（没有则上报空）                                |\n| en           | 日志类型start                                                |\n\n###  3.3 数据生成脚本\n\n![image-20191129165236180](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191129165236180.png)\n\n#### 3.3.1 java代码\n\n​\t\t[点击查看](https://github.com/orchid-ding/hadoop-example/tree/master/topkfly)\n\n### 3.4 数据采集模块\n\n#### 3.4.1 生成日志脚本\n\n- node01,node02节点分别生成日志\n\n```shell\n#! /bin/bash\nfor i in node01 node02\ndo\nssh $i \"source /etc/profile;java -classpath /kfly/topkfly/log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar appclient.AppMain > /kfly/topkfly/run.log\"\ndone\n```\n\n#### 3.4.2 日志采集Flume\n\n![image-20191129170733892](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191129170733892.png)\n\n#### 3.4.3 集群规划\n\n|                 | 服务器node01 | 服务器node02 | 服务器node03 |\n| --------------- | ------------ | ------------ | ------------ |\n| Flume(采集日志) | Flume        | Flume        | Flume        |\n\n#### 3.4.4 经验之谈\n\n1. Source\n   - Taildir Source相比Exec Source、Spooling Directory Source的优势\n\n     ​\tTailDir Source：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。\n\n     ​\tExec Source可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。\n\nSpooling Directory Source监控目录，不支持断点续传。\n\n2. batchSize大小如何设置？\n\n   ​\tEvent 1K左右时，500-1000合适（默认为100）\n\n3. Channel\n\n   ​\t保证数据的安全可靠，使用类型file，把数据缓存在磁盘中。\n\n## 4. Flume分类ETL和拦截器\n\n### 4.1 java代码\n\n​\t[点击查看](https://github.com/orchid-ding/hadoop-example/tree/master/topkfly)\n\n### 4.2 日志采集\n\n#### 4.2.1Flume配置分析\n\n- Flume直接读log日志的数据，log日志的格式是app-yyyy-mm-dd.log。\n\n![image-20191129172516667](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191129172516667.png)\n\n#### 4.2.2 具体配置如下\n\n- node01，node02\n\n  ```properties\n  # flume client配置\n  a1.sources=r1\n  a1.sinks=s1\n  a1.channels=c1\n  \n  # 配置sources\n  a1.sources.r1.type=taildir\n  a1.sources.r1.positionFile=/kfly/topkfly/index/log_position.json\n  a1.sources.r1.filegroups=f1\n  a1.sources.r1.filegroups.f1=/kfly/topkfly/logs/app.+\n  a1.sources.r1.fileHeader=true\n  a1.sources.r1.channels=c1\n  \n  # interceptors\n  a1.sources.r1.interceptors=i1 i2\n  a1.sources.r1.interceptors.i1.type=intercaptor.LogETLInterceptor$Builder\n  a1.sources.r1.interceptors.i2.type=intercaptor.LogTypeInterceptor$Builder\n  \n  # 配置channels\n  a1.channels.c1.type=file\n  a1.channels.c1.type.checkpointDir=/kfly/topkfly/flume_checkPoint\n  a1.channels.c1.dataDirs=/kfly/topkfly/flume_data\n  \n  # 配置sink\n  a1.sinks.s1.channel = c1\n  a1.sinks.s1.type=avro\n  # node03\n  a1.sinks.s1.hostname=node03\n  a1.sinks.s1.port=5211\n  ```\n\n- node03\n\n  ```properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  #配置source\n  a1.sources.r1.type = avro\n  a1.sources.r1.bind = node03\n  a1.sources.r1.port = 5211\n  a1.sources.r1.channels = c1\n  \n  #配置channel\n  a1.channels.c1.type = file\n  #检查点文件目录\n  a1.channels.c1.checkpointDir=/kfly/topkfly/flume_checkpoint\n  #缓存数据文件夹\n  a1.channels.c1.dataDirs=/kfly/topkfly/lume_data\n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  a1.sinks.k1.type = hdfs\n  a1.sinks.k1.hdfs.path = hdfs://node01:8020/origin_data/gmall/log/%{topic}/%Y-%m-%d\n  a1.sinks.k1.hdfs.filePrefix = logevent-\n  a1.sinks.k1.hdfs.round = true\n  a1.sinks.k1.hdfs.roundValue = 10\n  a1.sinks.k1.hdfs.roundUnit = second\n  \n  #不要产生大量小文件\n  a1.sinks.k1.hdfs.rollInterval = 10\n  a1.sinks.k1.hdfs.rollSize = 134217728\n  a1.sinks.k1.hdfs.rollCount = 1000\n  a1.sinks.k1.hdfs.useLocalTimeStamp = true\n  a1.sinks.k1.hdfs.minBlockReplicas=1\n  a1.sinks.k1.hdfs.fileType = CompressedStream\n  a1.sinks.k1.hdfs.codeC = lzop\n  ```\n\n#### 4.2.3 修改hadoop配置文件\n\n- 支持lzo压缩\n\n```shell\n # haddoop-lzo.jar \t放入hadoop目录\n scp hadoop-lzo-0.4.20.jar /kfly/install/hadoop-2.6.0/share/hadoop/common\n```\n\n\n\n- 修改hadoop 的core-site.xml\n\n```xml\n<property>\n        <name>io.compression.codecs</name>\n        <value>org.apache.hadoop.io.compress.GzipCodec,\n                org.apache.hadoop.io.compress.DefaultCodec,\n                org.apache.hadoop.io.compress.BZip2Codec,\n                com.hadoop.compression.lzo.LzoCodec,\n                com.hadoop.compression.lzo.LzopCodec\n        </value>\n</property>\n<property>\n        <name>io.compression.codec.lzo.class</name>\n        <value>com.hadoop.compression.lzo.LzoCodec</value>\n</property>\n```\n\n#### 4.2.4 Flume启动脚本\n\n```sh\n\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-hdfs.conf -Dflume.root.logger=info,console\n\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-client.conf -Dflume.root.logger=info,console\n\n\nssh node01 \"source /etc/profile; ps -ef | grep flume | grep -v grep |awk '{print \\$2}' | xargs kill\"\nssh node02 \"source /etc/profile; ps -ef | grep flume | grep -v grep |awk '{print \\$2}' | xargs kill\"\nssh node03\n```\n\n```sh\n#! /bin/bash\ncase $1 in\n\"start\"){\n        for i in node03 node02 node01\n    do\n                        echo \"------------启动 $i 采集日志，flume-------------\"\n                        if [ \"node03\" = $i ]; then\n                                ssh $i \"source /etc/profile; nohup /kfly/install/apache-flume-1.6.0-cdh5.14.2-bin/bin/flume-ng agent -n a1 -c /kfly/install/apache-flume-1.6.0-cdh5.14.2-bin/myconf -f /kfly/install/apache-flume-1.6.0-cdh5.14.2-bin/myconf/flume-hdfs.conf -Dflume.root.logger=info,console > /dev/null 2 >&1 &\"\n                                else\n                                ssh $i \"source /etc/profile; nohup /kfly/install/apache-flume-1.6.0-cdh5.14.2-bin/bin/flume-ng agent -n a1 -c /kfly/install/apache-flume-1.6.0-cdh5.14.2-bin/myconf -f /kfly/install/apache-flume-1.6.0-cdh5.14.2-bin/myconf/flume-client.conf -Dflume.root.logger=info,console > /dev/null 2 >&1 &\"\n                                fi\n    done\n};;\n\"stop\"){\n        for i in node01 node02 node03\n                do\n                        echo \"------------停止 $i 采集日志，flume-------------\"\n                        ssh $i \"source /etc/profile; ps -ef | grep flume | grep -v grep |awk '{print \\$2}' | xargs kill\"\n                done\n};;\nesac\n```\n\n## 5 数据仓库分层\n\n ### 5.1 为什么要分层\n\n![image-20191201172234918](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191201172234918.png)  \n\n### 5.2 数仓分层\n\n![image-20191201172314576](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191201172314576.png)\n\n![image-20191201172349777](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191201172349777.png)\n\n### 5.3 数据集市与数据仓库概念\n\n![image-20191201172510618](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191201172510618.png)\n\n## 6. 数仓搭建之ODS层\n\n​\t\t\t[创建数据库sql](https://github.com/orchid-ding/hadoop-example/blob/master/topkfly/db/create-table-hive.sql)\n\n![image-20191201172659677](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191201172659677.png)\n\n### 6.1  Shell中单引号和双引号区别  \n\n```sh\n# 1. sh 如下\n#!/bin/bash\ndo_date=$1\n\necho '$do_date'   # 单引号不取变量值\necho \"$do_date\"   # 双引号取变量值\necho \"'$do_date'\"\t# 双引号内部嵌套单引号，取出变量值\necho '\"$do_date\"'\t# 单引号内部嵌套双引号，不取出变量值\necho `date`\t\t\t\t# （3）反引号`，执行引号中命令\n\n# 2. 运行\nsh test.sh 2019-10-21\n\n# 3. 结果\n$do_date\n2019-10-21\n'2019-10-21'\n\"$do_date\"\nTue Oct 22 18:33:40 CST 2019\n```\n\n\n\n### 6.1 创建数据库ODS层\n\n- 原始数据层，存放原始数据，直接加载原始日志、数据，数据保持原貌不做处理。\n\n#### 6.1.1 创建启动日志ods_start_log  \n\n```sql\n--1.  建表\ndrop table if exists gmall.ods_start_log;\nCREATE EXTERNAL TABLE gmall.ods_start_log (`line` string)\n    PARTITIONED BY (`dt` string)\n    STORED AS\n        INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'\n        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n    LOCATION '/warehouse/gmall/ods/ods_start_log';\n\nload data inpath '/origin_data/gmall/log/topic_start/2019-11-18' into table gmall.ods_start_log partition(dt='2019-11-18');\n\n-- 2. 加载数据\n```\n\n#### 6.1.2 创建事件日志表ods_event_log\n\n```sql\n-- 1. 建表\ndrop table if exists gmall.ods_event_log;\nCREATE EXTERNAL TABLE gmall.ods_event_log\n(`line` string)\n    PARTITIONED BY (`dt` string)\n    STORED AS\n        INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'\n        OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n    LOCATION '/warehouse/gmall/ods/ods_event_log';\n\n-- 2. 加载数据\nload data inpath '/origin_data/gmall/log/topic_event/2019-11-18' into table gmall.ods_event_log partition(dt='2019-11-18');\n```\n\n#### 6.1.3 ods加载数据脚本\n\n```sh\n#!/bin/bash\n\n#  定义变量方便修改\nAPP=gmall\nhive=/kfly/install/hive-1.1.0-cdh5.14.2/bin/hive\n\n#  如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天\n# -n 判断是否为空\nif  [  -n  \"$1\"  ]  ;then\n        do_date=$1\nelse\n        # shellcheck disable=SC2006\n        do_date=`date  -d  \"-1  day\"  +%F`\nfi\necho \"--------日志日期为 $do_date ----------\"\nsql=\"\nload data inpath '/origin_data/gmall/log/topic_start/$do_date' into table \"$APP\".ods_start_log partition(dt='$do_date');\nload data inpath '/origin_data/gmall/log/topic_event/$do_date' into table \"$APP\".ods_event_log partition(dt='$do_date');\"\n\n$hive  -e  \"$sql\"\n```\n\n### 6.2 创建数据库DWD层\n\n- 对ODS层数据进行清洗（去除空值，脏数据，超过极限范围的数据，行式存储改为列存储，改压缩格式）  \n\n#### 6.2.1 DWD层启动表\n\n```sql\n-- 1. 创建启动表\ndrop  table  if  exists  gmall.dwd_start_log;\nCREATE  EXTERNAL  TABLE  gmall.dwd_start_log(\n                                             `mid_id`  string,\n                                             `user_id`  string,\n                                             `version_code`  string,\n                                             `version_name`  string,\n                                             `lang`  string,\n                                             `source`  string,\n                                             `os`  string,\n                                             `area`  string,\n                                             `model`  string,\n                                             `brand`  string,\n                                             `sdk_version`  string,\n                                             `gmail`  string,\n                                             `height_width`  string,\n                                             `app_time`  string,\n                                             `network`  string,\n                                             `lng`  string,\n                                             `lat`  string,\n                                             `entry`  string,\n                                             `open_ad_type`  string,\n                                             `action`  string,\n                                             `loading_time`  string,\n                                             `detail`  string,\n                                             `extend1`  string\n)\n    PARTITIONED  BY  (dt  string)\n    location  '/warehouse/gmall/dwd/dwd_start_log/';\n    \n-- 2. 加载数据\ninsert  overwrite  table  gmall.dwd_start_log\nPARTITION  (dt='2019-11-18')\nselect\n    get_json_object(line,'$.mid')  mid_id,\n    get_json_object(line,'$.uid')  user_id,\n    get_json_object(line,'$.vc')  version_code,\n    get_json_object(line,'$.vn')  version_name,\n    get_json_object(line,'$.l')  lang,\n    get_json_object(line,'$.sr')  source,\n    get_json_object(line,'$.os')  os,\n    get_json_object(line,'$.ar')  area,\n    get_json_object(line,'$.md')  model,\n    get_json_object(line,'$.ba')  brand,\n    get_json_object(line,'$.sv')  sdk_version,\n    get_json_object(line,'$.g')  gmail,\n    get_json_object(line,'$.hw')  height_width,\n    get_json_object(line,'$.t')  app_time,\n    get_json_object(line,'$.nw')  network,\n    get_json_object(line,'$.ln')  lng,\n    get_json_object(line,'$.la')  lat,\n    get_json_object(line,'$.entry')  entry,\n    get_json_object(line,'$.open_ad_type')  open_ad_type,\n    get_json_object(line,'$.action')  action,\n    get_json_object(line,'$.loading_time')  loading_time,\n    get_json_object(line,'$.detail')  detail,\n    get_json_object(line,'$.extend1')  extend1\nfrom  gmall.ods_start_log\nwhere  dt='2019-11-18';\n```\n\n```sh\n# 3. 加载数据脚本\n#!/bin/bash\n\n# 定义变量方便修改\nAPP=gmall\nhive=/kfly/install/hive-1.1.0-cdh5.14.2/bin/hive\n\n# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天\nif [ -n \"$1\" ] ;then\n    do_date=$1\nelse \n    # shellcheck disable=SC2006\n    do_date=`date -d \"-1 day\" +%F`\nfi \n\nsql=\"\ninsert overwrite table \"$APP\".dwd_start_log\nPARTITION (dt='$do_date')\nselect \n    get_json_object(line,'$.mid') mid_id,\n    get_json_object(line,'$.uid') user_id,\n    get_json_object(line,'$.vc') version_code,\n    get_json_object(line,'$.vn') version_name,\n    get_json_object(line,'$.l') lang,\n    get_json_object(line,'$.sr') source,\n    get_json_object(line,'$.os') os,\n    get_json_object(line,'$.ar') area,\n    get_json_object(line,'$.md') model,\n    get_json_object(line,'$.ba') brand,\n    get_json_object(line,'$.sv') sdk_version,\n    get_json_object(line,'$.g') gmail,\n    get_json_object(line,'$.hw') height_width,\n    get_json_object(line,'$.t') app_time,\n    get_json_object(line,'$.nw') network,\n    get_json_object(line,'$.ln') lng,\n    get_json_object(line,'$.la') lat,\n    get_json_object(line,'$.entry') entry,\n    get_json_object(line,'$.open_ad_type') open_ad_type,\n    get_json_object(line,'$.action') action,\n    get_json_object(line,'$.loading_time') loading_time,\n    get_json_object(line,'$.detail') detail,\n    get_json_object(line,'$.extend1') extend1\nfrom \"$APP\".ods_start_log \nwhere dt='$do_date';\n\"\n\n$hive -e \"$sql\"\n```\n\n#### 6.2.2 DWD层事件表数据解析\n\n-   明细表用于存储ODS层原始表转换过来的明细数据  \n\n##### 6.2.2.1 创建事件基础明细表\n\n```sql\n-- 1. 建表\ndrop  table  if  exists  gmall.dwd_base_event_log;\nCREATE  EXTERNAL  TABLE  gmall.dwd_base_event_log(\n                                                  `mid_id`  string,\n                                                  `user_id`  string,\n                                                  `version_code`  string,\n                                                  `version_name`  string,\n                                                  `lang`  string,\n                                                  `source`  string,\n                                                  `os`  string,\n                                                  `area`  string,\n                                                  `model`  string,\n                                                  `brand`  string,\n                                                  `sdk_version`  string,\n                                                  `gmail`  string,\n                                                  `height_width`  string,\n                                                  `app_time`  string,\n                                                  `network`  string,\n                                                  `lng`  string,\n                                                  `lat`  string,\n                                                  `event_name`  string,\n                                                  `event_json`  string,\n                                                  `server_time`  string)\n    PARTITIONED  BY  (`dt`  string)\n    stored  as  parquet\n    location  '/warehouse/gmall/dwd/dwd_base_event_log/';\n```\n\n##### 6.2.2.2 自定义UDF函数，解析（公公字段）\n\n​\t\t[代码](https://github.com/orchid-ding/hadoop-example/blob/master/topkfly/hive-function/src/main/java/udf/BaseFieldUDF.java)\n\n- 自定义拦截器，解析数据\n  - UDF拦截器实现公公字段解析\n\n![image-20191201210713207](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191201210713207.png)\n\n```java\npackage udf;\n\nimport org.apache.commons.lang.StringUtils;\nimport org.apache.hadoop.hive.ql.exec.UDF;\nimport org.json.JSONException;\nimport org.json.JSONObject;\n\n/**\n * @author dingchuangshi\n */\npublic class BaseFieldUDF extends UDF {\n    public String evaluate(String line, String jsonkeysString) {\n        // 0 准备一个sb\n        StringBuilder sb = new StringBuilder();\n        // 1 切割jsonkeys  mid uid vc vn l sr os ar md\n        String[] jsonkeys = jsonkeysString.split(\",\");\n        // 2 处理line   服务器时间 | json\n        String[] logContents = line.split(\"\\\\|\");\n        // 3 合法性校验\n        if (logContents.length != 2 || StringUtils.isBlank(logContents[1])) {\n            return \"\";\n        }\n        // 4 开始处理json\n        try {\n            JSONObject jsonObject = new JSONObject(logContents[1]);\n            // 获取cm里面的对象\n            JSONObject base = jsonObject.getJSONObject(\"cm\");\n            // 循环遍历取值\n            for (int i = 0; i < jsonkeys.length; i++) {\n                String filedName = jsonkeys[i].trim();\n                if (base.has(filedName)) {\n                    sb.append(base.getString(filedName)).append(\"\\t\");\n                } else {\n                    sb.append(\"\\t\");\n                }\n            }\n\n            sb.append(jsonObject.getString(\"et\")).append(\"\\t\");\n            sb.append(logContents[0]).append(\"\\t\");\n        } catch (JSONException e) {\n            e.printStackTrace();\n        }\n        return sb.toString();\n    }\n}\n\n```\n\n##### 6.2.2.3 自定义UDTF函数字段，（解析具体事件）\n\n​\t\t[代码](https://github.com/orchid-ding/hadoop-example/blob/master/topkfly/hive-function/src/main/java/udtf/EventJsonUDTF.java)\n\n![image-20191201211614119](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191201211614119.png)\n\n```java\npublic class EventJsonUDTF extends GenericUDTF {\n\n    /**\n     * 该方法中，我们将指定输出参数的名称和参数类型：\n     */\n    @Override\n    public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {\n\n        ArrayList<String> fieldNames = new ArrayList<String>();\n        ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();\n        fieldNames.add(\"event_name\");\n        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);\n        fieldNames.add(\"event_json\");\n        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);\n\n        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);\n    }\n\n    /**\n     * 输入1条记录，输出若干条结果\n     * @param objects\n     * @throws HiveException\n     */\n    @Override\n    public void process(Object[] objects) throws HiveException {\n        // 获取传入的et\n        String input = objects[0].toString();\n        // 如果传进来的数据为空，直接返回过滤掉该数据\n        if (StringUtils.isBlank(input)) {\n            return;\n        } else {\n\n            try {\n                // 获取一共有几个事件（ad/facoriters）\n                JSONArray ja = new JSONArray(input);\n\n                if (ja == null)\n                    return;\n\n                // 循环遍历每一个事件\n                for (int i = 0; i < ja.length(); i++) {\n                    //数组的第一个元素用于保存事件名称，第二个元素用于保存事件的信息\n                    String[] result = new String[2];\n\n                    try {\n                        // 取出每个的事件名称（ad/facoriters）\n                        result[0] = ja.getJSONObject(i).getString(\"en\");\n\n                        // 取出每一个事件整体\n                        result[1] = ja.getString(i);\n                    } catch (JSONException e) {\n                        continue;\n                    }\n\n                    // 将结果返回\n                    forward(result);\n                }\n            } catch (JSONException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n    //当没有记录处理的时候该方法会被调用，用来清理代码或者产生额外的输出\n    @Override\n    public void close() throws HiveException {\n    }\n}\n```\n\n##### 6.2.2.4 使用udf & udtf函数\n\n```sql\n-- 将jar 添加到hive\nadd jar /kfly/install/hive-1.1.0-cdh5.14.2/lib/hive-function-1.0-SNAPSHOT.jar;\n--创建函数 udf\ncreate temporary function base_analizer as 'udf.BaseFieldUDF';\n-- 创建udtf函数\ncreate temporary function flat_analizer as 'udtf.EventJsonUDTF';\n```\n\n##### 6.2.2.5 解析事件日志到明细表\n\n```sql\ninsert overwrite table gmall.dwd_base_event_log\n    PARTITION (dt='2019-02-12')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    event_name,\n    event_json,\n    server_time\nfrom\n    (\n        select\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[0]   as mid_id,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[1]   as user_id,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[2]   as version_code,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[3]   as version_name,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[4]   as lang,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[5]   as source,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[6]   as os,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[7]   as area,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[8]   as model,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[9]   as brand,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[10]   as sdk_version,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[11]  as gmail,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[12]  as height_width,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[13]  as app_time,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[14]  as network,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[15]  as lng,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[16]  as lat,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[17]  as ops,\n            split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\\t')[18]  as server_time\n        from gmall.ods_event_log where dt='2019-02-12'  and base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la')<>''\n    ) sdk_log lateral view flat_analizer(ops) tmp_k as event_name, event_json;\n\n```\n\n​\t[脚本](https://github.com/orchid-ding/hadoop-example/blob/master/topkfly/db/dwd_base_event_log.sh)\n\n#### 6.2.3 DWD层事件表\n\n##### 6.2.3.1 商品点击表\n\n```sql\ndrop  table  if  exists  gmall.dwd_display_log;\nCREATE  EXTERNAL  TABLE  gmall.dwd_display_log(\n                                               `mid_id`  string,\n                                               `user_id`  string,\n                                               `version_code`  string,\n                                               `version_name`  string,\n                                               `lang`  string,\n                                               `source`  string,\n                                               `os`  string,\n                                               `area`  string,\n                                               `model`  string,\n                                               `brand`  string,\n                                               `sdk_version`  string,\n                                               `gmail`  string,\n                                               `height_width`  string,\n                                               `app_time`  string,\n                                               `network`  string,\n                                               `lng`  string,\n                                               `lat`  string,\n                                               `action`  string,\n                                               `goodsid`  string,\n                                               `place`  string,\n                                               `extend1`  string,\n                                               `category`  string,\n                                               `server_time`  string\n)\n    PARTITIONED  BY  (dt  string)\n    location  '/warehouse/gmall/dwd/dwd_display_log/';\n\ninsert  overwrite  table  gmall.dwd_display_log\nPARTITION  (dt='2019-11-18')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    get_json_object(event_json,'$.kv.action')  action,\n    get_json_object(event_json,'$.kv.goodsid')  goodsid,\n    get_json_object(event_json,'$.kv.place')  place,\n    get_json_object(event_json,'$.kv.extend1')  extend1,\n    get_json_object(event_json,'$.kv.category')  category,\n    server_time\nfrom  gmall.dwd_base_event_log\nwhere  dt='2019-11-18'  and  event_name='display';\n```\n\n##### 6.2.3.2 商品详情页表\n\n```sql\ndrop  table  if  exists  gmall.dwd_newsdetail_log;\nCREATE  EXTERNAL  TABLE  gmall.dwd_newsdetail_log(\n                                                  `mid_id`  string,\n                                                  `user_id`  string,\n                                                  `version_code`  string,\n                                                  `version_name`  string,\n                                                  `lang`  string,\n                                                  `source`  string,\n                                                  `os`  string,\n                                                  `area`  string,\n                                                  `model`  string,\n                                                  `brand`  string,\n                                                  `sdk_version`  string,\n                                                  `gmail`  string,\n                                                  `height_width`  string,\n                                                  `app_time`  string,\n                                                  `network`  string,\n                                                  `lng`  string,\n                                                  `lat`  string,\n                                                  `entry`  string,\n                                                  `action`  string,\n                                                  `goodsid`  string,\n                                                  `showtype`  string,\n                                                  `news_staytime`  string,\n                                                  `loading_time`  string,\n                                                  `type1`  string,\n                                                  `category`  string,\n                                                  `server_time`  string)\n    PARTITIONED  BY  (dt  string)\n    location  '/warehouse/gmall/dwd/dwd_newsdetail_log/';\n\ninsert  overwrite  table  gmall.dwd_newsdetail_log\n    PARTITION  (dt='2019-11-18')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    get_json_object(event_json,'$.kv.entry')  entry,\n    get_json_object(event_json,'$.kv.action')  action,\n    get_json_object(event_json,'$.kv.goodsid')  goodsid,\n    get_json_object(event_json,'$.kv.showtype')  showtype,\n    get_json_object(event_json,'$.kv.news_staytime')  news_staytime,\n    get_json_object(event_json,'$.kv.loading_time')  loading_time,\n    get_json_object(event_json,'$.kv.type1')  type1,\n    get_json_object(event_json,'$.kv.category')  category,\n    server_time\nfrom  gmall.dwd_base_event_log\nwhere  dt='2019-11-18'  and  event_name='newsdetail';\n```\n\n##### 6.2.3.3 广告表\n```sql\n##### drop table if exists gmall.dwd_ad_log;\nCREATE EXTERNAL TABLE gmall.dwd_ad_log(\n                                          `mid_id` string,\n                                          `user_id` string,\n                                          `version_code` string,\n                                          `version_name` string,\n                                          `lang` string,\n                                          `source` string,\n                                          `os` string,\n                                          `area` string,\n                                          `model` string,\n                                          `brand` string,\n                                          `sdk_version` string,\n                                          `gmail` string,\n                                          `height_width` string,\n                                          `app_time` string,\n                                          `network` string,\n                                          `lng` string,\n                                          `lat` string,\n                                          `entry` string,\n                                          `action` string,\n                                          `content` string,\n                                          `detail` string,\n                                          `ad_source` string,\n                                          `behavior` string,\n                                          `newstype` string,\n                                          `show_style` string,\n                                          `server_time` string)\n    PARTITIONED BY (dt string)\n    location '/warehouse/gmall/dwd/dwd_ad_log/';\n\ninsert overwrite table gmall.dwd_ad_log\n    PARTITION (dt='2019-11-18')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    get_json_object(event_json,'$.kv.entry') entry,\n    get_json_object(event_json,'$.kv.action') action,\n    get_json_object(event_json,'$.kv.content') content,\n    get_json_object(event_json,'$.kv.detail') detail,\n    get_json_object(event_json,'$.kv.source') ad_source,\n    get_json_object(event_json,'$.kv.behavior') behavior,\n    get_json_object(event_json,'$.kv.newtypes') newstype,\n    get_json_object(event_json,'$.kv.show_style') show_style,\n    server_time\nfrom gmall.dwd_base_event_log\nwhere dt='2019-11-18' and event_name='ad';\n```\n\n##### 6.2.3.4 消息通知表\n\n```sql\ndrop table if exists gmall.dwd_notification_log;\nCREATE EXTERNAL TABLE gmall.dwd_notification_log(\n                                                    `mid_id` string,\n                                                    `user_id` string,\n                                                    `version_code` string,\n                                                    `version_name` string,\n                                                    `lang` string,\n                                                    `source` string,\n                                                    `os` string,\n                                                    `area` string,\n                                                    `model` string,\n                                                    `brand` string,\n                                                    `sdk_version` string,\n                                                    `gmail` string,\n                                                    `height_width` string,\n                                                    `app_time` string,\n                                                    `network` string,\n                                                    `lng` string,\n                                                    `lat` string,\n                                                    `action` string,\n                                                    `noti_type` string,\n                                                    `ap_time` string,\n                                                    `content` string,\n                                                    `server_time` string\n)\n    PARTITIONED BY (dt string)\n    location '/warehouse/gmall/dwd/dwd_notification_log/';\n\n\n-- 2）导入数据\ninsert overwrite table gmall.dwd_notification_log\n    PARTITION (dt='2019-11-18')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    get_json_object(event_json,'$.kv.action') action,\n    get_json_object(event_json,'$.kv.noti_type') noti_type,\n    get_json_object(event_json,'$.kv.ap_time') ap_time,\n    get_json_object(event_json,'$.kv.content') content,\n    server_time\nfrom gmall.dwd_base_event_log\nwhere dt='2019-11-18' and event_name='notification';\n\n\nselect * from gmall.dwd_notification_log limit 2;\n```\n\n##### 6.2.3.5 商品列表页面\n\n```sql\ndrop table if exists gmall.dwd_loading_log;\nCREATE EXTERNAL TABLE gmall.dwd_loading_log(\n                                               `mid_id` string,\n                                               `user_id` string,\n                                               `version_code` string,\n                                               `version_name` string,\n                                               `lang` string,\n                                               `source` string,\n                                               `os` string,\n                                               `area` string,\n                                               `model` string,\n                                               `brand` string,\n                                               `sdk_version` string,\n                                               `gmail` string,\n                                               `height_width` string,\n                                               `app_time` string,\n                                               `network` string,\n                                               `lng` string,\n                                               `lat` string,\n                                               `action` string,\n                                               `loading_time` string,\n                                               `loading_way` string,\n                                               `extend1` string,\n                                               `extend2` string,\n                                               `type` string,\n                                               `type1` string,\n                                               `server_time` string)\n    PARTITIONED BY (dt string)\n    location '/warehouse/gmall/dwd/dwd_loading_log/';\n\ninsert overwrite table gmall.dwd_loading_log\n    PARTITION (dt='2019-11-18')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    get_json_object(event_json,'$.kv.action') action,\n    get_json_object(event_json,'$.kv.loading_time') loading_time,\n    get_json_object(event_json,'$.kv.loading_way') loading_way,\n    get_json_object(event_json,'$.kv.extend1') extend1,\n    get_json_object(event_json,'$.kv.extend2') extend2,\n    get_json_object(event_json,'$.kv.type') type,\n    get_json_object(event_json,'$.kv.type1') type1,\n    server_time\nfrom gmall.dwd_base_event_log\nwhere dt='2019-11-18' and event_name='loading';\n\n select * from gmall.dwd_loading_log limit 2;\n```\n\n##### 6.2.3.6 用户后台活跃度\n\n```sql\ndrop table if exists gmall.dwd_active_background_log;\nCREATE EXTERNAL TABLE gmall.dwd_active_background_log(\n                                                         `mid_id` string,\n                                                         `user_id` string,\n                                                         `version_code` string,\n                                                         `version_name` string,\n                                                         `lang` string,\n                                                         `source` string,\n                                                         `os` string,\n                                                         `area` string,\n                                                         `model` string,\n                                                         `brand` string,\n                                                         `sdk_version` string,\n                                                         `gmail` string,\n                                                         `height_width` string,\n                                                         `app_time` string,\n                                                         `network` string,\n                                                         `lng` string,\n                                                         `lat` string,\n                                                         `active_source` string,\n                                                         `server_time` string\n)\n    PARTITIONED BY (dt string)\n    location '/warehouse/gmall/dwd/dwd_background_log/';\n\ninsert overwrite table gmall.dwd_active_background_log\n    PARTITION (dt='2019-11-18')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    get_json_object(event_json,'$.kv.active_source') active_source,\n    server_time\nfrom gmall.dwd_base_event_log\nwhere dt='2019-11-18' and event_name='active_background';\n\nselect * from gmall.dwd_active_background_log limit 2;\n```\n\n##### 6.2.3.7 用户前台活跃表\n\n```sql\ndrop table if exists gmall.dwd_active_foreground_log;\nCREATE EXTERNAL TABLE gmall.dwd_active_foreground_log(\n                                                         `mid_id` string,\n                                                         `user_id` string,\n                                                         `version_code` string,\n                                                         `version_name` string,\n                                                         `lang` string,\n                                                         `source` string,\n                                                         `os` string,\n                                                         `area` string,\n                                                         `model` string,\n                                                         `brand` string,\n                                                         `sdk_version` string,\n                                                         `gmail` string,\n                                                         `height_width` string,\n                                                         `app_time` string,\n                                                         `network` string,\n                                                         `lng` string,\n                                                         `lat` string,\n                                                         `push_id` string,\n                                                         `access` string,\n                                                         `server_time` string)\n    PARTITIONED BY (dt string)\n    location '/warehouse/gmall/dwd/dwd_foreground_log/';\n\ninsert overwrite table gmall.dwd_active_foreground_log\n    PARTITION (dt='2019-11-18')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    get_json_object(event_json,'$.kv.push_id') push_id,\n    get_json_object(event_json,'$.kv.access') access,\n    server_time\nfrom gmall.dwd_base_event_log\nwhere dt='2019-11-18' and event_name='active_foreground';\n\nselect * from gmall.dwd_active_foreground_log limit 2;\n```\n\n##### 6.2.3.8 评论表\n\n```sql\ndrop  table  if  exists  gmall.dwd_comment_log;\nCREATE  EXTERNAL  TABLE  gmall.dwd_comment_log(\n                                               `mid_id`  string,\n                                               `user_id`  string,\n                                               `version_code`  string,\n                                               `version_name`  string,\n                                               `lang`  string,\n                                               `source`  string,\n                                               `os`  string,\n                                               `area`  string,\n                                               `model`  string,\n                                               `brand`  string,\n                                               `sdk_version`  string,\n                                               `gmail`  string,\n                                               `height_width`  string,\n                                               `app_time`  string,\n                                               `network`  string,\n                                               `lng`  string,\n                                               `lat`  string,\n                                               `comment_id`  int,\n                                               `userid`  int,\n                                               `p_comment_id`  int,\n                                               `content`  string,\n                                               `addtime`  string,\n                                               `other_id`  int,\n                                               `praise_count`  int,\n                                               `reply_count`  int,\n                                               `server_time`  string\n)\n    PARTITIONED  BY  (dt  string)\n    location  '/warehouse/gmall/dwd/dwd_comment_log/';\n\n\ninsert  overwrite  table  gmall.dwd_comment_log\n    PARTITION  (dt='2019-11-18')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    get_json_object(event_json,'$.kv.comment_id')  comment_id,\n    get_json_object(event_json,'$.kv.userid')  userid,\n    get_json_object(event_json,'$.kv.p_comment_id')  p_comment_id,\n    get_json_object(event_json,'$.kv.content')  content,\n    get_json_object(event_json,'$.kv.addtime')  addtime,\n    get_json_object(event_json,'$.kv.other_id')  other_id,\n    get_json_object(event_json,'$.kv.praise_count')  praise_count,\n    get_json_object(event_json,'$.kv.reply_count')  reply_count,\n    server_time\nfrom  gmall.dwd_base_event_log\nwhere  dt='2019-11-18'  and  event_name='comment';\n\nselect  *  from  gmall.dwd_comment_log  limit  2;\t\n```\n\n##### 6.2.3.9 收藏表\n\n```sql\ndrop  table  if  exists  gmall.dwd_favorites_log;\nCREATE  EXTERNAL  TABLE  gmall.dwd_favorites_log(\n                                                 `mid_id`  string,\n                                                 `user_id`  string,\n                                                 `version_code`  string,\n                                                 `version_name`  string,\n                                                 `lang`  string,\n                                                 `source`  string,\n                                                 `os`  string,\n                                                 `area`  string,\n                                                 `model`  string,\n                                                 `brand`  string,\n                                                 `sdk_version`  string,\n                                                 `gmail`  string,\n                                                 `height_width`  string,\n                                                 `app_time`  string,\n                                                 `network`  string,\n                                                 `lng`  string,\n                                                 `lat`  string,\n                                                 `id`  int,\n                                                 `course_id`  int,\n                                                 `userid`  int,\n                                                 `add_time`  string,\n                                                 `server_time`  string\n)\n    PARTITIONED  BY  (dt  string)\n    location  '/warehouse/gmall/dwd/dwd_favorites_log/';\n\ninsert  overwrite  table  gmall.dwd_favorites_log\n    PARTITION  (dt='2019-11-18')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    get_json_object(event_json,'$.kv.id')  id,\n    get_json_object(event_json,'$.kv.course_id')  course_id,\n    get_json_object(event_json,'$.kv.userid')  userid,\n    get_json_object(event_json,'$.kv.add_time')  add_time,\n    server_time\nfrom  gmall.dwd_base_event_log\nwhere  dt='2019-11-18'  and  event_name='favorites';\n\nselect  *  from  gmall.dwd_favorites_log  limit  2;\n```\n\n##### 6.2.3.10 点赞表\n\n```sql\ndrop  table  if  exists  gmall.dwd_praise_log;\nCREATE  EXTERNAL  TABLE  gmall.dwd_praise_log(\n                                              `mid_id`  string,\n                                              `user_id`  string,\n                                              `version_code`  string,\n                                              `version_name`  string,\n                                              `lang`  string,\n                                              `source`  string,\n                                              `os`  string,\n                                              `area`  string,\n                                              `model`  string,\n                                              `brand`  string,\n                                              `sdk_version`  string,\n                                              `gmail`  string,\n                                              `height_width`  string,\n                                              `app_time`  string,\n                                              `network`  string,\n                                              `lng`  string,\n                                              `lat`  string,\n                                              `id`  string,\n                                              `userid`  string,\n                                              `target_id`  string,\n                                              `type`  string,\n                                              `add_time`  string,\n                                              `server_time`  string\n)\n    PARTITIONED  BY  (dt  string)\n    location  '/warehouse/gmall/dwd/dwd_praise_log/';\n\ninsert  overwrite  table  gmall.dwd_praise_log\n    PARTITION  (dt='2019-11-18')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    get_json_object(event_json,'$.kv.id')  id,\n    get_json_object(event_json,'$.kv.userid')  userid,\n    get_json_object(event_json,'$.kv.target_id')  target_id,\n    get_json_object(event_json,'$.kv.type')  type,\n    get_json_object(event_json,'$.kv.add_time')  add_time,\n    server_time\nfrom  gmall.dwd_base_event_log\nwhere  dt='2019-11-18'  and  event_name='praise';\n\nselect  *  from  gmall.dwd_praise_log  limit  2;\n```\n\n##### 6.2.3.11 错误日志表\n\n```sql\ndrop  table  if  exists  gmall.dwd_error_log;\nCREATE  EXTERNAL  TABLE  gmall.dwd_error_log(\n                                             `mid_id`  string,\n                                             `user_id`  string,\n                                             `version_code`  string,\n                                             `version_name`  string,\n                                             `lang`  string,\n                                             `source`  string,\n                                             `os`  string,\n                                             `area`  string,\n                                             `model`  string,\n                                             `brand`  string,\n                                             `sdk_version`  string,\n                                             `gmail`  string,\n                                             `height_width`  string,\n                                             `app_time`  string,\n                                             `network`  string,\n                                             `lng`  string,\n                                             `lat`  string,\n                                             `errorBrief`  string,\n                                             `errorDetail`  string,\n                                             `server_time`  string)\n    PARTITIONED  BY  (dt  string)\n    location  '/warehouse/gmall/dwd/dwd_error_log/';\n\n\ninsert  overwrite  table  gmall.dwd_error_log\n    PARTITION  (dt='2019-11-18')\nselect\n    mid_id,\n    user_id,\n    version_code,\n    version_name,\n    lang,\n    source,\n    os,\n    area,\n    model,\n    brand,\n    sdk_version,\n    gmail,\n    height_width,\n    app_time,\n    network,\n    lng,\n    lat,\n    get_json_object(event_json,'$.kv.errorBrief')  errorBrief,\n    get_json_object(event_json,'$.kv.errorDetail')  errorDetail,\n    server_time\nfrom  gmall.dwd_base_event_log\nwhere  dt='2019-11-18'  and  event_name='error';\n\n select  *  from  gmall.dwd_error_log  limit  2;\n```\n\n## 7 业务说明\n\n### 7.1. 用户\n\n​\t\t用户以设备为判断标准，在移动统计中，每个独立设备认为是一个独立用户。Android系统根据IMEI号，IOS系统根据OpenUDID来标识一个独立用户，每部手机一个用户。\n\n### 7.2. 新增用户\n\n​\t\t首次联网使用应用的用户。如果一个用户首次打开某APP，那这个用户定义为新增用户；卸载再安装的设备，不会被算作一次新增。新增用户包括日新增用户、周新增用户、月新增用户。\n\n### 7.3. 活跃用户\n\n​\t\t打开应用的用户即为活跃用户，不考虑用户的使用情况。每天一台设备打开多次会被计为一个活跃用户。\n\n### 7.4. 周（月）活跃用户\n\n​\t\t某个自然周（月）内启动过应用的用户，该周（月）内的多次启动只记一个活跃用户。\n\n### 7.5. 月活跃率\n\n​\t\t月活跃用户与截止到该月累计的用户总和之间的比例。\n\n### 7.6. 沉默用户\n\n​\t\t用户仅在安装当天（次日）启动一次，后续时间无再启动行为。该指标可以反映新增用户质量和用户与APP的匹配程度。\n\n### 7.7. 版本分布\n\n​\t\t不同版本的周内各天新增用户数，活跃用户数和启动次数。利于判断APP各个版本之间的优劣和用户行为习惯。\n\n### 7.8. 本周回流用户\n\n​\t\t上周未启动过应用，本周启动了应用的用户。\n\n### 7.9.  连续n周活跃用户\n\n​\t\t连续n周，每周至少启动一次。\n\n### 7.10. 忠诚用户\n\n​\t\t连续活跃5周以上的用户\n\n### 7.11. 连续活跃用户\n\n​\t\t连续2周及以上活跃的用户\n\n### 7.12. 近期流失用户\n\n​\t\t连续n(2<= n <= 4)周没有启动应用的用户。（第n+1周没有启动过）\n\n### 7.13.  留存用户\n\n​\t\t某段时间内的新增用户，经过一段时间后，仍然使用应用的被认作是留存用户；这部分用户占当时新增用户的比例即是留存率。\n\n​\t\t例如: 5月份新增用户200，这200人在6月份启动过应用的有100人，7月份启动过应用的有80人，8月份启动过应用的有50人；则5月份新增用户一个月后的留存率是50%，二个月后的留存率是40%，三个月后的留存率是25%。\n\n### 7.14. 用户新鲜度\n\n​\t\t每天启动应用的新老用户比例，即新增用户数占活跃用户数的比例。\n\n### 7.15.  单次使用时长\n\n​\t\t每次启动使用的时间长度。\n\n### 7.16. 日使用时长\n\n​\t\t累计一天内的使用时间长度。\n\n### 7.17. 启动次数计算标准\n\n​\t\tIOS平台应用退到后台就算一次独立的启动；平台我们规定，两次启动之间的间隔小于秒，被计算一次启动。用户在使用过程中，若因收发短信或接电话等退出应用秒又再次返回应用中，那这两次行为应该是延续而非独立的，所以可以被算作一次使用行为，即一次启动。业内大多使用秒这个标准，但用户还是可以自定义此时间间隔。\n\n## 8. 用户需求\n\n### 8.1 用户活跃主题\n\n#### 8.1.1 DWS层\n\n##### 8.1.1.1 每日活跃设备明细\n\n```sql\n-- 1. 建表\ndrop table if exists gmall.dws_uv_detail_day;\ncreate external table gmall.dws_uv_detail_day\n(\n    `mid_id` string COMMENT '设备唯一标识',\n    `user_id` string COMMENT '用户标识',\n    `version_code` string COMMENT '程序版本号',\n    `version_name` string COMMENT '程序版本名',\n    `lang` string COMMENT '系统语言',\n    `source` string COMMENT '渠道号',\n    `os` string COMMENT '安卓系统版本',\n    `area` string COMMENT '区域',\n    `model` string COMMENT '手机型号',\n    `brand` string COMMENT '手机品牌',\n    `sdk_version` string COMMENT 'sdkVersion',\n    `gmail` string COMMENT 'gmail',\n    `height_width` string COMMENT '屏幕宽高',\n    `app_time` string COMMENT '客户端日志产生时的时间',\n    `network` string COMMENT '网络模式',\n    `lng` string COMMENT '经度',\n    `lat` string COMMENT '纬度'\n)\n    partitioned by(dt string)\n    stored as parquet\n    location '/warehouse/gmall/dws/dws_uv_detail_day';\n-- 2. 插入数据\ninsert overwrite table gmall.dws_uv_detail_day\n    partition(dt='2019-02-10')\nselect\n    mid_id,\n    concat_ws('|', collect_set(user_id)) user_id,\n    concat_ws('|', collect_set(version_code)) version_code,\n    concat_ws('|', collect_set(version_name)) version_name,\n    concat_ws('|', collect_set(lang))lang,\n    concat_ws('|', collect_set(source)) source,\n    concat_ws('|', collect_set(os)) os,\n    concat_ws('|', collect_set(area)) area,\n    concat_ws('|', collect_set(model)) model,\n    concat_ws('|', collect_set(brand)) brand,\n    concat_ws('|', collect_set(sdk_version)) sdk_version,\n    concat_ws('|', collect_set(gmail)) gmail,\n    concat_ws('|', collect_set(height_width)) height_width,\n    concat_ws('|', collect_set(app_time)) app_time,\n    concat_ws('|', collect_set(network)) network,\n    concat_ws('|', collect_set(lng)) lng,\n    concat_ws('|', collect_set(lat)) lat\nfrom gmall.dwd_start_log\nwhere dt='2019-02-10'\ngroup by mid_id;\n\nselect * from gmall.dws_uv_detail_day limit 1;\n```\n\n##### 8.1.1.2 周活跃设备\n\n```sql\ndrop table if exists gmall.dws_uv_detail_wk;\ncreate external table gmall.dws_uv_detail_wk(\n              `mid_id` string COMMENT '设备唯一标识',\n              `user_id` string COMMENT '用户标识',\n              `version_code` string COMMENT '程序版本号',\n              `version_name` string COMMENT '程序版本名',\n              `lang` string COMMENT '系统语言',\n              `source` string COMMENT '渠道号',\n              `os` string COMMENT '安卓系统版本',\n              `area` string COMMENT '区域',\n              `model` string COMMENT '手机型号',\n              `brand` string COMMENT '手机品牌',\n              `sdk_version` string COMMENT 'sdkVersion',\n              `gmail` string COMMENT 'gmail',\n              `height_width` string COMMENT '屏幕宽高',\n              `app_time` string COMMENT '客户端日志产生时的时间',\n              `network` string COMMENT '网络模式',\n              `lng` string COMMENT '经度',\n              `lat` string COMMENT '纬度',\n              `monday_date` string COMMENT '周一日期',\n              `sunday_date` string COMMENT  '周日日期'\n) COMMENT '活跃用户按周明细'\n    PARTITIONED BY (`wk_dt` string)\n    stored as parquet\n    location '/warehouse/gmall/dws/dws_uv_detail_wk/';\n\ninsert overwrite table gmall.dws_uv_detail_wk partition(wk_dt)\nselect\n    mid_id,\n    concat_ws('|', collect_set(user_id)) user_id,\n    concat_ws('|', collect_set(version_code)) version_code,\n    concat_ws('|', collect_set(version_name)) version_name,\n    concat_ws('|', collect_set(lang)) lang,\n    concat_ws('|', collect_set(source)) source,\n    concat_ws('|', collect_set(os)) os,\n    concat_ws('|', collect_set(area)) area,\n    concat_ws('|', collect_set(model)) model,\n    concat_ws('|', collect_set(brand)) brand,\n    concat_ws('|', collect_set(sdk_version)) sdk_version,\n    concat_ws('|', collect_set(gmail)) gmail,\n    concat_ws('|', collect_set(height_width)) height_width,\n    concat_ws('|', collect_set(app_time)) app_time,\n    concat_ws('|', collect_set(network)) network,\n    concat_ws('|', collect_set(lng)) lng,\n    concat_ws('|', collect_set(lat)) lat,\n    date_add(next_day('2019-02-10','MO'),-7),\n    date_add(next_day('2019-02-10','MO'),-1),\n    concat(date_add( next_day('2019-02-10','MO'),-7), '_' , date_add(next_day('2019-02-10','MO'),-1)\n        )\nfrom gmall.dws_uv_detail_day\nwhere dt>=date_add(next_day('2019-02-10','MO'),-7) and dt<=date_add(next_day('2019-02-10','MO'),-1)\ngroup by mid_id;\n\nselect * from gmall.dws_uv_detail_wk limit 1;\n```\n\n##### 8.1.1.3 月活跃设备\n\n```sql\ndrop table if exists gmall.dws_uv_detail_mn;\n\ncreate external table gmall.dws_uv_detail_mn(\n                `mid_id` string COMMENT '设备唯一标识',\n                `user_id` string COMMENT '用户标识',\n                `version_code` string COMMENT '程序版本号',\n                `version_name` string COMMENT '程序版本名',\n                `lang` string COMMENT '系统语言',\n                `source` string COMMENT '渠道号',\n                `os` string COMMENT '安卓系统版本',\n                `area` string COMMENT '区域',\n                `model` string COMMENT '手机型号',\n                `brand` string COMMENT '手机品牌',\n                `sdk_version` string COMMENT 'sdkVersion',\n                `gmail` string COMMENT 'gmail',\n                `height_width` string COMMENT '屏幕宽高',\n                `app_time` string COMMENT '客户端日志产生时的时间',\n                `network` string COMMENT '网络模式',\n                `lng` string COMMENT '经度',\n                `lat` string COMMENT '纬度'\n) COMMENT '活跃用户按月明细'\n    PARTITIONED BY (`mn` string)\n    stored as parquet\n    location '/warehouse/gmall/dws/dws_uv_detail_mn/';\n\ninsert overwrite table gmall.dws_uv_detail_mn partition(mn)\nselect\n    mid_id,\n    concat_ws('|', collect_set(user_id)) user_id,\n    concat_ws('|', collect_set(version_code)) version_code,\n    concat_ws('|', collect_set(version_name)) version_name,\n    concat_ws('|', collect_set(lang)) lang,\n    concat_ws('|', collect_set(source)) source,\n    concat_ws('|', collect_set(os)) os,\n    concat_ws('|', collect_set(area)) area,\n    concat_ws('|', collect_set(model)) model,\n    concat_ws('|', collect_set(brand)) brand,\n    concat_ws('|', collect_set(sdk_version)) sdk_version,\n    concat_ws('|', collect_set(gmail)) gmail,\n    concat_ws('|', collect_set(height_width)) height_width,\n    concat_ws('|', collect_set(app_time)) app_time,\n    concat_ws('|', collect_set(network)) network,\n    concat_ws('|', collect_set(lng)) lng,\n    concat_ws('|', collect_set(lat)) lat,\n    date_format('2019-02-10','yyyy-MM')\nfrom gmall.dws_uv_detail_day\nwhere date_format(dt,'yyyy-MM') = date_format('2019-02-10','yyyy-MM')\ngroup by mid_id;\n\nselect * from gmall.dws_uv_detail_mn limit 1;\n```\n\n\n\n##### 8.1.1.4 加载数据脚本\n\n```sh\n#!/bin/bash\n\n# 定义变量方便修改\nAPP=gmall\nhive=/kfly/install/hive-1.1.0-cdh5.14.2/bin/hive\n\n# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天\nif [ -n \"$1\" ] ;then\n    do_date=$1\nelse\n    # shellcheck disable=SC2006\n    do_date=`date -d \"-1 day\" +%F`\nfi\n\nsql=\"\n\t# 设置动态分区规则，非严谨\n  set hive.exec.dynamic.partition.mode=nonstrict;\n\n  insert overwrite table \"$APP\".dws_uv_detail_day partition(dt='$do_date')\n  select\n    mid_id,\n    concat_ws('|', collect_set(user_id)) user_id,\n    concat_ws('|', collect_set(version_code)) version_code,\n    concat_ws('|', collect_set(version_name)) version_name,\n    concat_ws('|', collect_set(lang)) lang,\n    concat_ws('|', collect_set(source)) source,\n    concat_ws('|', collect_set(os)) os,\n    concat_ws('|', collect_set(area)) area,\n    concat_ws('|', collect_set(model)) model,\n    concat_ws('|', collect_set(brand)) brand,\n    concat_ws('|', collect_set(sdk_version)) sdk_version,\n    concat_ws('|', collect_set(gmail)) gmail,\n    concat_ws('|', collect_set(height_width)) height_width,\n    concat_ws('|', collect_set(app_time)) app_time,\n    concat_ws('|', collect_set(network)) network,\n    concat_ws('|', collect_set(lng)) lng,\n    concat_ws('|', collect_set(lat)) lat\n  from \"$APP\".dwd_start_log\n  where dt='$do_date'\n  group by mid_id;\n\n  insert overwrite table \"$APP\".dws_uv_detail_wk partition(wk_dt)\n  select\n    mid_id,\n    concat_ws('|', collect_set(user_id)) user_id,\n    concat_ws('|', collect_set(version_code)) version_code,\n    concat_ws('|', collect_set(version_name)) version_name,\n    concat_ws('|', collect_set(lang)) lang,\n    concat_ws('|', collect_set(source)) source,\n    concat_ws('|', collect_set(os)) os,\n    concat_ws('|', collect_set(area)) area,\n    concat_ws('|', collect_set(model)) model,\n    concat_ws('|', collect_set(brand)) brand,\n    concat_ws('|', collect_set(sdk_version)) sdk_version,\n    concat_ws('|', collect_set(gmail)) gmail,\n    concat_ws('|', collect_set(height_width)) height_width,\n    concat_ws('|', collect_set(app_time)) app_time,\n    concat_ws('|', collect_set(network)) network,\n    concat_ws('|', collect_set(lng)) lng,\n    concat_ws('|', collect_set(lat)) lat,\n    date_add(next_day('$do_date','MO'),-7),\n    date_add(next_day('$do_date','MO'),-1),\n    concat(date_add( next_day('$do_date','MO'),-7), '_' , date_add(next_day('$do_date','MO'),-1)\n  )\n  from \"$APP\".dws_uv_detail_day\n  where dt>=date_add(next_day('$do_date','MO'),-7) and dt<=date_add(next_day('$do_date','MO'),-1)\n  group by mid_id;\n\n  insert overwrite table \"$APP\".dws_uv_detail_mn partition(mn)\n  select\n    mid_id,\n    concat_ws('|', collect_set(user_id)) user_id,\n    concat_ws('|', collect_set(version_code)) version_code,\n    concat_ws('|', collect_set(version_name)) version_name,\n    concat_ws('|', collect_set(lang))lang,\n    concat_ws('|', collect_set(source)) source,\n    concat_ws('|', collect_set(os)) os,\n    concat_ws('|', collect_set(area)) area,\n    concat_ws('|', collect_set(model)) model,\n    concat_ws('|', collect_set(brand)) brand,\n    concat_ws('|', collect_set(sdk_version)) sdk_version,\n    concat_ws('|', collect_set(gmail)) gmail,\n    concat_ws('|', collect_set(height_width)) height_width,\n    concat_ws('|', collect_set(app_time)) app_time,\n    concat_ws('|', collect_set(network)) network,\n    concat_ws('|', collect_set(lng)) lng,\n    concat_ws('|', collect_set(lat)) lat,\n    date_format('$do_date','yyyy-MM')\n  from \"$APP\".dws_uv_detail_day\n  where date_format(dt,'yyyy-MM') = date_format('$do_date','yyyy-MM')\n  group by mid_id;\n\"\n\n$hive -e \"$sql\"\n```\n\n####8.1.2 ADS层\n\n##### 8.1.2.1 当日、当周、当月活跃设备数\n\n\n```sql\n-- 1. 建表\ndrop table if exists gmall.ads_uv_count;\ncreate external table gmall.ads_uv_count( \n`dt` string COMMENT '统计日期',\n`day_count` bigint COMMENT '当日用户数量',\n`wk_count`  bigint COMMENT '当周用户数量',\n`mn_count`  bigint COMMENT '当月用户数量',\n`is_weekend` string COMMENT 'Y,N是否是周末,用于得到本周最终结果',\n`is_monthend` string COMMENT 'Y,N是否是月末,用于得到本月最终结果' \n) COMMENT '活跃设备数' \nrow format delimited fields terminated by '\\t' \nlocation '/warehouse/gmall/ads/ads_uv_count/';\n\n-- 2. 导入\ninsert into table gmall.ads_uv_count \nselect  \n  '2019-02-10' dt,\n   daycount.ct,\n   wkcount.ct,\n   mncount.ct,\n   if(date_add(next_day('2019-02-10','MO'),-1)='2019-02-10','Y','N') ,\n   if(last_day('2019-02-10')='2019-02-10','Y','N') \nfrom \n(\n   select  \n      '2019-02-10' dt,\n       count(*) ct\n   from gmall.dws_uv_detail_day\n   where dt='2019-02-10'  \n)daycount join \n( \n   select  \n     '2019-02-10' dt,\n     count (*) ct\n   from gmall.dws_uv_detail_wk\n   where wk_dt=concat(date_add(next_day('2019-02-10','MO'),-7),'_' ,date_add(next_day('2019-02-10','MO'),-1) )\n) wkcount on daycount.dt=wkcount.dt\njoin \n( \n   select  \n     '2019-02-10' dt,\n     count (*) ct\n   from gmall.dws_uv_detail_mn\n   where mn=date_format('2019-02-10','yyyy-MM')  \n)mncount on daycount.dt=mncount.dt;\n```\n\n##### 8.1.2.2 当周、日、月活跃脚本\n\n```sh\n#!/bin/bash\n\n#ads 当日、当月、当周活跃设备书\n# 定义变量方便修改\nAPP=gmall\nhive=/kfly/install/hive-1.1.0-cdh5.14.2/bin/hive\n\n# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天\n    if [ -n \"$1\" ] ;then\n    do_date=$1\nelse\n    # shellcheck disable=SC2006\n    do_date=`date -d \"-1 day\" +%F`\nfi\n\nsql=\"\n  set hive.exec.dynamic.partition.mode=nonstrict;\n\ninsert into table \"$APP\".ads_uv_count\nselect\n  '$do_date' dt,\n   daycount.ct,\n   wkcount.ct,\n   mncount.ct,\n   if(date_add(next_day('$do_date','MO'),-1)='$do_date','Y','N') ,\n   if(last_day('$do_date')='$do_date','Y','N')\nfrom\n(\n   select\n      '$do_date' dt,\n       count(*) ct\n   from \"$APP\".dws_uv_detail_day\n   where dt='$do_date'\n)daycount   join\n(\n   select\n     '$do_date' dt,\n     count (*) ct\n   from \"$APP\".dws_uv_detail_wk\n   where wk_dt=concat(date_add(next_day('$do_date','MO'),-7),'_' ,date_add(next_day('$do_date','MO'),-1) )\n)  wkcount  on daycount.dt=wkcount.dt\njoin\n(\n   select\n     '$do_date' dt,\n     count (*) ct\n   from \"$APP\".dws_uv_detail_mn\n   where mn=date_format('$do_date','yyyy-MM')\n)mncount on daycount.dt=mncount.dt;\n\"\n\n$hive -e \"$sql\"\n```\n\n### 8.2 用户新增主题\n\n- 首次联网使用应用的用户。如果一个用户首次打开某APP，那这个用户定义为新增用户；卸载再安装的设备，不会被算作一次新增。新增用户包括日新增用户、周新增用户、月新增用户。\n\n#### 8.2.1 DWS层 每日新增设备明细\n\n​        ![image-20191202131344855](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191202131344855.png)\n\n```sql\n-- 1.日新增设备明细\ndrop table if exists gmall.dws_new_mid_day;\ncreate external table gmall.dws_new_mid_day\n(\n    `mid_id` string COMMENT '设备唯一标识',\n    `user_id` string COMMENT '用户标识', \n    `version_code` string COMMENT '程序版本号', \n    `version_name` string COMMENT '程序版本名', \n    `lang` string COMMENT '系统语言', \n    `source` string COMMENT '渠道号', \n    `os` string COMMENT '安卓系统版本', \n    `area` string COMMENT '区域', \n    `model` string COMMENT '手机型号', \n    `brand` string COMMENT '手机品牌', \n    `sdk_version` string COMMENT 'sdkVersion', \n    `gmail` string COMMENT 'gmail', \n    `height_width` string COMMENT '屏幕宽高',\n    `app_time` string COMMENT '客户端日志产生时的时间',\n    `network` string COMMENT '网络模式',\n    `lng` string COMMENT '经度',\n    `lat` string COMMENT '纬度',\n    `create_date`  string  comment '创建时间' \n)  COMMENT '每日新增设备信息'\nstored as parquet\nlocation '/warehouse/gmall/dws/dws_new_mid_day/';\n\n\n-- 2. 导入数据\ninsert into table gmall.dws_new_mid_day\nselect  \n    ud.mid_id,\n    ud.user_id , \n    ud.version_code , \n    ud.version_name , \n    ud.lang , \n    ud.source, \n    ud.os, \n    ud.area, \n    ud.model, \n    ud.brand, \n    ud.sdk_version, \n    ud.gmail, \n    ud.height_width,\n    ud.app_time,\n    ud.network,\n    ud.lng,\n    ud.lat,\n    '2019-02-10'\nfrom gmall.dws_uv_detail_day ud left join gmall.dws_new_mid_day nm on ud.mid_id=nm.mid_id \nwhere ud.dt='2019-02-10' and nm.mid_id is null;\n```\n\n#### 8.2.2  ADS层 每日新增设备表\n\n```sql\ndrop table if exists gmall.ads_new_mid_count;\ncreate external table gmall.ads_new_mid_count\n(\n`create_date` string comment '创建时间' ,\n`new_mid_count` BIGINT comment '新增设备数量' \n)  COMMENT '每日新增设备信息数量' \nrow format delimited fields terminated by '\\t' \nlocation '/warehouse/gmall/ads/ads_new_mid_count/';\n\ninsert into table gmall.ads_new_mid_count \nselect\ncreate_date,\ncount(*)\nfrom gmall.dws_new_mid_day\nwhere create_date='2019-02-10'\ngroup by create_date;\n```\n\n#### 8.2.3 用户新增主题脚本\n\n```sh\n#!/bin/bash\n\n# 新增设备数\n# 定义变量方便修改\nAPP=gmall\nhive=/kfly/install/hive-1.1.0-cdh5.14.2/bin/hive\n\n# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天\n    if [ -n \"$1\" ] ;then\n    do_date=$1\nelse\n    # shellcheck disable=SC2006\n    do_date=`date -d \"-1 day\" +%F`\nfi\n\nsql=\"\ninsert into table gmall.dws_new_mid_day\nselect\n    ud.mid_id,\n    ud.user_id ,\n    ud.version_code ,\n    ud.version_name ,\n    ud.lang ,\n    ud.source,\n    ud.os,\n    ud.area,\n    ud.model,\n    ud.brand,\n    ud.sdk_version,\n    ud.gmail,\n    ud.height_width,\n    ud.app_time,\n    ud.network,\n    ud.lng,\n    ud.lat,\n    '$do_date'\nfrom gmall.dws_uv_detail_day ud left join gmall.dws_new_mid_day nm on ud.mid_id=nm.mid_id\nwhere ud.dt='$do_date' and nm.mid_id is null;\n\ninsert into table gmall.ads_new_mid_count\nselect\ncreate_date,\ncount(*)\nfrom gmall.dws_new_mid_day\nwhere create_date='$do_date'\ngroup by create_date;\n\"\n\n$hive -e \"$sql\"\n```\n\n\n\n### 8.3 用户留存主题\n\n#### 8.3.1 用户留存概念\n\n![image-20191202133738802](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191202133738802.png)\n\n#### 8.3.2 需求描述\n\n![image-20191202135432430](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191202135432430.png)\n\n#### 8.3.3 DWS层\n\n##### 8.3.3.1 每日留存用户明细表\n\n```sql\ndrop table if exists gmall.dws_user_retention_day;\ncreate external table gmall.dws_user_retention_day \n(\n    `mid_id` string COMMENT '设备唯一标识',\n    `user_id` string COMMENT '用户标识', \n    `version_code` string COMMENT '程序版本号', \n    `version_name` string COMMENT '程序版本名', \n    `lang` string COMMENT '系统语言', \n    `source` string COMMENT '渠道号', \n    `os` string COMMENT '安卓系统版本', \n    `area` string COMMENT '区域', \n    `model` string COMMENT '手机型号', \n    `brand` string COMMENT '手机品牌', \n    `sdk_version` string COMMENT 'sdkVersion', \n    `gmail` string COMMENT 'gmail', \n    `height_width` string COMMENT '屏幕宽高',\n    `app_time` string COMMENT '客户端日志产生时的时间',\n    `network` string COMMENT '网络模式',\n    `lng` string COMMENT '经度',\n    `lat` string COMMENT '纬度',\n   `create_date`    string  comment '设备新增时间',\n   `retention_day`  int comment '截止当前日期留存天数'\n)  COMMENT '每日用户留存情况'\nPARTITIONED BY (`dt` string)\nstored as parquet\nlocation '/warehouse/gmall/dws/dws_user_retention_day/'\n;\n\ninsert overwrite table gmall.dws_user_retention_day \npartition(dt=\"2019-02-11\") \nselect \n nm.mid_id,\n nm.user_id , \n nm.version_code , \n nm.version_name , \n nm.lang , \n nm.source, \n nm.os, \n nm.area, \n nm.model, \n nm.brand, \n nm.sdk_version, \n nm.gmail, \n nm.height_width,\n nm.app_time,\n nm.network,\n nm.lng,\n nm.lat,\n nm.create_date,\n 1 retention_day \nfrom gmall.dws_uv_detail_day ud join gmall.dws_new_mid_day nm on ud.mid_id=nm.mid_id \nwhere ud.dt='2019-02-11' and nm.create_date=date_add('2019-02-11',-1);\n```\n\n##### 8.3.3.2 共1,2,3,n天留存用户明细表  \n\n```sql\ninsert overwrite table gmall.dws_user_retention_day\npartition(dt=\"2019-02-11\")\nselect\n    nm.mid_id,\n    nm.user_id,\n    nm.version_code,\n    nm.version_name,\n    nm.lang,\n    nm.source,\n    nm.os,\n    nm.area,\n    nm.model,\n    nm.brand,\n    nm.sdk_version,\n    nm.gmail,\n    nm.height_width,\n    nm.app_time,\n    nm.network,\n    nm.lng,\n    nm.lat,\n    nm.create_date,\n    1 retention_day \nfrom gmall.dws_uv_detail_day ud join gmall.dws_new_mid_day nm  on ud.mid_id =nm.mid_id \nwhere ud.dt='2019-02-11' and nm.create_date=date_add('2019-02-11',-1)\n\nunion all\nselect  \n    nm.mid_id,\n    nm.user_id , \n    nm.version_code , \n    nm.version_name , \n    nm.lang , \n    nm.source, \n    nm.os, \n    nm.area, \n    nm.model, \n    nm.brand, \n    nm.sdk_version, \n    nm.gmail, \n    nm.height_width,\n    nm.app_time,\n    nm.network,\n    nm.lng,\n    nm.lat,\n    nm.create_date,\n    2 retention_day \nfrom  gmall.dws_uv_detail_day ud join gmall.dws_new_mid_day nm   on ud.mid_id =nm.mid_id \nwhere ud.dt='2019-02-11' and nm.create_date=date_add('2019-02-11',-2)\n\nunion all\nselect  \n    nm.mid_id,\n    nm.user_id , \n    nm.version_code , \n    nm.version_name , \n    nm.lang , \n    nm.source, \n    nm.os, \n    nm.area, \n    nm.model, \n    nm.brand, \n    nm.sdk_version, \n    nm.gmail, \n    nm.height_width,\n    nm.app_time,\n    nm.network,\n    nm.lng,\n    nm.lat,\n    nm.create_date,\n    3 retention_day \nfrom  gmall.dws_uv_detail_day ud join gmall.dws_new_mid_day nm   on ud.mid_id =nm.mid_id \nwhere ud.dt='2019-02-11' and nm.create_date=date_add('2019-02-11',-3);\n```\n\n##### 8.3.3.3 留存用户脚本\n\n```sh\n#!/bin/bash\n\n#ads 当日、当月、当周活跃设备书\n# 定义变量方便修改\nAPP=gmall\nhive=/kfly/install/hive-1.1.0-cdh5.14.2/bin/hive\n\n# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天\n    if [ -n \"$1\" ] ;then\n    do_date=$1\nelse\n    # shellcheck disable=SC2006\n    do_date=`date -d \"-1 day\" +%F`\nfi\n\nsql=\"\ninsert overwrite table gmall.dws_user_retention_day\npartition(dt=\"$do_date\")\nselect\n nm.mid_id,\n nm.user_id ,\n nm.version_code ,\n nm.version_name ,\n nm.lang ,\n nm.source,\n nm.os,\n nm.area,\n nm.model,\n nm.brand,\n nm.sdk_version,\n nm.gmail,\n nm.height_width,\n nm.app_time,\n nm.network,\n nm.lng,\n nm.lat,\n nm.create_date,\n 1 retention_day\nfrom gmall.dws_uv_detail_day ud join gmall.dws_new_mid_day nm on ud.mid_id=nm.mid_id\nwhere ud.dt='$do_date' and nm.create_date=date_add('$do_date',-1);\n\ninsert overwrite table gmall.dws_user_retention_day\npartition(dt=\"$do_date\")\nselect\n    nm.mid_id,\n    nm.user_id,\n    nm.version_code,\n    nm.version_name,\n    nm.lang,\n    nm.source,\n    nm.os,\n    nm.area,\n    nm.model,\n    nm.brand,\n    nm.sdk_version,\n    nm.gmail,\n    nm.height_width,\n    nm.app_time,\n    nm.network,\n    nm.lng,\n    nm.lat,\n    nm.create_date,\n    1 retention_day\nfrom gmall.dws_uv_detail_day ud join gmall.dws_new_mid_day nm  on ud.mid_id =nm.mid_id\nwhere ud.dt='$do_date' and nm.create_date=date_add('$do_date',-1)\n\nunion all\nselect\n    nm.mid_id,\n    nm.user_id ,\n    nm.version_code ,\n    nm.version_name ,\n    nm.lang ,\n    nm.source,\n    nm.os,\n    nm.area,\n    nm.model,\n    nm.brand,\n    nm.sdk_version,\n    nm.gmail,\n    nm.height_width,\n    nm.app_time,\n    nm.network,\n    nm.lng,\n    nm.lat,\n    nm.create_date,\n    2 retention_day\nfrom  gmall.dws_uv_detail_day ud join gmall.dws_new_mid_day nm   on ud.mid_id =nm.mid_id\nwhere ud.dt='$do_date' and nm.create_date=date_add('$do_date',-2)\n\nunion all\nselect\n    nm.mid_id,\n    nm.user_id ,\n    nm.version_code ,\n    nm.version_name ,\n    nm.lang ,\n    nm.source,\n    nm.os,\n    nm.area,\n    nm.model,\n    nm.brand,\n    nm.sdk_version,\n    nm.gmail,\n    nm.height_width,\n    nm.app_time,\n    nm.network,\n    nm.lng,\n    nm.lat,\n    nm.create_date,\n    3 retention_day\nfrom  gmall.dws_uv_detail_day ud join gmall.dws_new_mid_day nm   on ud.mid_id =nm.mid_id\nwhere ud.dt='$do_date' and nm.create_date=date_add('$do_date',-3);\n\"\n\n$hive -e \"$sql\"\n\n```\n\n\n\n#### 8.3.4 ADS层\n\n##### 8.3.4.1 用户留存数\n\n```sql\ndrop table if exists gmall.ads_user_retention_day_count;\ncreate external table gmall.ads_user_retention_day_count \n(\n`create_date`     string  comment '设备新增日期',\n`retention_day`   int comment '截止当前日期留存天数',\n`retention_count` bigint comment  '留存数量' \n)  COMMENT '每日用户留存情况' \nrow format delimited fields terminated by '\\t' \nlocation '/warehouse/gmall/ads/ads_user_retention_day_count/';\n\ninsert into table gmall.ads_user_retention_day_count \nselect\n    create_date,\n    retention_day,\n    count(*) retention_count\nfrom gmall.dws_user_retention_day\nwhere dt='2019-02-11' \ngroup by create_date,retention_day;\n```\n\n##### 8.3.4.2 用户留存率\n\n```sql\ndrop table if exists gmall.ads_user_retention_day_rate;\n\ncreate external table gmall.ads_user_retention_day_rate \n(\n`stat_date`        string comment '统计日期',\n`create_date`      string  comment '设备新增日期',\n`retention_day`    int comment '截止当前日期留存天数',\n`retention_count`  bigint comment  '留存数量',\n`new_mid_count`    bigint comment '当日设备新增数量',\n`retention_ratio`  decimal(10,2) comment '留存率'\n)  COMMENT '每日用户留存情况'\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ads/ads_user_retention_day_rate/';\n\ninsert into table gmall.ads_user_retention_day_rate\nselect \n    '2019-02-11', \n    ur.create_date,\n    ur.retention_day, \n    ur.retention_count, \n    nc.new_mid_count,\n    ur.retention_count/nc.new_mid_count*100\nfrom gmall.ads_user_retention_day_count ur join gmall.ads_new_mid_count nc\non nc.create_date=ur.create_date;\n```\n\n##### 8.3.4.3 用户留存主题脚本\n\n```sh\n#!/bin/bash\n\n# 定义变量方便修改\nAPP=gmall\nhive=/kfly/install/hive-1.1.0-cdh5.14.2/bin/hive\n\n# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天\n    if [ -n \"$1\" ] ;then\n    do_date=$1\nelse\n    # shellcheck disable=SC2006\n    do_date=`date -d \"-1 day\" +%F`\nfi\n\nsql=\"\ninsert into table gmall.ads_user_retention_day_count\nselect\n    create_date,\n    retention_day,\n    count(*) retention_count\nfrom gmall.dws_user_retention_day\nwhere dt='$do_date'\ngroup by create_date,retention_day;\n\ninsert into table gmall.ads_user_retention_day_rate\nselect\n    '$do_date',\n    ur.create_date,\n    ur.retention_day,\n    ur.retention_count,\n    nc.new_mid_count,\n    ur.retention_count/nc.new_mid_count*100\nfrom gmall.ads_user_retention_day_count ur join gmall.ads_new_mid_count nc\non nc.create_date=ur.create_date;\n\"\n\n$hive -e \"$sql\"\n\n```\n\n### 8.4 沉默用户数\n\n![image-20191203170322315](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191203170322315.png)\n\n```sql\n-- 1. 使用日活明细表dws_uv_detail_day作为DWS层数据\n-- 2. 建表语句\ndrop table if exists gmall.ads_silent_count;\ncreate external table gmall.ads_silent_count( \n    `dt` string COMMENT '统计日期',\n    `silent_count` bigint COMMENT '沉默设备数'\n) \nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ads/ads_silent_count';\n\n-- 3. 插入数据\ninsert into table gmall.ads_silent_count\nselect \n    '2019-02-20' dt,\n    count(*) silent_count\nfrom \n(\n    select mid_id\n    from gmall.dws_uv_detail_day\n    where dt<='2019-02-20'\n    group by mid_id\n    having count(*)=1 and min(dt)<date_add('2019-02-20',-7)\n) t1;\n```\n\n```sh\n#!/bin/bash\n\nhive=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive\nAPP=gmall\n\nif [ -n \"$1\" ];then\n    do_date=$1\nelse\n    do_date=`date -d \"-1 day\" +%F`\nfi\n\necho \"-----------导入日期$do_date-----------\"\n\nsql=\"\ninsert into table \"$APP\".ads_silent_count\nselect \n    '$do_date' dt,\n    count(*) silent_count\nfrom \n(\n    select \n        mid_id\n    from \"$APP\".dws_uv_detail_day\n    where dt<='$do_date'\n    group by mid_id\n    having count(*)=1 and min(dt)<=date_add('$do_date',-7)\n)t1;\"\n\n$hive -e \"$sql\"\n```\n\n### 8.5 本周回流用户数\n\n![image-20191203172746206](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191203172746206.png)\n\n```sql\n-- 1. 使用日活明细表dws_uv_detail_day作为DWS层数据\n-- 2. 建表\ndrop table if exists gmall.ads_back_count;\ncreate external table ads_back_count( \n    `dt` string COMMENT '统计日期',\n    `wk_dt` string COMMENT '统计日期所在周',\n    `wastage_count` bigint COMMENT '回流设备数'\n) \nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ads/ads_back_count';\n\n-- 3. 岛入数据\ninsert into table gmall.ads_back_count\nselect \n   '2019-02-20' dt,\n   concat(date_add(next_day('2019-02-20','MO'),-7),'_',date_add(next_day('2019-02-20','MO'),-1)) wk_dt,\n   count(*)\nfrom \n(\n    select t1.mid_id\n    from \n    (\n        select  mid_id\n        from gmall.dws_uv_detail_wk\n        where wk_dt=concat(date_add(next_day('2019-02-20','MO'),-7),'_',date_add(next_day('2019-02-20','MO'),-1))\n    )t1\n    left join\n    (\n        select mid_id\n        from gmall.dws_new_mid_day\n        where create_date<=date_add(next_day('2019-02-20','MO'),-1) and create_date>=date_add(next_day('2019-02-20','MO'),-7)\n    )t2\n    on t1.mid_id=t2.mid_id\n    left join\n    (\n        select mid_id\n        from gmall.dws_uv_detail_wk\n        where wk_dt=concat(date_add(next_day('2019-02-20','MO'),-7*2),'_',date_add(next_day('2019-02-20','MO'),-7-1))\n    )t3\n    on t1.mid_id=t3.mid_id\n    where t2.mid_id is null and t3.mid_id is null\n)t4;\n```\n\n### 8.6 本周流失用户\n\n![image-20191203174523306](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191203174523306.png)\n\n```sql\n-- 使用日活明细表dws_uv_detail_day作为DWS层数据\ndrop table if exists gmall.ads_wastage_count;\ncreate external table ads_wastage_count( \n    `dt` string COMMENT '统计日期',\n    `wastage_count` bigint COMMENT '流失设备数'\n) \nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ads/ads_wastage_count';\n\n-- 2. 导入数据\ninsert into table gmall.ads_wastage_count\nselect\n     '2019-02-20',\n     count(*)\nfrom \n(\n    select mid_id\nfrom dws_uv_detail_day\n    group by mid_id\n    having max(dt)<=date_add('2019-02-20',-7)\n)t1;\n```\n\n### 8.7 连续三周活跃\n\n![image-20191203175551679](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191203175551679.png)\n\n```sql\ndrop table if exists  gmall.ads_continuity_wk_count;\ncreate external table gmall.ads_continuity_wk_count( \n    `dt` string COMMENT '统计日期,一般用结束周周日日期,如果每天计算一次,可用当天日期',\n    `wk_dt` string COMMENT '持续时间',\n    `continuity_count` bigint\n) \nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ads/ads_continuity_wk_count';\n\ninsert into table gmall.ads_continuity_wk_count\nselect \n     '2019-02-20',\n     concat(date_add(next_day('2019-02-20','MO'),-7*3),'_',date_add(next_day('2019-02-20','MO'),-1)),\n     count(*)\nfrom \n(\n    select mid_id\n    from gmall.dws_uv_detail_wk\n    where wk_dt>=concat(date_add(next_day('2019-02-20','MO'),-7*3),'_',date_add(next_day('2019-02-20','MO'),-7*2-1)) \n    and wk_dt<=concat(date_add(next_day('2019-02-20','MO'),-7),'_',date_add(next_day('2019-02-20','MO'),-1))\n    group by mid_id\n    having count(*)=3\n)t1;\n```\n\n```sh\n#!/bin/bash\n\nif [ -n \"$1\" ];then\n    do_date=$1\nelse\n    do_date=`date -d \"-1 day\" +%F`\nfi\n\nhive=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive\nAPP=gmall\n\necho \"-----------导入日期$do_date-----------\"\n\nsql=\"\ninsert into table \"$APP\".ads_continuity_wk_count\nselect \n     '$do_date',\n     concat(date_add(next_day('$do_date','MO'),-7*3),'_',date_add(next_day('$do_date','MO'),-1)),\n     count(*)\nfrom \n(\n    select mid_id\n    from \"$APP\".dws_uv_detail_wk\n    where wk_dt>=concat(date_add(next_day('$do_date','MO'),-7*3),'_',date_add(next_day('$do_date','MO'),-7*2-1)) \n    and wk_dt<=concat(date_add(next_day('$do_date','MO'),-7),'_',date_add(next_day('$do_date','MO'),-1))\n    group by mid_id\n    having count(*)=3\n)t1;\"\n\n$hive -e \"$sql\"\n```\n\n### 8.7 最近七天内连续三周活跃\n\n![image-20191203181811343](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191203181811343.png)\n\n```sql\ndrop table if exists gmall.ads_continuity_uv_count;\ncreate external table gmall.ads_continuity_uv_count( \n    `dt` string COMMENT '统计日期',\n    `wk_dt` string COMMENT '最近7天日期',\n    `continuity_count` bigint\n) COMMENT '连续活跃设备数'\nrow format delimited fields terminated by '\\t'\nlocation '/warehouse/gmall/ads/ads_continuity_uv_count';\n\ninsert into table gmall.ads_continuity_uv_count\nselect\n    '2019-02-12',\n    concat(date_add('2019-02-12',-6),'_','2019-02-12'),\n    count(*)\nfrom\n(\n    select mid_id\n    from\n    (\n        select mid_id      \n        from\n        (\n            select \n                mid_id,\n                date_sub(dt,rank) date_dif\n            from\n            (\n                select \n                    mid_id,\n                    dt,\n                    rank() over(partition by mid_id order by dt) rank\n                from gmall.dws_uv_detail_day\n                where dt>=date_add('2019-02-12',-6) and dt<='2019-02-12'\n            )t1\n        )t2 \n        group by mid_id,date_dif\n        having count(*)>=3\n    )t3 \n    group by mid_id\n)t4;\n\n```\n\n```sql\n#!/bin/bash\n\nif [ -n \"$1\" ];then\n    do_date=$1\nelse\n    do_date=`date -d \"-1 day\" +%F`\nfi\n\nhive=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive\nAPP=gmall\n\necho \"-----------导入日期$do_date-----------\"\n\nsql=\"\ninsert into table \"$APP\".ads_continuity_uv_count\nselect \n     '$do_date',\n     concat(date_add('$do_date',-6),'_','$do_date') dt,\n     count(*) \nfrom \n(\n    select mid_id\n    from\n    (\n        select mid_id\n        from \n        (\n            select\n                mid_id,\n                date_sub(dt,rank) date_diff\n            from \n            (\n                select \n                    mid_id,\n                    dt,\n                    rank() over(partition by mid_id order by dt) rank\n                from \"$APP\".dws_uv_detail_day\n                where dt>=date_add('$do_date',-6) and dt<='$do_date'\n            )t1\n        )t2\n        group by mid_id,date_diff\n        having count(*)>=3\n    )t3 \n    group by mid_id\n)t4;\n\"\n\n$hive -e \"$sql\"\n\n```\n\n## 9. Hive总结\n\n### 9.1 Hive的架构\n\n![image-20191203212352142](http://kfly.top/picture/kfly-top/数仓商城项目-用户行为数仓/assets/image-20191203212352142.png)\n\n### 9.2 Hive和数据库比较\n\nHive 和数据库除了拥有类似的查询语言，再无类似之处。\n\n-  数据存储位置\n\nHive 存储在 HDFS 。数据库将数据保存在块设备或者本地文件系统中。\n\n- 数据更新\n\nHive中不建议对数据的改写。而数据库中的数据通常是需要经常进行修改的， \n\n- 执行延迟\n\nHive 执行延迟较高。数据库的执行延迟较低。当然，这个是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。\n\n- 数据规模\n\nHive支持很大规模的数据计算；数据库可以支持的数据规模较小。\n\n### 9.3 内部表和外部表\n\n1. 管理表：当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。\n\n2. 外部表：删除该表并不会删除掉原始数据，删除的是表的元数据\n\n### 9.4 窗口函数\n\n1. 窗口函数\n\n-  OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。常用partition by 分区order by排序。\n\n- CURRENT ROW：当前行\n\n-  n PRECEDING：往前n行数据\n\n-  n FOLLOWING：往后n行数据\n\n- UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点\n\n-  LAG(col,n)：往前第n行数据\n\n- LEAD(col,n)：往后第n行数据\n\n-  NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。\n\n2. 排序函数：\n\n- RANK() 排序相同时会重复，总数不会变\n\n- DENSE_RANK() 排序相同时会重复，总数会减少\n\n- ROW_NUMBER() 会根据顺序计算\n\n### 9.5  在项目中是否自定义过UDF、UDTF函数，以及用他们处理了什么问题？\n\n- 1）自定义过。\n- 2）用UDF函数解析公共字段；用UDTF函数解析事件字段。\n- 3）自定义UDF步骤：定义一个类继承UDF，重写evaluate方法\n- 4）自定义UDTF步骤：定义一个类继承GenericUDTF，重写初始化方法、关闭方法和process方法。\n\n### 9.6 个By区别\n\n- 1）Sort By：分区内有序；\n- 2）Order By：全局排序，只有一个Reducer；\n- 3）Distrbute By：类似MR中Partition，进行分区，结合sort by使用。\n- 4） Cluster By：当Distribute by和Sorts by字段相同时，可以使用Cluster by方式。Cluster by除了具有Distribute by的功能外还兼具Sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。\n\n \n\n### 9.7 Hive优化\n\n- MapJoin\n\n  ​\t\t如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。\n\n- 行列过滤\n\n  ​\t列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。\n\n行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤。 \n\n- 采用分桶技术\n\n- 采用分区技术\n\n- 合理设置Map\n\n  ​\t\t通常情况下，作业会通过input****的目录产生一个或者多个map****任务。**\n\n主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。\n\n- 是不是map数越多越好？\n\n  ​\t\t答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。\n\n- 是不是保证每个map****处理接近128m****的文件块，就高枕无忧了？**\n\n  ​\t\t答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。\n\n针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；\n\n- 小文件进行合并\n\n  ​\t在Map执行前合并小文件，减少Map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。\n\n- 合理设置Reduce数\n\n- Reduce个数并不是越多越好\n\n  1. 过多的启动和初始化Reduce也会消耗时间和资源；\n\n  2. 另外，有多少个Reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；\n\n  3. 在设置Reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的Reduce数；使单个Reduce任务处理数据量大小要合适；\n\n- 常用参数-输出合并小文件\n\n```shell\n# 默认true，在map-only任务结束时合并小文件\nSET hive.merge.mapfiles = true;\n# 默认false，在map-reduce任务结束时合并小文件\nSET hive.merge.mapredfiles = true;\n# 默认256M\nSET hive.merge.size.per.task = 268435456; \n# 当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge\nSET hive.merge.smallfiles.avgsize = 16777216;\n```\n\n","tags":["电商项目"]},{"title":"azkaban工作流调度器","url":"/2019/11/26/it/flume-sqoop-zakaban/azkaban工作流调度器/","content":"\n## Azkaban工作流调度器\n\n### 1. 为什么需要工作流调度系统\n\n* 一个完整的数据分析系统通常都是由大量任务单元组成；\n  * shell脚本程序、java程序、mapreduce程序、hive脚本等\n* 各任务单元之间存在时间先后及前后依赖关系\n\n* 为了==很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行==\n\n\n\n### 2. Azkaban是什么\n\n* Azkaban是由Linkedin开源的一个==批量工作流任务调度器==。用于在一个工作流内以一个特定的顺序运行一组工作和流程。\n* Azkaban定义了一种==KV文件(properties)格式==来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。\n  * <https://azkaban.github.io/>\n\n![azkaban.github](assets/azkaban.github.png)\n\n* 功能特点\n  * 提供功能清晰、简单易用的web UI界面\n  * 提供job配置文件快速建立任务和任务之间的关系\n  * 提供模块化的可插拔机制，原生支持command、java、hive、hadoop\n  * 基于java开发，代码结构清晰，易于二次开发\n\n\n\n### 3. Azkaban基本架构\n\n![azkaban](assets/azkaban.png)\n\n\n\n* Azkaban由三部分构成\n\n  * 1、==Azkaban Web Server==\n    * 提供了Web UI，是azkaban的主要管理者，包括 project 的管理，认证，调度，对工作流执行过程的监控等。\n\n\n\n  * 2、==Azkaban Executor Server==\n    * 负责具体的工作流和任务的调度提交\n\n\n\n  * 3、==Mysql==\n    * 用于保存项目、日志或者执行计划之类的信息\n\n\n\n### 4. Azkaban架构的三种运行模式\n\n* 1、solo server mode(单机模式）\n\n~~~\nH2\nweb server 和 executor server运行在一个进程里\n最简单的模式，数据库内置的H2数据库，管理服务器和执行服务器都在一个进程中运行，任务量不大项目可以采用此模式。\n\n~~~\n\n\n\n* 2、two server mode\n\n~~~\nweb server 和 executor server运行在不同的进程\n数据库为mysql，管理服务器和执行服务器在不同进程，这种模式下，管理服务器和执行服务器互不影响。\n\n~~~\n\n\n\n* 3、multiple executor mode\n\n~~~\nweb server 和 executor server运行在不同的进程，executor server有多个\n该模式下，执行服务器和管理服务器在不同主机上，且执行服务器可以有多个。\n\n~~~\n\n\n\n### 5. Azkaban安装部署\n\n[点击查看](https://kfly.top/2019/11/26/hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n\n### 6. Azkaban实战\n\n#### 6.1 command类型单一job\n\n* 1、==创建job描述文件  以.job后缀结尾==\n  * 创建 command.job 文件\n\n~~~shell\n#command.job\ntype=command\ncommand=echo 'hello azkaban......'\n~~~\n\n* 2、将job资源文件打包成zip文件\n* 例如\n  * command.zip\n\n* 3、通过azkaban的web管理平台创建project并上传job压缩包\n\n![create_project](assets/create_project.png)\n\n![upload_zip](assets/upload_zip.png)\n\n* 3、点击运行\n\n![execute-flow](assets/execute-flow.png)\n\n![execute](assets/execute.png)\n\n\n\n* 4、运行完成\n\n![success](assets/success.png)\n\n#### 6.2 command类型多job工作流\n\n* 1、创建有依赖关系的多个job描述\n\n  * 第一个job：start1.job\n\n  ~~~shell\n  #start1.job\n  type=command\n  command= echo 'start1...start1...'\n  ~~~\n\n  * 第二个job：start2.job    它依赖start1.job\n\n  ~~~shell\n  #start2.job\n  type=command\n  dependencies=start1\n  command= echo 'start2...start2...'\n  ~~~\n\n* 2、将job资源文件打包成zip文件\n\n  * start12.zip\n\n  ![start12job](assets/start12job.png)\n\n* 3、创建工程，上传zip包，最后启动工作流\n\n![execute-start12](assets/execute-start12.png)\n\n* ==补充==\n  * 如果一个job有多个依赖的job，可以使用逗号隔开\n\n~~~shell\n例如： \n#start1.job\ntype=command\ncommand= echo \"start1 job\"\n\n#start2.job\ntype=command\ncommand= echo \"start2 job\"\n\n#stop.job\ntype=command\ndenpendencies=start1,start2\ncommand=echo \"stop job\"\n\n注意：有多个依赖的job，用逗号隔开\n~~~\n\n\n\n#### 6.3 HDFS操作任务\n\n* 1、创建job描述文件\n\n  * vim fs.job\n\n  ~~~shell\n  #fs.job\n  type=command\n  command=echo \"start execute\"\n  command.1=/kfly/install/hadoop-2.6.0-cdh5.14.2/bin/hdfs dfs -mkdir /azkaban\n  command.2=/kfly/install/hadoop-2.6.0-cdh5.14.2/bin/hdfs dfs -put /home/hadoop/source.txt /azkaban\n  ~~~\n\n* 2、将job资源文件打包成zip文件\n\n  * fs.zip\n\n* 3、创建工程，上传zip包，最后启动工作流\n\n![fs](assets/fs.png)\n\n![fs-hdfs](assets/fs-hdfs.png)\n\n#### 6.4 MAPREDUCE任务\n\n* MR任务依然可以使用command的job类型来执行\n\n* 1、创建job描述文件，及mr程序jar包\n\n  * 示例中直接使用hadoop自带的example jar\n  * hdfs dfs -mkdir -p  /wordcount/in\n\n  ~~~shell\n  #mr.job\n  type=command\n  command=/kfly/install/hadoop-2.6.0/bin/hadoop  jar hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar wordcount /wordcount/in /wordcount/out\n  ~~~\n\n* 2、将job资源文件打包成zip文件\n\n  - mr.zip\n\n* 3、创建工程，上传zip包，最后启动工作流\n\n![mr](assets/mr.png)\n\n#### 6.5 HIVE脚本任务\n\n* 1、 创建job描述文件和hive脚本\n\n  * Hive脚本： test.sql\n\n  ~~~sql\n  use default;\n  create table if not exists test_azkaban(id int,name string,address string) row format delimited fields terminated by ',';\n  load data local inpath '/home/hadoop/azkaban/test.txt' into table test_azkaban;\n  create table if not exists countaddress as select address,count(*) as num from test_azkaban group by address ;\n  \n  insert overwrite local directory '/home/hadoop/azkaban/out' select * from countaddress; \n  ~~~\n\n  * 准备数据\n\n    * vim /home/hadoop/azkaban/test.txt\n\n    ~~~properties\n    1,zhangsan,shanghai\n    2,lisi,beijing\n    3,xiaoming,shanghai\n    4,xiaozhang,shanghai\n    5,xiaogang,beijing\n    ~~~\n\n* 2、创建job描述文件\n\n  * hive.job\n\n  ~~~shell\n  # hive.job\n  type=command\n  command=/kkb/install/hive-1.1.0-cdh5.14.2/bin/hive -f 'test.sql'\n  ~~~\n\n\n\n* 3、将job资源文件打包成zip文件\n  \n  * hive.zip\n  \n### 7. 任务定时调度\n\n* 在启动工作流的时候，可以点击==Schedule==，实现定时调度一个工作流\n\n![schedule1](assets/schedule1.png)\n\n\n\n![schedule2](assets/schedule2.png)\n\n\n\n### 8. webUI 传递参数\n\n* 可以通过webUI动态给job传递参数\n\n* 1、创建一个job的描述文件\n\n  * parameter.job\n\n    ~~~shell\n    #parameter.job\n    type=command\n    parameter=${param}\n    command= echo ${parameter}\n    ~~~\n\n  * 其中\n\n    * ==${param}== 表示解析页面传递的参数param的值，通过声明一个变量parameter去接受\n    * ==${parameter}==表示获取该parameter变量的值\n\n* 2、将job资源文件打包成zip文件\n\n  * parameter.zip\n\n* 3、创建工程，上传zip包，最后启动工作流，并且设置参数\n\n![1572232468448](assets/1572232468448.png)\n\n* 4、运行完成后的结果\n\n  ![1572232525618](assets/1572232525618.png)\n","tags":["-azkaban"]},{"title":"sqoop数据迁移工具","url":"/2019/11/26/it/flume-sqoop-zakaban/sqoop数据迁移工具/","content":"\n### 1. Sqoop是什么\n\n* Sqoop是apache旗下的一款 ”==Hadoop和关系数据库之间传输数据==”的工具\n  * ==导入数据== import\n    * 将MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统\n  * ==导出数据== export\n    * 从Hadoop的文件系统中导出数据到关系数据库\n\n![sqoop](assets/sqoop.png)\n\n\n\n### 2. Sqoop的工作机制\n\n* 将导入和导出的命令翻译成mapreduce程序实现\n  * 在翻译出的mapreduce中主要是对inputformat和outputformat进行定制\n\n\n\n### 3. Sqoop基本架构\n\n* sqoop在发展中的过程中演进出来了两种不同的架构.[架构演变史](<https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop#comment-1561314193000>)\n\n* ==sqoop1的架构图==\n\n![sqoop1](assets/sqoop1.jpg)\n\n~~~\n版本号为1.4.x0\n~~~\n\n\n\n* ==sqoop2的架构图==\n\n\n\n![sqoop2](assets/sqoop2.jpg)\n\n~~~\n版本号为1.99x为sqoop2 \n在架构上：sqoop2引入了sqoop server，对connector实现了集中的管理 \n访问方式：REST API、 JAVA API、 WEB UI以及CLI控制台方式进行访问 \n~~~\n\n![sqoop1 VS sqoop2](assets/sqoop1 VS sqoop2.png)\n\n### 4. Sqoop安装部署\n\n​\t\t[点击查看](https://kfly.top/2019/11/26/hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n\n\n### 5. Sqooq数据的导入\n\n* 导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据） \n\n#### 5.1 列举出所有的数据库 \n\n* 命令行查看帮助文档\n\n~~~shell\nsqoop list-databases --help\n~~~\n\n* 列出node03上mysql数据库中所有的数据库名称\n\n~~~shell\nsqoop list-databases --connect jdbc:mysql://node03:3306/ --username root --password 123456\n~~~\n\n* 查看某一个数据库下面的所有数据表\n\n~~~shell\nsqoop list-tables --connect jdbc:mysql://node03:3306/hive --username root --password 123456\n~~~\n\n\n\n#### 5.2 导入数据库表数据到HDFS\n\n* 在MySQL数据库服务器中创建一个数据库userdb, 然后在创建一张表 emp，添加点测试数据到表中\n\n* 从MySQL数据库服务器中的userdb数据库下的emp表导入HDFS上\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node02:3306/userdb \\\n--username root   \\\n--password 123456 \\\n--table emp \\\n--m 1\n\n\n\n#参数解释\n--connect   指定mysql链接地址\n--username  连接mysql的用户名\n--password  连接mysql的密码\n--table     指定要导入的mysql表名称\n--m:        表示这个MR程序需要多少个MapTask去运行，默认为4\n默认路径是/user/hadoop下\n~~~\n\n* 提交之后，会运行一个MR程序，最后查看HDFS上的目录看是否有数据生成\n\n![table2hdfs](assets/table2hdfs.png)\n\n\n\n#### 5.3 导入数据库表数据到HDFS指定目录\n\n* 在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。\n* 使用参数 ==--target-dir==来指定导出目的地，\n* 使用参数==--delete-target-dir==来判断导出目录是否存在，如果存在就删掉\n\n~~~shell\nsqoop import  --connect jdbc:mysql://node03:3306/userdb --username root --password 123456  --table emp  --target-dir /sqoop/emp  --delete-target-dir --m 1\n~~~\n\n* 提交查看HDFS上的目录看是否有数据生成\n\n![table2hdfsDir](assets/table2hdfsDir.png)\n\n\n\n#### 5.4 导入数据库表数据到HDFS指定目录并且指定数据字段的分隔符\n\n* 这里使用参数 \n  * ==--fields-terminated-by 分隔符==\n\n~~~shell\nsqoop import  \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--delete-target-dir \\\n--table emp  \\\n--target-dir /sqoop/emp1 \\\n--fields-terminated-by '#' \\\n--m 1\n~~~\n\n#### 5.5 导入关系表到Hive中\n\n* (1) 将我们mysql表当中的数据直接导入到hive表中的话，需要将hive的一个叫做==hive-exec-1.1.0-cdh5.14.2.jar==包拷贝到sqoop的lib目录下\n\n~~~shell\ncp /kkb/install/hive-1.1.0-cdh5.14.2/lib/hive-exec-1.1.0-cdh5.14.2.jar /kkb/install/sqoop-1.4.6-cdh5.14.2/lib/\n~~~\n\n* (2) 准备hive数据库与表\n\n  * 在hive中创建一个数据库和表\n\n  ~~~sql\n  create database sqooptohive;\n  \n  create external table sqooptohive.emp_hive(id int,name string,deg string,salary double ,dept string) row format delimited fields terminated by '\\001';\n  ~~~\n\n* (3) 把mysql表数据导入到hive表中\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--fields-terminated-by '\\001' \\\n--hive-import \\\n--hive-table sqooptohive.emp_hive \\\n--hive-overwrite \\\n--delete-target-dir \\\n--m 1\n\n##参数解释\n--hive-table      指定要导入到hive表名\n--hive-import     导入数据到hive表中\n--hive-overwrite  覆盖hive表中已存有的数据\n\n~~~\n\n分为两步\n\n* (4) 执行完成了查看hive中表的数据\n  * **select * from sqooptohive.emp_hive;**\n\n![emp_hive](assets/emp_hive.png)\n\n\n\n#### 5.6 导入数据库表数据到hive中(并自动创建hive表)\n\n* 可以通过命令来将我们的mysql的表直接导入到hive表当中去，==不需要事先创建hive表==\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--hive-database sqooptohive \\\n--hive-table emp1 \\\n--hive-import \\\n--m 1 \n\n\ndrop table emp1\n~~~\n\n*  执行完成了查看hive中表的数据\n   *  **select * from sqooptohive.emp1;**\n\n![em1](assets/em1.png)\n\n\n\n#### 5.7 导入表数据子集\n\n* 我们可以导入表的使用Sqoop导入工具，\"where\"子句的一个子集。它执行在各自的数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。\n* 按照条件进行查找，通过==**--where**==参数来查找表emp当中==**dept**==字段的值为 **==TP==** 的所有数据导入到hdfs上面去\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--target-dir /sqoop/emp_where \\\n--delete-target-dir \\\n--where \"dept = 'TP'\" \\\n--m 1 \n~~~\n\n* 提交查看HDFS上的目录看是否有数据生成\n\n  ![](assets/emp_where.png)\n\n\n\n#### 5.8 sql语句查找导入hdfs\n\n* 我们还可以通过 -–query参数来指定我们的sql语句，通过sql语句来过滤我们的数据进行导入\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--target-dir /sqoop/emp_sql \\\n--delete-target-dir \\\n--query 'select * from emp where salary >30000 and $CONDITIONS' \\\n--m 1\n~~~\n\n* 提交查看HDFS上的目录看是否有数据生成\n\n![emp_conditions](assets/emp_conditions.png)\n\n* ==补充：==\n\n  ~~~shell\n  $CONTITONS是linux系统的变量，如果你想通过并行的方式导入结果，每个map task需要执行sql查询后脚语句的副本，结果会根据sqoop推测的边界条件分区。query必须包含$CONDITIONS。这样每个sqoop程序都会被替换为一个独立的条件。同时你必须指定 --split-by '字段'，后期是按照字段进行数据划分，最后可以达到多个MapTask并行运行。\n  \n  \n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456 \\\n  --target-dir /sqoop/emp_sql_2 \\\n  --delete-target-dir \\\n  --query 'select * from emp where salary >30000 and $CONDITIONS' \\\n  --split-by 'id' \\\n  --m 2\n  \n  \n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456 \\\n  --target-dir /sqoop/emp_sql_2 \\\n  --delete-target-dir \\\n  --query 'select * from emp where id >1 and $CONDITIONS' \\\n  --split-by 'salary' \\\n  --m 2\n  \n  \n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456 \\\n  --target-dir /sqoop/emp_sql_2 \\\n  --delete-target-dir \\\n  --query 'select * from emp where id >1 and $CONDITIONS' \\\n  --split-by 'id' \\\n  --m 7\n  \n  \n  \n  \n  --split-by '字段'： 后期按照字段进行数据划分实现并行运行多个MapTask。\n  ~~~\n\n\n#### 5.9 增量导入\n\n* 在实际工作当中，数据的导入很多时候都是==只需要导入增量数据即可==，并不需要将表中的数据全部导入到hive或者hdfs当中去，肯定会出现重复的数据的状况，所以我们一般都是选用一些字段进行增量的导入，为了支持增量的导入，sqoop也给我们考虑到了这种情况并且支持增量的导入数据\n\n* 增量导入是仅导入新添加的表中的行的技术。\n\n* 它需要添加 ==‘incremental’, ‘check-column’, 和 ‘last-value’==选项来执行增量导入。\n\n  ~~~\n  --incremental <mode>\n  --check-column <column name>\n  --last value <last check column value>\n  ~~~\n\n* ==第一种增量导入实现==\n\n  * ==基于递增列的增量数据导入（Append方式）==\n  * 导入emp表当中id大于1202的所有数据\n    * 注意：==这里不能加上 --delete-target-dir  参数，添加就报错==\n\n  ~~~shell\n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456 \\\n  --table emp \\\n  --incremental append \\\n  --check-column id \\\n  --last-value 1202  \\\n  --target-dir /sqoop/increment \\\n  --m 1\n  \n  \n  ##参数解释\n  --incremental   这里使用基于递增列的增量数据导入\n  --check-column  递增列字段\n  --last-value    指定上一次导入中检查列指定字段最大值\n  --target-dir    数据导入的目录\n  ~~~\n\n  * 提交查看HDFS上的目录看是否有数据生成\n\n![sqoop_increment1](assets/sqoop_increment1.png)\n\n\n\n* ==第二种增量导入实现==\n\n  * ==基于时间列的增量数据导入（LastModified方式）==\n\n    * 此方式要求原有表中有time字段，它能指定一个时间戳\n      * user表结构和数据\n\n    ![table_user](assets/table_user.png)\n\n  ~~~shell\n  sqoop import \\\n  --connect jdbc:mysql://node03:3306/userdb \\\n  --username root \\\n  --password 123456  \\\n  --table user \\\n  --incremental lastmodified  \\\n  --check-column createTime  \\\n  --last-value '2019-10-01 10:30:00'  \\\n  --target-dir /sqoop/increment2 \\\n  --m 1\n  \n  ##参数解释\n  --incremental   这里使用基于时间列的增量导入\n  --check-column  时间字段\n  --last-value    指定上一次导入中检查列指定字段最大值\n  --target-dir    数据导入的目录\n  \t\t\t\t如果该目录存在(可能已经有数据)\n  \t\t\t\t再使用的时候需要添加 --merge-key or --append\n  \t\t--merge-key 指定合并key（对于有修改的）\n  \t\t--append    直接追加修改的数据\n  ~~~\n\n  * 提交查看HDFS上的目录看是否有数据生成\n\n![sqoop_increment1](assets/sqoop_increment2.png)\n\n\n\n#### 5.10 mysql表的数据导入到hbase中\n\n* 实现把一张mysql表数据导入到hbase中\n\n~~~shell\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--table emp \\\n--hbase-table  mysqluser \\\n--column-family  f1 \\\n--hbase-create-table \\\n--hbase-row-key id  \\\n--m 1 \n\n\n#参数说明\n--hbase-table  \t\t\t指定hbase表名\n--column-family \t\t指定表的列族\n--hbase-create-table \t表不存在就创建\n--hbase-row-key \t\t指定hbase表的id\n--m  \t\t\t\t\t指定使用的MapTask个数\nlist\nscan 'mysqluser'\ndisable 'mysqluser'\ndrop 'mysqluser'\n\n\n# mysql导入hbase 不同的列族\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--columns id,salary,dept \\\n--table emp \\\n--hbase-table  mysqluser2 \\\n--column-family  f1 \\\n--hbase-create-table \\\n--hbase-row-key id  \\\n--m 1 \n\n\nsqoop import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--columns id,name,deg \\\n--table emp \\\n--hbase-table  mysqluser2 \\\n--column-family  f2 \\\n--hbase-create-table \\\n--hbase-row-key id  \\\n--m 1 \n\n\n~~~\n\n\n\n### 6. Sqoop数据的导出\n\n* 将数据从HDFS把文件导出到RDBMS数据库\n  * 导出前，目标表必须存在于目标数据库中。\n    * 默认操作是从将文件中的数据使用INSERT语句插入到表中\n    * 更新模式下，是生成UPDATE语句更新表数据\n\n\n\n#### 6.1 hdfs文件导出到mysql表中\n\n* 1、数据是在HDFS当中的如下目录/user/hive/warehouse/hive_source，数据内容如下\n\n~~~\n1 zhangsan 20 hubei\n2 lisi 30 hunan\n3 wangwu 40 beijing\n4 xiaoming 50 shanghai\n~~~\n\n* 2、创建一张mysql表\n  * 注意mysql中的这个表一定要先创建！ 不然报错！\n\n~~~sql\nCREATE TABLE  userdb.fromhdfs (\n   id INT DEFAULT NULL,\n   name VARCHAR(100) DEFAULT NULL,\n   age int DEFAULT NULL,\n   address VARCHAR(100) DEFAULT NULL\n) ENGINE=INNODB DEFAULT CHARSET=utf8;\n~~~\n\n* 3、执行导出命令\n\n~~~shell\nsqoop export \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table fromhdfs \\\n--export-dir /user/hive/warehouse/hive_source \\\n--input-fields-terminated-by \" \" \n\n\n##参数解释\n--table \t\t\t\t\t  指定导出的mysql表名\n--export-dir \t\t\t\t  指定hdfs数据文件目录\n--input-fields-terminated-by  指定文件数据字段的分隔符\n~~~\n\n* 4、验证mysql表数据\n\n![fromhdfs](assets/fromhdfs.png)\n\n### 7. Sqoop job\n\n* 将事先定义好的数据导入导出任务按照指定流程运行\n\n* 语法\n\n~~~\nsqoop job (generic-args) (job-args)\n   [-- [subtool-name] (subtool-args)]\n~~~\n\n#### 7.1 创建job\n\n* ==--create==\n  * 创建一个名为myjob,实现从mysql表数据导入到hdfs上的作业\n    * 注意\n      * 在创建job时，==命令\"-- import\" 中间有个空格==\n\n~~~shell\nsqoop job --help\n\n##创建一个sqoop作业\nsqoop job \\\n--create myjob1 \\\n-- import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456 \\\n--table emp \\\n--target-dir /sqoop/myjob \\\n--delete-target-dir \\\n--m 1\n\n##创建一个sqoop增量导入的作业\nsqoop  job  \\\n--create incrementJob1 \\\n-- import \\\n--connect jdbc:mysql://node03:3306/userdb \\\n--username root \\\n--password 123456  \\\n--table user \\\n--target-dir /sqoop/incrementJob \\\n--incremental append  \\\n--check-column createTime  \\\n--last-value '2019-11-19 16:40:21'  \\\n--m 1\n\n\n# incremental.last.value\n~~~\n\n#### 7.2 验证 job\n\n* ==--list==\n\n* 验证作业是否创建成功\n\n  * 执行如下命令\n\n  ~~~shell\n  sqoop job --list\n  \n  \n  最后显示：\n  Available jobs:\n    myjob\n  ~~~\n\n\n\n#### 7.3 查看job\n\n* ==--show==\n* 查看作业的详细信息\n  * 执行如下命令\n\n~~~shell\nsqoop job --show myjob1\n~~~\n\n\n\n#### 7.4 执行job\n\n* ==--exec==\n\n  * 用于执行保存的作业\n\n  ~~~shell\n  sqoop job --exec myjob1\n  ~~~\n\n  * 解决sqoop需要输入密码的问题\n    * 修改配置文件\n      * vi /kkb/install/sqoop-1.4.6-cdh5.14.2/conf/sqoop-site.xml\n\n  ~~~xml\n  <property>\n      <name>sqoop.metastore.client.record.password</name>\n      <value>true</value>\n      <description>If true, allow saved passwords in the metastore.\n      </description>\n  </property>\n  ~~~\n\n\n\n#### 7.5 删除job\n\n- ==--delete==\n  - 用于删除保存作业\n\n```shell\nsqoop job --delete myjob\n```\n\n","tags":["sqoop"]},{"title":"大数据环境搭建","url":"/2019/11/26/it/hadoop/大数据环境搭建/","content":"\n# 大数据环境搭建(MAC)\n\n## 1. Linux 环境配置\n\n### 1.1 Vmware Funsion Linux网络配置\n\n#### 1.1.1 查看本机网络\n\n```shell\n# 1. 查看vmnet网络配置，本机mac上\ncat /Library/Preferences/VMware\\ Fusion/vmnet8/nat.conf \n# 2. 可以看到如下部分信息，记住下面信息，用来配置linux\n[host]\n# NAT gateway address\nip = 192.168.83.2\nnetmask = 255.255.255.0\n```\n\n#### 1.1.2 配置linux网络\n\n1. setting -> Network Adapter -> Share with my msc  如下图\n\n<img src=\"assets/image-20191127172534764.png\" alt=\"image-20191127172534764\" style=\"zoom:50%;\" />\n\n2. 点击上图左下角 Advanced options，点击Generate（生成MAC Address）\n\n   <img src=\"assets/image-20191127172700292.png\" alt=\"image-20191127172700292\" style=\"zoom:50%;\" />\n\n3. 启动虚拟机\n\n```shell\n# 1. 修改配置文件\nsudo vi /etc/sysconf/network-scripts/ifcg-ens192\n\n# 2. 修改下面内容\n# 改为静态\nBOOTPROTO=static\nONBOOT=yes\n# 与上面网段保持一致\nIPPADDR=192.168.83.100\n# 与上面一致\nNETMASK=255.255.255.0\n# 与上面ip一致\nGATEWAY=192.168.83.2\nDNS1=8.8.8.8\n\n#3. 修改之后，重启network\nservice network restart\n```\n\n4. 关闭防火墙，selinux\n\n```shell\n# 关闭\nsystemctl stop firewalld\n# 永久关闭\nsystemctl disable firewalld\n\n# 关闭selinux，修改下面文件内容\nvi /etc/selinux/config #进入selinux设置文件\nSELINUX=disabled\n```\n\n### 1.2. Linux 设置免密登陆\n\n#### 1.2.1 设置用户 & 权限\n\n```shell\n# 1. 创建hadoop用户\nuseradd hadoop #添加hadoop用户\npasswd hadoop #给hadoop用户添加密码\nhadoop #密码设为hadoop\n# 2. 设置用户权限 \nvisudo #进入用户权限配置文件\n## Allow root to run any commands anywhere\nroot    ALL=(ALL)       ALL\nhadoop  ALL=(ALL)\t    ALL # 给hadoop用户添加所有权限\n# 3. 切换到hadoop用户\nsu - hadoop # 加上- 表示切换同时拥有权限\n```\n\n#### 1.2.2 免密登陆\n\n1. 修改/etc/hosts 文件\n\n```shell\n   sudo vi /etc/hosts\n   \n   # 添加如下内容\n   192.168.83.100 node01\n   192.168.83.110 node02\n   192.168.83.120 node03\n```\n\n2. 配置免密登陆\n\n```shell\n# 1. hadoop用户下执行下列命令，必须！\n# 下面操作node01，node02，node03 都执行，回车next。\nssh-keygen -t rsa #生成公钥\n\n# 2. 三台机器的公钥全部拷贝到node01\nssh-copy-id node01 \n\n# 3. 第一台机器执行，“:PWD”的意思是：拷贝目标文件位置和node01的位置一致。\ncd /home/hadoop/.ssh/\nscp authorized_keys node02:$PWD #将node01的授权文件拷贝到node02\nscp authorized_keys node03:$PWD #将node01的授权文件拷贝到node03\n\n#4. 验证免密登录\n#在node01执行\nssh node02 #在node01登录node02，不需要密码就ok\nssh node03 #在node01登录node02，不需要密码就ok\n#回到node01\nlogout\t\t\n```\n\n### 1.3 时间同步\n\n#### 1.3.1 同步阿里云\n\n```shell\n# 安装ntpdate\nsudo yum -y install ntpdate\ncrontab -e \n*/1 * * * * /usr/sbin/ntpdate time1.aliyun.com\n\n# 如果时间不通可以执行\nsudo ntpdate -u asia.pool.ntp.org\n```\n\n#### 1.3.2 同步node01时间\n\n- 以下命令在root用户操作\n\n```shell\n# 1. 安装ntp软件（所有）\n# (注意：ntpd作为node01服务端，node02、node03 ntpdate作为客户端软件不同)\nsudo yum  -y  install ntpd\n\n# 2. 设置时区为中国上海（所有）\ntimedatectl set-timezone Asia/Shanghai\n\n# 3. node01启动ntp服务，作为服务端供其他节点同步时间\nsystemctl start ntpd\n\n# 4. node01设置开机启动\nsystemctl enable ntpd\n\n# 5.修改配置 node01上\nsudo vi /etc/ntp.conf\n# 注释掉以下四行，添加最后两行。对应自己的vmnt8内容\n#server 0.centos.pool.ntp.org iburst\n#server 1.centos.pool.ntp.org iburst\n#server 2.centos.pool.ntp.org iburst\n#server 3.centos.pool.ntp.org iburst\n\nrestrict 192.168.83.2 mask 255.255.255.0 nomodify notrap\nserver 127.127.1.0\n\n# 5.1 node02 node03修改配置\nsudo vi /etc/sysconfig/ntpdate\n# 修改为 yes\nSYNC_HWCLOCK=yes\n\ncrontab -e\n*/1 * * * * /usr/sbin/ntpdate 192.168.83.100\n\n# 6. node02、node03重启服务\nsystemctl restart ntpdate\n\n```\n\n## 2.Hadoop环境搭建\n\n### 2.1 配置hadoop-env.sh\n\n```shell\n# hadoop 用户下\nsu - hadoop\n\nvi /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/hadoop-env.sh \nexport JAVA_HOME=/kfly/install/jdk1.8.0_141 #修改为此变量\n```\n\n### 2.2 配置core-site.xml\n\n```shell\n#在hadoop用户下打开配置文件：\nvi /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/core-site.xml\n```\n\n```xml\n<configuration>\n\t<property>\n\t\t<name>fs.defaultFS</name>\n\t\t<value>hdfs://node01:8020</value>\n\t</property>\n\t<property>\n\t\t<name>hadoop.tmp.dir</name>\n\t\t<value>/kfly/install/hadoop-2.6.0/hadoopDatas/tempDatas</value>\n\t</property>\n\t<!--  缓冲区大小，实际工作中根据服务器性能动态调整 -->\n\t<property>\n\t\t<name>io.file.buffer.size</name>\n\t\t<value>4096</value>\n\t</property>\n  <property>\n    <name>fs.trash.interval</name>\n    <value>10080</value>\n    <description>检查点被删除后的分钟数。 如果为零，垃圾桶功能将被禁用。 \n      该选项可以在服务器和客户端上配置。 如果垃圾箱被禁用服务器端，则检查客户端配置。 \n      如果在服务器端启用垃圾箱，则会使用服务器上配置的值，并忽略客户端配置值。\n    </description>\n  </property>\n\n  <property>\n       <name>fs.trash.checkpoint.interval</name>\n       <value>0</value>\n       <description>垃圾检查点之间的分钟数。 应该小于或等于fs.trash.interval。 \n       如果为零，则将该值设置为fs.trash.interval的值。 每次检查指针运行时，\n       它都会从当前创建一个新的检查点，并删除比fs.trash.interval更早创建的检查点。\n    </description>\n  </property>\n</configuration>\n```\n\n### 2.3 配置hdfs-site.xml\n\n```shell\n#在hadoop用户下打开配置文件：\nvi /kkb/install/hadoop-2.6.0/etc/hadoop/hdfs-site.xml\n```\n\n```xml\n<configuration>\n\t<!-- NameNode存储元数据信息的路径，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割   --> \n\t<!--   集群动态上下线 \n\t<property>\n\t\t<name>dfs.hosts</name>\n\t\t<value>/kfly/install/hadoop-2.6.0/etc/hadoop/accept_host</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.hosts.exclude</name>\n\t\t<value>/kfly/install/hadoop-2.6.0/etc/hadoop/deny_host</value>\n\t</property>\n\t -->\n\t <property>\n\t\t\t<name>dfs.namenode.secondary.http-address</name>\n\t\t\t<value>node01:50090</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.http-address</name>\n\t\t<value>node01:50070</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.name.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/namenodeDatas</value>\n\t</property>\n\t<!--  定义dataNode数据存储的节点位置，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割  -->\n\t<property>\n\t\t<name>dfs.datanode.data.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/datanodeDatas</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.edits.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/dfs/nn/edits</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.checkpoint.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/dfs/snn/name</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.namenode.checkpoint.edits.dir</name>\n\t\t<value>file:///kfly/install/hadoop-2.6.0/hadoopDatas/dfs/nn/snn/edits</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.replication</name>\n\t\t<value>3</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.permissions</name>\n\t\t<value>false</value>\n\t</property>\n\t<property>\n\t\t<name>dfs.blocksize</name>\n\t\t<value>134217728</value>\n\t</property>\n</configuration>\n```\n\n### 2.4 配置mapred-site.xml\n\n```shell\n#在hadoop用户下操作,进入指定文件夹：\ncd /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/\n#由于原来没有mapred-site.xml配置文件，需要根据模板复制一份：\ncp  mapred-site.xml.template mapred-site.xml\nvi /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/mapred-site.xml\n```\n\n```xml\n<!--指定运行mapreduce的环境是yarn -->\n<configuration>\n   <property>\n\t\t<name>mapreduce.framework.name</name>\n\t\t<value>yarn</value>\n\t</property>\n\t<property>\n\t\t<name>mapreduce.job.ubertask.enable</name>\n\t\t<value>true</value>\n\t</property>\n\t<property>\n\t\t<name>mapreduce.jobhistory.address</name>\n\t\t<value>node01:10020</value>\n\t</property>\n\t<property>\n\t\t<name>mapreduce.jobhistory.webapp.address</name>\n\t\t<value>node01:19888</value>\n\t</property>\n</configuration>\n```\n\n### 2.5 配置yarn-site.xml\n\n```shell\n#在hadoop用户下操作\nvi /kfly/install/hadoop-2.6.0/etc/hadoop/yarn-site.xml\n```\n\n```xml\n<configuration>\n\t<property>\n\t\t<name>yarn.resourcemanager.hostname</name>\n\t\t<value>node01</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.nodemanager.aux-services</name>\n\t\t<value>mapreduce_shuffle</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.log-aggregation-enable</name>\n\t\t<value>true</value>\n\t</property>\n\t<property>\n\t\t <name>yarn.log.server.url</name>\n\t\t <value>http://node01:19888/jobhistory/logs</value>\n\t</property>\n\t<!--多长时间聚合删除一次日志 此处-->\n\t<property>\n        <name>yarn.log-aggregation.retain-seconds</name>\n        <value>2592000</value><!--30 day-->\n\t</property>\n\t<!--时间在几秒钟内保留用户日志。只适用于如果日志聚合是禁用的-->\n\t<property>\n        <name>yarn.nodemanager.log.retain-seconds</name>\n        <value>604800</value><!--7 day-->\n\t</property>\n\t<!--指定文件压缩类型用于压缩汇总日志-->\n\t<property>\n        <name>yarn.nodemanager.log-aggregation.compression-type</name>\n        <value>gz</value>\n\t</property>\n\t<!-- nodemanager本地文件存储目录-->\n\t<property>\n        <name>yarn.nodemanager.local-dirs</name>\n        <value>/kkb/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/yarn/local</value>\n\t</property>\n\t<!-- resourceManager  保存最大的任务完成个数 -->\n\t<property>\n        <name>yarn.resourcemanager.max-completed-applications</name>\n        <value>1000</value>\n\t</property>\n</configuration>\n```\n\n### 2.6 编辑slaves\n\n此文件用于配置集群有多少个数据节点,我们把node2，node3作为数据节点,node1作为集群管理节点\n\n```shell\n#在hadoop用户下操作\nvim /kfly/install/hadoop-2.6.0/etc/hadoop/slaves\n\nnode01 #添加\nnode02 #添加\nnode03 #添加\n```\n\n### 2.7 创建文件存放目录\n\n```shell\n#在hadoop用户下操作\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/tempDatas\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name\nmkdir -p /kfly/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits\n```\n\n### 2.8 拷贝到其他节点\n\n```shell\n# 分发到各个节点下,$PWD 相同目录\nscp -r hadoop-2.6.0 node02:$PWD\n\n#分发配置文件\nscp /etc/profile node02:$PWD\n```\n\n### 2.9 格式化节点\n\n```shell\n#下面命令只在node01上执行\nhdfs namenode -format #格式化\n\n# 启动\nstart-all.sh\n```\n\n### 2.10 访问\n\n- hadoop webui http://node01:50070/\n\n- hadoop application http://node01:8088\n\n  ```shell\n  # 启动 jobhistory\n  mr-jobhistory-daemon.sh start historyserver\n  ```\n\n- hadoop job http://node01:19888\n\n### 2.11. Hadoop Ha高可用\n\n​\t\t[hadoop ha 高可用](https://kfly.top/2019/10/28/zookeeper/zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%B0%83%E6%A1%86%E6%9E%B6%EF%BC%88%E4%BA%8C%EF%BC%89hadoop%20ha%E9%AB%98%E5%8F%AF%E7%94%A8%E5%AE%89%E8%A3%85/)\n\n## 3. hive环境搭建\n\n### 3.1 mysql安装\n\n```shell\n# 安装wget\nsudo yum install wget\n\n# 1. 换源\n# 1.1 备份系统源\nsudo mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup\n# 1.2 下载阿里云CENTOS7镜像文件\nwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n# 1.3 清理缓存、生成新的缓存\nsudo yum clean all\nsudo yum makecache\n# 1.4 更新源\nsudo yum update -y\n\n# 2. 使用yum安装MySQL,下载并安装MySQL官方的 Yum Repository\nsudo wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm\nsudo yum -y install mysql57-community-release-el7-10.noarch.rpm\nsudo yum -y install mysql-community-server\n\n# 3. 启动mysql，查看运行状态\nsystemctl start  mysqld.service\nsystemctl status mysqld.service\n\n# 4. 找出默认密码如下图所示\nsudo grep \"password\" /var/log/mysqld.log\n\n# 5. 登陆、修改密码\nmysql -uroot -p     # 回车后会提示输入密码\nALTER USER 'root'@'localhost' IDENTIFIED BY 'new password';\n # 注意： 如果失败，修改密码策略,先修改一个复杂的密码，在修改策略，修改密码\nALTER USER 'root'@'localhost' IDENTIFIED BY 'z?guwrBhH7p>';\nset global validate_password_policy=0;\nset global validate_password_policy=1;\n\n# 6. 设置mysql可以外部连接\ngrant all on *.* to root@'%' identified by '数据库密码';\n```\n\n### 3.2 hive安装\n\n#### 3.2.1 下载hive的安装包\n\n- http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.14.2.tar.gz\n\n#### 3.2.2 hive-env.sh\n\n```shell\nvim hive-env.sh\n\n#配置HADOOP_HOME路径\nexport HADOOP_HOME=/kfly/install/hadoop-2.6.0\n#配置HIVE_CONF_DIR路径\nexport HIVE_CONF_DIR=/kfly/install/hive-1.1.0-cdh5.14.2/conf\n```\n\n#### 3.2.3 hive-site.xml\n\n```xml\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n        <property>\n                <name>javax.jdo.option.ConnectionURL</name>\n                <value>jdbc:mysql://node02:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=latin1&amp;useSSL=false</value>\n        </property>\n\n        <property>\n                <name>javax.jdo.option.ConnectionDriverName</name>\n                <value>com.mysql.jdbc.Driver</value>\n        </property>\n        <property>\n                <name>javax.jdo.option.ConnectionUserName</name>\n                <value>root</value>\n        </property>\n        <property>\n                <name>javax.jdo.option.ConnectionPassword</name>\n                <value>123456</value>\n        </property>\n        <property>\n                <name>hive.cli.print.current.db</name>\n                <value>true</value>\n        </property>\n        <property>\n                <name>hive.cli.print.header</name>\n            <value>true</value>\n        </property>\n    <property>\n                <name>hive.server2.thrift.bind.host</name>\n                <value>node02</value>\n        </property>\n</configuration>\n```\n\n#### 3.2.4 日志路径\n\n```shell\nvim hive-log4j.properties\n\n#更改以下内容，设置我们的日志文件存放的路径\nhive.log.dir=/kkb/install/hive-1.1.0-cdh5.14.2/logs/\n```\n\n#### 3.4.5  lib包\n\n```shell\n ps: ==需要将mysql的驱动包上传到hive的lib目录下==\n  * 例如 mysql-connector-java-5.1.38.jar\n```\n\n### 3.3 hive使用Spark on Yarn作为计算引擎\n\n​\t\t[查看](http://lxw1234.com/archives/2016/05/673.htm)\n\n## 4. zookeeper环境搭建\n\n### 4.1 下载软件\n\n​\t\t\t[点击下载 zookeeper-3.4.5-cdh5.14.2.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.14.2.tar.gz)\n\n### 4.2 修改配置文件\n\n```shell\n# 1. copy配置文件\ncd /kfly/install/zookeeper-3.4.5-cdh5.14.2/conf\ncp zoo_sample.cfg zoo.cfg\n# 2. 创建存放数据目录\nmkdir -p /kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas\n# 编辑\nvim  zoo.cfg\n# 3. 文件内容如下\ndataDir=/kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas\nautopurge.snapRetainCount=3\nautopurge.purgeInterval=1\nserver.1=node01:2888:3888\nserver.2=node02:2888:3888\nserver.3=node03:2888:3888\n\n# 4 分发到各个节点\n# 5. 写入myid文件，myid分别对应，node01:1，node02:2,node03.3、一次累加\necho 1 >  /kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid\n\n# 6. 启动服务、查看状态\nbin/zkServer.sh start\nbin/zkServer.sh status\n```\n\n## 5. HBase环境搭建\n\n### 5.1 下载软件\n\n​\t\t[点击下载hbase-1.2.0-cdh5.14.2.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.14.2.tar.gz)\n\n### 5.2  hbase-env.sh\n\n```shell\nexport JAVA_HOME=/kfly/install/jdk1.8.0_141\n# 使用外部的zookeeper集群\nexport HBASE_MANAGES_ZK=false\n```\n\n### 5.3 hbase-site.xml\n\n```xml\n<configuration>\n\t<property>\n\t\t<name>hbase.rootdir</name>\n\t\t<value>hdfs://node01:8020/HBase</value>  \n\t</property>\n\t<property>\n\t\t<name>hbase.cluster.distributed</name>\n\t\t<value>true</value>\n\t</property>\n\t<!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 -->\n\t<property>\n\t\t<name>hbase.master.port</name>\n\t\t<value>16000</value>\n\t</property>\n\t<property>\n\t\t<name>hbase.zookeeper.quorum</name>\n\t\t<value>node01,node02,node03</value>\n\t</property>\n\t<property>\n\t\t<name>hbase.zookeeper.property.clientPort</name>\n\t\t<value>2181</value>\n\t</property>\n\t<property>\n\t\t<name>hbase.zookeeper.property.dataDir</name>\n\t\t<value>/kfly/install/zookeeper-3.4.5-cdh5.14.2/zkdatas</value>\n\t</property>\n\t<property>\n\t\t<name>zookeeper.znode.parent</name>\n\t\t<value>/HBase</value>\n\t</property>\n</configuration>\n```\n\n### 5.4  regionservers\n\n```shell\n# 配置文件 目录 conf下\n vim regionservers\n \n# 内容如下\nnode01\nnode02\nnode03\n```\n\n### 5.5  back-masters\n\n- 创建back-masters配置文件，里边包含备份HMaster节点的主机名，每个机器独占一行，实现HMaster的高可用\n\n```shell\n[hadoop@node01 conf]$ vim backup-masters\n```\n\n- 将node02作为备份的HMaster节点，问价内容如下\n\n```properties\nnode02\n```\n\n### 5.6  创建软连接\n\n- **<font color='red'>注意：三台机器</font>**均做如下操作\n\n- 因为HBase集群需要读取hadoop的core-site.xml、hdfs-site.xml的配置文件信息，所以我们==三台机器==都要执行以下命令，在相应的目录创建这两个配置文件的软连接\n\n```shell\nln -s /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/core-site.xml  /kfly/install/hbase-1.2.0-cdh5.14.2/conf/core-site.xml\n\nln -s /kfly/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/hdfs-site.xml  /kfly/install/hbase-1.2.0-cdh5.14.2/conf/hdfs-site.xml\n\n```\n\n### 5.7 添加HBase环境变量\n\n- **<font color='red'>注意：三台机器</font>**均执行以下命令，添加环境变量\n\n```shell\nsudo vim /etc/profile\n# 添加如下\nexport HBASE_HOME=/kkb/install/hbase-1.2.0-cdh5.14.2\nexport PATH=$PATH:$HBASE_HOME/bin\n\n# 立即生效\nsource /etc/profile\n```\n\n### 5.8  HBase的启动与停止\n\n- <font color='red'>需要提前启动HDFS及ZooKeeper集群</font>\n\n- 第一台机器node01（HBase主节点）执行以下命令，启动HBase集群\n\n```shell\n[hadoop@node01 ~]$ start-hbase.sh\n\n#HMaster节点上启动HMaster命令\nhbase-daemon.sh start master\n\n#启动HRegionServer命令\nhbase-daemon.sh start regionserver\n```\n\n- 浏览器页面访问\n\n  http://node01:60010\n\n## 6. Flume环境搭建\n\n### 1. 下载软件\n\n​\t\t\t[点击下载 flume-ng-1.6.0-cdh5.14.2.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.2.tar.gz)\n\n### 2. flume-env.sh\n\n```properties\nexport JAVA_HOME=/kfly/install/jdk1.8.0_141\n```\n\n## 7. Sqoop环境搭建\n\n### 7.1. 下载软件\n\n​\t[点击下载 sqoop-1.4.6-cdh5.14.2.tar.gz ](http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.14.2.tar.gz)\n\n### 7.2. sqoop-env.sh\n\n```shell\n#Set path to where bin/hadoop is available\nexport HADOOP_COMMON_HOME=/kfly/install/hadoop-2.6.0\n\n#Set path to where hadoop-*-core.jar is available\nexport HADOOP_MAPRED_HOME=/kfly/install/hadoop-2.6.0\n\n#set the path to where bin/hbase is available\nexport HBASE_HOME=/kfly/install/hbase-1.2.0-cdh5.14.2\n\n#Set the path to where bin/hive is available\nexport HIVE_HOME=/kfly/install/hive-1.1.0-cdh5.14.2\n```\n\n## 8. zakaban环境搭建\n\n### 8.1 下载软件\n\n​\t[点击下载 ](https://azkaban.github.io/downloads.html)\n\n- azkaban-web-server-2.5.0.tar.gz\n- azkaban-executor-server-2.5.0.tar.gz\n- azkaban-sql-script-2.5.0.tar.gz\n\n### 8.2 azkaban web服务器安装\n\n#### 8.2.1 配置SSL安全访问协议\n\n~~~shell\n# 1. 创建安装目录，解压、解压文件重命名\nmkdir /kfly/install/azkaban\ntar –zxvf azkaban-web-server-2.5.0.tar.gz -C /kfly/install/azkaban\nmv /kkb/install/azkaban/azkaban-web-2.5.0 /kkb/install/azkaban/server\n\n# 2. 在server目下执行下边的命令\nkeytool -keystore keystore -alias jetty -genkey -keyalg RSA\n          # Keytool:   是java数据证书的管理工具，使用户能够管理自己的公/私钥对及相关证书。\n          # -keystore：指定密钥库的名称及位置(产生的各类信息将不在.keystore文件中)\n          # -alias：   对我们生成的.keystore 进行指认别名；如果没有默认是mykey\n          # -genkey：  在用户主目录中创建一个默认文件\".keystore\" \n          # -keyalg：  指定密钥的算法 RSA/DSA 默认是DSA\n\n          # 运行此命令后,会提示输入当前生成 keystore的密码及相应信息,输入的密码请劳记\n         -------------------------------------------------------------------\n            输入keystore密码： \n            再次输入新密码:\n            您的名字与姓氏是什么？\n              [Unknown]： \n            您的组织单位名称是什么？\n              [Unknown]： \n            您的组织名称是什么？\n              [Unknown]： \n            您所在的城市或区域名称是什么？\n              [Unknown]： \n            您所在的州或省份名称是什么？\n              [Unknown]： \n            该单位的两字母国家代码是什么\n              [Unknown]：  CN\n            CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？\n              [否]：  y\n\n              输入<jetty>的主密码\n                      （如果和 keystore 密码相同，按回车）： \n              再次输入新密码:\n          #完成上述工作后,将在当前目录生成 keystore 证书文件,\n# 3. 将keystore 考贝到 azkaban webserver 服务器根目录中.\ncp keystore /kfly/install/azkaban/server\n\n# 4. 配置时区、先生成时区配置文件Asia/Shanghai，用交互式命令 tzselect 即可.\ntzselect \n  \t# 选5 --->选9---->选1----->选1\n# 4.1、拷贝该时区文件，覆盖系统本地时区配置\ncp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  \n\n# 5.修改配置文件\n\n# 5.2 \n~~~\n\n#### 8.2.2 修改配置文件\n\n- 1. azkaban.properties\n\n~~~shell\nvim /kfly/install/azkaban/server/conf/azkaban.properties\n\n#内容说明如下:\n#Azkaban Personalization Settings\nazkaban.name=Test                   #服务器UI名称,用于服务器上方显示的名字\nazkaban.label=My Local Azkaban      #描述\nazkaban.color=#FF3601               #UI颜色\nazkaban.default.servlet.path=/index    \nweb.resource.dir=web/                 #默认根web目录\ndefault.timezone.id=Asia/Shanghai     #默认时区,已改为亚洲/上海 默认为美国\n \n#Azkaban UserManager class\nuser.manager.class=azkaban.user.XmlUserManager   #用户权限管理默认类\nuser.manager.xml.file=conf/azkaban-users.xml     #用户配置,具体配置参加下文\n \n#Loader for projects\nexecutor.global.properties=conf/global.properties    #global配置文件所在位置\nazkaban.project.dir=projects                                             \n \ndatabase.type=mysql               #数据库类型\nmysql.port=3306                   #端口号\nmysql.host=node03                 #数据库连接IP\nmysql.database=azkaban            #数据库实例名\nmysql.user=root                   #数据库用户名\nmysql.password=123456             #数据库密码\n \n# Velocity dev mode\nvelocity.dev.mode=false          #Jetty服务器属性.\njetty.maxThreads=25              #最大线程数\njetty.ssl.port=8443              #Jetty SSL端口\njetty.port=8081                  #Jetty端口\njetty.keystore=keystore          #SSL文件名\njetty.password=123456            #SSL文件密码\njetty.keypassword=123456         #Jetty主密码 与 keystore文件相同\njetty.truststore=keystore        #SSL文件名\njetty.trustpassword=123456       #SSL文件密码\n \n# 执行服务器属性\nexecutor.port=12321               #执行服务器端口\n \n# 邮件设置\nmail.sender=xxxxxxxx@163.com        #发送邮箱\nmail.host=smtp.163.com              #发送邮箱smtp地址\nmail.user=xxxxxxxx                  #发送邮件时显示的名称\nmail.password=**********            #邮箱密码\njob.failure.email=xxxxxxxx@163.com  #任务失败时发送邮件的地址\njob.success.email=xxxxxxxx@163.com  #任务成功时发送邮件的地址\nlockdown.create.projects=false       \ncache.directory=cache                #缓存目录\n~~~\n\n- 2. azkaban-users.xml\n\n```shell\nvim /kfly/install/azkaban/server/conf/azkaban-users.xml\n```\n\n~~~xml\n<azkaban-users>\n<user username=\"azkaban\" password=\"azkaban\" roles=\"admin\"groups=\"azkaban\"/>\n<user username=\"metrics\" password=\"metrics\" roles=\"metrics\"/>\n <!--新增admin用户--> \n<user username=\"admin\" password=\"admin\" roles=\"admin,metrics\" />\n<role name=\"admin\" permissions=\"ADMIN\" />\n<role name=\"metrics\" permissions=\"METRICS\"/>\n</azkaban-users>\n~~~\n\n### 8.3 azkaban 执行服器安装\n\n```shell\n# 1. 解压azkaban-executor-server-2.5.0.tar.gz,重命名文件\ntar -zxvf azkaban-executor-server-2.5.0.tar.gz -C /kfly/install/azkaban\nmv /kfly/install/azkaban/azkaban-executor-2.5.0 /kfly/install/azkaban/executor\n\n# 2. 修改配置文件\nvim /kfly/install/azkaban/executor/conf/azkaban.properties\n      # 内容如下----------\n      #Azkaban   #时区\n      default.timezone.id=Asia/Shanghai          \n\n      #数据库设置----->需要修改的信息\n      mysql.host=node3          #数据库IP地址\n      mysql.database=azkaban    #数据库实例名\n      mysql.user=root           #数据库用户名\n      mysql.password=123456     #数据库密码\n```\n\n### 8.4 azkaban脚本导入\n\n~~~shell\n# 1. 解压azkaban-sql-script-2.5.0.tar.gz\ntar -zxvf azkaban-sql-script-2.5.0.tar.gz -C /kfly/install/azkaban\n\n# 2. 把解压后的脚本导入到mysql中、进入到mysql\nmysql> create database azkaban;\nmysql> use azkaban;\nmysql> source /kfly/install/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql;\n~~~\n\n### 8.5 Azkaban启动\n\n* 在azkaban web server服务器目录下执行启动命令\n\n~~~shell\n# 1. 启动web server服务、在azkaban web server服务器目录下执行启动命令\nbin/azkaban-web-start.sh\n# 2. 启动executor执行服务、在azkaban executor服务器目录下执行启动命令\nbin/azkaban-executor-start.sh\n\n\n~~~\n\n* 启动完成后,在浏览器(建议使用谷歌浏览器)中输入==https://服务器IP地址:8443== ,即可访问azkaban服务了.在登录中输入刚才新的户用名及密码,点击 login.\n* 输入“IP地址:8443”无法访问 web 页面, 且后台报错，原因是浏览器安全证书限制\n* 解决办法：使用“https://ip:8443”访问, 发现已经可以访问了, 后台会报证数问题的错误, 忽略即可, 不影响使用, 选择高级 ------> 继续访问该网站\n\n<img src=\"assets/azkaban-web.png\" alt=\"azkaban-web\" style=\"zoom: 33%;\" />\n\n\n\n<img src=\"assets/azkaban界面.png\" alt=\"azkaban界面\" style=\"zoom:33%;\" />\n\n~~~\n（1）、projects：azkaban最重要的一部分，创建一个工程，将所有的工作流放在工程中执行\n（2）、scheduling：定时调度任务用的\n（3）、executing:  显示当前运行的任务\n（4）、History : 显示历史运行任务\n\n一个project由3个按钮：\n（1）、Flows：一个工作流，由多个job组成\n（2）、Permissions:权限管理\n（3）、Project Logs：工程日志信息\n~~~\n\n## 9. Spark环境安装\n\n### 9.1 下载软件\n\n​\t[点击下载](https://archive.apache.org/dist/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz)\n\n### 9.2 修改配置文件\n\n#### 9.2.1 spark-env.sh \n\n```sh\n#配置java的环境变量\nexport JAVA_HOME=/kkb/install/jdk1.8.0_141\n#配置zk相关信息\nexport SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER  -Dspark.deploy.zookeeper.url=node01:2181,node02:2181,node03:2181  -Dspark.deploy.zookeeper.dir=/spark\"\n```\n\n- 注意\n\n```properties\n## 如果想运行spark-shell --master local[N] 读取HDFS上文件，则加上如下配置文件\nexport HADOOP_CONF_DIR=/kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop\n# 这就是读取hdfs下，hdfs://node01:8020/words.txt 文件\nsc.textFile(\"/words.txt\")\n```\n\n  \n\n#### 9.2.2  slaves\n\n```shell\n#指定spark集群的worker节点\nnode02\nnode03\n```\n\n#### 9.2.3 分发到各个节点\n\n```sh\n# 1. 分发到各个节点\nscp -r /kfly/install/spark node02:/kkb/install\n# 2. 修改source /etc/profile环境\nexport SPARK_HOME=/kfly/install/spark\nexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n```\n\n### 9.3 SparkSql整合Hive\n\n- 步骤\n  - 1、需要把hive安装目录下的配置文件hive-site.xml拷贝到每一个spark安装目录下对应的conf文件夹中\n  - 2、需要一个连接mysql驱动的jar包拷贝到spark安装目录下对应的jars文件夹中\n  - 3、可以使用spark-sql脚本 后期执行sql相关的任务\n\n```shell\nspark-sql \\\n--master spark://node01:7077 \\\n--executor-memory 1g \\\n--total-executor-cores 4 \\\n--conf spasrk.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse \n```\n\n```sh\n#!/bin/sh\n#定义sparksql提交脚本的头信息\nSUBMITINFO=\"spark-sql --master spark://node01:7077 --executor-memory 1g --total-executor-cores 4 --conf spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse\" \n#定义一个sql语句\nSQL=\"select * from kfly.psrson;\" \n#执行sql语句   类似于 hive -e sql语句\necho \"$SUBMITINFO\" \necho \"$SQL\"\n$SUBMITINFO -e \"$SQL\"\n```\n\n\n\n### 9.4 Spark on Yarn\n\n- [官网资料](http://spark.apache.org/docs/2.3.3/running-on-yarn.html)\n\n```shell\n## 注意这里不需要安装spark集群、只需要解压spark安装包到任意一台服务器\n#修改文件 spark-env.sh\n\n#指定java的环境变量\nexport JAVA_HOME=/kkb/install/jdk1.8.0_141\n#指定hadoop的配置文件目录\nexport HADOOP_CONF_DIR=/kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop\n```\n\n```shell\nspark-submit --class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode cluster \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 1 \\\n/kfly/install/spark-2.3.3-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.3.jar \\\n10\n```\n\n- 如果运行出现错误，可能是虚拟内存不足，可以添加参数\n\n  - vim yarn-site.xml\n\n  ```xml\n  <!--容器是否会执行物理内存限制默认为True-->\n  <property>\n      <name>yarn.nodemanager.pmem-check-enabled</name>\n      <value>false</value>\n  </property>\n  \n  <!--容器是否会执行虚拟内存限制    默认为True-->\n  <property>\n      <name>yarn.nodemanager.vmem-check-enabled</name>\n      <value>false</value>\n  </property>\n  ```\n\n#### ","tags":["环境搭建"]},{"title":"大数据之批流处理的未来之路Flink","url":"/2019/11/22/it/flink/大数据之批流处理的未来之路Flink/","content":"\n# 大数据批流处理的未来之路\n\n## 1 Flink介绍\n\n```\n早起柏林工业大学联合发起的一个关于数据库的研究项目，叫做：stratorsphere。直到2014年4月份捐献给apache基金会，称为apache基金会的孵化项目， 在孵化期间项目stratosphere改名为Flink 随后到2014年12月，该项目成为了apache基金会的顶级项目。\n目前新的flink版本已经到了1.9\n```\n\n- Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态的计算。 \n- 官网地址:http://flink.apache.org \n\n![img](https://flink.apache.org/img/flink-home-graphic.png)\n\n- 上图分为三部分\n\n  1. 首先要有数据，负责接收数据\n\n  2. 中间就是进行计算的部分，具体对数据处理的地方\n\n  3. 最终数据输出的地方，把结果存储在某地方\n\n### 1.1 核心功能\n\n- 同时支持高吞吐、低延迟、高性能\n  - Sparck Core： 支持高吞吐、高性能： 相对延迟较高\n  - Stream ：低延迟、高性能框架\n- 支持事件时间（Event Time）概念\n- 支持有状态的计算\n- 支持高度灵活的窗口操作（流式计算）\n- 基于轻量级的分布式快照（Snapshot）容错\n- 基于JVM实现独立的内存管理\n- SavePoints保存点\n\n### 1.2 应用场景\n\n- 实时智能推荐系统\n  - 今日头条广告： 与淘宝共享检索数据，实时推送广告\n- 复杂事件处理\n- 实时欺诈监测\n- 实时数仓ETL\n- 流数据分析\n- 实时报表分析\n\n## 2 Flink的基础架构\n### 2.1 基础组建栈\n\n![image-20191122163707117](assets/image-20191122163707117.png)\n\n### 2.2 基础架构图\n\n![image-20191122163747496](assets/image-20191122163747496.png)\n\n## 3 Flient API开发\n\n### 3.1 批处理开发\n\n==使用DataSet Api开发批处理程序==\n\n- 创建maven工程依赖\n\n```xml\n  <dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-scala_2.12</artifactId>\n    <version>1.8.1</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-clients_2.12</artifactId>\n    <version>1.8.1</version>\n  </dependency>\n```\n\n  - Scala代码\n\n```scala\n    package flink\n    \n    import org.apache.flink.api.scala._\n    \n    /**\n     * 通过Scala语言开发Flink 批处理程序\n     */\n    object ScalaWorkCount {\n    \n      def main(args: Array[String]): Unit = {\n    \n        // 获取Flink批处理执行环境\n        var env:ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment;\n    \n        // 初始化原数据\n        var data:DataSet[String] = env.fromCollection(List(\"hadoop mapreduce\",\"hadoop spark\",\"spark core\"))\n    \n        // 数据处理，切分每一行数据获取所有单词\n        var words : DataSet[String] = data.flatMap(_.split(\" \"))\n    \n        // 把每个单词记为1，封装成元组\n        var wordAndOne:DataSet[(String,Int)] = words.map((_,1))\n    \n        // 按照单词进行分组\n        var groupByWord:GroupedDataSet[(String,Int)] = wordAndOne.groupBy(0)\n    \n        // 对相同的单词进行分组\n        var aggregateDataSet: AggregateDataSet[(String,Int)] = groupByWord.sum(1)\n    \n        // 打印\n        aggregateDataSet.print()\n      }\n    }\n    \n```\n\n  - Java代码\n\n```java\n    package flink;\n    \n    import org.apache.flink.api.common.functions.FlatMapFunction;\n    import org.apache.flink.api.java.DataSet;\n    import org.apache.flink.api.java.ExecutionEnvironment;\n    import org.apache.flink.api.java.operators.DataSource;\n    import org.apache.flink.api.java.tuple.Tuple2;\n    \n    /**\n     * @author dingchuangshi\n     */\n    public class ScalaWorkCountJava {\n        public static void main(String[] args) throws Exception {\n    \n            // 获取Flink批处理执行环境\n            ExecutionEnvironment env = ExecutionEnvironment.createCollectionsEnvironment();\n    \n            // 初始化原数据\n            DataSource<String> data = env.fromElements(\"hadoop mapreduce\", \"hadoop spark\", \"spark core\");\n    \n            // 数据处理\n            DataSet<Tuple2<String, Integer>> words = data\n                    .flatMap((FlatMapFunction<String, Tuple2<String,Integer>>)(s,out)->{\n                        for (String word: s.split(\" \")) {\n                            out.collect(new Tuple2<String,Integer>(word,1));\n                        }\n                    })\n                    .groupBy(0)\n                    .sum(1);\n    \n            // sink\n            words.print();\n        }\n    }\n    \n```\n\n### 3.2 流式处理开发\n\n```xml\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-scala_2.12</artifactId>\n  <version>1.8.1</version>\n</dependency>\n```\n\n- Scala代码\n\n```scala\n  package flink\n\n\nimport org.apache.flink.streaming.api.scala._\nimport org.apache.flink.streaming.api.windowing.time.Time\nimport org.apache.flink.streaming.api.windowing.windows.TimeWindow\n\n/**\n  * 基于scala语言开发Flink的流式处理程序\n */\nobject ScalaStreamWordCount {\n\n  def main(args: Array[String]): Unit = {\n\n       //1. 获取flink的流式处理环境\n      val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n\n      //2. 构建source数据源\n      val socketTextStream: DataStream[String] = env.socketTextStream(\"192.168.18.238\",9999)\n\n      //3. 数据处理\n      //3.1 切分每一行，获取所有的单词\n      val words: DataStream[String] = socketTextStream.flatMap(_.split(\" \"))\n\n      //3.2 把每个单词计为1 封装成元组(单词，1)\n      val wordAndOne: DataStream[(String, Int)] = words.map((_,1))\n\n      //3.3 按照单词进行分组\n      val groupByWord: KeyedStream[(String, Int), String] = wordAndOne.keyBy(_._1)\n\n      //3.4 设置时间窗口\n      val timeWindow: WindowedStream[(String, Int), String, TimeWindow] = groupByWord.timeWindow(Time.seconds(5))\n\n      //3.5 相同单词出现的1累加\n      val result: DataStream[(String, Int)] = timeWindow.reduce((v1,v2)=> (v1._1,v1._2+v2._2))\n\n      //4. 构建Sink\n      result.print()\n\n      //5. 启动流式应用程序\n      env.execute(\"ScalaStreamWordCount\")\n  }\n}\n```\n\n  \n\n- 模拟一个socket服务器\n\n```shell\nnc -lk 9999\n```\n\n\n\n\n## 4 Flink的Table & SQL API\n\n![image-20191122170247587](assets/image-20191122170247587.png)\n\n- Table Api是一种关系型Api，类SQL的API。 用户可以像操作表一样的操作数据，非常直观和方便。\n\n- Table & SQL API的出现可以解决流处理和批处理统一的API层\n\n  - 批处理上的查询会随着输入数据的结束而结束并生成有限的结果集。\n  - 流处理上的查询会一直运行并生成结果流\n  - Table & SQL API 做到了批与流上的查询具有相同的语法，因此不用修改代码就能同时实现批和流。\n\n  ```xml\n  <dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-table-planner_2.11</artifactId> \n    <version>1.8.1</version>\n  </dependency>\n  ```\n  \n  \n  \n### 4.1 批处理开发\n\n  ```scala\n  package flink\n  \n  import org.apache.flink.api.scala._\n  import org.apache.flink.core.fs.FileSystem.WriteMode\n  import org.apache.flink.table.api.{Table, Types}\n  import org.apache.flink.table.api.scala.BatchTableEnvironment\n  import org.apache.flink.table.sinks.CsvTableSink\n  import org.apache.flink.table.sources.CsvTableSource\n  \n  /**\n   * 基于table & sql api开发 Flink的批处理程序\n   */\n  object ScalaTableSQLBatchWordCount {\n  \n    def main(args: Array[String]): Unit = {\n  \n        //1. 获取flink的table批处理环境\n        val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment\n  \n        val tEnv: BatchTableEnvironment = BatchTableEnvironment.create(env)\n  \n        //2.构建Source数据源\n        val tableSource: CsvTableSource = CsvTableSource.builder()\n          .path(\"./data/person.txt\")\n          .fieldDelimiter(\" \")\n          .field(\"id\", Types.INT)\n          .field(\"name\", Types.STRING)\n          .field(\"age\", Types.INT)\n          .ignoreParseErrors().lineDelimiter(\"\\r\\n\")\n          .build()\n  \n        //将tableSource注册成表 tEnv.registerTableSource(\"person\",tableSource)\n        //3. 查询\n        //3.1 查询年龄大于30岁的人\n        val result1: Table = tEnv.scan(\"person\").filter(\"age > 30\")\n  \n        //3.2 统计不同的年龄用户数\n        val result2: Table = tEnv.sqlQuery(\"select age,count(*) from person group by age \")\n  \n        //4. 构建sink //打印表的元数据schema信息\n        result1.printSchema()\n  \n        //保存结果数据到文件中 val tableSink1 = new\n        var tableSink1 = new CsvTableSink(\"./out/result1.txt\", \"\\t\", 1, WriteMode.OVERWRITE)\n        result1.writeToSink(tableSink1)\n  \n        val tableSink2 = new CsvTableSink(\"./out/result2.txt\", \"\\t\", 1, WriteMode.OVERWRITE)\n        result2.writeToSink(tableSink2)\n  \n        //开启计算\n        env.execute()\n    }\n  }\n  ```\n\n  \n\n### 4.2 流处理开发\n\n```scala\npackage flink\n\nimport org.apache.flink.core.fs.FileSystem.WriteMode\nimport org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}\nimport org.apache.flink.table.api.Table\nimport org.apache.flink.table.api.scala.StreamTableEnvironment\nimport org.apache.flink.table.sinks.CsvTableSink\nimport org.apache.flink.api.scala._\n\n/**\n * 基于table & sql api开发 Flink的流式处理程序\n */\nobject ScalaTableSQLStreamWordCount {\n\n  case class User(id:Int,name:String,age:Int)\n\n  def main(args: Array[String]): Unit = {\n\n    //1. 获取flink的table流式处理的环境\n    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment\n\n    val streamSQLEnv: StreamTableEnvironment = StreamTableEnvironment.create(env)\n    //2. 构建Source数据源\n    /**\n      * 101,zhangsan,18\n      * 102,lisi,20\n      * 103,wangwu,25\n      * 104,zhaoliu,15\n      */\n    val socketDataStream: DataStream[String] = env.socketTextStream(\"node01\",9999)\n\n    val userDataStream: DataStream[User] = socketDataStream.map(x=>x.split(\",\")).map(y=>User(y(0).toInt,y(1),y(2).toInt ))\n\n    //3. 将流注册成一张表\n    streamSQLEnv.registerDataStream(\"userTable\",userDataStream)\n\n    //4. 使用table && sql api来查询数据\n\n     // 使用table 的api查询年龄大于20岁的人\n    val result1: Table = streamSQLEnv.scan(\"userTable\").filter(\"age >20\")\n\n    //使用sql 的api查询\n    val result2: Table = streamSQLEnv.sqlQuery(\" select * from userTable \")\n\n    //5. 构建Sink\n    val tableSink1 = new CsvTableSink(\"./out/tableSink1.txt\",\"\\t\",1,WriteMode.OVERWRITE)\n    result1.writeToSink(tableSink1)\n\n    val tableSink2 = new CsvTableSink(\"./out/tableSink2.txt\",\"\\t\",1,WriteMode.OVERWRITE)\n    result2.writeToSink(tableSink2) //开启执行流式计算\n\n    env.execute() }\n}\n```\n\n### 4.3 批流一体处理逻辑\n\n#### 4.3.1 初始化环境\n\n![image-20191122192954640](assets/image-20191122192954640.png)\n\n#### 4.3.2 获取一个table\n\n![image-20191122193054844](assets/image-20191122193054844.png)\n\n#### 4.2.3 输出一个table\n\n![image-20191122193124451](assets/image-20191122193124451.png)\n\n#### 4.2.4 Table API\n\n![image-20191122193206331](assets/image-20191122193206331.png)","tags":["hadoop","flink"]},{"title":"Flume日志采集框架","url":"/2019/11/21/it/flume-sqoop-zakaban/Flume日志采集框架/","content":"\n# Flume日志采集框架\n\n### 1. Flume是什么 \n\n![image-20191126223347760](assets/image-20191126223347760.png)\n\n~~~\n\t在一个完整的离线大数据处理系统中，除了hdfs+mapreduce+hive组成分析系统的核心之外，还需要数据采集、结果数据导出、任务调度等不可或缺的辅助系统，而这些辅助工具在hadoop生态体系中都有便捷的开源框架。\n~~~\n\n* Flume是Cloudera提供的一个高可用的，高可靠的，分布式的==海量日志采集、聚合和传输的系统==\n* Flume支持在日志系统中定制各类数据发送方，用于收集数据；\n* Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。\n\n重构后的版本统称为 Flume NG（next generation）；改动的另一原因是将 Flume 纳入 apache 旗下，cloudera Flume 改名为 Apache Flume。\n\n　　备注：Flume参考资料\n\n　　　　官方网站： http://flume.apache.org/\n　　　　用户文档： http://flume.apache.org/FlumeUserGuide.html\n　　　　开发文档： http://flume.apache.org/FlumeDeveloperGuide.html\n\n### 2. Flume的架构\n\n* Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。\n* Flume分布式系统中==最核心的角色是agent==，flume采集系统就是由一个个agent所连接起来形成。\n\n#### 2.1 核心概念\n\n- Client：\n  \n  - Client生产数据，运行在一个独立的线程。\n  \n- Event：\n  \n  -  一个数据单元，消息头和消息体组成。（Events可以是日志记录、 avro 对象等。）\n  \n- Flow：\n  \n  -  Event从源点到达目的点的迁移的抽象。\n  \n- Agent：\n  \n  - 一个独立的Flume进程，包含组件Source、 Channel、 Sink。（Agent使用JVM 运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。）\n  \n- Source：\n  \n     -  数据收集组件。（source从Client收集数据，传递给Channel）\n     \n- Channel：\n\n     -  中转Event的一个临时存储，保存由Source组件传递过来的Event。（Channel连接 sources 和 sinks ，这个有点像一个队列。）\n\n- Sink： \n\n     - 从Channel中读取并移除Event， 将Event传递到FlowPipeline中的下一个Agent（如果有的话）（Sink从Channel收集数据，运行在一个独立线程。） \n\n#### 2.2 Agent结构\n\n　Flume 运行的核心是 Agent。Flume以agent为最小的独立运行单位。一个agent就是一个JVM。它是一个完整的数据收集工具，含有三个核心组件，分别是\n\n　　 source、 channel、 sink。通过这些组件， Event 可以从一个地方流向另一个地方，如下图所示。\n\n　　![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108130603872-780242084.png)\n\n#### 2.3 \t Source\n\n- 　\tSource是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入Channel中。 Flume提供了很多内置的Source， 支持 Avro， log4j， syslog 和 http post(body为json格式)。可以让应用程序同已有的Source直接打交道，如AvroSource，SyslogTcpSource。 如果内置的Source无法满足需要， Flume还支持自定义Source。\n  　　![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108130818481-670496307.png)\n\n- 　　source类型：\n\n![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108130931325-512757774.png)\n\n#### 2.4、Channel\n\n- 　　Channel是连接Source和Sink的组件，大家可以将它看做一个数据的缓冲区（数据队列），它可以将事件暂存到内存中也可以持久化到本地磁盘上， 直到Sink处理完该事件。介绍两个较为常用的Channel， MemoryChannel和FileChannel。\n\n- 　　Channel类型：\n\n　　![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108131248653-2068131817.png)\n\n#### 2.4、Sink\n\n- 　　Sink从Channel中取出事件，然后将数据发到别处，可以向文件系统、数据库、 hadoop存数据， 也可以是其他agent的Source。在日志数据较少时，可以将数据存储在文件系统中，并且设定一定的时间间隔保存数据。\n\n![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108131438466-1752568401.png)\n\n- 　　Sink类型：\n\n　　![img](https://images2017.cnblogs.com/blog/999804/201711/999804-20171108131516809-1930573599.png)\n\n### 3. Flume采集系统结构图\n\n#### 3.1 简单结构\n\n* 单个agent采集数据\n\n![](assets/flume.png)\n\n\n\n#### 3.2 复杂结构\n\n* 2个agent串联\n\n![UserGuide_image03](assets/UserGuide_image03.png)\n\n\n\n* 多个agent串联\n\n![](assets/UserGuide_image02.png)\n\n\n\n* 多个channel\n\n![](assets/UserGuide_image04.png)\n\n\n\n### 4. Flume安装部署\n\n==Flume安装很简单，解压好基本上就可以使用==\n\n* 1、下载安装包\n\n  - <http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.2.tar.gz>\n  - flume-ng-1.6.0-cdh5.14.2.tar.gz\n\n* 2、规划安装目录\n\n  - /kkb/install\n\n* 3、上传安装包到服务器\n\n* 4、解压安装包到指定的规划目录\n\n  - tar -zxvf flume-ng-1.6.0-cdh5.14.2.tar.gz -C /kkb/install\n\n* 5、重命名解压目录\n\n  - mv apache-flume-1.6.0-cdh5.14.2-bin  flume-1.6.0-cdh5.14.2\n\n* 6、修改配置\n\n  * 进入到flume安装目录下的conf文件夹中\n\n    * 先重命名文件\n\n      * mv flume-env.sh.template flume-env.sh\n\n    * 修改文件，添加java环境变量\n\n      * vim flume-env.sh\n\n      ~~~shell\n      export JAVA_HOME=/kkb/install/jdk1.8.0_141\n      ~~~\n\n\n\n### 5. Flume实战\n\n#### 5.1 采集文件到控制台\n\n- 1、需求描述\n\n  ```\n  监控一个文件如果有新增的内容就把数据采集之后打印控制台，通常用于测试/调试目的\n  ```\n\n- 2、==flume配置文件开发==\n\n  * 在flume的安装目录下创建一个文件夹myconf， 后期存放flume开发的配置文件\n    * mkdir /kfly/install/flume-1.6.0-cdh5.14.2/myconf\n\n  - vim tail-memory-logger.conf\n\n  ```properties\n  # Name the components on this agent\n  #定义一个agent，分别指定source、channel、sink别名\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  #配置source\n  #指定source的类型为exec，通过Unix命令来传输结果数据\n  a1.sources.r1.type = exec\n  #监控一个文件，有新的数据产生就不断采集走\n  a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n  #指定source的数据流入的channel中\n  a1.sources.r1.channels = c1\n  \n  #配置channel\n  #指定channel的类型为memory\n  a1.channels.c1.type = memory\n  #指定channel的最多可以存放数据的容量\n  a1.channels.c1.capacity = 1000\n  #指定在一个事务中source写数据到channel或者sink从channel取数据最大条数\n  a1.channels.c1.transactionCapacity = 100\n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  #类型是日志格式，结果会打印在控制台\n  a1.sinks.k1.type = logger\n  ```\n\n- **3、启动agent**\n\n  * 进入到node01上的/kkb/install/flume-1.6.0-cdh5.14.2目录下执行\n\n  ```shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/tail-memory-logger.conf -Dflume.root.logger=info,console\n  \n  \n  其中：\n  -n表示指定该agent名称\n  -c表示配置文件所在的目录\n  -f表示配置文件的路径名称\n  -D表示指定key=value键值对---这里指定的是启动的日志输出级别\n  ```\n\n\n\n#### 5.2 采集文件到HDFS\n\n- 1、需求描述\n\n  ```\n  监控一个文件如果有新增的内容就把数据采集到HDFS上\n  ```\n\n- 2、结构示意图\n\n![file-Flume-HDFS](assets/file-Flume-HDFS.png)\n\n- ==3、flume配置文件开发==\n\n  - vim file2Hdfs.conf\n\n  ```properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  #配置source\n  a1.sources.r1.type = exec\n  a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n  a1.sources.r1.channels = c1\n  \n  #配置channel\n  a1.channels.c1.type = file\n  #设置检查点目录--该目录是记录下event在数据目录下的位置\n  a1.channels.c1.checkpointDir=/kfly/data/flume_checkpoint\n  #数据存储所在的目录\n  a1.channels.c1.dataDirs=/kfly/data/flume_data\n  \n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  #指定sink类型为hdfs\n  a1.sinks.k1.type = hdfs\n  #指定数据收集到hdfs目录\n  a1.sinks.k1.hdfs.path = hdfs://node01:8020/tailFile/%Y-%m-%d/%H%M\n  #指定生成文件名的前缀\n  a1.sinks.k1.hdfs.filePrefix = events-\n  \n  #是否启用时间上的”舍弃”   -->控制目录 \n  a1.sinks.k1.hdfs.round = true\n  #时间上进行“舍弃”的值\n  # 如 12:10 -- 12:19 => 12:10\n  # 如 12:20 -- 12:29 => 12:20\n  a1.sinks.k1.hdfs.roundValue = 10\n  #时间上进行“舍弃”的单位\n  a1.sinks.k1.hdfs.roundUnit = minute\n  \n  # 控制文件个数\n  #60s或者50字节或者10条数据，谁先满足，就开始滚动生成新文件\n  a1.sinks.k1.hdfs.rollInterval = 60\n  a1.sinks.k1.hdfs.rollSize = 50\n  a1.sinks.k1.hdfs.rollCount = 10\n  \n  #每个批次写入的数据量\n  a1.sinks.k1.hdfs.batchSize = 100\n  \n  #开始本地时间戳--开启后就可以使用%Y-%m-%d去解析时间\n  a1.sinks.k1.hdfs.useLocalTimeStamp = true\n  \n  #生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\n  a1.sinks.k1.hdfs.fileType = DataStream\n  ```\n\n- **4、启动agent**\n\n  - 进入到node01上的/kkb/install/flume-1.6.0-cdh5.14.2目录下执行\n\n    ```shell\n    bin/flume-ng agent -n a1 -c myconf -f myconf/file2Hdfs.conf -Dflume.root.logger=info,console\n    ```\n\n\n\n#### 5.3 采集目录到HDFS \n\n* 1、需求描述\n\n  ~~~\n  一个目录中不断有新的文件产生，需要把目录中的文件不断地进行数据收集保存到HDFS上\n  \n  ~~~\n\n* 2、结构示意图\n\n  ![Flume-HDFS](/Users/dingchuangshi/Documents/hexo-kfly-blog/source/_posts/flume/assets/Dir-Flume-HDFS.png)\n\n* ==3、flume配置文件开发==\n\n  * 在myconf目录中创建配置文件添加内容\n    * vim  dir2Hdfs.conf\n\n  ~~~properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  # 配置source\n  ##注意：不能往监控目中重复丢同名文件\n  a1.sources.r1.type = spooldir\n  a1.sources.r1.spoolDir = /kfly/install/flumeData/files\n  # 是否将文件的绝对路径添加到header\n  a1.sources.r1.fileHeader = true\n  a1.sources.r1.channels = c1\n  \n  \n  #配置channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  \n  \n  #配置sink\n  a1.sinks.k1.type = hdfs\n  a1.sinks.k1.channel = c1\n  a1.sinks.k1.hdfs.path = hdfs://node01:8020/spooldir/%Y-%m-%d/%H%M\n  a1.sinks.k1.hdfs.filePrefix = events-\n  a1.sinks.k1.hdfs.round = true\n  a1.sinks.k1.hdfs.roundValue = 10\n  a1.sinks.k1.hdfs.roundUnit = minute\n  a1.sinks.k1.hdfs.rollInterval = 60\n  a1.sinks.k1.hdfs.rollSize = 50\n  a1.sinks.k1.hdfs.rollCount = 10\n  a1.sinks.k1.hdfs.batchSize = 100\n  a1.sinks.k1.hdfs.useLocalTimeStamp = true\n  #生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\n  a1.sinks.k1.hdfs.fileType = DataStream\n  \n  \n  ~~~\n\n\n* **4、启动agent**\n\n\n  * 进入到node01上的/kkb/install/flume-1.6.0-cdh5.14.2目录下执行\n\n  ```shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/dir2Hdfs.conf -Dflume.root.logger=info,console\n\n  ```\n\n\n\n\n#### 5.4 两个agent级联\n\n* 1、需求描述\n\n~~~\n\t第一个agent负责监控某个目录中新增的文件进行数据收集，通过网络发送到第二个agent当中去，第二个agent负责接收第一个agent发送的数据，并将数据保存到hdfs上面去。\n\n~~~\n\n* 2、结构示意图\n\n![](assets/2个agent级联.png)\n\n* 3、在node01和node02上分别都安装flume\n\n* 4、创建node01上的flume配置文件\n\n  * vim dir2avro.conf\n\n  ~~~properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  # 配置source\n  ##注意：不能往监控目中重复丢同名文件\n  a1.sources.r1.type = spooldir\n  a1.sources.r1.spoolDir = /kfly/install/flumeData/files\n  a1.sources.r1.fileHeader = true\n  a1.sources.r1.channels = c1\n  \n  \n  #配置channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  #AvroSink是用来通过网络来传输数据的,可以将event发送到RPC服务器(比如AvroSource)\n  a1.sinks.k1.type = avro\n  \n  #node02 注意修改为自己的hostname\n  a1.sinks.k1.hostname = node02\n  a1.sinks.k1.port = 5211\n  \n  ~~~\n\n* 5、创建node02上的flume配置文件\n\n  * vim avro2Hdfs.conf\n\n  ~~~properties\n  # Name the components on this agent\n  a1.sources = r1\n  a1.sinks = k1\n  a1.channels = c1\n  \n  #配置source\n  #通过AvroSource接受AvroSink的网络数据\n  a1.sources.r1.type = avro\n  a1.sources.r1.channels = c1\n  #AvroSource服务的ip地址\n  a1.sources.r1.bind = node02\n  #AvroSource服务的端口\n  a1.sources.r1.port = 5211\n  \n  #配置channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  \n  #配置sink\n  a1.sinks.k1.channel = c1\n  a1.sinks.k1.type = hdfs\n  a1.sinks.k1.hdfs.path = hdfs://node01:8020/avro-hdfs/%Y-%m-%d/%H-%M\n  a1.sinks.k1.hdfs.filePrefix = events-\n  a1.sinks.k1.hdfs.round = true\n  a1.sinks.k1.hdfs.roundValue = 10\n  a1.sinks.k1.hdfs.roundUnit = minute\n  a1.sinks.k1.hdfs.rollInterval = 60\n  a1.sinks.k1.hdfs.rollSize = 50\n  a1.sinks.k1.hdfs.rollCount = 10\n  a1.sinks.k1.hdfs.batchSize = 100\n  a1.sinks.k1.hdfs.useLocalTimeStamp = true\n  #生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\n  a1.sinks.k1.hdfs.fileType = DataStream\n  \n  ~~~\n\n* **6、启动agent**\n\n  * 先启动node02上的flume。然后在启动node01上的flume\n\n    * 在node02上的flume安装目录下执行\n\n    ~~~shell\n    bin/flume-ng agent -n a1 -c myconf -f myconf/avro2Hdfs.conf -Dflume.root.logger=info,console\n    \n    ~~~\n\n    * 在node01上的flume安装目录下执行\n\n    ~~~shell\n    bin/flume-ng agent -n a1 -c myconf -f myconf/dir2avro.conf -Dflume.root.logger=info,console\n    \n    ~~~\n\n    * 最后在node01上的/kfly/install/flumeData/files目录下创建一些数据文件，最后去HDFS上查看数据。\n\n\n\n### 6. 高可用配置案例\n\n#### 6.1 failover故障转移\n\n![flume-failover](assets/flume-failover.png)\n\n* 1、节点分配\n\n|    名称    | 服务器主机名 |     ip地址      |    角色    |\n| :--------: | :----------: | :-------------: | :--------: |\n|   Agent1   |    node01    | 192.168.200.200 | WebServer  |\n| Collector1 |    node02    | 192.168.200.210 | AgentMstr1 |\n| Collector2 |    node03    | 192.168.200.220 | AgentMstr2 |\n\n~~~\nAgent1数据分别流入到Collector1和Collector2，Flume NG本身提供了Failover机制，可以自动切换和恢复。\n\n~~~\n\n* 2、开发配置文件\n\n  * node01、node02、node03分别都要安装flume\n\n  * ==创建node01上的flume配置文件==\n\n    * vim flume-client-failover.conf\n\n    ~~~properties\n    #agent name\n    a1.channels = c1\n    a1.sources = r1\n    #定义了2个sink\n    a1.sinks = k1 k2\n    \n    #set gruop\n    #设置一个sink组，一个sink组下可以包含很多个sink\n    a1.sinkgroups = g1\n    \n    #set sink group\n    #指定g1这个sink组下有k1  k2 这2个sink\n    a1.sinkgroups.g1.sinks = k1 k2\n    \n    #set source\n    a1.sources.r1.channels = c1\n    a1.sources.r1.type = exec\n    a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n    \n    #set channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    \n    # set sink1    指定sink1的数据会传输给node02\n    a1.sinks.k1.channel = c1\n    a1.sinks.k1.type = avro\n    a1.sinks.k1.hostname = node02\n    a1.sinks.k1.port = 52020\n    \n    # set sink2    指定sink2的数据会传输给node03\n    a1.sinks.k2.channel = c1\n    a1.sinks.k2.type = avro\n    a1.sinks.k2.hostname = node03\n    a1.sinks.k2.port = 52020\n    \n    #set failover\n    #指定sink组高可用的策略---failover故障转移\n    a1.sinkgroups.g1.processor.type = failover\n    #指定k1这个sink的优先级\n    a1.sinkgroups.g1.processor.priority.k1 = 10\n    #指定k2这个sink的优先级\n    a1.sinkgroups.g1.processor.priority.k2 = 5\n    #指定故障转移的最大时间，如果超时会出现异常\n    a1.sinkgroups.g1.processor.maxpenalty = 10000\n    \n    ~~~\n\n    ~~~properties\n    说明：\n    #这里首先要申明一个sinkgroups,然后再设置2个sink ,k1与k2,其中2个优先级是10和5。\n    #而processor的maxpenalty被设置为10秒，默认是30秒.表示故障转移的最大时间\n    \n    ~~~\n\n    \n\n  * ==创建node02和node03上的flume配置文件==\n  \n  * node02和node03上配置信息相同\n    * vim flume-server-failover.conf\n  \n    ~~~properties\n    #set Agent name\n    a1.sources = r1\n    a1.channels = c1\n    a1.sinks = k1\n    \n    #set channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    # set source\n    a1.sources.r1.type = avro\n    a1.sources.r1.bind = 0.0.0.0\n    a1.sources.r1.port = 52020\n    a1.sources.r1.channels = c1\n    \n    #配置拦截器\n    #指定2个拦截器  i1  i2 \n    a1.sources.r1.interceptors = i1 i2\n    #i1的类型为时间戳拦截器  可以解析%Y-%m-%d 时间\n    a1.sources.r1.interceptors.i1.type = timestamp\n    #i2的类型为主机拦截器，可以获取当前event中携带的主机名\n    a1.sources.r1.interceptors.i2.type = host\n    #指定主机名变量\n    a1.sources.r1.interceptors.i2.hostHeader=hostname\n    \n    #set sink to hdfs\n    a1.sinks.k1.channel = c1\n    a1.sinks.k1.type=hdfs\n    a1.sinks.k1.hdfs.path=hdfs://node01:8020/failover/logs/%{hostname}\n    a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d\n    a1.sinks.k1.hdfs.round = true\n    a1.sinks.k1.hdfs.roundValue = 10\n    a1.sinks.k1.hdfs.roundUnit = minute\n    a1.sinks.k1.hdfs.rollInterval = 60\n    a1.sinks.k1.hdfs.rollSize = 50\n    a1.sinks.k1.hdfs.rollCount = 10\n    a1.sinks.k1.hdfs.batchSize = 100\n    a1.sinks.k1.hdfs.fileType = DataStream\n    \n    ~~~\n\n\n* 3、启动flume配置\n\n  * 先分别在node02和node03上启动flume\n    * 分别进入到flume的安装目录下执行命令\n\n  ~~~shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/flume-server-failover.conf -Dflume.root.logger=info,console\n  \n  ~~~\n\n\n\n  * 然后在node01上启动flume\n    * 进入到flume的安装目录下执行命令\n\n  ~~~shell\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-client-failover.conf -Dflume.root.logger=info,console\n\n  ~~~\n\n\n\n  * 最后在hdfs目录上观察数据\n\n  ~~~shell\nhdfs://node01:8020/failover/logs\n\n  ~~~\n\n\n\n\n#### 6.2 load balance负载均衡\n\n* 实现多个flume采集数据的时候避免单个flume的负载比较高，实现多个flume采集器负载均衡。\n\n* 1、节点分配\n\n  * 与failover故障转移的节点分配\n\n* 2、开发配置文件\n\n  * ==创建node01上的flume配置文件==\n\n    * vim  flume-client-loadbalance.conf\n\n    ~~~properties\n    #agent name\n    a1.channels = c1\n    a1.sources = r1\n    a1.sinks = k1 k2\n    \n    #set gruop\n    a1.sinkgroups = g1\n    \n    #set sink group\n    a1.sinkgroups.g1.sinks = k1 k2\n    \n    #set source\n    a1.sources.r1.channels = c1\n    a1.sources.r1.type = exec\n    a1.sources.r1.command = tail -F /kfly/install/flumeData/tail.log\n    \n    #set channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    \n    # set sink1\n    a1.sinks.k1.channel = c1\n    a1.sinks.k1.type = avro\n    a1.sinks.k1.hostname = node02\n    a1.sinks.k1.port = 52020\n    \n    # set sink2\n    a1.sinks.k2.channel = c1\n    a1.sinks.k2.type = avro\n    a1.sinks.k2.hostname = node03\n    a1.sinks.k2.port = 52020\n    \n    #set load-balance\n    #指定sink组高可用的策略---load_balance负载均衡\n    a1.sinkgroups.g1.processor.type =load_balance\n    # 默认是round_ robin，还可以选择random\n    a1.sinkgroups.g1.processor.selector = round_robin\n    #如果backoff被开启，则sink processor会屏蔽故障的sink\n    a1.sinkgroups.g1.processor.backoff = true\n    \n    ~~~\n\n  * ==创建node02和node03上的flume配置文件==\n\n    * vim  flume-server-loadbalance.conf\n\n\n  ~~~properties\n  #set Agent name\n  a1.sources = r1\n  a1.channels = c1\n  a1.sinks = k1\n  \n  #set channel\n  a1.channels.c1.type = memory\n  a1.channels.c1.capacity = 1000\n  a1.channels.c1.transactionCapacity = 100\n  \n  # set source\n  a1.sources.r1.type = avro\n  a1.sources.r1.bind = 0.0.0.0\n  a1.sources.r1.port = 52020\n  a1.sources.r1.channels = c1\n  \n  #配置拦截器\n  a1.sources.r1.interceptors = i1 i2\n  a1.sources.r1.interceptors.i1.type = timestamp\n  a1.sources.r1.interceptors.i2.type = host\n  a1.sources.r1.interceptors.i2.hostHeader=hostname\n  #hostname不使用ip显示，直接就是该服务器对应的主机名\n  a1.sources.r1.interceptors.i2.useIP=false\n  \n  #set sink to hdfs\n  a1.sinks.k1.channel = c1\n  a1.sinks.k1.type=hdfs\n  a1.sinks.k1.hdfs.path=hdfs://node01:8020/loadbalance/logs/%{hostname}\n  a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d\n  a1.sinks.k1.hdfs.round = true\n  a1.sinks.k1.hdfs.roundValue = 10\n  a1.sinks.k1.hdfs.roundUnit = minute\n  a1.sinks.k1.hdfs.rollInterval = 60\n  a1.sinks.k1.hdfs.rollSize = 50\n  a1.sinks.k1.hdfs.rollCount = 10\n  a1.sinks.k1.hdfs.batchSize = 100\n  a1.sinks.k1.hdfs.fileType = DataStream\n\n  ~~~\n\n\n* 3、启动flume配置\n\n  - 先分别在node02和node03上启动flume\n    - 分别进入到flume的安装目录下执行命令\n\n  ```shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/flume-server-loadbalance.conf -Dflume.root.logger=info,console\n  \n  ```\n\n\n  * 然后在node01上启动flume\n\n    * 分别进入到flume的安装目录下执行命令\n\n  ~~~shell\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-client-loadbalance.conf -Dflume.root.logger=info,console\n\n  ~~~\n\n\n  - 最后在hdfs上目录观察数据\n\n  ~~~shell\nhdfs://node01:8020/loadbalance/logs\n\n  ~~~\n\n\n### 7. flume企业案例\n\n#### 7.1 flume案例之静态拦截器使用\n\n* 1、案例场景\n\n~~~\nA、B两台日志服务机器实时生产日志主要类型为access.log、nginx.log、web.log \n现在需要把A、B 机器中的access.log、nginx.log、web.log 采集汇总到C机器上然后统一收集到hdfs中。\n但是在hdfs中要求的目录为：\n/source/logs/access/20180101/**\n/source/logs/nginx/20180101/**\n/source/logs/web/20180101/**\n\n~~~\n\n* 2、场景分析\n\n![flume采集不同的日志数据](assets/flume采集不同的日志数据.png)\n\n* 3、数据流程处理分析\n\n![](assets/flume采集不同的日志数据流程分析.png)\n\n* 4、开发配置文件\n\n  * ==在node01与node02服务器开发flume的配置文件==\n\n    * vim exec_source_avro_sink.conf\n\n    ~~~properties\n    # Name the components on this agent\n    #定义三个source\n    a1.sources = r1 r2 r3\n    a1.sinks = k1\n    a1.channels = c1\n    \n    # Describe/configure the source\n    a1.sources.r1.type = exec\n    a1.sources.r1.command = tail -F /kfly/install/flumeData/access.log\n    #指定source r1 使用拦截器i1\n    a1.sources.r1.interceptors = i1\n    #拦截器类型static静态\n    a1.sources.r1.interceptors.i1.type = static\n    ## static拦截器的功能就是往采集到的数据的header中插入自己定义的key-value对\n    # 自己进行设置,我们这里的key和value相当于键值对,k=type v=access\n    a1.sources.r1.interceptors.i1.key = type\n    a1.sources.r1.interceptors.i1.value = access\n    \n    a1.sources.r2.type = exec\n    a1.sources.r2.command = tail -F /kfly/install/flumeData/nginx.log\n    #指定source r2 使用拦截器i2\n    a1.sources.r2.interceptors = i2\n    #拦截器类型static静态\n    a1.sources.r2.interceptors.i2.type = static\n    # 自己进行设置\n    a1.sources.r2.interceptors.i2.key = type\n    a1.sources.r2.interceptors.i2.value = nginx\n    \n    a1.sources.r3.type = exec\n    a1.sources.r3.command = tail -F /kfly/install/flumeData/web.log\n    #指定source r3 使用拦截器i3\n    a1.sources.r3.interceptors = i3\n    #拦截器类型static静态\n    a1.sources.r3.interceptors.i3.type = static\n    # 自己进行设置\n    a1.sources.r3.interceptors.i3.key = type\n    a1.sources.r3.interceptors.i3.value = web\n    \n    # Use a channel which buffers events in memory\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 20000\n    a1.channels.c1.transactionCapacity = 10000\n    \n    # Describe the sink\n    a1.sinks.k1.type = avro\n    a1.sinks.k1.hostname = node03\n    a1.sinks.k1.port = 41414\n    a1.sinks.k1.channel = c1\n    \n    # Bind the source and sink to the channel\n    a1.sources.r1.channels = c1\n    a1.sources.r2.channels = c1\n    a1.sources.r3.channels = c1\n    \n    \n    ~~~\n\n  * ==在node03服务器上开发flume配置文件==\n\n    * vim avro_source_hdfs_sink.conf\n\n    ~~~properties\n    a1.sources = r1\n    a1.sinks = k1\n    a1.channels = c1\n    #定义source\n    a1.sources.r1.type = avro\n    a1.sources.r1.bind = node03\n    a1.sources.r1.port =41414\n    \n    #定义channels\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 20000\n    a1.channels.c1.transactionCapacity = 1000\n    \n    #定义sink\n    a1.sinks.k1.type = hdfs\n    # 此处的%{type} 这里是取我们在node01和node02定义的type的值,也就是value\n    a1.sinks.k1.hdfs.path=hdfs://node01:8020/source/logs/%{type}/%Y%m%d\n    a1.sinks.k1.hdfs.filePrefix =events-\n    a1.sinks.k1.hdfs.fileType = DataStream\n    a1.sinks.k1.hdfs.writeFormat = Text\n    #时间类型\n    a1.sinks.k1.hdfs.useLocalTimeStamp = true\n    #生成的文件不按条数生成\n    a1.sinks.k1.hdfs.rollCount = 0\n    #生成的文件按时间生成\n    a1.sinks.k1.hdfs.rollInterval = 30\n    #生成的文件按大小生成\n    a1.sinks.k1.hdfs.rollSize  = 10485760\n    #批量写入hdfs的个数\n    a1.sinks.k1.hdfs.batchSize = 1000\n    #flume操作hdfs的线程数（包括新建，写入等）\n    a1.sinks.k1.hdfs.threadsPoolSize=10\n    #操作hdfs超时时间\n    a1.sinks.k1.hdfs.callTimeout=30000\n    \n    #组装source、channel、sink\n    a1.sources.r1.channels = c1\n    a1.sinks.k1.channel = c1\n    \n    ~~~\n\n* 5、启动flume配置\n\n  * 先在node03上启动flume\n\n  ```shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/avro_source_hdfs_sink.conf -Dflume.root.logger=info,console\n  \n  ```\n\n\n#### 7.2 flume案例之自定义拦截器\n\n* 1、案例场景\n\n~~~\n在数据采集之后，通过flume的拦截器，实现不需要的数据过滤掉，并将指定的第一个字段进行加密，加密之后再往hdfs上面保存\n\n~~~\n\n* 2、数据文件 user.txt\n\n~~~\n13901007610,male,30,sing,beijing\n18600000035,male,40,dance,shanghai\n13366666659,male,20,Swimming,wuhan\n13801179888,female,18,dance,tianjin\n18511111114,male,35,sing,beijing\n13718428888,female,40,Foodie,shanghai\n13901057088,male,50,Basketball,taiwan\n13671057777,male,60,Bodybuilding,xianggang\n\n~~~\n\n![](assets/flume-custom-interceptor.png)\n\n* 3、创建maven工程添加依赖\n\n~~~xml\n<dependency>\n        <groupId>org.apache.flume</groupId>\n        <artifactId>flume-ng-core</artifactId>\n        <version>1.6.0-cdh5.14.2</version>\n    </dependency>\n~~~\n\n\n\n* 4、代码开发\n\n~~~java\npackage bigdata.flume;\n\nimport com.google.common.base.Charsets;\nimport org.apache.flume.Context;\nimport org.apache.flume.Event;\nimport org.apache.flume.interceptor.Interceptor;\nimport java.math.BigInteger;\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.ArrayList;\nimport java.util.List;\n\n/**\n * @author dingchuangshi\n */\npublic class CustomInterceptor implements Interceptor {\n\n    /**\n     * encrypted_field_index.\n     * 指定需要加密的字段下标\n     */\n    private final String encrypted_field_index;\n\n\n    /**\n     * The out_index.\n     * 指定不需要对应列的下标\n     */\n    private final String out_index;\n\n\n    /**\n     * 提供构建方法，后期可以接受配置文件中的参数\n     * @param encrypted_field_index\n     * @param out_index\n     */\n    public CustomInterceptor(String encrypted_field_index, String out_index) {\n        this.encrypted_field_index = encrypted_field_index;\n        this.out_index = out_index;\n    }\n\n    /**\n     * 定义拦截器规则\n     * @param event\n     * @return\n     */\n    @Override\n    public Event intercept(Event event) {\n        if(event == null){\n            return null;\n        }\n       try{\n           String line = new String(event.getBody(), Charsets.UTF_8);\n           String newLine = \"\";\n           String[] splits = line.split(\",\");\n           for (int i = 0; i < splits.length; i++) {\n               // 加密索引\n               int encryptedField = Integer.parseInt(encrypted_field_index);\n               // 忽略索引\n               int outIndex = Integer.parseInt(out_index);\n\n               if(i == encryptedField){\n                   // 加密\n                   newLine += md5(splits[encryptedField]) + \",\";\n               }else if(i != outIndex){\n                   // 忽略取消数据\n                   newLine += splits[i] + \",\";\n               }\n           }\n           // 去掉最后一个'，'符号\n           newLine = newLine.substring(0,newLine.length() - 1);\n           event.setBody(newLine.getBytes(Charsets.UTF_8));\n       }catch (Exception e){\n           e.printStackTrace();\n       }\n        return event;\n    }\n\n    @Override\n    public List<Event> intercept(List<Event> events) {\n        List<Event> out = new ArrayList<Event>();\n        for (Event event : events) {\n            Event outEvent = intercept(event);\n            if (outEvent != null) {\n                out.add(outEvent);\n            }\n        }\n        return out;\n    }\n\n    @Override\n    public void initialize() {\n\n    }\n\n    @Override\n    public void close() {\n\n    }\n\n    /**\n     * md5加密\n     * @return\n     */\n    public String md5(String plainText){\n        byte[] secretBytes = null;\n        try {\n            MessageDigest instance = MessageDigest.getInstance(\"md5\");\n            instance.update(plainText.getBytes());\n            secretBytes = instance.digest();\n        } catch (NoSuchAlgorithmException e) {\n            System.out.println(\"没有md5这个算法\");\n            e.printStackTrace();\n        }\n        String md5Code = new BigInteger(1,secretBytes).toString(16);\n        for (int i = 0; i < 32 - md5Code.length(); i++) {\n            md5Code =\"0\" + md5Code;\n        }\n        return md5Code;\n    }\n\n\n    public static class MyBuilder  implements CustomInterceptor.Builder {\n        /**\n         * encrypted_field_index.\n         * 指定需要加密的字段下标\n         */\n        private String encrypted_field_index;\n\n        /**\n         * The out_index.\n         * 指定不需要对应列的下标\n         */\n        private String out_index;\n\n        @Override\n        public CustomInterceptor build() {\n            return new CustomInterceptor(encrypted_field_index, out_index);\n        }\n\n\n        @Override\n        public void configure(Context context) {\n            this.encrypted_field_index = context.getString(\"encrypted_field_index\", \"\");\n            this.out_index = context.getString(\"out_index\", \"\");\n        }\n    }\n}\n\n~~~\n\n* 5、打成jar包后放到flume安装目录下的lib中\n\n\n\n* 6、创建配置文件 flume-interceptor-hdfs.conf\n\n~~~properties\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n#配置source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /kfly/install/flumeData/user.txt\na1.sources.r1.channels = c1\na1.sources.r1.interceptors =i1\na1.sources.r1.interceptors.i1.type =bigdata.flume.CustomInterceptor$MyBuilder\na1.sources.r1.interceptors.i1.encrypted_field_index=0\na1.sources.r1.interceptors.i1.out_index=3\n\n#配置channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n\n\n#配置sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.channel = c1\na1.sinks.k1.hdfs.path = hdfs://node01:8020/interceptor/files/%Y-%m-%d/%H%M\na1.sinks.k1.hdfs.filePrefix = events-\n# 时间舍弃\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = minute\n# 限定生成文件时机\na1.sinks.k1.hdfs.rollInterval = 5\na1.sinks.k1.hdfs.rollSize = 50\na1.sinks.k1.hdfs.rollCount = 10\na1.sinks.k1.hdfs.batchSize = 100\na1.sinks.k1.hdfs.useLocalTimeStamp = true\n#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本\na1.sinks.k1.hdfs.fileType = DataStream\n\n~~~\n\n* 7、进入到flume安装目录下启动flume\n\n~~~shell\nbin/flume-ng agent -n a1 -c myconf -f myconf/flume-interceptor-hdfs.conf -Dflume.root.logger=info,console\n\n~~~\n\n\n\n### 8. flume自定义Source\n\n#### 8.1 场景描述\n\n~~~\n\t官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。如：实时监控MySQL，从MySQL中获取数据传输到HDFS或者其他存储框架，所以此时需要我们自己实现MySQLSource。\n\n官方也提供了自定义source的接口：\n官网说明：https://flume.apache.org/FlumeDeveloperGuide.html#source\n\n~~~\n\n\n\n#### 8.2 自定义MysqlSource步骤\n\n* 1、根据官方说明自定义mysqlsource需要继承AbstractSource类并实现Configurable和PollableSource接口。\n\n\n\n* 2、实现对应的方法\n  * configure(Context context)\n    * 初始化context\n  * process()\n    * 从mysql表中获取数据，然后把数据封装成event对象写入到channel，该方法被一直调用\n  * stop()\n    * 关闭相关资源\n\n\n\n* 3、开发流程\n\n  * 3.1 创建mysql数据库以及mysql数据库表\n\n  ~~~sql\n  --创建一个数据库\n  CREATE DATABASE IF NOT EXISTS mysqlsource DEFAULT CHARACTER SET utf8 ;\n  \n  --创建一个表，用户保存拉取目标表位置的信息\n  CREATE TABLE mysqlsource.flume_meta (\n    source_tab varchar(255) NOT NULL,\n    currentIndex varchar(255) NOT NULL,\n    PRIMARY KEY (source_tab)\n  ) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n  \n  --插入数据\n  insert  into mysqlsource.flume_meta(source_tab,currentIndex) values ('student','4');\n  \n  \n  --创建要拉取数据的表\n  CREATE TABLE mysqlsource.student(\n    id int(11) NOT NULL AUTO_INCREMENT,\n    name varchar(255) NOT NULL,\n    PRIMARY KEY (id)\n  ) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;\n  \n  --向student表中添加测试数据\n  insert  into mysqlsource.student(id,name) values (1,'zhangsan'),(2,'lisi'),(3,'wangwu'),(4,'zhaoliu');\n  \n  ~~~\n\n  * 3.2 代码开发实现\n\n    *  构建maven工程，添加依赖\n\n    ~~~xml\n        <dependencies>\n            <dependency>\n                <groupId>mysql</groupId>\n                <artifactId>mysql-connector-java</artifactId>\n                <version>5.1.38</version>\n            </dependency>\n            <dependency>\n                <groupId>org.apache.commons</groupId>\n                <artifactId>commons-lang3</artifactId>\n                <version>3.6</version>\n            </dependency>\n        </dependencies>\n    \n    \n    ~~~\n\n    * 在resources资源文件夹下添加jdbc.properties\n\n      * ==jdbc.properties==\n\n      ~~~~properties\n      dbDriver=com.mysql.jdbc.Driver\n      dbUrl=jdbc:mysql://node03:3306/mysqlsource?useUnicode=true&characterEncoding=utf-8\n      dbUser=root\n      dbPassword=123456\n      \n      ~~~~\n\n    * ==核心代码==\n\n      ~~~java\n      /**\n       * 自定义source\n       * @author dingchuangshi\n       */\n      public class CustomSource extends AbstractSource implements Configurable, PollableSource {\n      \n          private static Logger logger = LoggerFactory.getLogger(CustomSource.class);\n      \t\t// 自定义的查询类\n          private QueryMysql sqlSourceHelp;\n      \n          @Override\n          public Status process() throws EventDeliveryException {\n              try {\n                  //查询数据表\n                  List<List<Object>> result = sqlSourceHelp.executeQuery();\n                  //存放event的集合\n                  List<Event> events = new ArrayList<>();\n                  //存放event头集合\n                  Map<String, String> header = new HashMap<>();\n                  //如果有返回数据，则将数据封装为event\n                  if (!result.isEmpty()) {\n                      List<String> allRows = sqlSourceHelp.getAllRows(result);\n                      Event event = null;\n                      for (String row : allRows) {\n                          event = new SimpleEvent();\n                          event.setBody(row.getBytes());\n                          event.setHeaders(header);\n                          events.add(event);\n                      }\n                      //将event写入channel\n                      this.getChannelProcessor().processEventBatch(events);\n                      //更新数据表中的offset信息\n                      sqlSourceHelp.updateOffset2DB(result.size());\n                  }\n                  //等待时长\n                  Thread.sleep(sqlSourceHelp.getRunQueryDelay());\n                  return Status.READY;\n              } catch (InterruptedException e) {\n                  logger.error(\"Error procesing row\", e);\n                  return Status.BACKOFF;\n              }\n          }\n      }\n      \n      ~~~\n\n\n\n* 4、测试\n\n  * 4.1 ==程序打成jar包，上传jar包到flume的lib目录下==\n\n  * 4.2 ==配置文件准备==\n\n    * vim mysqlsource.conf\n\n    ~~~properties\n    # Name the components on this agent\n    a1.sources = r1\n    a1.sinks = k1\n    a1.channels = c1\n    \n    # Describe/configure the source\n    a1.sources.r1.type = bigdata.flume.source.CustomSource\n    # 老师的是node01,同学们改成自己的节点 一定要注意\n    a1.sources.r1.connection.url = jdbc:mysql://node03:3306/mysqlsource\n    a1.sources.r1.connection.user = root\n    a1.sources.r1.connection.password = 123456\n    a1.sources.r1.table = student\n    a1.sources.r1.columns.to.select = *\n    a1.sources.r1.start.from=0\n    a1.sources.r1.run.query.delay=3000\n    \n    # Describe the channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    # Describe the sink\n    a1.sinks.k1.type = logger\n    \n    \n    # Bind the source and sink to the channel\n    a1.sources.r1.channels = c1\n    a1.sinks.k1.channel = c1\n    \n    ~~~\n\n* 4.3 ==启动flume配置==\n\n  ~~~shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/mysqlsource.conf -Dflume.root.logger=info,console\n  \n  ~~~\n\n  \n  * 4.4 最后向表添加数据，观察控制台信息\n\n\n\n### 9. flume自定义Sink\n\n#### 9.1 场景描述\n\n```\n\t官方提供的sink类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些sink。如：需要把接受到的数据按照规则进行过滤之后写入到某张mysql表中，所以此时需要我们自己实现MySQLSink。\n\n官方也提供了自定义sink的接口：\n官网说明：https://flume.apache.org/FlumeDeveloperGuide.html#sink\n```\n\n\n\n#### 9.2 自定义MysqlSink步骤\n\n- 1、根据官方说明自定义MysqlSink需要继承AbstractSink类并实现Configurable\n\n- 2、实现对应的方法\n  - configure(Context context)\n\n    - 初始化context\n\n  - start()\n\n    - 启动准备操作\n\n  - process()\n\n    - 从channel获取数据，然后解析之后，保存在mysql表中\n\n  - stop()\n\n    - 关闭相关资源\n\n\n- 3、开发流程\n\n  - 3.1 ==创建mysql数据库以及mysql数据库表==\n\n  ~~~SQL\n  --创建一个数据库\n  CREATE DATABASE IF NOT EXISTS mysqlsource DEFAULT CHARACTER SET utf8 ;\n  \n  --创建一个表，用户保存拉取目标表位置的信息\n  CREATE TABLE mysqlsource.flume2mysql (\n    id int(11) NOT NULL AUTO_INCREMENT,\n    create_time varchar(64) NOT NULL,\n    content varchar(255) NOT NULL,\n    PRIMARY KEY (id)\n  ) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n  ~~~\n\n\n\n  - 3.2  代码开发实现\n\n    - ==定义MysqlSink类==\n\n\n```java\npackage com.kaikeba.sink;\n\nimport org.apache.flume.conf.Configurable;\nimport org.apache.flume.*;\nimport org.apache.flume.sink.AbstractSink;\n\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.PreparedStatement;\nimport java.sql.SQLException;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * 自定义MysqlSink\n */\npublic class MysqlSink extends AbstractSink implements Configurable {\n    private String mysqlurl = \"\";\n    private String username = \"\";\n    private String password = \"\";\n    private String tableName = \"\";\n\n    Connection con = null;\n\n    @Override\n    public Status process(){\n        Status status = null;\n        // Start transaction\n        Channel ch = getChannel();\n        Transaction txn = ch.getTransaction();\n        txn.begin();\n        try\n        {\n            Event event = ch.take();\n\n            if (event != null)\n            {\n                    //获取body中的数据\n                    String body = new String(event.getBody(), \"UTF-8\");\n\n                    //如果日志中有以下关键字的不需要保存，过滤掉\n                if(body.contains(\"delete\") || body.contains(\"drop\") || body.contains(\"alert\")){\n                    status = Status.BACKOFF;\n                }else {\n\n                    //存入Mysql\n                    SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n                    String createtime = df.format(new Date());\n\n                    PreparedStatement stmt = con.prepareStatement(\"insert into \" + tableName + \" (createtime, content) values (?, ?)\");\n                    stmt.setString(1, createtime);\n                    stmt.setString(2, body);\n                    stmt.execute();\n                    stmt.close();\n                    status = Status.READY;\n                }\n           }else {\n                    status = Status.BACKOFF;\n                }\n\n            txn.commit();\n        } catch (Throwable t){\n            txn.rollback();\n            t.getCause().printStackTrace();\n            status = Status.BACKOFF;\n        } finally{\n            txn.close();\n        }\n\n        return status;\n    }\n    /**\n     * 获取配置文件中指定的参数\n     * @param context\n     */\n    @Override\n    public void configure(Context context) {\n        mysqlurl = context.getString(\"mysqlurl\");\n        username = context.getString(\"username\");\n        password = context.getString(\"password\");\n        tableName = context.getString(\"tablename\");\n    }    \n    \n    @Override\n    public synchronized void start() {\n        try{\n              //初始化数据库连接\n            con = DriverManager.getConnection(mysqlurl, username, password);\n            super.start();\n            System.out.println(\"finish start\");\n        }catch (Exception ex){\n            ex.printStackTrace();\n        }\n    }\n    \n    @Override\n    public synchronized void stop(){\n        try{\n            con.close();\n        }catch(SQLException e) {\n            e.printStackTrace();\n        }\n        super.stop();\n    }\n\n}\n\n```\n\n\n\n\n* 4、测试\n\n  * 4.1 ==程序打成jar包，上传jar包到flume的lib目录下==\n\n  * 4.2 ==配置文件准备==\n\n    * vim mysqlsink.conf\n\n~~~properties\n    a1.sources = r1\n    a1.sinks = k1\n    a1.channels = c1\n    \n    #配置source\n    a1.sources.r1.type = exec\n    a1.sources.r1.command = tail -F /kfly/install/flumeData/data.log\n    a1.sources.r1.channels = c1\n    \n    #配置channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    #配置sink\n    a1.sinks.k1.channel = c1\n    a1.sinks.k1.type = bigdata.flume.sink.CustomSink\n    a1.sinks.k1.mysqlurl=jdbc:mysql://node03:3306/mysqlsource?useSSL=false\n    a1.sinks.k1.username=root\n    a1.sinks.k1.password=123456\n    a1.sinks.k1.tablename=flume2mysql\n~~~\n\n  * 4.3 ==启动flume配置==\n\n  ~~~shell\n  bin/flume-ng agent -n a1 -c myconf -f myconf/mysqlsink.conf -Dflume.root.logger=info,console\n  ~~~\n\n  * 4.4 最后向文件中添加数据，观察mysql表中的数据\n\n### 10. Flume实际使用注意事项\n\n#### 1、注意启动脚本命名的书写\n\n```\nagent 的名称别写错了，后台执行加上 nohup ... &\n```\n\n#### 2、channel参数\n\n```\ncapacity：默认该通道中最大的可以存储的event数量\ntrasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量\n注意：capacity > trasactionCapacity\n\n```\n\n#### 3、日志采集到HDFS配置说明1（sink端）\n\n```shell\n#定义sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.path=hdfs://node01:8020/source/logs/%{type}/%Y%m%d\na1.sinks.k1.hdfs.filePrefix =events\na1.sinks.k1.hdfs.fileType = DataStream\na1.sinks.k1.hdfs.writeFormat = Text\n#时间类型\na1.sinks.k1.hdfs.useLocalTimeStamp = true\n#生成的文件不按条数生成\na1.sinks.k1.hdfs.rollCount = 0\n#生成的文件按时间生成\na1.sinks.k1.hdfs.rollInterval = 0\n#生成的文件按大小生成\na1.sinks.k1.hdfs.rollSize  = 10485760\n#批量写入hdfs的个数\na1.sinks.k1.hdfs.batchSize = 10000\n#flume操作hdfs的线程数（包括新建，写入等）\na1.sinks.k1.hdfs.threadsPoolSize=10\n#操作hdfs超时时间\na1.sinks.k1.hdfs.callTimeout=30000\n\n\n```\n\n#### 4、日志采集到HDFS配置说明2（sink端）\n\n| hdfs.round          | false  | Should the timestamp be rounded down (if true, affects all time based escape sequences except %t) |\n| ------------------- | ------ | ------------------------------------------------------------ |\n| **hdfs.roundValue** | 1      | Rounded down to the highest multiple of this (in the unit configured usinghdfs.roundUnit), less than current time. |\n| **hdfs.roundUnit**  | second | The unit of the round down value - second, minute or hour.   |\n\nØ round： 默认值：false 是否启用时间上的”舍弃”，这里的”舍弃”，类似于”四舍五入”\n\nØ roundValue：默认值：1  时间上进行“舍弃”的值；\n\nØ roundUnit： 默认值：seconds时间上进行”舍弃”的单位，包含：second,minute,hour\n\n```properties\n# 案例一：\na1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d/%H:%M/%S\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = minute\n# 当时间为2015-10-16 17:38:59时候，hdfs.path依然会被解析为：\n# /flume/events/2015-10-16/17:30/00\n# /flume/events/2015-10-16/17:40/00\n# /flume/events/2015-10-16/17:50/00\n# 因为设置的是舍弃10分钟内的时间，因此，该目录每10分钟新生成一个。\n\n# 案例二：\na1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d/%H:%M/%S\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = second\n# 现象：10秒为时间梯度生成对应的目录，目录下面包括很多小文件！！！\n# 格式如下：\n# /flume/events/2016-07-28/18:45/10\n# /flume/events/2016-07-28/18:45/20\n# /flume/events/2016-07-28/18:45/30\n# /flume/events/2016-07-28/18:45/40\n# /flume/events/2016-07-28/18:45/50\n# /flume/events/2016-07-28/18:46/10\n# /flume/events/2016-07-28/18:46/20\n# /flume/events/2016-07-28/18:46/30\n# /flume/events/2016-07-28/18:46/40\n# /flume/events/2016-07-28/18:46/50\n\n```\n\n#### 5、实现数据的断点续传\n\n- 当一个flume挂掉之后重启的时候还是可以接着上一次的数据继续收集\n  - flume在1.7版本之前使用的监控一个文件（source exec）、监控一个目录（source spooldir）都无法直接实现\n- flume在1.7版本之后已经集成了该功能\n  - 其本质就是记录下每一次消费的位置，把消费信息的位置保存到文件中，后续程序挂掉了再重启的时候，可以接着上一次消费的数据位置继续拉取。\n- 配置文件\n  - ==vim taildir.conf==\n    - source 类型---->taildir\n\n```properties\na1.sources = s1\na1.channels = ch1\na1.sinks = hdfs-sink1\n\n#channel\na1.channels.ch1.type = memory\na1.channels.ch1.capacity=10000\na1.channels.ch1.transactionCapacity=500\n\n#source\na1.sources.s1.channels = ch1\n#监控一个目录下的多个文件新增的内容\na1.sources.s1.type = taildir\n#通过 json 格式存下每个文件消费的偏移量，避免从头消费\na1.sources.s1.positionFile = /k/install/flumeData/index/taildir_position.json\na1.sources.s1.filegroups = f1 f2 f3 \na1.sources.s1.filegroups.f1 = /home/hadoop/taillogs/access.log\na1.sources.s1.filegroups.f2 = /home/hadoop/taillogs/nginx.log\na1.sources.s1.filegroups.f3 = /home/hadoop/taillogs/web.log\na1.sources.s1.headers.f1.headerKey = access\na1.sources.s1.headers.f2.headerKey = nginx\na1.sources.s1.headers.f3.headerKey = web\na1.sources.s1.fileHeader  = true\n\n##sink\na1.sinks.hdfs-sink1.channel = ch1\na1.sinks.hdfs-sink1.type = hdfs\na1.sinks.hdfs-sink1.hdfs.path =hdfs://node01:8020/demo/data/%{headerKey}\na1.sinks.hdfs-sink1.hdfs.filePrefix = event_data\na1.sinks.hdfs-sink1.hdfs.fileSuffix = .log\na1.sinks.hdfs-sink1.hdfs.rollSize = 1048576\na1.sinks.hdfs-sink1.hdfs.rollInterval =20\na1.sinks.hdfs-sink1.hdfs.rollCount = 10\na1.sinks.hdfs-sink1.hdfs.batchSize = 1500\na1.sinks.hdfs-sink1.hdfs.round = true\na1.sinks.hdfs-sink1.hdfs.roundUnit = minute\na1.sinks.hdfs-sink1.hdfs.threadsPoolSize = 25\na1.sinks.hdfs-sink1.hdfs.fileType =DataStream\na1.sinks.hdfs-sink1.hdfs.writeFormat = Text\na1.sinks.hdfs-sink1.hdfs.callTimeout = 60000\n\n```\n\n\n\n~~~json\n# 运行后生成的 taildir_position.json文件信息如下：\n[\n{\"inode\":102626782,\"pos\":123,\"file\":\"/home/hadoop/taillogs/access.log\"},{\"inode\":102626785,\"pos\":123,\"file\":\"/home/hadoop/taillogs/web.log\"},{\"inode\":102626786,\"pos\":123,\"file\":\"/home/hadoop/taillogs/nginx.log\"}\n]\n\n#这里inode就是标记文件的，文件名称改变，这个iNode不会变，pos记录偏移量，file就是绝对路径\n\n~~~\n\n\n\n#### 6、flume的header参数配置讲解\n\n- ==vim test-header.conf==\t\n\n```properties\n#配置信息test-header.conf\na1.channels=c1\na1.sources=r1\na1.sinks=k1\n\n#source\na1.sources.r1.channels=c1\na1.sources.r1.type= spooldir\na1.sources.r1.spoolDir= /home/hadoop/test\na1.sources.r1.batchSize= 100\na1.sources.r1.inputCharset= UTF-8\n#是否添加一个key存储目录下文件的绝对路径\na1.sources.r1.fileHeader= true\n#指定存储目录下文件的绝对路径的key\na1.sources.r1.fileHeaderKey= mm\n#是否添加一个key存储目录下的文件名称\na1.sources.r1.basenameHeader= true\n#指定存储目录下文件的名称的key\na1.sources.r1.basenameHeaderKey= nn\n\n#channel\na1.channels.c1.type= memory\na1.channels.c1.capacity=10000\na1.channels.c1.transactionCapacity=500\n\n\n#sink\na1.sinks.k1.type=logger\na1.sinks.k1.channel=c1\n\n```\n\n- 准备数据文件，添加内容\n\n```\n/home/hadoop/test/abc.txt\n/home/hadoop/test/def.txt\n```\n\n- 启动flume配置\n\n```\nbin/flume-ng agent -n a1 -c myconf -f myconf/test-header.conf -Dflume.root.logger=info,console\n```\n\n- 查看控制台\n\n```\nEvent: { headers:{mm=/home/hadoop/test/abc.txt, nn=abc.txt} body: 68 65 6C 6C 6F 20 73 70 61 72 6B                hello spark }\n19/08/30 19:23:15 INFO sink.LoggerSink: Event: { headers:{mm=/home/hadoop/test/abc.txt, nn=abc.txt} body: 68 65 6C 6C 6F 20 68 61 64 6F 6F 70             hello hadoop }\n```\n\n####  7 tail / tail -f /tail -F区别\n\n```\ntail -f\n\n等同于--follow=descriptor，根据文件描述符进行追踪，当文件改名或被删除，追踪停止\n\ntail -F\n\n等同于--follow=name --retry，根据文件名进行追踪，并保持重试，即该文件被删除或改名后，如果再次创建相同的文件名，会继续追踪\n\ntailf\n\n等同于tail -f -n 10（貌似tail -f或-F默认也是打印最后10行，然后追踪文件），与tail -f不同的是，如果文件不增长，它不会去访问磁盘文件，所以tailf特别适合那些便携机上跟踪日志文件，因为它减少了磁盘访问，\n```","tags":["flume"]},{"title":"大数据开发之HBase（四）微博案例","url":"/2019/11/19/it/hbase/大数据开发之HBase（四）微博案例/","content":"\n#  1. hbase微博实战案例\n\n## 1.1 需求分析\n\n> 1. hbase属于OLAP数据库，表设计与OLTP数据库不同，支持百万列。列的动态变化。\n>\n> 2. 设计规范\n>\n>    | **项目**     | **说明**                                                     | **示例**                                                     |\n>    | ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n>    | **命名空间** | 采用英文单词、阿拉伯数字的组合形式，其中，单词必须大写，并且首字符必须为英文字符，不能是数字。不建议用连接符（下划线）拼接多个单词，简单语义的可采用单个单词，复杂语义的可采用多个单词的首字母拼接。长度尽量限制在4~8字符之间。命名空间一般可与项目名称、组织机构名称等保持一致。 | 根据项目名称构建命名空间：DLQX（电力气象首字母拼接形式），简短明了。不建议过长的命名空间名称，譬如不推荐采用以下形式：USER_INFO_MANAGE等。 |\n>    | **表名称**   | 采用英文单词、阿拉伯数字、连接符（_）的组合形式，其中，单词必须大写，并且首字符必须为英文字符，不能是数字，可用连接符拼接多个单词。长度尽量限制在8~16字符之间。尽量采用具有明确意义的英文单词，而不建议采用汉字的拼音字母或者拼音首字母组合。 | 符合规范的表名称：USER_INFO_MANAGE、WEATHER_DATA、T_ELECTRIC_GATHER等。 |\n>    | **列族名称** | 采用英文单词、阿拉伯数字的组合形式，其中，单词必须大写，并且首字符必须为英文字符，不能是数字。长度尽量限制在1~6字符之间，过长的列族名称将占用更多的存储空间。 | 符合规范的列族名称：D1、D2、DATA等。不推荐的列族名称：USER_INFO、D_1等。 |\n>    | **列名称**   | 采用英文单词、阿拉伯数字、连接符（_）的组合形式，其中，单词必须大写，并且首字符必须为英文字符，不能是数字，可用连接符拼接多个单词。长度尽量限制在1~16字符之间。尽量采用具有明确意义的英文单词，而不建议采用汉字的拼音字母或者拼音首字母组合。 | 符合规范的列名称：USER_ID、DATA_1、REMARK等。不推荐的列名称：UserID、1_DA |\n\n\n\n1) 微博内容的浏览，数据库表设计\n\n- rowKey: uid + timestamp\n\n![image-20191119172809987](assets/image-20191119172809987.png)\n\n2) 用户社交体现：关注用户，取关用户\n\n- rowKey： uid\n\n- ColumnFamily： 关注（attends）、粉丝（fans）两个列族\n\n- 列族下列名称和cell值直接使用uid，当新增关注或者粉丝时直接动态增加列即可\n\n![image-20191119172951520](assets/image-20191119172951520.png)\n\n3) 拉取关注的人的微博内容\n\n- 发送微博直接推送给粉丝，需要表微博收件箱\n- rowKey 和列都用uid，动态增加列接收发送的微博\n- cell值使用微博id，设置微博版本ß\n\n![image-20191119174811329](assets/image-20191119174811329.png)\n\n## 1.2 代码实现\n\n### 1.2.1 准备工作\n\n- 第一步：创建maven工程并导入jar包\n\n  直接使用在版本确界当中创建的工程以及导入的jar包即可\n\n- 第二步：拷贝三个配置文件到maven工程的下\n\n  将node01服务器的三个配置文件，分别是\n\n  core-site.xml、hdfs-site.xml、hbase-site.xml三个配置文件，拷贝到maven工程的resources资源目录下\n\n### 1.2.2 代码设计总览：\n\n1) 创建命名空间以及表名的定义\n\n2) 创建微博内容表\n\n3) 创建用户关系表\n\n4) 创建用户微博内容接收邮件表\n\n5) 发布微博内容\n\n6) 添加关注用户\n\n7) 移除（取关）用户\n\n8) 获取关注的人的微博内容\n\n### 1.2.3 创建命名空间以及表名的定义\n\n**代码实现：**\n\n```java\n//微博内容表的表名\nprivate static final byte[] TABLE_CONTENT = Bytes.toBytes(\"weibo:content\");\n//用户关系表的表名\nprivate static final byte[] TABLE_RELATIONS = Bytes.toBytes(\"weibo:relations\");\n//微博收件箱表的表名\nprivate static final byte[] TABLE_RECEIVE_CONTENT_EMAIL = Bytes.toBytes(\"weibo:receive_content_email\");\n/**\n * 初始化命名空间\n */\npublic void initNameSpace() throws IOException {\n    Connection connection = getConnection();\n    Admin admin = connection.getAdmin();\n    NamespaceDescriptor namespaceDescriptor = NamespaceDescriptor.create(\"weibo2\").addConfiguration(\"creator\",\"jim\").build();\n    admin.createNamespace(namespaceDescriptor);\n    admin.close();\n    connection.close();\n}\n\npublic Connection getConnection() throws IOException {\n    Configuration configuration = HBaseConfiguration.create();\n    configuration.set(\"hbase.zookeeper.quorum\",\"node01:2181,node02:2181,node03:2181\");\n    Connection connection = ConnectionFactory.createConnection();\n    return connection;\n}\n\n```\n\n### 1.2.4 创建微博内容表\n\n**表结构：**\n\n| **方法名**   | creatTableeContent |\n| ------------ | ------------------ |\n| Table Name   | weibo:content      |\n| RowKey       | 用户ID_时间戳      |\n| ColumnFamily | info               |\n| ColumnLabel  | 标题,内容,图片     |\n| Version      | 1个版本            |\n\n**代码实现：**\n\n```java\n/**\n * 创建微博内容存储表\n *\n * 方法名 creatTableeContent\n Table Name    weibo:content\n RowKey    用户ID_时间戳\n ColumnFamily  info\n ColumnLabel   标题,内容,图片\n Version   1个版本\n\n *\n */\npublic void creatTableeContent() throws IOException {\n    //获取连接\n    Connection connection = getConnection();\n    //获取管理员对象\n    Admin admin = connection.getAdmin();\n    //得到HTableDescriptor对象\n    HTableDescriptor hTableDescriptor = new HTableDescriptor(TableName.valueOf(\"weibo:content\"));\n    //添加列族info\n    HColumnDescriptor info = new HColumnDescriptor(\"info\");\n    //设置版本确界\n    info.setMaxVersions(1);\n    info.setMinVersions(1);\n    info.setBlockCacheEnabled(true);\n    //设置数据压缩\n    //   info.setCompressionType(Compression.Algorithm.SNAPPY);\n    info.setBlocksize(2048*1024);\n    hTableDescriptor.addFamily(info);\n    //创建表\n    admin.createTable(hTableDescriptor);\n    admin.close();\n    connection.close();\n}\n\n```\n\n### 1.2.5 创建用户关系表\n\n**表结构：**\n\n| **方法名**   | createTableRelations   |\n| ------------ | ---------------------- |\n| Table Name   | weibo:relations        |\n| RowKey       | 用户ID                 |\n| ColumnFamily | attends、fans          |\n| ColumnLabel  | 关注用户ID，粉丝用户ID |\n| ColumnValue  | 用户ID                 |\n| Version      | 1个版本                |\n\n**代码实现：**\n\n```java\n/**\n * 创建用户关系表\n * 方法名 createTableRelations\n Table Name    weibo:relations\n RowKey    用户ID\n ColumnFamily  attends、fans\n ColumnLabel   关注用户ID，粉丝用户ID\n ColumnValue   用户ID\n Version   1个版本\n\n */\npublic void createTableRelations() throws IOException {\n    //获取连接\n    Connection connection = getConnection();\n    //获取管理员对象\n    Admin admin = connection.getAdmin();\n    //得到表定义\n    HTableDescriptor hTableDescriptor = new HTableDescriptor(TableName.valueOf(TABLE_RELATIONS));\n    HColumnDescriptor attends = new HColumnDescriptor(\"attends\");\n    HColumnDescriptor fans = new HColumnDescriptor(\"fans\");\n\n    attends.setBlocksize(2048*1024);\n    attends.setBlockCacheEnabled(true);\n    attends.setMinVersions(1);\n    attends.setMaxVersions(1);\n\n    fans.setBlocksize(2048*1024);\n    fans.setBlockCacheEnabled(true);\n    fans.setMinVersions(1);\n    fans.setMaxVersions(1);\n\n    hTableDescriptor.addFamily(attends);\n    hTableDescriptor.addFamily(fans);\n    admin.createTable(hTableDescriptor);\n    admin.close();\n    connection.close();\n}\n\n```\n\n### 1.2.6 创建微博收件箱表\n\n**表结构：**\n\n| **方法名**   | createTableReceiveContentEmails |\n| ------------ | ------------------------------- |\n| Table Name   | weibo:receive_content_email     |\n| RowKey       | 用户ID                          |\n| ColumnFamily | info                            |\n| ColumnLabel  | 用户ID                          |\n| ColumnValue  | 取微博内容的RowKey              |\n| Version      | 1000                            |\n\n**代码实现：**\n\n```java\n/**\n * 表结构：\n 方法名   createTableReceiveContentEmails\n Table Name    weibo:receive_content_email\n RowKey    用户ID\n ColumnFamily  info\n ColumnLabel   用户ID\n ColumnValue   取微博内容的RowKey\n Version   1000\n\n */\n\npublic void createTableReceiveContentEmails() throws IOException {\n    //获取连接\n    Connection connection = getConnection();\n    //得到管理员对象\n    Admin admin = connection.getAdmin();\n    //获取HTableDescriptor\n    HTableDescriptor hTableDescriptor = new HTableDescriptor(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));\n    //定义列族名称\n    HColumnDescriptor info = new HColumnDescriptor(\"info\");\n    info.setMaxVersions(1000);\n    info.setMinVersions(1000);\n    info.setBlockCacheEnabled(true);\n    info.setBlocksize(2048*1024);\n    hTableDescriptor.addFamily(info);\n    //创建表\n    admin.createTable(hTableDescriptor);\n    admin.close();\n    connection.close();\n}\n\n```\n\n### 1.2.7 发布微博内容\n\na、微博内容表中添加1条数据\n\nb、微博收件箱表对所有粉丝用户添加数据\n\n**代码实现：**\n\n```java\n/**\n * 发布微博内容\n * @param uid\n * @param content\n */\npublic void publishWeiBo(String uid ,String content) throws IOException {\n    Connection connection = getConnection();\n    Table table = connection.getTable(TableName.valueOf(TABLE_CONTENT));\n    String rowkey = uid + \"_\"+ System.currentTimeMillis();\n    Put put = new Put(rowkey.getBytes());\n    put.addColumn(\"info\".getBytes(),\"content\".getBytes(),System.currentTimeMillis(),content.getBytes());\n    table.put(put);\n    //微博用户关系表\n    Table table_relations = connection.getTable(TableName.valueOf(TABLE_RELATIONS));\n    Get get = new Get(uid.getBytes());\n    get.addFamily(\"fans\".getBytes());\n    Result result = table_relations.get(get);\n    Cell[] cells = result.rawCells();\n    if(cells.length <= 0){\n        return ;\n    }\n    List<byte[]>  allFans = new ArrayList<byte[]>();\n    //将所有的粉丝都获取到，然后将数据保存到粉丝表当中去\n    for (Cell cell : cells) {\n        byte[] bytes = CellUtil.cloneQualifier(cell);\n        allFans.add(bytes);\n    }\n    Table table_receive_content_email = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));\n    List<Put> putFansList = new ArrayList<>();\n    for (byte[] allFan : allFans) {\n        Put put1 = new Put(allFan);\n        put1.addColumn(\"info\".getBytes(),Bytes.toBytes(uid),System.currentTimeMillis(),rowkey.getBytes());\n        putFansList.add(put1);\n    }\n    table_receive_content_email.put(putFansList);\n}\n\n```\n\n### 1.2.8 添加关注用户\n\na、在微博用户关系表中，对当前主动操作的用户添加新关注的好友\n\nb、在微博用户关系表中，对被关注的用户添加新的粉丝\n\nc、微博收件箱表中添加所关注的用户发布的微博\n\n**代码实现：**\n\n```java\n/**\n * 添加关注用户，一次可能添加多个关注用户\n * A 关注一批用户 B,C ,D\n * 第一步：A是B,C,D的关注者   在weibo:relations 当中attend列族当中以A作为rowkey，B,C,D作为列名，B,C,D作为列值，保存起来\n * 第二步：B,C,D都会多一个粉丝A  在weibo:relations 当中fans列族当中分别以B,C,D作为rowkey，A作为列名，A作为列值，保存起来\n * 第三步：A需要获取B,C,D 的微博内容存放到 receive_content_email 表当中去，以A作为rowkey，B,C,D作为列名，获取B,C,D发布的微博rowkey，放到对应的列值里面去\n *\n *\n * @param uid\n * @param attends\n */\npublic void addAttends(String uid ,String ...attends) throws IOException {\n    Connection connection = getConnection();\n    Table relation_table = connection.getTable(TableName.valueOf(TABLE_RELATIONS));\n    //用户关注人,attend列族当中添加数据\n    Put put = new Put(uid.getBytes());\n    for (String attend : attends) {\n        put.addColumn(\"attends\".getBytes(),attend.getBytes(),attend.getBytes());\n    }\n    relation_table.put(put);\n    //粉丝fans添加粉丝，A 关注B，那么自然B就需要添加一个粉丝A\n    for (String attend : attends) {\n        Put put1 = new Put(attend.getBytes());\n        put1.addColumn(\"fans\".getBytes(),uid.getBytes(),uid.getBytes());\n        relation_table.put(put1);\n    }\n\n    //获取uid的所有关注人的收件箱，放到收件箱列表weibo:receive_content_email里面去\n    //A 关注B，那么A需要获取B所有的微博内容\n    Table table_content = connection.getTable(TableName.valueOf(TABLE_CONTENT));\n    Scan scan = new Scan();\n    List<byte[]> rowkeyBytes = new ArrayList<>();\n    for (String attend : attends) {\n        RowFilter rowFilter = new RowFilter(CompareFilter.CompareOp.EQUAL\n,new SubstringComparator(attend+\"_\"));\n        scan.setFilter(rowFilter);\n        ResultScanner scanner = table_content.getScanner(scan);\n        for (Result result : scanner) {\n            //获取到数据的rowkey\n            byte[] rowkey = result.getRow();\n            rowkeyBytes.add(rowkey);\n        }\n    }\n    if(rowkeyBytes.size() > 0){\n        Table table_receive_content = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));\n        List<Put> recPuts = new ArrayList<Put>();\n        for (byte[] rowkeyByte : rowkeyBytes) {\n            Put put1 = new Put(uid.getBytes());\n            String rowKeyStr = Bytes.toString(rowkeyByte);\n            String attendUid = rowKeyStr.substring(0, rowKeyStr.indexOf(\"_\"));\n            long timestamp = Long.parseLong(rowKeyStr.substring(rowKeyStr.indexOf(\"_\") + 1));\n\n            put1.addColumn(\"info\".getBytes(),attendUid.getBytes(), timestamp,rowkeyByte);\n            recPuts.add(put1);\n        }\n        table_receive_content.put(recPuts);\n    }\n}\n\n```\n\n### 1.2.9 移除（取关）用户\n\na、在微博用户关系表中，对当前主动操作的用户移除取关的好友(attends)\n\nb、在微博用户关系表中，对被取关的用户移除粉丝\n\nc、微博收件箱中删除取关的用户发布的微博\n\n**代码实现：**\n\n```java\n/**\n * 取消关注 A取消关注 B,C,D这三个用户\n * 其实逻辑与关注B,C,D相反即可\n * 第一步：在weibo:relation关系表当中，在attends列族当中删除B,C,D这三个列\n * 第二步：在weibo:relation关系表当中，在fans列族当中，以B,C,D为rowkey，查找fans列族当中A这个粉丝，给删除掉\n * 第三步：A取消关注B,C,D,在收件箱中，删除取关的人的微博的rowkey\n */\npublic void attendCancel(String uid,String ...cancelAttends) throws IOException {\n    Connection connection = getConnection();\n    Table table_relations = connection.getTable(TableName.valueOf(TABLE_RELATIONS));\n\n    //移除A关注的B,C,D这三个用户\n    for (String cancelAttend : cancelAttends) {\n        Delete delete = new Delete(uid.getBytes());\n        delete.addColumn(\"attends\".getBytes(),cancelAttend.getBytes());\n        table_relations.delete(delete);\n    }\n\n    //B,C,D这三个用户移除粉丝A\n    for (String cancelAttend : cancelAttends) {\n        Delete delete = new Delete(cancelAttend.getBytes());\n        delete.addColumn(\"fans\".getBytes(),uid.getBytes());\n        table_relations.delete(delete);\n    }\n\n    //收件箱表当中  A移除掉B,C,D的信息\n    Table table_receive_connection = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));\n\n    for (String cancelAttend : cancelAttends) {\n        Delete delete = new Delete(uid.getBytes());\n        delete.addColumn(\"info\".getBytes(),cancelAttend.getBytes());\n        table_receive_connection.delete(delete);\n\n    }\n    table_receive_connection.close();\n    table_relations.close();\n    connection.close();\n}\n\n```\n\n### 1.2.10 获取关注的人的微博内容\n\na、从微博收件箱中获取所关注的用户的微博RowKey \n\nb、根据获取的RowKey，得到微博内容\n\n**代码实现：**\n\n```java\n/**\n * 某个用户获取收件箱表内容\n * 例如A用户刷新微博，拉取他所有关注人的微博内容\n * A 从 weibo:receive_content_email  表当中获取所有关注人的rowkey\n * 通过rowkey从weibo:content表当中获取微博内容\n */\npublic void getContent(String uid) throws IOException {\n    //从weibo:receive_content_email 表当中获取用户id为uid的人的所有的微博列表\n    Connection connection = getConnection();\n    //从 weibo:receive_content_email\n    Table table_receive_content_email = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL));\n    //定义list集合里面存储我们的所有的Get对象，用于下一步的查询\n    List<Get>  rowkeysList = new ArrayList<Get>();\n    Get get = new Get(uid.getBytes());\n    //设置最大版本为5个\n    get.setMaxVersions(5);\n    Result result = table_receive_content_email.get(get);\n    Cell[] cells = result.rawCells();\n    for (Cell cell : cells) {\n        byte[] rowkeys = CellUtil.cloneValue(cell);\n        Get get1 = new Get(rowkeys);\n        rowkeysList.add(get1);\n    }\n    //从weibo:content表当中通过用户id进行查询微博内容\n    //table_content内容表\n    Table table_content = connection.getTable(TableName.valueOf(TABLE_CONTENT));\n    //所有查询出来的内容进行打印出来\n    Result[] results = table_content.get(rowkeysList);\n    for (Result result1 : results) {\n        byte[] value = result1.getValue(\"info\".getBytes(), \"content\".getBytes());\n        byte[] row = result1.getRow();\n        String rowkey = Bytes.toString(row);\n        String[] split = rowkey.split(\"_\");\n        Content content = new Content();\n        content.setUid(split[0]);\n        content.setTimeStamp(Long.parseLong(split[1]));\n        content.setContent(Bytes.toString(value));\n        System.out.println(content.toString());\n    }\n}\n```\n\n\n\n[点击获取代码示例](https://github.com/orchid-ding/hadoop-example/tree/master/src/main/java/hbase/weibo)","tags":["hbase","项目练习"]},{"title":"大数据开发之HBase（三）","url":"/2019/11/17/it/hbase/大数据开发之HBase（三）/","content":"\n\n\n# 大数据数据库之HBase\n\n## 1. HBase协处理器\n\n- http://hbase.apache.org/book.html#cp\n- 起源：\n  - Hbase 作为列族数据库最经常被人诟病的特性包括：无法轻易建立“二级索引”，难以执行求和、计数、排序等操作。比如，在旧版本的(<0.92)Hbase 中，统计数据表的总行数，需 要使用 Counter 方法，执行一次 MapReduce Job 才能得到。\n  - 虽然 HBase 在数据存储层中集成了 MapReduce，能够有效用于数据表的分布式计算。然而在很多情况下，做一些简单的相加或者聚合计算的时候， 如果直接将计算过程放置在 server端，能够减少通讯开销，从而获得很好的性能提升。\n  - 于是， HBase 在 0.92 之后引入了协处理器(coprocessors)，实现一些激动人心的新特性：能够轻易建立二次索引、复杂过滤器(谓词下推)以及访问控制等。\n\n### 1.1 两种协处理器\n\n- 协处理器有两种：observer和endpoint\n\n#### 1.1.1 observer\n\n- Observer 类似于传统数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。\n- Observer Coprocessor就是一些散布在 HBase Server 端代码中的 hook 钩子， 在固定的事件发生时被调用。\n  - 比如： put 操作之前有钩子函数 prePut，该函数在put操作执行前会被Region Server调用；在 put 操作之后则有 postPut 钩子函数\n\n- 以 HBase0.92 版本为例，它提供了三种观察者接口：\n  - RegionObserver：提供客户端的数据操纵事件钩子： Get、 Put、 Delete、 Scan 等。\n  - WALObserver：提供 WAL 相关操作钩子。\n  - MasterObserver：提供 DDL类型的操作钩子。如创建、删除、修改数据表等。\n  - 到 0.96 版本又新增一个 RegionServerObserver\n\n![](assets/Image201911151202.png)\n\n- 下图是以 RegionObserver 为例子讲解 Observer 这种协处理器的原理：\n\n![1122015-20170511100919222-711579099](assets/1122015-20170511100919222-711579099.png)\n\n#### 1.1.2 endpoint\n\n- Endpoint协处理器类似传统数据库中的存储过程，客户端可以调用这些 Endpoint 协处理器执行一段 Server 端代码，并将 Server 端代码的结果返回给客户端进一步处理\n- 最常见的用法就是进行聚集操作。\n  - 如果没有协处理器，当用户需要找出一张表中的最大数据，即max 聚合操作，就必须进行全表扫描，在客户端代码内遍历扫描结果，并执行求最大值的操作。\n  - 这样的方法无法利用底层集群的并发能力，而将所有计算都集中到 Client 端统一执 行，势必效率低下。\n  - 利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，HBase将利用底层cluster 的多个节点并发执行求最大值的操作。即在每个 Region范围内执行求最大值的代码，将每个 Region 的最大值在 Region Server 端计算出，仅仅将该 max 值返回给客户端。\n  - 在客户端进一步将多个 Region 的最大值进一步处理而找到其中的最大值。这样整体的执行效率就会提高很多\n\n#### 1.1.3 总结\n\n- Observer允许集群在正常的客户端操作过程中可以有不同的行为表现\n- Endpoint 允许扩展集群的能力，对客户端应用开放新的运算命令\n- observer 类似于 RDBMS 中的触发器，主要在服务端工作\n- endpoint 类似于 RDBMS 中的存储过程，主要在 client 端工作\n- observer 可以实现权限管理、优先级设置、监控、 ddl 控制、 二级索引等功能\n- endpoint 可以实现 min、 max、 avg、 sum、 distinct、 group by 等功能\n\n### 1.2 协处理器加载方式  \n\n- 协处理器的加载方式有两种\n  - 静态加载方式（ Static Load）；静态加载的协处理器称之为 System Coprocessor\n  - 动态加载方式 （ Dynamic Load）；动态加载的协处理器称 之为 Table Coprocessor\n\n#### 1.2.1 静态加载 \n\n- 通过修改 hbase-site.xml 这个文件来实现， 如启动全局 aggregation，能过操纵所有的表数据。只需要在hbase-site.xml里面添加以下配置即可\n- ==注意==：修改完配置之后需要**重启HBase集群**\n\n```xml\n<property>\n\t<name>hbase.coprocessor.user.region.classes</name>\n\t<value>org.apache.hadoop.hbase.coprocessor.AggregateImplementation</value>\n</property>\n```\n\n- 为所有table加载了一个 cp class，可以用” ,”分割加载多个 class，修改\n\n#### 1.2.2 动态加载\n\n- 启用表aggregation，只对特定的表生效。\n- 通过 HBase Shell 来实现。\n-  disable 指定表。\n\n```ruby\nhbase> disable 'mytable'\n```\n\n-  添加 aggregation\n\n```ruby\nhbase> alter 'mytable', METHOD => 'table_att','coprocessor'=>'|org.apache.Hadoop.hbase.coprocessor.AggregateImplementation||'\n```\n\n- 重启指定表 \n\n```ruby\nhbase> enable 'mytable'\n```\n\n- 协处理器卸载\n\n   ![xxx](assets/xxx.png)\n\n### 1.3 协处理器Observer实战\n\n<img src=\"assets/xdfsdfsdf.png\" alt=\"xdfsdfsdf\" style=\"zoom:80%;\" />\n\n- 通过协处理器Observer实现向hbase当中一张表插入数据时，通过协处理器，将数据复制一份保存到另外一张表当中去；但是只取第一张表当中的部分列数据，保存到第二张表当中去\n\n#### 1.3.1 创建第一张表proc1\n\n- 打开hbase shell\n\n```shell\ncd //install/hbase-1.2.0-cdh5.14.2/\nbin/hbase shell\n```\n\n- 在HBase当中创建一张表，表名user2，并只有一个列族info\n\n```ruby\nhbase(main):053:0> create 'proc1','info'\n```\n\n#### 1.3.2 创建第二张表proc2\n\n- 创建第二张表proc2，作为目标表\n- 将第一张表当中插入数据的部分列，使用协处理器，复制到proc2表当中来\n\n```ruby\nhbase(main):054:0> create 'proc2','info'\n```\n\n#### 1.3.3 开发HBase协处理器\n\n- 创建maven工程所用的repositories、dependencies、plugins跟之前的一样\n\n```xml\n<dependency>\n  <groupId>org.apache.hadoop</groupId>\n  <artifactId>hadoop-client</artifactId>\n  <version>2.6.0-mr1-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-client</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-server</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n```\n\n- 开发HBase的协处理器\n\n```\nimport org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;\n\npublic class MyProcessor extends BaseRegionObserver {\n    /**\n     *\n     * @param e\n     * @param put   插入到proc1表里面的数据，都是封装在put对象里面了\n     *              插入到proc1表里面的数据都在put对象里面，就可以解析put对象，获取数据，获取到了数据之后，插入到proc2表里面去\n     * @param edit\n     * @param durability\n     * @throws IOException\n     */\n    @Override\n    public void prePut(ObserverContext<RegionCoprocessorEnvironment> e, Put put, WALEdit edit, Durability durability) throws IOException {\n        //获取连接\n        Configuration configuration = HBaseConfiguration.create();\n        configuration.set(\"hbase.zookeeper.quorum\", \"node01:2181,node02:2181,node03:2181\");\n        Connection connection = ConnectionFactory.createConnection(configuration);\n\n        //涉及到多个版本问题\n        List<Cell> cells = put.get(\"info\".getBytes(), \"name\".getBytes());\n        Cell nameCell = cells.get(0);//获取最新的那个版本数据\n        //Cell nameCell = put.get(\"info\".getBytes(), \"name\".getBytes()).get(0);\n        Put put1 = new Put(put.getRow());\n        put1.add(nameCell);\n        Table reverseuser = connection.getTable(TableName.valueOf(\"proc2\"));\n        reverseuser.put(put1);\n        reverseuser.close();\n        connection.close();\n    }\n}\n```\n\n#### 1.3.4 将项目打成jar包，并上传到HDFS上面\n\n- 将我们的协处理器打成一个jar包，此处不需要用任何的打包插件即可\n- 然后将打好的jar包上传到linux的/kfly/install路径下\n- 再将jar包上传到HDFS\n\n```shell\ncd /kfly/install\n# 名称必须为processor.jar\nmv original-hbaseStudy-1.0-SNAPSHOT.jar  processor.jar\nhdfs dfs -mkdir -p /processor\nhdfs dfs -put processor.jar /processor\n```\n\n\n\n#### 1.3.5 将jar包挂载到proc1表\n\n```ruby\nhbase(main):056:0> describe 'proc1'\nhbase(main):055:0> alter 'observer:source',METHOD => 'table_att','Coprocessor'=>'hdfs://node01:8020/processor/processor.jar|top.kfly.hbasemr.MyProcessor|1001|'\n```\n\n- 再次查看'proc1'表；可以查看到我们的卸载器已经加载了\n\n```ruby\nhbase(main):043:0> describe 'proc1'\n```\n\n#### 1.3.6 向proc1表添加数据\n\n```\npackage com.kaikeba.hbase.cp.test;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.Connection;\nimport org.apache.hadoop.hbase.client.ConnectionFactory;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.client.Table;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.testng.annotations.Test;\n\npublic class TestObserver {\n    @Test\n    public void testPut() throws Exception{\n        //获取连接\n        Configuration configuration = HBaseConfiguration.create();\n        configuration.set(\"hbase.zookeeper.quorum\", \"node01,node02\");\n\n        Connection connection = ConnectionFactory.createConnection(configuration);\n\n        Table user5 = connection.getTable(TableName.valueOf(\"proc1\"));\n\n        Put put1 = new Put(Bytes.toBytes(\"hello_world\"));\n\n        put1.addColumn(Bytes.toBytes(\"info\"),\"name\".getBytes(),\"helloworld\".getBytes());\n        put1.addColumn(Bytes.toBytes(\"info\"),\"gender\".getBytes(),\"abc\".getBytes());\n        put1.addColumn(Bytes.toBytes(\"info\"),\"nationality\".getBytes(),\"test\".getBytes());\n        user5.put(put1);\n        byte[] row = put1.getRow();\n        System.out.println(Bytes.toString(row));\n        user5.close();\n    }\n}\n```\n\n- 注意：如果需要卸载我们的协处理器，那么进入hbase的shell命令行，执行以下命令即可\n\n```ruby\ndisable 'proc1'\nalter 'proc1',METHOD=>'table_att_unset',NAME=>'coprocessor$1'\nenable 'proc1'\n```\n\n\n\n## 2. HBase表的rowkey设计\n\n- rowkey设计三原则\n\n### 2.1 rowkey长度原则\n\n- rowkey是一个二进制码流，可以是任意字符串，最大长度64kb，实际应用中一般为10-100bytes，以byte[]形式保存，一般设计成定长。\n\n* 建议尽可能短；但是也不能太短，否则rowkey前缀重复的概率增大\n* 设计过长会降低memstore内存的利用率和HFile存储数据的效率。\n\n### 2.2 rowkey散列原则\n\n- 建议将rowkey的高位作为散列字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。\n- 如果没有散列字段，首字段直接是时间信息。所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。\t\n\n### 2.3 rowkey唯一原则\n\n- 必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的\n- 因此，设计rowkey的时候，要充分利用这个排序的特点，可以将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块\n- 下图为电信上网详单数据，保存在HBase的一个应用场景\n\n![2019-10-16_112336](assets/2019-10-16_112336.png)\n\n![2019-10-16_112157](assets/2019-10-16_112157.png)\n\n\n\n![2019-10-16_112529](assets/2019-10-16_112529.png)\n\n\n\n## 3. HBase表的热点\n\n### 3.2 什么是热点\n\n- 检索habse的记录首先要通过row key来定位数据行。\n- 当大量的client访问hbase集群的一个或少数几个节点，造成少数region server的读/写请求过多、负载过大，而其他region server负载却很小，就造成了“热点”现象。\n\n### 3.2 热点的解决方案\n\n#### 3.2.1 预分区\n\n- 预分区的目的让表的数据可以均衡的分散在集群中，而不是默认只有一个region分布在集群的一个节点上。\n\n#### 3.2.3 加盐             \n\n- 这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同\n\n#### 3.2.4 哈希\n\n- 哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。\n\n~~~\nrowkey=MD5(username).subString(0,10)+时间戳\t\n~~~\n\n#### 3.2.4 反转\n\n- 反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。\n- 这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。\n\n~~~\n电信公司：\n移动-----------> 136xxxx9301  ----->1039xxxx631\n\t\t\t\t136xxxx1234  \n\t\t\t\t136xxxx2341 \n电信\n联通\n\nuser表\nrowkey    name    age   sex    address\n\t\t  lisi1    21     m       beijing\n\t\t  lisi2    22     m       beijing\n\t\t  lisi3    25     m       beijing\n\t\t  lisi4    30     m       beijing\n\t\t  lisi5    40     f       shanghai\n\t\t  lisi6    50     f       tianjin\n\t          \n需求：后期想经常按照居住地和年龄进行查询？\t\nrowkey= address+age+随机数\n        beijing21+随机数\n        beijing22+随机数\n        beijing25+随机数\n        beijing30+随机数\n   \nrowkey= address+age+随机数\n~~~\n\n\n\n## 4. HBase的数据备份\n\n### 4.1 基于HBase提供的类对表进行备份\n\n* 使用HBase提供的类把HBase中某张表的数据导出到HDFS，之后再导出到测试hbase表中。\n\n* (1)  ==从hbase表导出到HDFS==\n\n  ~~~shell\n  [hadoop@node01 shells]$ hbase org.apache.hadoop.hbase.mapreduce.Export myuser /hbase_data/myuser_bak\n  \n  ~~~\n  \n* (2) ==文件导入hbase表==\n\n  hbase shell中创建备份目标表\n  \n  ```ruby\n  create 'myuser_bak','f1','f2'\n  ```\n  \n* 将HDFS上的数据导入到备份目标表中\n\n  ~~~shell\n  hbase org.apache.hadoop.hbase.mapreduce.Driver import myuser_bak /hbase_data/myuser_bak/*\n  ~~~\n\n* 补充说明\n\n  以上都是对数据进行了全量备份，后期也可以实现表的**增量数据备份**，增量备份跟全量备份操作差不多，只不过要在后面加上时间戳。\n  \n  例如：\n  HBase数据导出到HDFS\n  \n  ~~~\n  hbase org.apache.hadoop.hbase.mapreduce.Export test /hbase_data/test_bak_increment 开始时间戳  结束时间戳\n  ~~~\n\n\n### 4.2 基于snapshot快照对表进行备份\n\n* 通过snapshot快照的方式实现HBase数据的迁移和拷贝。这种方式比较常用，效率高，也是最为推荐的数据迁移方式。\n\n*  HBase的snapshot其实就是一组==metadata==信息的集合（文件列表），通过这些metadata信息的集合，就能将表的数据回滚到snapshot那个时刻的数据。\n\n  * 首先我们要了解一下所谓的HBase的LSM类型的系统结构，我们知道在HBase中，数据是先写入到Memstore中，当Memstore中的数据达到一定条件，就会flush到HDFS中，形成HFile，后面就不允许原地修改或者删除了。\n  * 如果要更新或者删除的话，只能追加写入新文件。既然数据写入以后就不会在发生原地修改或者删除，这就是snapshot做文章的地方。做snapshot的时候，只需要给快照表对应的所有文件创建好指针（元数据集合），恢复的时候只需要根据这些指针找到对应的文件进行恢复就Ok。这是原理的最简单的描述，下图是描述快照时候的简单流程：\t\n  \n  ![snapshot](assets/snapshot.png)\n\n\n\n### 4.3 **快照实战**\n\n* 1、创建表的snapshot\n\n~~~\nsnapshot 'tableName', 'snapshotName'\n~~~\n\n  * 2、查看snapshot\n\n  ~~~\nlist_snapshots  \n  ~~~\n\n​\t\t查找以test开头的snapshot\n\n```\nlist_snapshots 'test.*'\n```\n\n  * 3、恢复snapshot\n\n​\t\tps:这里需要对表进行disable操作，先把表置为不可用状态，然后在进行进行restore_snapshot的操作\n\n```\ndisable 'tableName'\nrestore_snapshot 'snapshotName'\nenable 'tableName'\n```\n\n  * 4、删除snapshot\n\n  ~~~\ndelete_snapshot 'snapshotName'\n  ~~~\n\n  * 5、迁移 snapshot\n\n  ~~~\n  hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \\\n  -snapshot snapshotName  \\\n  -copy-from hdfs://src-hbase-root-dir/hbase \\\n  -copy-to hdfs://dst-hbase-root-dir/hbase \\\n  -mappers 1 \\\n  -bandwidth 1024\n  \n  例如：\n  hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \\\n  -snapshot test  \\\n  -copy-from hdfs://node01:8020/hbase \\\n  -copy-to hdfs://node01:8020/hbase1 \\\n  -mappers 1 \\\n  -bandwidth 1024\n  ~~~\n\n​\t\t注意：这种方式用于将快照表迁移到另外一个集群的时候使用，使用MR进行数据的拷贝，速度很快，使用的时候记得设置好bandwidth参数，以免由于网络打满导致的线上业务故障。\n\n  * 6、将snapshot使用bulkload的方式导入\n\n  ~~~\n  hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles \\\n  hdfs://dst-hbase-root-dir/hbase/archive/datapath/tablename/filename \\\n  tablename\n  \n  例如：\n  创建一个新表\n  create 'newTest','f1','f2'\n  hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles hdfs://node1:9000/hbase1/archive/data/default/test/6325fabb429bf45c5dcbbe672225f1fb newTest\n  ~~~\n\n\n\n## 5. HBase二级索引\n\n![hbase寻址](assets/hbase寻址.png)\n\n- HBase表后期按照rowkey查询性能是最高的。rowkey就相当于hbase表的一级索引\n- 但是在实际的工作中，我们做的查询基本上都是按照一定的条件进行查找，无法事先知道满足这些条件的rowkey是什么，正常是可以通过hbase过滤器去实现。但是效率非常低，这是由于查询的过程中需要在底层进行大量的文件扫描。\n\n- HBase的二级索引\n- 为了HBase的数据查询更高效、适应更多的场景，诸如使用非rowkey字段检索也能做到秒级响应，或者支持各个字段进行模糊查询和多字段组合查询等， 因此需要在HBase上面构建二级索引， 以满足现实中更复杂多样的业务需求。\n  - hbase的二级索引其本质就是建立HBase表中列与行键之间的映射关系。\n\n\n\n![](assets/二级索引思想.png)\n\n- 构建hbase二级索引方案\n  - MapReduce方案 \n  - Hbase Coprocessor(协处理器)方案 \n  - Solr+hbase方案\n  - ES+hbase方案\n  - Phoenix+hbase方案\n    - [点击查看](https://kfly.top/2019/11/17/phoenix/Phoenix%E6%9E%84%E5%BB%BA%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95/) \n\n## 6. HBase的namespace\n\n### 6.1 namespace基本介绍\n\n- 在HBase中，namespace命名空间指对一组表的逻辑分组，类似RDBMS中的database，方便对表在业务上划分。\n- Apache HBase从0.98.0, 0.95.2两个版本号开始支持namespace级别的授权操作，HBase**全局管理员**能够创建、改动和回收namespace的授权。\n\n### 6.2 namespace的作用\n\n- 配额管理：限制一个namespace可以使用的资源，包括region和table\n- 命名空间安全管理：提供了另一个层面的多租户安全管理\n\n- Region服务器组：一个命名或一张表，可以被固定到一组RegionServers上，从而保证了数据隔离性\n\n### 6.3 namespace的基本操作\n\n```sql\n创建namespace\nhbase>create_namespace 'nametest'  \n\n查看namespace\nhbase>describe_namespace 'nametest'  \n\n列出所有namespace\nhbase>list_namespace  \n\n在namespace下创建表\nhbase>create 'nametest:testtable', 'fm1' \n\n查看namespace下的表\nhbase>list_namespace_tables 'nametest'  \n\n删除namespace\nhbase>drop_namespace 'nametest'  \n```\n\n \n\n## 7. HBase的数据版本的确界以及TTL\n\n### 7.1 数据的确界\n\n- 在HBase当中，我们可以为数据设置上界和下界，其实就是定义数据的历史版本保留多少个，通过自定义历史版本保存的数量，我们可以实现数据多个历史版本的数据查询\n\n- 版本的下界\n  - 默认的版本下界是0，即禁用。row版本使用的最小数目是与生存时间（TTL Time To Live）相结合的，并且我们根据实际需求可以有0或更多的版本，使用0，即只有1个版本的值写入cell。\n\n- 版本的上界\n  - 之前默认的版本上界是3，也就是一个row保留3个副本（基于时间戳的插入）。\n  - 该值不要设计的过大，一般的业务不会超过100。如果cell中存储的数据版本号超过了3个，再次插入数据时，最新的值会将最老的值覆盖。（现版本已默认为1）\n\n### 7.2 数据的TTL\n\n- 在实际工作当中经常会遇到有些数据过了一段时间我们可能就不需要了，那么这时候我们可以使用定时任务去定时的删除这些数据\n- 或者我们也可以使用Hbase的TTL（Time  To  Live）功能，让我们的数据定期的会进行清除\n\n- 使用代码来设置数据的确界以及设置数据的TTL如下\n\n####  7.2.1 创建maven工程\n\n- 创建maven工程，导入jar包坐标\n\n```xml\n<dependency>\n  <groupId>org.apache.hadoop</groupId>\n  <artifactId>hadoop-client</artifactId>\n  <version>2.6.0-mr1-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-client</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-server</artifactId>\n  <version>1.2.0-cdh5.14.2</version>\n</dependency>\n```\n\n#### 7.2.2 代码开发\n\n```java\n /**\n     * 初始化连接\n     * @throws IOException\n     */\n    public void init() throws IOException {\n        Configuration conf = HBaseConfiguration.create();\n        conf.set(\"hbase.zookeeper.quorum\",\"node01:2181,node02:2181,node03:2181\");\n        conf.set(\"zookeeper.znode.parent\",\"/HBase\");\n        conf.set(\"fs.fefaultFS\",\"hadoop.hdfs://node01:8020\");\n        connection = ConnectionFactory.createConnection(conf);\n    }\n\n    /**\n     * 创建表\n     */\n    public void createTable() throws IOException {\n        Admin admin = connection.getAdmin();\n        if(!admin.tableExists(TableName.valueOf(TABLE_NAME))){\n            // table\n            HTableDescriptor table = new HTableDescriptor(TableName.valueOf(TABLE_NAME));\n            // columa family\n            HColumnDescriptor column = new HColumnDescriptor(\"f1\");\n            // version\n            column.setMaxVersions(5);\n            column.setMinVersions(3);\n            // ttl unit s\n            column.setTimeToLive(30);\n            table.addFamily(column);\n            admin.createTable(table);\n        }\n        admin.close();\n    }\n\n    /**\n     * insert data\n     * @throws IOException\n     */\n    public void insertData() throws IOException {\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        for (int i = 0; i < 6 ; i++) {\n            Put put = new Put((\"column\").getBytes());\n            put.addColumn(\"f1\".getBytes(),\"col1\".getBytes(), System.currentTimeMillis(),Bytes.toBytes(\"column\" + i));\n            table.put(put);\n        }\n    }\n\n    /**\n     * get raw cell\n     * @throws IOException\n     */\n    public void getRawCell() throws IOException {\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        Get get = new Get(\"column\".getBytes());\n        get.setMaxVersions();\n        Result result = table.get(get);\n        Cell[] cells = result.rawCells();\n        for (int i = 0; i < cells.length; i++) {\n            System.out.println(Bytes.toString(CellUtil.cloneValue(cells[i])));\n        }\n    }\n```\n\n","tags":["hbase","协处理器","rowkey设计"]},{"title":"Phoenix构建二级索引","url":"/2019/11/17/it/phoenix/Phoenix构建二级索引/","content":"\n\n\n## Phoenix构建二级索引\n\n### 1、为什么需要用二级索引？\n\n- 对于HBase而言，如果想精确地定位到某行记录，唯一的办法是通过rowkey来查询。如果不通过rowkey来查找数据，就必须逐行地比较每一列的值，即全表扫瞄。\n- 对于较大的表，全表扫描的代价是不可接受的。但是，很多情况下，需要从多个角度查询数据。例如，在定位某个人的时候，可以通过姓名、身份证号、学籍号等不同的角度来查询，要想把这么多角度的数据都放到rowkey中几乎不可能（业务的灵活性不允许，对rowkey长度的要求也不允许）。所以需要secondary index（二级索引）来完成这件事。secondary index的原理很简单，但是如果自己维护的话则会麻烦一些。现在，Phoenix已经提供了对HBase secondary index的支持。\n\n### 2、Phoenix Global Indexing And Local Indexing\n\n#### 2.1 Global Indexing\n\n- Global indexing，全局索引，适用于读多写少的业务场景。\n- 使用Global indexing在写数据的时候开销很大，因为所有对数据表的更新操作（DELETE, UPSERT VALUES and UPSERT SELECT），都会引起索引表的更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。\n- 在读数据的时候Phoenix会选择索引表来降低查询消耗的时间。在默认情况下如果想查询的字段不是索引字段的话索引表不会被使用，也就是说不会带来查询速度的提升。\n\n#### 2.2 Local Indexing\n\n- Local indexing，本地索引，适用于写操作频繁以及空间受限制的场景。\n- 与Global indexing一样，Phoenix会自动判定在进行查询的时候是否使用索引。使用Local indexing时，索引数据和数据表的数据存放在相同的服务器中，这样避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。使用Local indexing的时候即使查询的字段不是索引字段索引表也会被使用，这会带来查询速度的提升，这点跟Global indexing不同。对于Local Indexing，一个数据表的所有索引数据都存储在一个单一的独立的可共享的表中\n\n### 3 Immutable index And Mutable index\n\n#### 3.1 immutable index\n\n- immutable index，不可变索引，适用于数据只增加不更新并且按照时间先后顺序存储（time-series data）的场景，如保存日志数据或者事件数据等。\n- 不可变索引的存储方式是write one，append only。当在Phoenix使用create table语句时指定IMMUTABLE_ROWS = true表示该表上创建的索引将被设置为不可变索引。Phoenix默认情况下如果在create table时不指定IMMUTABLE_ROW = true时，表示该表为mutable。不可变索引分为Global immutable index和Local immutable index两种。\n\n#### 3.2 mutable index\n\n- mutable index，可变索引，适用于数据有增删改的场景。\n- Phoenix默认情况创建的索引都是可变索引，除非在create table的时候显式地指定IMMUTABLE_ROWS = true。可变索引同样分为Global immutable index和Local immutable index两种。\n\n### 4、配置HBase支持Phoenix二级索引\n\n#### 4.1 修改配置文件\n\n- 如果要启用phoenix的二级索引功能，需要修改配置文件hbase-site.xml\n\n- vim hbase-site.xml      \n\n  ~~~xml\n  <!-- 添加配置 -->\n  <property>\n  \t<name>hbase.regionserver.wal.codec</name>\n  \t<value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>\n  </property>\n  <property>\n     <name>hbase.region.server.rpc.scheduler.factory.class</name>\n     <value>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory</value>\n  </property>\n  <property>\n  \t\t<name>hbase.rpc.controllerfactory.class</name>\n  \t\t<value>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</value>\n  </property>\n  ~~~\n\n#### 4.2 重启hbase\n\n- 完成上述修改后重启hbase集群使配置生效。\n\n\n\n### 5、实战\n\n#### 5.1 在phoenix中创建表\n\n- 首先，在phoenix中创建一个user table\n\n- node02执行以下命令，进入phoenix客户端\n\n~~~~\ncd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/\nbin/sqlline.py node01:2181\t\n~~~~\n\n- 创建表\n\n```sql\ncreate  table user (\n\"session_id\" varchar(100) not null primary key, \n\"f\".\"cookie_id\" varchar(100), \n\"f\".\"visit_time\" varchar(100), \n\"f\".\"user_id\" varchar(100), \n\"f\".\"age\" varchar(100), \n\"f\".\"sex\" varchar(100), \n\"f\".\"visit_url\" varchar(100), \n\"f\".\"visit_os\" varchar(100), \n\"f\".\"browser_name\" varchar(100),\n\"f\".\"visit_ip\" varchar(100), \n\"f\".\"province\" varchar(100),\n\"f\".\"city\" varchar(100),\n\"f\".\"page_id\" varchar(100), \n\"f\".\"goods_id\" varchar(100),\n\"f\".\"shop_id\" varchar(100)) column_encoded_bytes=0;\n```\n\n#### 5.2 导入测试数据\n\n- 将课件当中的user50w.csv 这个文件上传到node02的/kkb/install/phoenixsql 这个路径下 \n  该CSV文件中有250万条记录\n  node02执行以下命令，导入50W的测试数据\n\n~~~shell\ncd /kkb/install/apache-phoenix-4.14.0-cdh5.14.2-bin/\nbin/psql.py -t USER node01:2181 /kkb/install/phoenixsql/user50w.csv\n~~~\n\n#### 5.3 Global Indexing的二级索引测试\n\n##### 5.3.1 正常查询一条数据所需的时间\n\n- 在为表USER创建secondary index之前，先看看查询一条数据所需的时间\n  在node02服务器，进入phoenix的客户端\n\n~~~shell\nbin/sqlline.py node01:2181\ncd /kkb/install/apache-phoenix-4.14.0-cdh5.14.2-bin\n~~~\n\n- 然后执行以下sql语句，查询数据，查看耗费时间\n\n  可以看到，对名为cookie_id的列进行按值查询需要10秒左右。\n\n```sql\nselect * from user where \"cookie_id\" = '99738fd1-2084-44e9';\n```\n\n- 我们可以通过explain来查看执行计划\n  EXPLAIN(语句的执行逻辑及计划):\n\n  由图看知先进行了全表扫描再通过过滤器来筛选出目标数据，显示这种查询方式效率是很低的。\n\n\n~~~\nexplain select * from user where \"cookie_id\" = '99738fd1-2084-44e9';\n~~~\n\n##### 5.3.2 给表USER创建基于Global Indexing的二级索引\n\n- 进入到phoenix的客户端，然后执行以下命令进行创建索引\n  在cookie_id列上面创建二级索引：\n\n  查看当前所有表会发现多一张USER_COOKIE_ID_INDEX索引表，查询该表数据。\n\n~~~\n0: jdbc:phoenix:node01:2181> create index USER_COOKIE_ID_INDEX on USER (\"f\".\"cookie_id\"); \n~~~\n\n![](https://kfly.top/picture/kfly-top/Phoenix构建二级索引/assets/user_cookie_id_index.PNG)\n\n* 再次执行查询\"cookie_id\"='99738fd1-2084-44e9'的数据记录\n\n  ```\nselect \"cookie_id\" from user where \"cookie_id\" = '99738fd1-2084-44e9';\n  ```\n\n  ![](https://kfly.top/picture/kfly-top/Phoenix构建二级索引/assets/3.PNG)\n\n  此时：查询速度由10秒左右减少到了毫秒级别。\n  \n  注意：select所带的字段必须包含在覆盖索引内\n  \n  EXPLAIN(语句的执行逻辑及计划):\n\n  可以看到使用到了创建的索引USER_COOKIE_ID_INDEX。\n  \n  ~~~\n  explain select \"cookie_id\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n  ~~~\n  \n  ![](https://kfly.top/picture/kfly-top/Phoenix构建二级索引/assets/4.PNG)\t\n\n\n##### 5.3.3 以下查询不会用到索引表\n\n  ~~~~\nselect \"cookie_id\",\"age\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n  ~~~~\n\n- (虽然cookie_id是索引字段，但age不是索引字段，所以不会使用到索引)\n    也可以通过EXPLAIN查询语句的执行逻辑及计划\n\n```\nEXPLAIN\tselect \"cookie_id\",\"age\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n```\n\n- 同理要查询的字段不是索引字段，也不会使用到索引表。\n\n  ~~~\nselect \"sex\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n  ~~~\n\n\n\n#### 5.4  Local Indexing的二级索引测试\n\n##### 5.4.1 正常查询一条数据所需的时间\n\n- 在为表USER创建secondary index之前，先看看查询一条数据所需的时间\n\n~~~\nselect * from user where \"user_id\"='371e963d-c-487065';\n~~~\n\n可以看到，对名为user_id的列进行按值查询需要11秒左右。\n\n- EXPLAIN(语句的执行逻辑及计划):\n\n~~~\nexplain select * from user where \"user_id\"='371e963d-c-487065';\n~~~\n\n由图看知先进行了全表扫描再通过过滤器来筛选出目标数据，显示这种查询方式效率是很低的。\n\n##### 5.4.2 给表USER创建基于Local Indexing的二级索引\n\n- 在user_id列上面创建二级索引：\n\n~~~\ncreate local index USER_USER_ID_INDEX on USER (\"f\".\"user_id\");\n~~~\n\n查看当前所有表会发现多一张USER_USER_ID_INDEX索引表，查询该表数据。\n\n- 再次执行查询\"user_id\"='371e963d-c-487065'的数据记录\n\n  ```\nselect * from user where \"user_id\"='371e963d-c-487065';\n  可以看到，对名为user_id的列进行按值查询需要0.3秒左右。\n  ```\n\n- EXPLAIN(语句的执行逻辑及计划):\n\n```\n    explain select * from user where \"user_id\"='371e963d-c-487065';\n```\n\n查看执行计划，没有执行全表扫描，效率更高了\n\n此时：查询速度由11秒左右减少到了毫秒级别。\n\nEXPLAIN(语句的执行逻辑及计划):\n\n  ~~~\nexplain select \"user_id\",\"age\",\"sex\" from user where \"user_id\"='371e963d-c-487065';\n  ~~~\n\n可以看到使用到了创建的索引USER_USER_ID_INDEX.\n\n#### 5.5 如何确保query查询使用Index\n\n​\t要想让一个查询使用index，有三种方式实现。\n\n##### 5.5.1  创建 convered index\n\n~~~\n\t如果在某次查询中，查询项或者查询条件中包含除被索引列之外的列（主键MY_PK除外）。默认情况下，该查询会触发full table scan（全表扫描），但是使用covered index则可以避免全表扫描。 \n\t创建包含某个字段的覆盖索引,创建方式如下：\n\t create index USER_COOKIE_ID_AGE_INDEX on USER (\"f\".\"cookie_id\") include(\"f\".\"age\");\n\t \n\t 查看当前所有表会发现多一张USER_COOKIE_ID_AGE_INDEX索引表，查询该表数据。\n~~~\n\n\n\n~~~\n查询数据\nselect \"age\" from user where  \"cookie_id\"='99738fd1-2084-44e9';\nselect \"age\",\"sex\" from user where  \"cookie_id\"='99738fd1-2084-44e9';\n~~~\n\n![](https://kfly.top/picture/kfly-top/Phoenix构建二级索引/assets/16.PNG)\n\n![](https://kfly.top/picture/kfly-top/Phoenix构建二级索引/assets/17.PNG)\n\n##### 5.5.2 在查询中提示其使用index\n\n~~~\n在select和column_name之间加上/*+ Index(<表名> <index名>)*/，通过这种方式强制使用索引。\n例如：\nselect /*+ index(user,USER_COOKIE_ID_AGE_INDEX) */ \"age\" from user where \"cookie_id\"='99738fd1-2084-44e9';\n\n如果sex是索引字段，那么就会直接从索引表中查询\n如果sex不是索引字段，那么将会进行全表扫描，所以当用户明确知道表中数据较少且符合检索条件时才适用，此时的性能才是最佳的。\n~~~\n\n![](https://kfly.top/picture/kfly-top/Phoenix构建二级索引/assets/18.PNG)\n\n![](https://kfly.top/picture/kfly-top/Phoenix构建二级索引/assets/19.PNG)\n\n##### 5.5.3 使用本地索引 (创建Local Indexing 索引)\n\n* 详细见上面\n\n\n\n#### 5.6 索引重建\n\n~~~\nPhoenix的索引重建是把索引表清空后重新装配数据。\nalter index USER_COOKIE_ID_INDEX on user rebuild;\n~~~\n\n\n\n#### 5.7 删除索引\n\n~~~\n删除某个表的某张索引：\n语法\tdrop index 索引名称 on 表名\n例如：  \ndrop index USER_COOKIE_ID_INDEX on user;\n\n如果表中的一个索引列被删除，则索引也将被自动删除，如果删除的是\n覆盖索引上的列，则此列将从覆盖索引中被自动删除。\n~~~\n\n### 6、索引性能调优\n\n​\t一般来说，索引已经很快了，不需要特别的优化。这里也提供了一些方法，让你在面对特定的环境和负载的时候可以进行一些调优。下面的这些需要在hbase-site.xml文件中设置，针对所有的服务器。\n\n~~~\n1. index.builder.threads.max \n创建索引时，使用的最大线程数。 \n默认值: 10。\n\n2. index.builder.threads.keepalivetime \n创建索引的创建线程池中线程的存活时间，单位：秒。 \n默认值: 60\n\n3. index.writer.threads.max \n写索引表数据的写线程池的最大线程数。 \n更新索引表可以用的最大线程数，也就是同时可以更新多少张索引表，数量最好和索引表的数量一致。 \n默认值: 10\n\n4. index.writer.threads.keepalivetime \n索引写线程池中，线程的存活时间，单位：秒。\n默认值：60\n \n\n5. hbase.htable.threads.max \n每一张索引表可用于写的线程数。 \n默认值: 2,147,483,647\n\n6. hbase.htable.threads.keepalivetime \n索引表线程池中线程的存活时间，单位：秒。 \n默认值: 60\n\n7. index.tablefactory.cache.size \n允许缓存的索引表的数量。 \n增加此值，可以在写索引表时不用每次都去重复的创建htable，这个值越大，内存消耗越多。 \n默认值: 10\n\n8. org.apache.phoenix.regionserver.index.handler.count \n处理全局索引写请求时，可以使用的线程数。 \n默认值: 30\n~~~\n\n","tags":["phoenix","二级索引"]},{"title":"Phoenix环境部署安装","url":"/2019/11/15/it/phoenix/Phoenix环境部署安装/","content":"\n# Phoenix安装部署\n\n- 需要先安装好HBase集群\n- phoenix只是一个工具，只需要在一台机器上安装就可以了，这里我们选择node02服务器来进行安装一台即可\n\n# 1、下载安装包\n\n- 从对应的地址下载：http://archive.apache.org/dist/phoenix/\n- 注意版本兼容问题；这里我们使用的是\n  - apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz\n\n# 2、上传解压\n\n- 将安装包上传到node02服务器的/kfly/soft路径下，然后进行解压\n\n```shell\ncd /kfly/soft/\ntar -zxf apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz -C /kfly/install/\n```\n\n# 3、修改配置\n\n## 3.1 拷贝jar包\n\n- 将phoenix目录下的==phoenix-4.8.2-HBase-1.2-server.jar==、==phoenix-core-4.8.2-HBase-1.2.jar==拷贝到==各个 HBase节点的lib目录==下。\n\n- node02执行以下命令\n\n  ```shell\n  cd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin\n  \n  scp phoenix-4.14.0-cdh5.14.2-server.jar phoenix-core-4.14.0-cdh5.14.2.jar node01:/kfly/install/hbase-1.2.0-cdh5.14.2/lib/ \n  \n  scp phoenix-4.14.0-cdh5.14.2-server.jar phoenix-core-4.14.0-cdh5.14.2.jar node02:/kfly/install/hbase-1.2.0-cdh5.14.2/lib/ \n  \n  scp phoenix-4.14.0-cdh5.14.2-server.jar phoenix-core-4.14.0-cdh5.14.2.jar node03:/kfly/install/hbase-1.2.0-cdh5.14.2/lib/ \n  ```\n\n## 3.2 拷贝配置文件\n\n- 将HBase的配置文件==hbase-site.xml==、 hadoop下的配置文件==core-site.xml== 、==hdfs-site.xml==放到phoenix/bin/下，替换phoenix原来的配置文件。\n\n  node02执行以下命令，进行拷贝配置文件\n\n  ```shell\n  cp /kfly/install/hadoop-2.6.0/etc/hadoop/core-site.xml  /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/\n  \n  scp /kfly/install/hadoop-2.6.0/etc/hadoop/hdfs-site.xml /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/\n  \n  scp /kfly/install/hbase-1.2.0-cdh5.14.2/conf/hbase-site.xml /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/\n  ```\n\n- 重启hbase集群，使Phoenix的jar包生效。\n\n  node01执行以下命令来重启hbase的集群\n\n  ```\n  cd /kfly/install/hbase-1.2.0-cdh5.14.2/\n  bin/stop-hbase.sh \n  bin/start-hbase.sh \n  ```\n\n\n\n# 4、验证是否成功\n\n- 在phoenix/bin下输入命令, 进入到命令行，接下来就可以操作了\n\n- node02执行以下命令，进入phoenix客户端\n\n  ~~~shell\n  cd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/\n  bin/sqlline.py node01:2181\n  ~~~\n\n# 5、Phoenix使用\n\n## 5.1 批处理方式\n\n* node02执行以下命令创建user_phoenix.sql文件\n\n~~~\nmkdir -p /kfly/doc/phoenixsql\ncd /kfly/install/phoenixsql/\nvi user_phoenix.sql\n~~~\n\n​\t内容如下\n\n```sql\ncreate table if not exists user_phoenix (state varchar(10) NOT NULL,  city varchar(20) NOT NULL,  population BIGINT  CONSTRAINT my_pk PRIMARY KEY (state, city));\n```\n\n* node02执行以下命令，创建user_phoenix.csv数据文件\n\n~~~\ncd /kfly/install/phoenixsql/\nvi user_phoenix.csv\n~~~\n\n​\t内容如下\n\n```\nNY,New York,8143197\nCA,Los Angeles,3844829\nIL,Chicago,2842518\nTX,Houston,2016582\nPA,Philadelphia,1463281\nAZ,Phoenix,1461575\nTX,San Antonio,1256509\nCA,San Diego,1255540\nTX,Dallas,1213825\nCA,San Jose,912332\n```\n\n* 创建user_phoenix_query.sql文件\n\n~~~\ncd /kfly/doc/phoenixsql\nvi user_phoenix_query.sql\n~~~\n\n​\t\t内容如下\n\n```sql\nselect state as \"userState\",count(city) as \"City Count\",sum(population) as \"Population Sum\" FROM user_phoenix GROUP BY state; \n```\n\n* 执行sql语句\n\n~~~shell\ncd /kfly/doc/phoenixsql\n\n/kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/bin/psql.py  node01:2181 user_phoenix.sql user_phoenix.csv user_phoenix_query.sql\n~~~\n\n\n\n## 5.2 命令行方式\n\n* 执行命令\n\n~~~shell\ncd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin/\nbin/sqlline.py node01:2181\n~~~\n\n* 退出命令行方式\n\n  phoenix的命令都需要一个==感叹号==\n\n~~~\n!quit\n~~~\n\n- 查看phoenix的帮助文档，显示所有命令；用!help\n\n  ```\n  0: jdbc:phoenix:node01:2181> !help\n  !all                Execute the specified SQL against all the current\n                    connections\n  !autocommit         Set autocommit mode on or off\n  !batch              Start or execute a batch of statements\n  !brief              Set verbose mode off\n  !call               Execute a callable statement\n  !close              Close the current connection to the database\n  !closeall           Close all current open connections\n  !columns            List all the columns for the specified table\n  !commit             Commit the current transaction (if autocommit is off)\n  !connect            Open a new connection to the database.\n  !dbinfo             Give metadata information about the database\n  !describe           Describe a table\n  !dropall            Drop all tables in the current database\n  !exportedkeys       List all the exported keys for the specified table\n  !go                 Select the current connection\n  !help               Print a summary of command usage\n  !history            Display the command history\n  !importedkeys       List all the imported keys for the specified table\n  !indexes            List all the indexes for the specified table\n  !isolation          Set the transaction isolation for this connection\n  !list               List the current connections\n  !manual             Display the SQLLine manual\n  !metadata           Obtain metadata information\n  !nativesql          Show the native SQL for the specified statement\n  !outputformat       Set the output format for displaying results\n                    (table,vertical,csv,tsv,xmlattrs,xmlelements)\n  !primarykeys        List all the primary keys for the specified table\n  !procedures         List all the procedures\n  !properties         Connect to the database specified in the properties file(s)\n  !quit               Exits the program\n  !reconnect          Reconnect to the database\n  !record             Record all output to the specified file\n  !rehash             Fetch table and column names for command completion\n  !rollback           Roll back the current transaction (if autocommit is off)\n  !run                Run a script from the specified file\n  !save               Save the current variabes and aliases\n  !scan               Scan for installed JDBC drivers\n  !script             Start saving a script to a file\n  !set                Set a sqlline variable\n  ```\n\nVariable        Value      Description\n=============== ========== ================================\nautoCommit      true/false Enable/disable automatic\n                           transaction commit\nautoSave        true/false Automatically save preferences\ncolor           true/false Control whether color is used\n                           for display\nfastConnect     true/false Skip building table/column list\n                           for tab-completion\nforce           true/false Continue running script even\n                           after errors\nheaderInterval  integer    The interval between which\n                           headers are displayed\nhistoryFile     path       File in which to save command\n                           history. Default is\n                           $HOME/.sqlline/history (UNIX,\n                           Linux, Mac OS),\n                           $HOME/sqlline/history (Windows)\nincremental     true/false Do not receive all rows from\n                           server before printing the first\n                           row. Uses fewer resources,\n                           especially for long-running\n                           queries, but column widths may\n                           be incorrect.\nisolation       LEVEL      Set transaction isolation level\nmaxColumnWidth  integer    The maximum width to use when\n                           displaying columns\nmaxHeight       integer    The maximum height of the\n                           terminal\nmaxWidth        integer    The maximum width of the\n                           terminal\nnumberFormat    pattern    Format numbers using\n                           DecimalFormat pattern\noutputFormat    table/vertical/csv/tsv Format mode for\n                           result display\npropertiesFile  path       File from which SqlLine reads\n                           properties on startup; default is\n                           $HOME/.sqlline/sqlline.properties\n                           (UNIX, Linux, Mac OS),\n                           $HOME/sqlline/sqlline.properties\n                           (Windows)\nrowLimit        integer    Maximum number of rows returned\n                           from a query; zero means no\n                           limit\nshowElapsedTime true/false Display execution time when\n                           verbose\nshowHeader      true/false Show column names in query\n                           results\nshowNestedErrs  true/false Display nested errors\nshowWarnings    true/false Display connection warnings\nsilent          true/false Be more silent\ntimeout         integer    Query timeout in seconds; less\n                           than zero means no timeout\ntrimScripts     true/false Remove trailing spaces from\n                           lines read from script files\nverbose         true/false Show verbose error messages and\n                           debug info\n!sql                Execute a SQL command\n!tables             List all the tables in the database\n!typeinfo           Display the type map for the current connection\n!verbose            Set verbose mode on\n\n  ```\n* 1、建立employee的映射表\n\n  进入hbase客户端，创建一个普通表employee，并且有两个列族 company 和family\n\n  ~~~\n  cd /kfly/install/hbase-1.2.0-cdh5.14.2/\n  bin/hbase shell\n  hbase(main):001:0> create 'employee','company','family'\n  ~~~\n\n* 2、添加数据\n\n  ~~~\n  put 'employee','row1','company:name','ted'\n  put 'employee','row1','company:position','worker'\n  put 'employee','row1','family:tel','13600912345'\n  put 'employee','row2','company:name','michael'\n  put 'employee','row2','company:position','manager'\n  put 'employee','row2','family:tel','1894225698'\n  ~~~\n\n* 3、建立hbase到phoenix的映射表\n\n  node02进入到phoenix的客户端，然后创建映射表\n\n  ~~~\n  cd kflyb/install/apache-phoenix-4.14.0-cdh5.14.2-bin\n  bin/sqlline.py node01:2181\n  ~~~\n\n  执行语句\n\n  ```sql\n  CREATE TABLE IF NOT EXISTS \"employee\" (\"no\" VARCHAR(10) NOT NULL PRIMARY KEY, \"company\".\"name\" VARCHAR(30),\"company\".\"position\" VARCHAR(20), \"family\".\"tel\" VARCHAR(20), \"family\".\"age\" INTEGER) column_encoded_bytes=0;\n\n  ```\n\n  > 说明\n  >\n  > 在建立映射表之前要说明的是，Phoenix是==大小写敏感==的，并且所有命令都是大写，如果你建的表名没有用双引号括起来，那么无论你输入的是大写还是小写，建立出来的表名都是大写的，如果你需要建立出同时包含大写和小写的表名和字段名，请把表名或者字段名用双引号括起来。 \n\n* 4、查询映射表数据\n\n~~~\n0: jdbc:phoenix:node1:2181> select * from \"employee\";\n+-------+----------+-----------+--------------+-------+\n|  no   |   name   | position  |     tel      |  age  |\n+-------+----------+-----------+--------------+-------+\n| row1  | ted      | worker    | 13600912345  | null  |\n| row2  | michael  | manager   | 1894225698   | null  |\n+-------+----------+-----------+--------------+-------+\n\n0: jdbc:phoenix:node01:2181> select * from \"employee\" where \"tel\" = '13600912345';\n+-------+-------+-----------+--------------+-------+\n|  no   | name  | position  |     tel      |  age  |\n+-------+-------+-----------+--------------+-------+\n| row1  | ted   | worker    | 13600912345  | null  |\n+-------+-------+-----------+--------------+-------+\n\n\n~~~\n\n## 5.3 GUI方式\n\n- 通过dbeaver来连接phoenix\n- [点击下载]( https://dbeaver.io/files/6.2.4/dbeaver-ce-6.2.4-macos.dmg)\n\n### 5.3.1 准备两个文件\n\n- 我们通过dbeaver来连接phoenix需要两个文件\n\n  - 第一个文件是phoenix-4.14.0-cdh5.14.2-client.jar\n  - 第二个文件是hbase-site.xml\n\n- 进入到phoenix的安装目录，获取第一个文件\n\n  node02执行以下命令，进入到以下路径，获取第一个文件\n\n  找到 phoenix-4.14.0-cdh5.14.2-client.jar  这个jar包，并将其下载下来备用\n\n```shell\ncd /kfly/install/apache-phoenix-4.14.0-cdh5.14.2-bin\n\n```\n\n- 然后进入到node02服务器的hbase的安装配置文件路径，获取hbase-site.xml这个文件\n\n  找到hbase-site.xml，将其下载下来备用\n\n```shell\ncd /kfly/install/hbase-1.2.0-cdh5.14.2/conf/\n\n```\n\n\n\n### 5.3.2 更新jar包\n\n- 将hbase-site.xml放到phoenix-4.14.0-cdh5.14.2-client.jar这个jar包里面去\n\n- 我们在第一步找到了hbase-site.xml和phoenix-4.14.0-cdh5.14.2-client.jar 这两个文件之后，我们需要使用解压缩工具，将phoenix-4.14.0-cdh5.14.2-client.jar 这个jar包解压开，然后将hbase-site.xml放入到phoenix-4.14.0-cdh5.14.2-client.jar 这个jar包里面去\n\n![1571282342354](/Users/dingchuangshi/Downloads/20191115-HBase第四次课/phoenix安装部署/assets/1571282342354.png)\n\n### 5.3.3 通过dbeaver连接phoenix\n\n![1571282405612](/Users/dingchuangshi/Downloads/20191115-HBase第四次课/phoenix安装部署/assets/1571282405612.png)\n\n![1571282452922](/Users/dingchuangshi/Downloads/20191115-HBase第四次课/phoenix安装部署/assets/1571282452922.png)\n\n![1571282684917](/Users/dingchuangshi/Downloads/20191115-HBase第四次课/phoenix安装部署/assets/1571282684917.png)\n\n- 注意：如果连接不上，可能不是操作配置的问题，有可能是因为dbeaver软件的问题，将dbeaver软件重启几次试试看\n\n### 5.4.4 创建数据库表，并实现sql进行操作\n\n- 直接在phoenix当中通过sql语句的方式来创建表并\n\n```sql\nCREATE TABLE IF NOT EXISTS US_POPULATION (\n      state CHAR(2) NOT NULL,\n      city VARCHAR NOT NULL,\n      population BIGINT\n      CONSTRAINT my_pk PRIMARY KEY (state, city));\n\nUPSERT INTO US_POPULATION (state, city, population) values ('NY','New York',8143197);\nUPSERT INTO US_POPULATION (state, city, population) values ('CA','Los Angeles',3844829);\n\nSELECT * FROM US_POPULATION WHERE population > 8000000;\n```\n\n## 5.4 JDBC调用方式\n\n* 创建maven工程并导入jar包\n\n~~~xml\n<dependencies>\n    <dependency>\n        <groupId>org.apache.phoenix</groupId>\n        <artifactId>phoenix-core</artifactId>\n        <version>4.14.0-cdh5.14.2</version>\n    </dependency>\n    <dependency>\n        <groupId>junit</groupId>\n        <artifactId>junit</artifactId>\n        <version>4.12</version>\n    </dependency>\n    <dependency>\n        <groupId>org.testng</groupId>\n        <artifactId>testng</artifactId>\n        <version>6.14.3</version>\n    </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <!-- 限制jdk版本插件 -->\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                    <encoding>UTF-8</encoding>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n~~~\n\n* 代码开发\n\n~~~java\nimport org.testng.annotations.BeforeTest;\nimport org.testng.annotations.Test;\nimport java.sql.*;\npublic class PhoenixSearch {\n    private Connection connection;\n    private Statement statement;\n    private ResultSet rs;\n    @BeforeTest\n    public void init() throws SQLException {\n        //定义phoenix的连接url地址\n        String url=\"jdbc:phoenix:node01:2181\";\n        connection = DriverManager.getConnection(url);\n        //构建Statement对象\n        statement = connection.createStatement();\n    }\n    @Test\n    public void queryTable() throws SQLException {\n        //定义查询的sql语句，注意大小写\n        String sql=\"select * from US_POPULATION\";\n        //执行sql语句\n        try {\n            rs=statement.executeQuery(sql);\n            while(rs.next()){\n                System.out.println(\"state:\"+rs.getString(\"state\"));\n                System.out.println(\"city:\"+rs.getString(\"city\"));\n                System.out.println(\"population:\"+rs.getInt(\"population\"));\n                System.out.println(\"-------------------------\");\n            }\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }finally {\n            if(connection!=null){\n                connection.close();\n            }\n        }\n    }\n}\n~~~\n\n","tags":["环境搭建","phoenix"]},{"title":"大数据问题集合","url":"/2019/11/15/it/problems/大数据问题集合/","content":"\n# 1. Hive\n\n- 运行MR时检查到用过多虚拟内存而被NodeManager杀死进程问题：\n\n  ```shell\n  # 1. 这种问题是从机上运行的Container试图使用过多的内存，而被NodeManager kill掉了。\n  \n  Caused by: org.apache.tez.dag.api.SessionNotRunning: TezSession has already shutdown. Application application_1546781144082_0005 failed 2 times due to AM Container for appattempt_1546781144082_0005_000002 exited with  exitCode: -103\n  For more detailed output, check application t racking page:http://node01:8088/cluster/app/application_1546781144082_0005Then, click on links to logs of each attempt.\n  Diagnostics: Container [pid=11116,containerID=container_1546781144082_0005_02_000001] is running beyond virtual memory limits. Current usage: 216.3 MB of 1 GB physical memory used; 2.6 GB of 2.1 GB virtual memory used. Killing container.\n  \n  # 描述\n  The NodeManager is killing your container. It sounds like you are trying to use hadoop streaming which is running as a child process of the map-reduce task. The NodeManager monitors the entire process tree of the task and if it eats up more memory than the maximum set in mapreduce.map.memory.mb or mapreduce.reduce.memory.mb respectively, we would expect the Nodemanager to kill the task, otherwise your task is stealing memory belonging to other containers, which you don't want.\n  ​```\n  ​```\n  # 方案一：关掉虚拟机内存检查  yarn-site.xml ，修改后重启yarnß\n  <property>\n      <name>yarn.nodemanager.vmem-check-enabled</name>\n      <value>false</value>\n  </property>\n  \n  # 方案二：mapred-site.xml中设置Map和Reduce任务的内存配置如下：(value中实际配置的内存需要根据自己机器内存大小及应用情况进行修改)\n  \n  <property>\n  　　<name>mapreduce.map.memory.mb</name>\n  　　<value>1536</value>\n  </property>\n  <property>\n  　　<name>mapreduce.map.java.opts</name>\n  　　<value>-Xmx1024M</value>\n  </property>\n  <property>\n  　　<name>mapreduce.reduce.memory.mb</name>\n  　　<value>3072</value>\n  </property>\n  <property>\n  　　<name>mapreduce.reduce.java.opts</name>\n  　　<value>-Xmx2560M</value>\n  </property>\n  ```\n\n\n\n#2. Maven\n\n-  maven仓库中、cdh文件无法下载： 修改maven setting.xml\n\n```xml\n<mirror>\n    <id>nexus-aliyun</id>\n    <mirrorOf>*,!cloudera</mirrorOf>\n    <name>Nexus aliyun</name>                     \n    <url>\n      http://maven.aliyun.com/nexus/content/groups/public\n    </url>\n</mirror>\n```\n\n# sqoop\n\n### Sqoop数据导出一致性问题\n\n- 如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。\n\n官网：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html\n\n```shell\n# Since Sqoop breaks down export process into multiple transactions, it is possible that a failed export job may result in partial data being committed to the database. This can further lead to subsequent jobs failing due to insert collisions in some cases, or lead to duplicated data in others. You can overcome this problem by specifying a staging table via the --staging-table option which acts as an auxiliary table that is used to stage exported data. The staged data is finally moved to the destination table in a single transaction.\n\n–staging-table方式\n\nsqoop export \n--connect jdbc:mysql://node1:3306/user_behavior \n--username root \n--password 123456 \n--table app_cource_study_report \n--columns watch_video_cnt,complete_video_cnt,dt \n--fields-terminated-by \"\\t\" \n--export-dir \"/user/hive/warehouse/tmp.db/app_cource_study_analysis_${day}\" \n--staging-table app_cource_study_report_tmp \n--clear-staging-table --input-null-string '\\N'\n```\n\n\n\n","tags":["问题集合"]},{"title":"大数据开发之HBase（二）","url":"/2019/11/13/it/hbase/大数据开发之HBase（二）/","content":"\n# 大数据数据库之hbase\n\n##  1. HBase的数据存储原理\n\n\n\n![hbase存储架构](assets/hbase%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84.png?lastModify=1573631775)\n\n![img](assets/hbase_data_storage-1565601156263.png?lastModify=1573631775)\n\n- 一个HRegionServer会负责管理很多个region\n- 一个**==region==**包含很多个==store==\n  - 一个**==列族==**就划分成一个**==store==**\n  - 如果一个表中只有1个列族，那么每一个region中只有一个store\n  - 如果一个表中有N个列族，那么每一个region中有N个store\n- ==一个store==里面只有==一个memstore==\n  - memstore是一块**内存区域**，写入的数据会先写入memstore进行缓冲，然后再把数据刷到磁盘\n- 一个store里面有很多个**==StoreFile==**, 最后数据是以很多个**==HFile==**这种数据结构的文件保存在HDFS上\n  - StoreFile是HFile的抽象对象，如果说到StoreFile就等于HFile\n  - ==每次memstore刷写数据到磁盘，就生成对应的一个新的HFile文件出来==\n\n![region](assets/region.png?lastModify=1573631775)\n\n\n\n## 2. HBase读数据流程\n\n![img](assets/hbase%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png?lastModify=1573631775)\n\n> 说明：HBase集群，只有一张meta表，此表只有一个region，该region数据保存在一个HRegionServer上\n\n- 1、客户端首先与zk进行连接；从zk找到meta表的region位置，即meta表的数据存储在某一HRegionServer上；客户端与此HRegionServer建立连接，然后读取meta表中的数据；meta表中存储了所有用户表的region信息，我们可以通过`scan  'hbase:meta'`来查看meta表信息\n- 2、根据要查询的namespace、表名和rowkey信息。找到写入数据对应的region信息\n- 3、找到这个region对应的regionServer，然后发送请求\n- 4、查找并定位到对应的region\n- 5、先从memstore查找数据，如果没有，再从BlockCache上读取\n  - HBase上Regionserver的内存分为两个部分\n    - 一部分作为Memstore，主要用来写；\n    - 另外一部分作为BlockCache，主要用于读数据；\n- 6、如果BlockCache中也没有找到，再到StoreFile上进行读取\n  - 从storeFile中读取到数据之后，不是直接把结果数据返回给客户端，而是把数据先写入到BlockCache中，目的是为了加快后续的查询；然后在返回结果给客户端。\n\n\n\n## 3. HBase写数据流程\n\n![img](assets/hbase%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png?lastModify=1573631775)\n\n- 1、客户端首先从zk找到meta表的region位置，然后读取meta表中的数据，meta表中存储了用户表的region信息\n- 2、根据namespace、表名和rowkey信息。找到写入数据对应的region信息\n- 3、找到这个region对应的regionServer，然后发送请求\n- 4、把数据分别写到HLog（write ahead log）和memstore各一份\n- 5、memstore达到阈值后把数据刷到磁盘，生成storeFile文件\n- 6、删除HLog中的历史数据\n\n```\n补充：\nHLog（write ahead log）：\n  也称为WAL意为Write ahead log，类似mysql中的binlog,用来做灾难恢复时用，HLog记录数据的所有变更,一旦数据修改，就可以从log中进行恢复。\n```\n\n\n\n## 4. HBase的flush、compact机制\n\n![](assets/hbase-split-compaction.png)\n\n### 4.1 Flush触发条件\n\n#### 4.1.1 memstore级别限制\n\n- 当Region中任意一个MemStore的大小达到了上限（hbase.hregion.memstore.flush.size，默认128MB），会触发Memstore刷新。\n\n```xml\n<property>\n\t<name>hbase.hregion.memstore.flush.size</name>\n\t<value>134217728</value>\n</property>\n```\n\n  #### 4.1.2 region级别限制\n\n- 当Region中所有Memstore的大小总和达到了上限（hbase.hregion.memstore.block.multiplier * hbase.hregion.memstore.flush.size，默认 2* 128M = 256M），会触发memstore刷新。\n\n```xml\n<property>\n\t<name>hbase.hregion.memstore.flush.size</name>\n\t<value>134217728</value>\n</property>\n<property>\n\t<name>hbase.hregion.memstore.block.multiplier</name>\n\t<value>2</value>\n</property>   \n```\n\n#### 4.1.3 Region Server级别限制\n\n- 当一个Region Server中所有Memstore的大小总和超过低水位阈值hbase.regionserver.global.memstore.size.lower.limit*hbase.regionserver.global.memstore.size（前者默认值0.95），RegionServer开始强制flush；\n- 先Flush Memstore最大的Region，再执行次大的，依次执行；\n- 如写入速度大于flush写出的速度，导致总MemStore大小超过高水位阈值hbase.regionserver.global.memstore.size（默认为JVM内存的40%），此时RegionServer会阻塞更新并强制执行flush，直到总MemStore大小低于低水位阈值\n\n```xml\n<property>\n\t<name>hbase.regionserver.global.memstore.size.lower.limit</name>\n\t<value>0.95</value>\n</property>\n<property>\n\t<name>hbase.regionserver.global.memstore.size</name>\n\t<value>0.4</value>\n</property>\n```\n\n#### 4.1.4 HLog数量上限\n\n- 当一个Region Server中HLog数量达到上限（可通过参数hbase.regionserver.maxlogs配置）时，系统会选取最早的一个 HLog对应的一个或多个Region进行flush\n\n#### 4.1.5 定期刷新Memstore\n\n- 默认周期为1小时，确保Memstore不会长时间没有持久化。为避免所有的MemStore在同一时间都进行flush导致的问题，定期的flush操作有20000左右的随机延时。\n\n#### 4.1.6 手动flush\n\n- 用户可以通过shell命令`flush ‘tablename’`或者`flush ‘region name’`分别对一个表或者一个Region进行flush。\n\n### 4.2 flush的流程\n\n- 为了减少flush过程对读写的影响，将整个flush过程分为三个阶段：\n  - prepare阶段：遍历当前Region中所有的Memstore，将Memstore中当前数据集CellSkipListSet做一个**快照snapshot**；然后再新建一个CellSkipListSet。后期写入的数据都会写入新的CellSkipListSet中。prepare阶段需要加一把updateLock对**写请求阻塞**，结束之后会释放该锁。因为此阶段没有任何费时操作，因此持锁时间很短。\n\n  - flush阶段：遍历所有Memstore，将prepare阶段生成的snapshot持久化为**临时文件**，临时文件会统一放到目录.tmp下。这个过程因为涉及到磁盘IO操作，因此相对比较耗时。\n  - commit阶段：遍历所有Memstore，将flush阶段生成的临时文件移到指定的ColumnFamily目录下，针对HFile生成对应的storefile和Reader，把storefile添加到HStore的storefiles列表中，最后再**清空**prepare阶段生成的snapshot。\n\n### 4.3  Compact合并机制\n\n- hbase为了==防止小文件过多==，以保证查询效率，hbase需要在必要的时候将这些小的store file合并成相对较大的store file，这个过程就称之为compaction。\n\n- 在hbase中主要存在两种类型的compaction合并\n  - **==minor compaction 小合并==**\n  - **==major compaction 大合并==**\n\n#### 4.3.1 minor compaction 小合并\n\n- 在将Store中多个HFile合并为一个HFile\n\n  在这个过程中会选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，对于超过了TTL的数据、更新的数据、删除的数据仅仅只是做了标记。并没有进行物理删除，一次Minor Compaction的结果是更少并且更大的StoreFile。这种合并的触发频率很高。\n\n- minor compaction触发条件由以下几个参数共同决定：\n\n~~~xml\n<!--表示至少需要三个满足条件的store file时，minor compaction才会启动-->\n<property>\n\t<name>hbase.hstore.compactionThreshold</name>\n\t<value>3</value>\n</property>\n\n<!--表示一次minor compaction中最多选取10个store file-->\n<property>\n\t<name>hbase.hstore.compaction.max</name>\n\t<value>10</value>\n</property>\n\n<!--默认值为128m,\n表示文件大小小于该值的store file 一定会加入到minor compaction的store file中\n-->\n<property>\n\t<name>hbase.hstore.compaction.min.size</name>\n\t<value>134217728</value>\n</property>\n\n<!--默认值为LONG.MAX_VALUE，\n表示文件大小大于该值的store file 一定会被minor compaction排除-->\n<property>\n\t<name>hbase.hstore.compaction.max.size</name>\n\t<value>9223372036854775807</value>\n</property>\n~~~\n\n#### 4.3.2 major compaction 大合并\n\n* 合并Store中所有的HFile为一个HFile\n\n  将所有的StoreFile合并成一个StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL过期数据、版本号超过设定版本号的数据。合并频率比较低，默认7天执行一次，并且性能消耗非常大，建议生产关闭(设置为0)，在应用空闲时间手动触发。一般可以是手动控制进行合并，防止出现在业务高峰期。\n\n* major compaction触发时间条件\n\n  ~~~xml\n  <!--默认值为7天进行一次大合并，-->\n  <property>\n  \t<name>hbase.hregion.majorcompaction</name>\n  \t<value>604800000</value>\n  </property>\n  ~~~\n\n* 手动触发\n\n  ~~~ruby\n  ##使用major_compact命令\n  major_compact tableName\n  ~~~\n\n\n\n## 5. region 拆分机制\n\n- region中存储的是大量的rowkey数据 ,当region中的数据条数过多的时候,直接影响查询效率.当region过大的时候.hbase会拆分region , 这也是Hbase的一个优点 .\n\n- HBase的region split策略一共有以下几种：\n\n* 1、**ConstantSizeRegionSplitPolicy**\n\n  * 0.94版本前默认切分策略\n\n* 当region大小大于某个阈值(hbase.hregion.max.filesize=10G)之后就会触发切分，一个region等分为2个region。\n\n  * 但是在生产线上这种切分策略却有相当大的弊端：切分策略对于大表和小表没有明显的区分。阈值(hbase.hregion.max.filesize)设置较大对大表比较友好，但是小表就有可能不会触发分裂，极端情况下可能就1个，这对业务来说并不是什么好事。如果设置较小则对小表友好，但一个大表就会在整个集群产生大量的region，这对于集群的管理、资源使用、failover来说都不是一件好事。\n\n  \n\n* 2、**IncreasingToUpperBoundRegionSplitPolicy**\n\n  *  0.94版本~2.0版本默认切分策略\n\n  - 切分策略稍微有点复杂，总体看和ConstantSizeRegionSplitPolicy思路相同，一个region大小大于设置阈值就会触发切分。但是这个阈值并不像ConstantSizeRegionSplitPolicy是一个固定的值，而是会在一定条件下不断调整，调整规则和region所属表在当前regionserver上的region个数有关系.\n\n  - region split的计算公式是：\n    regioncount^3 * 128M * 2，当region达到该size的时候进行split\n    例如：\n    第一次split：1^3 * 256 = 256MB \n    第二次split：2^3 * 256 = 2048MB \n    第三次split：3^3 * 256 = 6912MB \n    第四次split：4^3 * 256 = 16384MB > 10GB，因此取较小的值10GB \n    后面每次split的size都是10GB了\n\n* 3、**SteppingSplitPolicy**\n\n  * 2.0版本默认切分策略\n\n  - 这种切分策略的切分阈值又发生了变化，相比 IncreasingToUpperBoundRegionSplitPolicy 简单了一些，依然和待分裂region所属表在当前regionserver上的region个数有关系，如果region个数等于1，\n    切分阈值为flush size * 2，否则为MaxRegionFileSize。这种切分策略对于大集群中的大表、小表会比 IncreasingToUpperBoundRegionSplitPolicy 更加友好，小表不会再产生大量的小region，而是适可而止。\n\n* 4、**KeyPrefixRegionSplitPolicy**\n\n  - 根据rowKey的前缀对数据进行分组，这里是指定rowKey的前多少位作为前缀，比如rowKey都是16位的，指定前5位是前缀，那么前5位相同的rowKey在进行region split的时候会分到相同的region中。\n\n* 5、**DelimitedKeyPrefixRegionSplitPolicy**\n\n  - 保证相同前缀的数据在同一个region中，例如rowKey的格式为：userid_eventtype_eventid，指定的delimiter为 _ ，则split的的时候会确保userid相同的数据在同一个region中。\n\n\n* 6、**DisabledRegionSplitPolicy**\n  * 不启用自动拆分, 需要指定手动拆分\n\n\n\n## 6. HBase表的预分区\n\n- 当一个table刚被创建的时候，Hbase默认的分配一个region给table。也就是说这个时候，所有的读写请求都会访问到同一个regionServer的同一个region中，这个时候就达不到负载均衡的效果了，集群中的其他regionServer就可能会处于比较空闲的状态。\n- 解决这个问题可以用**pre-splitting**,在创建table的时候就配置好，生成多个region。\n\n### 6.1 为何要预分区？\n\n* 增加数据读写效率\n* 负载均衡，防止数据倾斜\n* 方便集群容灾调度region\n* 优化Map数量\n\n### 6.2 预分区原理\n\n- 每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。\n\n### 6.3 手动指定预分区\n\n- 两种方式\n\n- 方式一\n\n~~~ruby\ncreate 'person','info1','info2',SPLITS => ['1000','2000','3000','4000']\n~~~\n\n![personSplit](assets/personSplit.png)\n\n* 方式二：也可以把分区规则创建于文件中\n\n  ~~~shell\n  cd /kfly/doc\n  \n  vim split.txt\n  ~~~\n\n  - 文件内容\n\n  ~~~\n  aaa\n  bbb\n  ccc\n  ddd\n  ~~~\n\n  - hbase shell中，执行命令\n\n  ~~~ruby\n  create 'student','info',SPLITS_FILE => '/kfly/install/split.txt'\n  ~~~\n\n  - 成功后查看web界面\n\n  ![splitFile](assets/splitFile.png)\n\n### 6.2.2 HexStringSplit 算法\n\n- HexStringSplit会将数据从“00000000”到“FFFFFFFF”之间的数据长度按照**n等分**之后算出每一段的其实rowkey和结束rowkey，以此作为拆分点。\n\n- 例如：\n\n  ```ruby\n  create 'mytable', 'base_info',' extra_info', {NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}\n  ```\n\n![hbasePreSplit](assets/hbasePreSplit.png)\n\n\n\n## 7. region 合并\n\n### 7.1 region合并说明\n\n- Region的合并不是为了性能,  而是出于维护的目的 .\n- 比如删除了大量的数据 ,这个时候每个Region都变得很小 ,存储多个Region就浪费了 ,这个时候可以把Region合并起来，进而可以减少一些Region服务器节点 \n\n### 7.2 如何进行region合并\n\n#### 7.2.1 通过Merge类冷合并Region\n\n- 执行合并前，==需要先关闭hbase集群==\n\n- 创建一张hbase表：\n\n```ruby\ncreate 'test','info1',SPLITS => ['1000','2000','3000']\n```\n\n- 查看表region\n\n![testRegion](assets/testRegion.png)\n\n- 需求：\n\n  需要把test表中的2个region数据进行合并：\n  test,,1565940912661.62d28d7d20f18debd2e7dac093bc09d8.\n  test,1000,1565940912661.5b6f9e8dad3880bcc825826d12e81436.\n\n- 这里通过org.apache.hadoop.hbase.util.Merge类来实现，不需要进入hbase shell，直接执行（==需要先关闭hbase集群==）：\n  hbase org.apache.hadoop.hbase.util.Merge test test,,1565940912661.62d28d7d20f18debd2e7dac093bc09d8. test,1000,1565940912661.5b6f9e8dad3880bcc825826d12e81436.\n\n- 成功后界面观察\n\n![testMerge](assets/testMerge.png)\n\n#### 7.2.2  通过online_merge热合并Region\n\n- ==不需要关闭hbase集群==，在线进行合并\n\n- 与冷合并不同的是，online_merge的传参是Region的hash值，而Region的hash值就是Region名称的最后那段在两个.之间的字符串部分。\n\n- 需求：需要把test表中的2个region数据进行合并：\n  test,2000,1565940912661.c2212a3956b814a6f0d57a90983a8515.\n  test,3000,1565940912661.553dd4db667814cf2f050561167ca030.\n\n- 需要进入hbase shell：\n\n  ```ruby\n  merge_region 'c2212a3956b814a6f0d57a90983a8515','553dd4db667814cf2f050561167ca030'\n  ```\n\n- 成功后观察界面\n\n![online_merge](assets/online_merge.png)\n\n## 8. HBase集成MapReduce\n\n* HBase表中的数据最终都是存储在HDFS上，HBase天生的支持MR的操作，我们可以通过MR直接处理HBase表中的数据，并且MR可以将处理后的结果直接存储到HBase表中。\n  * 参考地址：<http://hbase.apache.org/book.html#mapreduce>\n\n### 8.1 实战一\n\n* 需求：==读取HBase当中myuser这张表的数据，将数据写入到另外一张myuser2表里面去==\n\n- 第一步：创建myuser2这张hbase表\n\n  **注意：**列族的名字要与myuser表的列族名字相同\n\n```ruby\nhbase(main):010:0> create 'myuser2','f1'\n```\n\n- 第二步：创建maven工程并导入jar包\n\n```xml\n\t<repositories>\n        <repository>\n            <id>cloudera</id>\n            <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n        </repository>\n    </repositories>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.6.0-mr1-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hbase</groupId>\n            <artifactId>hbase-client</artifactId>\n            <version>1.2.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hbase</groupId>\n            <artifactId>hbase-server</artifactId>\n            <version>1.2.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.testng</groupId>\n            <artifactId>testng</artifactId>\n            <version>6.14.3</version>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                    <encoding>UTF-8</encoding>\n                    <!--    <verbal>true</verbal>-->\n                </configuration>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.2</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <filters>\n                                <filter>\n                                    <artifact>*:*</artifact>\n                                    <excludes>\n                                        <exclude>META-INF/*.SF</exclude>\n                                        <exclude>META-INF/*.DSA</exclude>\n                                        <exclude>META-INF/*/RSA</exclude>\n                                    </excludes>\n                                </filter>\n                            </filters>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n- 第三步：开发MR程序实现功能\n\n~~~java\npackage com.kaikeba;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.hbase.Cell;\nimport org.apache.hadoop.hbase.CellUtil;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.client.Result;\nimport org.apache.hadoop.hbase.client.Scan;\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable;\nimport org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;\nimport org.apache.hadoop.hbase.mapreduce.TableMapper;\nimport org.apache.hadoop.hbase.mapreduce.TableReducer;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\n\nimport java.io.IOException;\n\npublic class HBaseMR {\n\n    public static class HBaseMapper extends TableMapper<Text,Put>{\n        @Override\n        protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException {\n             //获取rowkey的字节数组\n            byte[] bytes = key.get();\n            String rowkey = Bytes.toString(bytes);\n            //构建一个put对象\n            Put put = new Put(bytes);\n            //获取一行中所有的cell对象\n            Cell[] cells = value.rawCells();\n            for (Cell cell : cells) {\n                  // f1列族\n                if(\"f1\".equals(Bytes.toString(CellUtil.cloneFamily(cell)))){\n                    // name列名\n                     if(\"name\".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))){\n                          put.add(cell);\n                     }\n                     // age列名\n                    if(\"age\".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))){\n                        put.add(cell);\n                    }\n                }\n            }\n            if(!put.isEmpty()){\n              context.write(new Text(rowkey),put);\n            }\n        }\n    }\n\n     public  static  class HbaseReducer extends TableReducer<Text,Put,ImmutableBytesWritable>{\n         @Override\n         protected void reduce(Text key, Iterable<Put> values, Context context) throws IOException, InterruptedException {\n             for (Put put : values) {\n                 context.write(null,put);\n             }\n         }\n     }\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        Configuration conf = new Configuration();\n\n        Scan scan = new Scan();\n\n        Job job = Job.getInstance(conf);\n        job.setJarByClass(HBaseMR.class);\n        //使用TableMapReduceUtil 工具类来初始化我们的mapper\n        TableMapReduceUtil.initTableMapperJob(TableName.valueOf(args[0]),scan,HBaseMapper.class,Text.class,Put.class,job);\n        //使用TableMapReduceUtil 工具类来初始化我们的reducer\n        TableMapReduceUtil.initTableReducerJob(args[1],HbaseReducer.class,job);\n        //设置reduce task个数\n         job.setNumReduceTasks(1);\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n~~~\n\n* 打成jar包提交到集群中运行\n\n  ~~~shell\n  hadoop jar hbase_java_api-1.0-SNAPSHOT.jar com.kaikeba.HBaseMR t1 t2\n  ~~~\n\n\n### 8.2 实战二\n\n* 需求 读取hdfs上面的数据，写入到hbase表里面去\n\n  node03执行以下命令准备数据文件，并将数据文件上传到HDFS上面去\n\n  ~~~\n  hdfs dfs -mkdir -p /hbase/input\n  cd /kfly/install\n  vim \n  user.txt\n  \n  0007\tzhangsan\t18\n  0008\tlisi\t25\n  0009\twangwu\t20\n  \n  将文件上传到hdfs的路径下面去\n  hdfs dfs -put kflyb/install/user.txt   /hbase/input/\n  ~~~\n\n* 代码开发\n\n ~~~java\npackage com.kaikeba;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable;\nimport org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;\nimport org.apache.hadoop.hbase.mapreduce.TableReducer;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport java.io.IOException;\n\n\n\npublic class Hdfs2Hbase {\n\n    public static class HdfsMapper extends Mapper<LongWritable,Text,Text,NullWritable> {\n\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            context.write(value,NullWritable.get());\n        }\n    }\n\n    public static class HBASEReducer extends TableReducer<Text,NullWritable,ImmutableBytesWritable> {\n\n        protected void reduce(Text key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {\n            String[] split = key.toString().split(\" \");\n            Put put = new Put(Bytes.toBytes(split[0]));\n            put.addColumn(\"f1\".getBytes(),\"name\".getBytes(),split[1].getBytes());\n            put.addColumn(\"f1\".getBytes(),\"age\".getBytes(), split[2].getBytes());\n            context.write(new ImmutableBytesWritable(Bytes.toBytes(split[0])),put);\n        }\n    }\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        job.setJarByClass(Hdfs2Hbase.class);\n\n        job.setInputFormatClass(TextInputFormat.class);\n        //输入文件路径\n        TextInputFormat.addInputPath(job,new Path(args[0]));\n        job.setMapperClass(HdfsMapper.class);\n        //map端的输出的key value 类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(NullWritable.class);\n\n        //指定输出到hbase的表名\n        TableMapReduceUtil.initTableReducerJob(args[1],HBASEReducer.class,job);\n\n        //设置reduce个数\n        job.setNumReduceTasks(1);\n\n        System.exit(job.waitForCompletion(true)?0:1);\n    }\n}\n\n ~~~\n\n* 创建hbase表 t3\n\n~~~ruby\ncreate 't3','f1'\n~~~\n\n\n\n* 打成jar包提交到集群中运行\n\n~~~shell\nhadoop jar hbase_java_api-1.0-SNAPSHOT.jar com.kaikeba.Hdfs2Hbase /data/user.txt t3\n\n~~~\n\n\n\n### 8.3 实战三\n\n* 需求\n\n  * ==通过bulkload的方式批量加载数据到HBase表中==\n  * ==将我们hdfs上面的这个路径/hbase/input/user.txt的数据文件，转换成HFile格式，然后load到myuser2这张表里面去==\n\n* 知识点描述\n\n  - 加载数据到HBase当中去的方式多种多样，我们可以使用HBase的javaAPI或者使用sqoop将我们的数据写入或者导入到HBase当中去，但是这些方式不是慢就是在导入的过程的占用Region资源导致效率低下\n  - 我们也可以通过MR的程序，将我们的数据直接转换成HBase的最终存储格式HFile，然后直接load数据到HBase当中去即可\n\n* HBase数据正常写流程回顾\n\n  ![hbase-write](assets/hbase-write.png)\n\n* bulkload方式的处理示意图\n\n![](assets/bulkload.png)\n\n\n\n* 好处\n\n  - 导入过程不占用Region资源\n  - 能快速导入海量的数据\n  - 节省内存\n\n* ==1、开发生成HFile文件的代码==\n\n~~~java\npackage com.kaikeba;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.Connection;\nimport org.apache.hadoop.hbase.client.ConnectionFactory;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.client.Table;\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable;\nimport org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class HBaseLoad {\n\n    public static class LoadMapper  extends Mapper<LongWritable,Text,ImmutableBytesWritable,Put> {\n        @Override\n        protected void map(LongWritable key, Text value, Mapper.Context context) throws IOException, InterruptedException {\n            String[] split = value.toString().split(\" \");\n            Put put = new Put(Bytes.toBytes(split[0]));\n            put.addColumn(\"f1\".getBytes(),\"name\".getBytes(),split[1].getBytes());\n            put.addColumn(\"f1\".getBytes(),\"age\".getBytes(), split[2].getBytes());\n            context.write(new ImmutableBytesWritable(Bytes.toBytes(split[0])),put);\n        }\n    }\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n            final String INPUT_PATH=  \"hdfs://node01:8020/hbase/input\";\n            final String OUTPUT_PATH= \"hdfs://node01:8020/hbase/output_file\";\n            Configuration conf = HBaseConfiguration.create();\n\n            Connection connection = ConnectionFactory.createConnection(conf);\n            Table table = connection.getTable(TableName.valueOf(\"t4\"));\n            Job job= Job.getInstance(conf);\n\n            job.setJarByClass(HBaseLoad.class);\n            job.setMapperClass(LoadMapper.class);\n            job.setMapOutputKeyClass(ImmutableBytesWritable.class);\n            job.setMapOutputValueClass(Put.class);\n        \n            //指定输出的类型HFileOutputFormat2\n            job.setOutputFormatClass(HFileOutputFormat2.class);\n\n         HFileOutputFormat2.configureIncrementalLoad(job,table,connection.getRegionLocator(TableName.valueOf(\"t4\")));\n            FileInputFormat.addInputPath(job,new Path(INPUT_PATH));\n            FileOutputFormat.setOutputPath(job,new Path(OUTPUT_PATH));\n            System.exit(job.waitForCompletion(true)?0:1);\n\n\n    }\n}\n\n~~~\n\n* ==2、打成jar包提交到集群中运行==\n\n~~~shell\nhadoop jar hbase_java_api-1.0-SNAPSHOT.jar com.kaikeba.HBaseLoad\n~~~\n\n* ==3、观察HDFS上输出的结果==\n\n![f1](assets/f1.png)\n\n![HFile文件](assets/HFile文件.png)\n\n* ==4、加载HFile文件到hbase表中==\n\n  * 方式一：代码加载\n\n  ~~~java\n  package com.kaikeba;\n  \n  import org.apache.hadoop.conf.Configuration;\n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.hbase.HBaseConfiguration;\n  import org.apache.hadoop.hbase.TableName;\n  import org.apache.hadoop.hbase.client.Admin;\n  import org.apache.hadoop.hbase.client.Connection;\n  import org.apache.hadoop.hbase.client.ConnectionFactory;\n  import org.apache.hadoop.hbase.client.Table;\n  import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;\n  \n  public class LoadData {\n      public static void main(String[] args) throws Exception {\n          Configuration configuration = HBaseConfiguration.create();\n          configuration.set(\"hbase.zookeeper.quorum\", \"node01,node02,node03\");\n      //获取数据库连接\n      Connection connection =  ConnectionFactory.createConnection(configuration);\n      //获取表的管理器对象\n      Admin admin = connection.getAdmin();\n      //获取table对象\n      TableName tableName = TableName.valueOf(\"t4\");\n      Table table = connection.getTable(tableName);\n      //构建LoadIncrementalHFiles加载HFile文件\n      LoadIncrementalHFiles load = new LoadIncrementalHFiles(configuration);\n      load.doBulkLoad(new Path(\"hdfs://node01:8020/hbase/output_file\"), admin,table,connection.getRegionLocator(tableName));\n   }\n  }\n  ~~~\n\n  * 方式二：命令加载\n\n  先将hbase的jar包添加到hadoop的classpath路径下\n\n  ```shell\n  先将hbase的jar包添加到hadoop的classpath路径下\n  export HBASE_HOME=/kfly/install/hbase-1.2.0-cdh5.14.2/\n  export HADOOP_HOME=/kfly/install/hadoop-2.6.0/\n  export HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase mapredcp`\n  ```\n\n- 运行命令\n\n  ```shell\n  yarn jar /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar   completebulkload /hbase/output_hfile myuser2\n  ```\n\n​\t\n\n## 8. HBase集成Hive\n\n- Hive提供了与HBase的集成，使得能够在HBase表上使用hive sql 语句进行查询、插入操作以及进行Join和Union等复杂查询，同时也可以将hive表中的数据映射到Hbase中\n\n### 8.1 HBase与Hive的对比（\n\n#### 8.1.1 Hive\n\n- 数据仓库\n\n  Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。\n\n- 用于数据分析、清洗                \n\n  Hive适用于离线的数据分析和清洗，延迟较高\n\n- 基于HDFS、MapReduce\n\n  Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。（不要钻不需要执行MapReduce代码的情况的牛角尖）\n\n#### 8.1.2 HBase\n\n- 数据库\n\n  是一种面向列存储的非关系型数据库。\n\n- 用于存储结构化和非结构话的数据\n\n  适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。\n\n- 基于HDFS\n\n  数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。\n\n- 延迟较低，接入在线业务使用\n\n  面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。\n\n##### 8.1.3 总结：Hive与HBase\n\n- Hive和Hbase是两种基于Hadoop的不同技术，Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到HBase，或者从HBase写回Hive。\n\n### 9.2 整合配置\n\n#### 9.2.1 拷贝jar包\n\n- 将我们HBase的五个jar包拷贝到hive的lib目录下\n\n- hbase的jar包都在/kfly/install/hbase-1.2.0-cdh5.14.2/lib\n\n- 我们需要拷贝五个jar包名字如下\n\n```\nhbase-client-1.2.0-cdh5.14.2.jar                  \nhbase-hadoop2-compat-1.2.0-cdh5.14.2.jar \nhbase-hadoop-compat-1.2.0-cdh5.14.2.jar  \nhbase-it-1.2.0-cdh5.14.2.jar    \nhbase-server-1.2.0-cdh5.14.2.jar\n```\n\n- 我们直接在node03执行以下命令，通过创建软连接的方式来进行jar包的依赖\n\n```shell\nln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar              /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar   \n\nln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-hadoop2-compat-1.2.0-cdh5.14.2.jar      /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-hadoop2-compat-1.2.0-cdh5.14.2.jar             \nln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-hadoop-compat-1.2.0-cdh5.14.2.jar       /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-hadoop-compat-1.2.0-cdh5.14.2.jar            \nln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-it-1.2.0-cdh5.14.2.jar     /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-it-1.2.0-cdh5.14.2.jar    \n\nln -s /kfly/install/hbase-1.2.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar          /kfly/install/hive-1.1.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar  \n```\n\n#### 9.2.2 修改hive的配置文件\n\n- 编辑**node03**服务器上面的hive的配置文件hive-site.xml\n\n```shell\ncd /kfly/install/hive-1.1.0-cdh5.14.2/conf\nvim hive-site.xml\n```\n\n-  添加以下两个属性的配置\n\n```xml\n<property>\n\t\t<name>hive.zookeeper.quorum</name>\n\t\t<value>node01,node02,node03</value>\n</property>\n <property>\n\t\t<name>hbase.zookeeper.quorum</name>\n\t\t<value>node01,node02,node03</value>\n</property>\n```\n\n#### 9.2.3 修改hive-env.sh配置文件\n\n```shell\ncd /kfly/install/hive-1.1.0-cdh5.14.2/conf\nvim hive-env.sh\n```\n\n- 添加以下配置\n\n```\nexport HADOOP_HOME=/export/servers/hadoop-2.6.0\nexport HBASE_HOME=/export/servers/hbase-1.2.0-cdh5.14.2\nexport HIVE_CONF_DIR=/export/servers/hive-1.1.0-cdh5.14.2/conf\n```\n\n\n\n### 9.3 需求一：将hive表当中分析的结果保存到hbase表当中去\n\n#### 9.3.1 hive当中建表\n\n- node03执行以下命令，进入hive客户端，并创建hive表\n\n```shell\ncd /kfly/install/hive-1.1.0-cdh5.14.2/\nbin/hive\n```\n\n- 创建hive数据库与hive对应的数据库表\n\n```mysql\ncreate database course;\nuse course;\n\ncreate external table if not exists course.score(id int,cname string,score int) row format delimited fields terminated by '\\t' stored as textfile ;\n```\n\n#### 9.3.2 准备数据内容如下并加载到hive表\n\n- node03执行以下命令，创建数据文件\n\n```shell\ncd /kfly/install/hivedatas\nvi hive-hbase.txt\n```\n\n- 文件内容如下\n\n```\n1\tzhangsan\t80\n2\tlisi\t60\n3\twangwu\t30\n4\tzhaoliu\t70\n```\n\n- 进入hive客户端进行加载数据\n\n```mysql\nhive (course)> load data local inpath '/kfly/doc/hive-hbase.txt' into table score;\nhive (course)> select * from score;\n```\n\n#### 9.3.3 创建hive管理表与HBase进行映射\n\n- 我们可以创建一个hive的管理表与hbase当中的表进行映射，hive管理表当中的数据，都会存储到hbase上面去\n\n- hive当中创建内部表\n\n```sql\ncreate table course.hbase_score(id int,cname string,score int) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'  with serdeproperties(\"hbase.columns.mapping\" = \"cf:name,cf:score\") tblproperties(\"hbase.table.name\" = \"hbase_score\");\n```\n\n- 通过insert  overwrite select  插入数据\n\n```mysql\ninsert overwrite table course.hbase_score select id,cname,score from course.score;\n```\n\n#### 9.3.4 hbase当中查看表hbase_score\n\n- 进入hbase的客户端查看表hbase_score，并查看当中的数据\n\n```ruby\nhbase(main):023:0> list\n\nTABLE                                                                                 hbase_score                                                                           myuser                                                                                 myuser2                                                                               student                                                                               user                                                                                   5 row(s) in 0.0210 seconds\n=> [\"hbase_score\", \"myuser\", \"myuser2\", \"student\", \"user\"]\n\nhbase(main):024:0> scan 'hbase_score'\n\nROW                      COLUMN+CELL                                                   \n 1                       column=cf:name, timestamp=1550628395266, value=zhangsan       \n 1                       column=cf:score, timestamp=1550628395266, value=80           \n 2                       column=cf:name, timestamp=1550628395266, value=lisi           \n 2                       column=cf:score, timestamp=1550628395266, value=60           \n 3                       column=cf:name, timestamp=1550628395266, value=wangwu         \n 3                       column=cf:score, timestamp=1550628395266, value=30           \n 4                       column=cf:name, timestamp=1550628395266, value=zhaoliu       \n 4                       column=cf:score, timestamp=1550628395266, value=70           \n4 row(s) in 0.0360 seconds\n```\n\n### 9.4 需求二：创建hive外部表，映射HBase当中已有的表模型（5分钟）\n\n#### 9.4.1 HBase当中创建表并手动插入加载一些数据\n\n- 进入HBase的shell客户端，\n\n```shell\nbin/hbase shell\n```\n\n- 手动创建一张表，并插入加载一些数据进去\n\n```ruby\n# 创建一张表\ncreate 'hbase_hive_score',{ NAME =>'cf'}\n# 通过put插入数据到hbase表\nput 'hbase_hive_score','1','cf:name','zhangsan'\nput 'hbase_hive_score','1','cf:score', '95'\nput 'hbase_hive_score','2','cf:name','lisi'\nput 'hbase_hive_score','2','cf:score', '96'\nput 'hbase_hive_score','3','cf:name','wangwu'\nput 'hbase_hive_score','3','cf:score', '97'\n```\n\n#### 9.4.2 建立hive的外部表，映射HBase当中的表以及字段\n\n- 在hive当中建立外部表\n\n- 进入hive客户端，然后执行以下命令进行创建hive外部表，就可以实现映射HBase当中的表数据\n\n```mysql\nCREATE external TABLE course.hbase2hive(id int, name string, score int) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf:name,cf:score\") TBLPROPERTIES(\"hbase.table.name\" =\"hbase_hive_score\");\n```\n\n","tags":["hadoop","hbase","hbase与hive整合"]},{"title":"大数据开发之Hbase（一）","url":"/2019/11/11/it/hbase/大数据开发之Hbase（一）/","content":"\n# 大数据开发之HBase\n\n## 1. HBase是什么\n\n- [漫画学习HBase----最易懂的Hbase架构原理解析](<http://developer.51cto.com/art/201904/595698.htm>)\n\n### 1.1 HBase的概念\n\n* HBase基于Google的BigTable论文，是建立的==HDFS==之上，提供**高可靠性**、**高性能**、**列存储**、**可伸缩**、**实时读写**的分布式数据库系统。\n* 在需要==实时读写随机访问==超大规模数据集时，可以使用HBase。\n\n### 1.2 HBase的特点\n\n* ==**海量存储**==\n  * 可以存储大批量的数据\n* ==**列式存储**==\n  * HBase表的数据是基于列族进行存储的，列族是在列的方向上的划分。\n* ==**极易扩展**==\n  * 底层依赖HDFS，当磁盘空间不足的时候，只需要动态增加datanode节点就可以了\n  * 可以通过增加服务器来对集群的存储进行扩容\n* ==**高并发**==\n  * 支持高并发的读写请求\n* ==**稀疏**==\n  * 稀疏主要是针对HBase列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的。\n* ==**数据的多版本**==\n  - HBase表中的数据可以有多个版本值，默认情况下是根据版本号去区分，版本号就是插入数据的时间戳\n* ==**数据类型单一**==\n  * 所有的数据在HBase中是以==字节数组==进行存储\n\n\n\n## 2. HBase集群安装部署\n\n​\t[点击跳转](https://kfly.top/2019/11/26/hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\n\n## 3. HBase表的数据模型\n\n![](assets/HBase-data-model.png.png)\n\n### 3.1 rowkey行键\n\n- table的主键，table中的记录按照rowkey 的字典序进行排序\n- Row key行键可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)\n\n### 3.2 Column Family列族\n\n- 列族或列簇\n- HBase表中的每个列，都归属与某个列族\n- 列族是表的schema的一部分(而列不是)，即建表时至少指定一个列族\n- 比如创建一张表，名为`user`，有两个列族，分别是`info`和`data`，建表语句`create 'user', 'info', 'data'`\n\n### 3.3 Column列\n\n- 列肯定是表的某一列族下的一个列，用`列族名:列名`表示，如`info`列族下的`name`列，表示为`info:name`\n- 属于某一个ColumnFamily,类似于我们mysql当中创建的具体的列\n\n### 3.4 cell单元格\n\n- 指定row key行键、列族、列，可以确定的一个cell单元格\n\n- cell中的数据是没有类型的，全部是以字节数组进行存储\n\n![](assets/Image201911072218.png)\n\n### 3.5 Timestamp时间戳\n\n- 可以对表中的Cell多次赋值，每次赋值操作时的时间戳timestamp，可看成Cell值的版本号version number\n- 即一个Cell可以有多个版本的值\n\n\n\n## 4. HBase整体架构\n\n![](assets/HBase-Hmaster-Hregionserver.png)\n\n### 4.1 Client客户端\n\n* Client是操作HBase集群的入口\n  * 对于管理类的操作，如表的增、删、改操纵，Client通过RPC与HMaster通信完成\n  * 对于表数据的读写操作，Client通过RPC与RegionServer交互，读写数据\n* Client类型：\n  * HBase shell\n  * Java编程接口\n  * Thrift、Avro、Rest等等\n\n### 4.2 ZooKeeper集群\n\n* 作用\n  * 实现了HMaster的高可用，多HMaster间进行主备选举\n\n  * 保存了HBase的元数据信息meta表，提供了HBase表中region的寻址入口的线索数据\n\n  * 对HMaster和HRegionServer实现了监控\n\n### 4.3 HMaster\n\n* HBase集群也是主从架构，HMaster是主的角色，是老大\n* 主要负责Table表和Region的相关管理工作：\n* 关于Table\n  * 管理Client对Table的增删改的操作\n  * 关于Region\n    * 在Region分裂后，负责新Region分配到指定的HRegionServer上\n    * 管理HRegionServer间的负载均衡，迁移region分布\n    * 当HRegionServer宕机后，负责其上的region的迁移\n\n### 4.4 HRegionServer\n\n* HBase集群中从的角色，是小弟\n* 作用\n\n- 响应客户端的读写数据请求\n  - 负责管理一系列的Region\n  - 切分在运行过程中变大的region\n\n### 4.5 Region\n\n* HBase集群中分布式存储的最小单元\n* 一个Region对应一个Table表的部分数据\n\n\n\n> HBase使用，主要有两种形式：①命令；②Java编程\n\n## 5. HBase shell 命令基本操作\n\n### 5.1 进入HBase客户端命令操作界面\n\n- node01执行以下命令，进入HBase的shell客户端\n\n```shell\ncd /kfly/install/hbase-1.2.0-cdh5.14.2/\nbin/HBase shell\n\n```\n\n### 5.2 help 帮助命令\n\n```shell\nHBase(main):001:0> help\n\n```\n\n### 5.3 list 查看有哪些表\n\n- 查看当前数据库中有哪些表\n\n```shell\nHBase(main):002:0> list\n\n```\n\n### 5.4 create 创建表\n\n- 创建user表，包含info、data两个列族\n- 使用create命令\n\n```\nHBase(main):010:0> create 'user', 'info', 'data'\n\n或者\n\nHBase(main):010:0> create 'user', {NAME => 'info', VERSIONS => '3'}，{NAME => 'data'}\n\n```\n\n### 5.5 put 插入数据操作\n\n- 向表中插入数据\n- 使用put命令\n\n```\n向user表中插入信息，row key为rk0001，列族info中添加名为name的列，值为zhangsan\nHBase(main):011:0> put 'user', 'rk0001', 'info:name', 'zhangsan'\n\n向user表中插入信息，row key为rk0001，列族info中添加名为gender的列，值为female\nHBase(main):012:0> put 'user', 'rk0001', 'info:gender', 'female'\n\n向user表中插入信息，row key为rk0001，列族info中添加名为age的列，值为20\nHBase(main):013:0> put 'user', 'rk0001', 'info:age', 20\n\n向user表中插入信息，row key为rk0001，列族data中添加名为pic的列，值为picture\nHBase(main):014:0> put 'user', 'rk0001', 'data:pic', 'picture'\n\n```\n\n### 5.6 查询数据操作\n\n#### 5.6.1 通过rowkey进行查询\n\n- 获取user表中row key为rk0001的所有信息（即所有cell的数据）\n- 使用get命令\n\n```\nHBase(main):015:0> get 'user', 'rk0001'\n\n```\n\n#### 5.6.2 查看rowkey下某个列族的信息\n\n- 获取user表中row key为rk0001，info列族的所有信息\n\n```\nHBase(main):016:0> get 'user', 'rk0001', 'info'\n\n```\n\n#### 5.6.3 查看rowkey指定列族指定字段的值\n\n- 获取user表中row key为rk0001，info列族的name、age列的信息\n\n```shell\nHBase(main):017:0> get 'user', 'rk0001', 'info:name', 'info:age'\n\n```\n\n![](assets/Image201911080715.png)\n\n#### 5.6.4 查看rowkey指定多个列族的信息\n\n- 获取user表中row key为rk0001，info、data列族的信息\n\n```shell\nHBase(main):018:0> get 'user', 'rk0001', 'info', 'data'\n\n或者你也可以这样写\nHBase(main):019:0> get 'user', 'rk0001', {COLUMN => ['info', 'data']}\n\n或者你也可以这样写，也行\nHBase(main):020:0> get 'user', 'rk0001', {COLUMN => ['info:name', 'data:pic']}\n\n```\n\n#### 5.6.5 指定rowkey与列值过滤器查询\n\n- 获取user表中row key为rk0001，cell的值为zhangsan的信息\n\n```shell\nHBase(main):030:0> get 'user', 'rk0001', {FILTER => \"ValueFilter(=, 'binary:zhangsan')\"}\n\n```\n\n#### 5.6.6 指定rowkey与列值模糊查询\n\n- 获取user表中row key为rk0001，列标示符中含有a的信息\n\n```shell\nHBase(main):031:0> get 'user', 'rk0001', {FILTER => \"(QualifierFilter(=,'substring:a'))\"}\n\n继续插入一批数据\n\nHBase(main):032:0> put 'user', 'rk0002', 'info:name', 'fanbingbing'\nHBase(main):033:0> put 'user', 'rk0002', 'info:gender', 'female'\nHBase(main):034:0> put 'user', 'rk0002', 'info:nationality', '中国'\nHBase(main):035:0> get 'user', 'rk0002', {FILTER => \"ValueFilter(=, 'binary:中国')\"}\n\n```\n\n#### 5.6.7 查询所有行的数据\n\n- 查询user表中的所有信息\n- 使用scan命令\n\n```\nHBase(main):032:0>  scan 'user'\n\n```\n\n#### 5.6.8 列族查询\n\n- 查询user表中列族为info的信息\n\n```\nscan 'user', {COLUMNS => 'info'}\n\nscan 'user', {COLUMNS => 'info', RAW => true, VERSIONS => 5}\n\nscan 'user', {COLUMNS => 'info', RAW => true, VERSIONS => 3}\n\n```\n\n#### 5.6.9 多列族查询\n\n- 查询user表中列族为info和data的信息\n\n```\nscan 'user', {COLUMNS => ['info', 'data']}\n\n```\n\n#### 5.6.10 指定列族与某个列名查询\n\n- 查询user表中列族为info、列标示符为name的信息\n\n```\nscan 'user', {COLUMNS => 'info:name'}\n\n```\n\n- 查询info:name列、data:pic列的数据\n\n```\nscan 'user', {COLUMNS => ['info:name', 'data:pic']}\n\n```\n\n- 查询user表中列族为info、列标示符为name的信息,并且版本最新的5个\n\n```\nscan 'user', {COLUMNS => 'info:name', VERSIONS => 5}\n\n```\n\n#### 5.6.11 指定多个列族与按照数据值模糊查询\n\n- 查询user表中列族为info和data且列标示符中含有a字符的信息\n\n```\nscan 'user', {COLUMNS => ['info', 'data'], FILTER => \"(QualifierFilter(=,'substring:a'))\"}\n\n```\n\n#### 5.6.12 指定rowkey的范围查询\n\n- 查询user表中列族为info，rk范围是[rk0001, rk0003)的数据\n\n```\nscan 'user', {COLUMNS => 'info', STARTROW => 'rk0001', ENDROW => 'rk0003'}\n\n```\n\n#### 5.6.13 指定rowkey模糊查询\n\n- 查询user表中row key以rk字符开头的数据\n\n```\nscan 'user',{FILTER=>\"PrefixFilter('rk')\"}\n\n```\n\n#### 5.6.14 指定数据版本的范围查询\n\n- 查询user表中指定范围的数据（前闭后开）\n\n```\nscan 'user', {TIMERANGE => [1392368783980, 1392380169184]}\n\n```\n\n\n\n### 5.7 更新数据操作\n\n#### 5.7.1 更新数据值\n\n- 更新操作同插入操作一模一样，只不过有数据就更新，没数据就添加\n- 使用put命令\n\n#### 5.7.2 更新版本号\n\n- 将user表的f1列族版本数改为5\n\n```\nHBase(main):050:0> alter 'user', NAME => 'info', VERSIONS => 5\n\n```\n\n### 5.8 删除数据以及删除表操作\n\n#### 5.8.1 指定rowkey以及列名进行删除\n\n- 删除user表row key为rk0001，列标示符为info:name的数据\n\n```\nHBase(main):045:0> delete 'user', 'rk0001', 'info:name'\n\n```\n\n#### 5.8.2 指定rowkey，列名以及版本号进行删除\n\n- 删除user表row key为rk0001，列标示符为info:name，timestamp为1392383705316的数据\n\n```\ndelete 'user', 'rk0001', 'info:name', 1392383705316\n\n```\n\n#### 5.8.3 删除一个列族\n\n- 删除一个列族：\n\n```\nalter 'user', NAME => 'info', METHOD => 'delete' \n\n或 alter 'user', 'delete' => 'info'\n\n```\n\n#### 5.8.4 清空表数据\n\n```\nHBase(main):017:0> truncate 'user'\n\n```\n\n#### 5.8.5 删除表\n\n- 首先需要先让该表为disable状态，使用命令：\n\n```\nHBase(main):049:0> disable 'user'\n\n```\n\n- 然后使用drop命令删除这个表\n\n```\n HBase(main):050:0> drop 'user'\n\n```\n\n> (注意：如果直接drop表，会报错：Drop the named table. Table must first be disabled)\n\n### 5.9 统计一张表有多少行数据\n\n```\nHBase(main):053:0> count 'user'\n\n```\n\n \n\n## 6. HBase的高级shell管理命令\n\n### 6.1 status\n\n- 例如：显示服务器状态\n\n```\nHBase(main):058:0> status 'node01'\n\n```\n\n### 6.2 whoami\n\n- 显示HBase当前用户，例如：\n\n```\nHBase> whoami\n\n```\n\n### 6.3 list\n\n- 显示当前所有的表\n\n```\nHBase >  list\n\n```\n\n### 6.4 count\n\n- 统计指定表的记录数，例如：\n\n```\nHBase> count 'user' \n\n```\n\n### 6.5 describe\n\n- 展示表结构信息\n\n```\nHBase> describe 'user'\n\n```\n\n### 6.6 exists\n\n- 检查表是否存在，适用于表量特别多的情况\n\n```\nHBase> exists 'user'\n\n```\n\n### 6.7 is_enabled、is_disabled\n\n- 检查表是否启用或禁用\n\n```\nHBase> is_enabled 'user'\nHBase> is_disabled 'user'\n\n```\n\n### 6.8 alter\n\n- 该命令可以改变表和列族的模式，例如：\n\n- **为当前表增加列族：**\n\n```\nHBase> alter 'user', NAME => 'CF2', VERSIONS => 2\n\n```\n\n- **为当前表删除列族：**\n\n```\nHBase(main):002:0>  alter 'user', 'delete' => 'CF2'\n\n```\n\n### 6.9 disable/enable\n\n- 禁用一张表/启用一张表\n\n```\nHBase> disable 'user'\nHBase> enable 'user'\n\n```\n\n### 6.10 drop\n\n- 删除一张表，记得在删除表之前必须先禁用\n\n### 6.11 truncate\n\n- 禁用表-删除表-创建表\n\n\n\n## 7. HBase的JavaAPI操作（重点）\n\n- HBase是一个分布式的NoSql数据库，在实际工作当中，我们一般都可以通过JavaAPI来进行各种数据的操作，包括创建表，以及数据的增删改查等等\n\n### 7.1 创建maven工程\n\n- 讲如下内容作为maven工程中pom.xml的repositories的内容\n- 自动导包（需要从cloudera仓库下载，耗时较长，**耐心等待**）\n\n```xml\n\t<repositories>\n        <repository>\n            <id>cloudera</id>\n            <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n        </repository>\n    </repositories>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.6.0-mr1-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.HBase</groupId>\n            <artifactId>hbase-client</artifactId>\n            <version>1.2.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.HBase</groupId>\n            <artifactId>hbase-server</artifactId>\n            <version>1.2.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.testng</groupId>\n            <artifactId>testng</artifactId>\n            <version>6.14.3</version>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                    <encoding>UTF-8</encoding>\n                </configuration>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.2</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <filters>\n                                <filter>\n                                    <artifact>*:*</artifact>\n                                    <excludes>\n                                        <exclude>META-INF/*.SF</exclude>\n                                        <exclude>META-INF/*.DSA</exclude>\n                                        <exclude>META-INF/*/RSA</exclude>\n                                    </excludes>\n                                </filter>\n                            </filters>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n\n```\n\n### 7.2 创建myuser表\n\n- 创建myuser表，此表有两个列族f1和f2\n\n```java\n\t//操作数据库  第一步：获取连接  第二步：获取客户端对象   第三步：操作数据库  第四步：关闭\n    /**\n     * 创建一张表  myuser  两个列族  f1   f2\n     */\n    @Test\n    public void createTable() throws IOException {\n        Configuration configuration = HBaseConfiguration.create();\n        //连接HBase集群不需要指定HBase主节点的ip地址和端口号\n        configuration.set(\"HBase.zookeeper.quorum\",\"node01:2181,node02:2181,node03:2181\");\n        //创建连接对象\n        Connection connection = ConnectionFactory.createConnection(configuration);\n        //获取连接对象，创建一张表\n        //获取管理员对象，来对手数据库进行DDL的操作\n        Admin admin = connection.getAdmin();\n        //指定我们的表名\n        TableName myuser = TableName.valueOf(\"myuser\");\n        HTableDescriptor hTableDescriptor = new HTableDescriptor(myuser);\n        //指定两个列族\n        HColumnDescriptor f1 = new HColumnDescriptor(\"f1\");\n        HColumnDescriptor f2 = new HColumnDescriptor(\"f2\");\n        hTableDescriptor.addFamily(f1);\n        hTableDescriptor.addFamily(f2);\n        admin.createTable(hTableDescriptor);\n        admin.close();\n        connection.close();\n\n    }\n\n```\n\n### 7.3 向表中添加数据\n\n```java\n \tprivate Connection connection ;\n    private final String TABLE_NAME = \"myuser\";\n    private Table table ;\n\n    @BeforeTest\n    public void initTable () throws IOException {\n        Configuration configuration = HBaseConfiguration.create();\n        configuration.set(\"HBase.zookeeper.quorum\",\"node01:2181,node02:2181\");\n        connection= ConnectionFactory.createConnection(configuration);\n        table = connection.getTable(TableName.valueOf(TABLE_NAME));\n    }\n\n    @AfterTest\n    public void close() throws IOException {\n        table.close();\n        connection.close();\n    }\n\n\n    /**\n     *  向myuser表当中添加数据\n     */\n    @Test\n    public void addData() throws IOException {\n        //获取表\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        Put put = new Put(\"0001\".getBytes());//创建put对象，并指定rowkey值\n        put.addColumn(\"f1\".getBytes(),\"name\".getBytes(),\"zhangsan\".getBytes());\n        put.addColumn(\"f1\".getBytes(),\"age\".getBytes(), Bytes.toBytes(18));\n        put.addColumn(\"f1\".getBytes(),\"id\".getBytes(), Bytes.toBytes(25));\n        put.addColumn(\"f1\".getBytes(),\"address\".getBytes(), Bytes.toBytes(\"地球人\"));\n        table.put(put);\n        table.close();\n    } \n\n```\n\n### 7.4 查询数据\n\n- 初始化一批数据到HBase表当中，用于查询\n\n```java\n \t@Test\n    public void insertBatchData() throws IOException {\n\n        //获取连接\n        Configuration configuration = HBaseConfiguration.create();\n        configuration.set(\"HBase.zookeeper.quorum\", \"node01:2181,node02:2181\");\n        Connection connection = ConnectionFactory.createConnection(configuration);\n        //获取表\n        Table myuser = connection.getTable(TableName.valueOf(\"myuser\"));\n        //创建put对象，并指定rowkey\n        Put put = new Put(\"0002\".getBytes());\n        put.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(1));\n        put.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"曹操\"));\n        put.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(30));\n        put.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"沛国谯县\"));\n        put.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"16888888888\"));\n        put.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"helloworld\"));\n\n        Put put2 = new Put(\"0003\".getBytes());\n        put2.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(2));\n        put2.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"刘备\"));\n        put2.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(32));\n        put2.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put2.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"幽州涿郡涿县\"));\n        put2.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"17888888888\"));\n        put2.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"talk is cheap , show me the code\"));\n\n        Put put3 = new Put(\"0004\".getBytes());\n        put3.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(3));\n        put3.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"孙权\"));\n        put3.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(35));\n        put3.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put3.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"下邳\"));\n        put3.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"12888888888\"));\n        put3.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"what are you 弄啥嘞！\"));\n\n        Put put4 = new Put(\"0005\".getBytes());\n        put4.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(4));\n        put4.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"诸葛亮\"));\n        put4.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(28));\n        put4.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put4.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"四川隆中\"));\n        put4.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"14888888888\"));\n        put4.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"出师表你背了嘛\"));\n\n        Put put5 = new Put(\"0006\".getBytes());\n        put5.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(5));\n        put5.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"司马懿\"));\n        put5.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(27));\n        put5.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put5.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"哪里人有待考究\"));\n        put5.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"15888888888\"));\n        put5.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"跟诸葛亮死掐\"));\n\n\n        Put put6 = new Put(\"0007\".getBytes());\n        put6.addColumn(\"f1\".getBytes(),\"id\".getBytes(),Bytes.toBytes(5));\n        put6.addColumn(\"f1\".getBytes(),\"name\".getBytes(),Bytes.toBytes(\"xiaobubu—吕布\"));\n        put6.addColumn(\"f1\".getBytes(),\"age\".getBytes(),Bytes.toBytes(28));\n        put6.addColumn(\"f2\".getBytes(),\"sex\".getBytes(),Bytes.toBytes(\"1\"));\n        put6.addColumn(\"f2\".getBytes(),\"address\".getBytes(),Bytes.toBytes(\"内蒙人\"));\n        put6.addColumn(\"f2\".getBytes(),\"phone\".getBytes(),Bytes.toBytes(\"15788888888\"));\n        put6.addColumn(\"f2\".getBytes(),\"say\".getBytes(),Bytes.toBytes(\"貂蝉去哪了\"));\n\n        List<Put> listPut = new ArrayList<Put>();\n        listPut.add(put);\n        listPut.add(put2);\n        listPut.add(put3);\n        listPut.add(put4);\n        listPut.add(put5);\n        listPut.add(put6);\n\n        myuser.put(listPut);\n        myuser.close();\n        connection.close();\n    }\n\n```\n\n#### 7.4.1 Get查询\n\n- 按照rowkey进行查询，获取所有列的所有值\n- 查询主键rowkey为0003的人\n\n```java\n\t /**\n     * 查询rowkey为0003的人\n     */\n    @Test\n    public void getData() throws IOException {\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        //通过get对象，指定rowkey\n        Get get = new Get(Bytes.toBytes(\"0003\"));\n\n        get.addFamily(\"f1\".getBytes());//限制只查询f1列族下面所有列的值\n        //查询f2  列族 phone  这个字段\n        get.addColumn(\"f2\".getBytes(),\"phone\".getBytes());\n        //通过get查询，返回一个result对象，所有的字段的数据都是封装在result里面了\n        Result result = table.get(get);\n        List<Cell> cells = result.listCells();  //获取一条数据所有的cell，所有数据值都是在cell里面 的\n        for (Cell cell : cells) {\n            byte[] family_name = CellUtil.cloneFamily(cell);//获取列族名\n            byte[] column_name = CellUtil.cloneQualifier(cell);//获取列名\n            byte[] rowkey = CellUtil.cloneRow(cell);//获取rowkey\n            byte[] cell_value = CellUtil.cloneValue(cell);//获取cell值\n            //需要判断字段的数据类型，使用对应的转换的方法，才能够获取到值\n            if(\"age\".equals(Bytes.toString(column_name))  || \"id\".equals(Bytes.toString(column_name))){\n                System.out.println(Bytes.toString(family_name));\n                System.out.println(Bytes.toString(column_name));\n                System.out.println(Bytes.toString(rowkey));\n                System.out.println(Bytes.toInt(cell_value));\n            }else{\n                System.out.println(Bytes.toString(family_name));\n                System.out.println(Bytes.toString(column_name));\n                System.out.println(Bytes.toString(rowkey));\n                System.out.println(Bytes.toString(cell_value));\n            }\n        }\n        table.close();\n    }\n```\n\n#### 7.4.2 Scan查询\n\n```java\n\t/**\n     * 不知道rowkey的具体值，我想查询rowkey范围值是0003  到0006\n     * select * from myuser  where age > 30  and id < 8  and name like 'zhangsan'\n     *\n     */\n    @Test\n    public void scanData() throws IOException {\n        //获取table\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        Scan scan = new Scan();//没有指定startRow以及stopRow  全表扫描\n        //只扫描f1列族\n        scan.addFamily(\"f1\".getBytes());\n        //扫描 f2列族 phone  这个字段\n        scan.addColumn(\"f2\".getBytes(),\"phone\".getBytes());\n        scan.setStartRow(\"0003\".getBytes());\n        scan.setStopRow(\"0007\".getBytes());\n        //通过getScanner查询获取到了表里面所有的数据，是多条数据\n        ResultScanner scanner = table.getScanner(scan);\n        //遍历ResultScanner 得到每一条数据，每一条数据都是封装在result对象里面了\n        for (Result result : scanner) {\n            List<Cell> cells = result.listCells();\n            for (Cell cell : cells) {\n                byte[] family_name = CellUtil.cloneFamily(cell);\n                byte[] qualifier_name = CellUtil.cloneQualifier(cell);\n                byte[] rowkey = CellUtil.cloneRow(cell);\n                byte[] value = CellUtil.cloneValue(cell);\n                //判断id和age字段，这两个字段是整形值\n                if(\"age\".equals(Bytes.toString(qualifier_name))  || \"id\".equals(Bytes.toString(qualifier_name))){\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toInt(value));\n                }else{\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toString(value));\n                }\n            }\n        }\n        table.close();\n    }\n```\n\n### 7.5 HBase过滤器查询\n\n#### 7.5.1 过滤器\n\n- 过滤器的作用是在服务端判断数据是否满足条件，然后只将满足条件的数据返回给客户端\n\n- 过滤器的类型很多，但是可以分为两大类\n  - ==比较过滤器==\n  - ==专用过滤器==\n\n- HBase过滤器的**比较运算符**：\n\n```\nLESS  <\nLESS_OR_EQUAL <=\nEQUAL =\nNOT_EQUAL <>\nGREATER_OR_EQUAL >=\nGREATER >\nNO_OP 排除所有\n```\n\n- Hbase过滤器的**比较器**（指定比较机制）：\n\n```\nBinaryComparator  按字节索引顺序比较指定字节数组，采用Bytes.compareTo(byte[])\nBinaryPrefixComparator 跟前面相同，只是比较左端的数据是否相同\nNullComparator 判断给定的是否为空\nBitComparator 按位比较\nRegexStringComparator 提供一个正则的比较器，仅支持 EQUAL 和非EQUAL\nSubstringComparator 判断提供的子串是否出现在中。\n```\n\n#### 7.5.2 比较过滤器使用\n\n##### 1、rowKey过滤器RowFilter\n\n- 通过RowFilter过滤比rowKey  0003小的所有值出来\n\n```java\n\t/**\n     * 查询所有的rowkey比0003小的所有的数据\n     */\n    @Test\n    public void rowFilter() throws IOException {\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        Scan scan = new Scan();\n        //获取我们比较对象\n        BinaryComparator binaryComparator = new BinaryComparator(\"0003\".getBytes());\n        /***\n         * rowFilter需要加上两个参数\n         * 第一个参数就是我们的比较规则\n         * 第二个参数就是我们的比较对象\n         */\n        RowFilter rowFilter = new RowFilter(CompareFilter.CompareOp.GREATER, binaryComparator);\n        //为我们的scan对象设置过滤器\n        scan.setFilter(rowFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        for (Result result : scanner) {\n            List<Cell> cells = result.listCells();\n            for (Cell cell : cells) {\n                byte[] family_name = CellUtil.cloneFamily(cell);\n                byte[] qualifier_name = CellUtil.cloneQualifier(cell);\n                byte[] rowkey = CellUtil.cloneRow(cell);\n                byte[] value = CellUtil.cloneValue(cell);\n                //判断id和age字段，这两个字段是整形值\n                if(\"age\".equals(Bytes.toString(qualifier_name))  || \"id\".equals(Bytes.toString(qualifier_name))){\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toInt(value));\n                }else{\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toString(value));\n                }\n            }\n        }\n    }\n```\n\n##### 2、列族过滤器FamilyFilter\n\n- 查询列族名包含f2的所有列族下面的数据\n\n```JAVA\n\t/**\n     * 通过familyFilter来实现列族的过滤\n     * 需要过滤，列族名包含f2\n     * f1  f2   hello   world\n     */\n    @Test\n    public void familyFilter() throws IOException {\n        Table table = connection.getTable(TableName.valueOf(TABLE_NAME));\n        Scan scan = new Scan();\n        SubstringComparator substringComparator = new SubstringComparator(\"f2\");\n        //通过familyfilter来设置列族的过滤器\n        FamilyFilter familyFilter = new FamilyFilter(CompareFilter.CompareOp.EQUAL, substringComparator);\n        scan.setFilter(familyFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        for (Result result : scanner) {\n            List<Cell> cells = result.listCells();\n            for (Cell cell : cells) {\n                byte[] family_name = CellUtil.cloneFamily(cell);\n                byte[] qualifier_name = CellUtil.cloneQualifier(cell);\n                byte[] rowkey = CellUtil.cloneRow(cell);\n                byte[] value = CellUtil.cloneValue(cell);\n                //判断id和age字段，这两个字段是整形值\n                if(\"age\".equals(Bytes.toString(qualifier_name))  || \"id\".equals(Bytes.toString(qualifier_name))){\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toInt(value));\n                }else{\n                    System.out.println(\"数据的rowkey为\" +  Bytes.toString(rowkey)   +\"======数据的列族为\" +  Bytes.toString(family_name)+\"======数据的列名为\" +  Bytes.toString(qualifier_name) + \"==========数据的值为\" +Bytes.toString(value));\n                }\n            }\n        }\n    }\n```\n\n##### 3、列过滤器QualifierFilter\n\n- 只查询name列的值\n\n```java\n/**\n     * 列名过滤器 只查询包含name列的值\n     */\n    @Test\n    public void  qualifierFilter() throws IOException {\n        Scan scan = new Scan();\n        SubstringComparator substringComparator = new SubstringComparator(\"name\");\n        //定义列名过滤器，只查询列名包含name的列\n        QualifierFilter qualifierFilter = new QualifierFilter(CompareFilter.CompareOp.EQUAL, substringComparator);\n        scan.setFilter(qualifierFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        printlReult(scanner);\n    }\n```\n\n##### 4、列值过滤器ValueFilter\n\n- 查询所有列当中包含8的数据\n\n```java\n\t/**\n     * 查询哪些字段值  包含数字8\n     */\n    @Test\n    public void contains8() throws IOException {\n        Scan scan = new Scan();\n        SubstringComparator substringComparator = new SubstringComparator(\"8\");\n        //列值过滤器，过滤列值当中包含数字8的所有的列\n        ValueFilter valueFilter = new ValueFilter(CompareFilter.CompareOp.EQUAL, substringComparator);\n        scan.setFilter(valueFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        printlReult(scanner);\n    }\n```\n\n#### 7.5.3 专用过滤器使用\n\n##### 1、单列值过滤器 SingleColumnValueFilter\n\n- SingleColumnValueFilter会返回满足条件的整列值的所有字段\n\n- 查询名字为刘备的数据\n\n```java\n\t/**\n     * select  *  from  myuser where name  = '刘备'\n     * 会返回我们符合条件数据的所有的字段\n     *\n     * SingleColumnValueExcludeFilter  列值排除过滤器\n     *  select  *  from  myuser where name  ！= '刘备'\n     */\n    @Test\n    public void singleColumnValueFilter() throws IOException {\n        //查询 f1  列族 name  列  值为刘备的数据\n        Scan scan = new Scan();\n        //单列值过滤器，过滤  f1 列族  name  列  值为刘备的数据\n        SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter(\"f1\".getBytes(), \"name\".getBytes(), CompareFilter.CompareOp.EQUAL, \"刘备\".getBytes());\n        scan.setFilter(singleColumnValueFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        printlReult(scanner);\n    }\n```\n\n##### 2、列值排除过滤器SingleColumnValueExcludeFilter\n\n- 与SingleColumnValueFilter相反，会排除掉指定的列，其他的列全部返回\n\n##### 3、rowkey前缀过滤器PrefixFilter\n\n- 查询以00开头的所有前缀的rowkey\n\n```java\n\t/**\n     * 查询rowkey前缀以  00开头的所有的数据\n     */\n    @Test\n    public  void  prefixFilter() throws IOException {\n        Scan scan = new Scan();\n        //过滤rowkey以  00开头的数据\n        PrefixFilter prefixFilter = new PrefixFilter(\"00\".getBytes());\n        scan.setFilter(prefixFilter);\n        ResultScanner scanner = table.getScanner(scan);\n        printlReult(scanner);\n    }\n```\n\n##### 4、分页过滤器PageFilter（15分钟）\n\n- 通过pageFilter实现分页过滤器\n\n```java\n\t/**\n     * HBase当中的分页\n     */\n    @Test\n    public void hbasePageFilter() throws IOException {\n        int pageNum= 3;\n        int pageSize = 2;\n        Scan scan = new Scan();\n        if(pageNum == 1 ){\n            //获取第一页的数据\n            scan.setMaxResultSize(pageSize);\n            scan.setStartRow(\"\".getBytes());\n            //使用分页过滤器来实现数据的分页\n            PageFilter filter = new PageFilter(pageSize);\n            scan.setFilter(filter);\n            ResultScanner scanner = table.getScanner(scan);\n            printlReult(scanner);\n        }else{\n            String  startRow = \"\";\n            //扫描数据的调试 扫描五条数据\n            int scanDatas = (pageNum - 1) * pageSize + 1;\n            scan.setMaxResultSize(scanDatas);//设置一步往前扫描多少条数据\n            PageFilter filter = new PageFilter(scanDatas);\n            scan.setFilter(filter);\n            ResultScanner scanner = table.getScanner(scan);\n            for (Result result : scanner) {\n                byte[] row = result.getRow();//获取rowkey\n                //最后一次startRow的值就是0005\n                startRow= Bytes.toString(row);//循环遍历我们多有获取到的数据的rowkey\n                //最后一条数据的rowkey就是我们需要的起始的rowkey\n            }\n            //获取第三页的数据\n            scan.setStartRow(startRow.getBytes());\n            scan.setMaxResultSize(pageSize);//设置我们扫描多少条数据\n            PageFilter filter1 = new PageFilter(pageSize);\n            scan.setFilter(filter1);\n            ResultScanner scanner1 = table.getScanner(scan);\n            printlReult(scanner1);\n        }\n    }\n```\n\n#### 3、多过滤器综合查询FilterList\n\n- 需求：使用SingleColumnValueFilter查询f1列族，name为刘备的数据，并且同时满足rowkey的前缀以00开头的数据（PrefixFilter）\n\n```java\n\t/**\n     * 查询  f1 列族  name  为刘备数据值\n     * 并且rowkey 前缀以  00开头数据\n     */\n    @Test\n    public  void filterList() throws IOException {\n        Scan scan = new Scan();\n        SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter(\"f1\".getBytes(), \"name\".getBytes(), CompareFilter.CompareOp.EQUAL, \"刘备\".getBytes());\n        PrefixFilter prefixFilter = new PrefixFilter(\"00\".getBytes());\n        FilterList filterList = new FilterList();\n        filterList.addFilter(singleColumnValueFilter);\n        filterList.addFilter(prefixFilter);\n        scan.setFilter(filterList);\n        ResultScanner scanner = table.getScanner(scan);\n        printlReult(scanner);\n    }\n```\n\n### 7.6 HBase的删除操作\n\n#### 1、根据rowkey删除数据\n\n- 删除rowkey为003的数据\n\n```java\n\t/**\n     * 删除数据\n     */\n    @Test\n    public  void  deleteData() throws IOException {\n        Delete delete = new Delete(\"0003\".getBytes());\n        table.delete(delete);\n    }\n```\n\n#### 2、删除表操作\n\n```java\n \t/**\n     * 删除表\n     */\n    @Test\n    public void deleteTable() throws IOException {\n        //获取管理员对象，用于表的删除\n        Admin admin = connection.getAdmin();\n        //删除一张表之前，需要先禁用表\n        admin.disableTable(TableName.valueOf(TABLE_NAME));\n        admin.deleteTable(TableName.valueOf(TABLE_NAME));\n    }\n```\n\n\n\n# 五、拓展点、未来计划、行业趋势\n\n### 8. Hbase在实际场景中的应用\n\n##### 8.1 交通方面\n\n- 船舶GPS信息，全长江的船舶GPS信息，每天有1千万左右的数据存储。\n\n##### 8.2 金融方面\n\n- 消费信息、贷款信息、信用卡还款信息等\n\n\n##### 8.3 电商方面\n\n- 电商网站的交易信息、物流信息、游览信息等\n\n##### 8.4 电信方面\n\n- 通话信息、语音详单等\n\n==**总结：海量明细数据的存储，并且后期需要有很好的查询性能**==","tags":["hadoop","hbase"]},{"title":"Hadoop数据分析之Hive（四）","url":"/2019/11/11/it/hive/Hadoop数据分析之Hive（四）/","content":"\n## 数据仓库分析及hive中数据倾斜介绍\n\n## 一、数据仓库\n\n### 1. 数据仓库基本介绍\n\n英文名称为==Data Warehouse==，可简写为DW或DWH。数据仓库的目的是==构建面向分析的集成化数据环境==，为企业提供==决策支持==（Decision Support）。它出于分析性报告和决策支持目的而创建。\n\n数据仓库本身并不“生产”任何数据，同时自身也不需要“消费”任何的数据，数据来源于外部，并且开放给外部应用，这也是为什么叫“仓库”，而不叫“工厂”的原因。\n\n\n\n\n\n### 2. 数据仓库的定义\n\n数据仓库是==面向主题的==（Subject-Oriented ）、==集成的==（Integrated）、==稳定性的==（Non-Volatile）和==时变的==（Time-Variant ）数据集合，用以支持管理决策。 \n\n\n\n#### 2.1、面向主题\n\n数据仓库中的数据是按照一定的主题域进行组织。\n\n主题是一个抽象的概念，是指用户使用数据仓库进行决策时所关心的重点方面，一个主题通常与多个操作型信息系统相关。\n\n> 以电商为例：\n>\n> 用户主题：主要是用于分析用户的行为\n>\n> 商品主题：针对商品进行分析    指标：昨日新增商品，昨日下架商品 最近七天流量最高的哪些商品\n>\n> 财务主题：财务分析\n>\n> 订单主题：订单分析\n>\n> 货运主题：针对快递分析\n\n​\t\n\n#### 2.2、集成性\n\n根据决策分析的要求，将分散于各处的源数据进行抽取、筛选、清理、综合等工作，最终集成到数据仓库中。\n\n![](img/2019-08-17_17-04-09.png)\n\n\n\n#### 2.3、稳定性\n\n数据的相对稳定性，数据仓库中的数据只进行新增，没有更新操作、删除操作处理。\n\n反映历史变化，以查询分析为主。\n\n\n\n#### 2.4、时变性\n\n数据仓库的数据一般都带有时间属性，随着时间的推移而发生变化，不断地生成主题的新快照\n\n![](img/2019-08-17_17-09-51.png)\n\n\n\n### 3. 数据仓库与数据库的区别\n\n数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别。\n\n**==OLTP==**： On-Line Transaction Processing  叫==联机事务处理==， 也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理 .\n\n **==OLAP==**：On-Line Analytical Processing  叫==联机分析处理==，一般针对某些主题的历史数据进行分析，支持管理决策。\n\n简而言之，==数据库是面向事务的设计，数据仓库是面向主题设计的==。 \n\n数据库一般存储在线交易数据，有很高的事务要求；数据仓库存储的一般是历史数据。 \n\n数据库设计是尽量避免冗余，一般采用符合范式的规则来设计，数据仓库在设计是有意引入冗余，采用反范式的方式来设计。 \n\n数据库是==为捕获数据而设计==，数据仓库是==为分析数据而设计==，它的两个基本的元素是维表和事实表。维是看问题的角度，比如时间，部门，维表放的就是这些东西的定义，事实表里放着要查询的数据，同时有维的ID。\n\n\n\n| **功能** | **数据仓库**                           | **数据库**                             |\n| -------- | -------------------------------------- | -------------------------------------- |\n| 数据范围 | 存储历史的、完整的、反应历史变化的     | 当前状态数据                           |\n| 数据变化 | 可添加、无删除、无变更的、反应历史变化 | 支持频繁的增、删、改、查操作           |\n| 应用场景 | 面向分析、支持战略决策                 | 面向业务交易流程                       |\n| 设计理论 | 违范式、适当冗余                       | 遵照范式(第一、二、三等范式)、避免冗余 |\n| 处理量   | 非频繁、大批量、高吞吐、有延迟         | 频繁、小批次、高并发、低延迟           |\n\n\n\n### . 构建数据仓库常用手段\n\n• 传统数仓建设更多的基于成熟的商业数据集成平台，比如Teradata、Oracle、Informatica等，技术体系比较成熟完善，但相对比较封闭，对实施者技术面要求也相对专业且单一，一般更多应用于银行、保险、电信等“有钱”行业.\n\n• 基于大数据的数仓建设一般是基于非商业、开源的技术，常见的是基于hadoop生态构建，涉及技术较广泛、复杂，同时相对于商业产品，稳定性、服务支撑较弱，需要自己维护更多的技术框架。在大数据领域，==常用的数据仓库构建手段很多基于hive，sparkSQL，impala等各种技术框架==.\n\n\n\n### 5. 数据仓库分层\n\n\n\n#### 5.1 数据仓库分层描述\n\n* 数据仓库更多代表的是一种对数据的管理和使用的方式，它是一整套包括了etl、调度、建模在内的完整的理论体系。现在所谓的大数据更多的是一种数据量级的增大和工具的上的更新。 两者并无冲突，相反，而是一种更好的结合。数据仓库在构建过程中通常都需要进行分层处理。业务不同，分层的技术处理手段也不同。\n* 分层是数据仓库解决方案中，数据架构设计的一种数据逻辑结构 ，通过分层理念建立的数据仓库，它的可扩展性非常好，这样设计出来的模型架构，可以任意地增减、替换数据仓库中的各个组成部分。\n\n![](img/数据仓库层.png)\n\n\n\n~~~\n从整体的逻辑划分来讲，数据仓库模型实际上就是这三层架构。\n\n接入层：底层的数据源或者是操作数据层，一般在公司的话，统一都是称为ODS层\n\n中间层：是做数据仓库同学需要花费更多精力的一层，这一层包括的内容是最多的、最复杂的。\n\n应用层：对不同的应用提供对应的数据。该层主要是提供数据产品和数据分析使用的数据，\n\t   比如我们经常说的报表数据\n\t\n~~~\n\n\n\n* 针对于这三层架构，这里给出比较典型的一个做数据仓库在实施的时候，具体的层次划分。\n\n![dw](img/dw.png)\n\n\n\n* ==ODS==：\n\n  * Operation Data Store 原始数据层\n\n* ==DWD==\n\n  * data warehouse detail 数据明细层\n\n  * 它主要是针对于接入层的数据进行数据的清洗和转换。还有就是一些维度的补充。\n\n* ==DWS==\n  * data warehouse summary 数据汇总层\n\n  * 它是在DWD明细层之上，也有公司叫DW层\n\n  * 它是按照一定的粒度进行了汇总聚合操作。它是单业务场景。\n\n* ==DWM==\n\n  * data warehouse market 数据集市层\n  * 它是在DWS数据汇总层之上，集市层它是多业务场景的。\n\n* ==APP==\n\n  - Application 应用层\n  - 这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用\n\n~~~\nTMP临时表：在做一些中间层表计算的时候，大量使用tmp临时表。\nDIM维度层：基于ODS层和DWD层抽象出一些公共的维度，\n\t\t  典型的公共维度主要包括城市信息、渠道信息、个人基础属性信息。\n~~~\n\n\n\n#### 5.2 为什么要进行数据仓库分层\n\n* 分层的主要原因是在管理数据的时候，能对数据有一个更加清晰的掌控，主要有下面几个原因：\n  * **空间换时间**\n    * 通过建设多层次的数据模型供用户使用，避免用户直接使用底层操作型数据，可以更高效的访问数据。\n  * **把复杂问题简单化**\n    * 讲一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。\n  * **便于处理业务的变化**\n    * 随着业务的变化，只需要调整底层的数据，对应用层对业务的调整零感知。\n\n\n\n### 6. 数据仓库建模\n\n​\t目前业界较为流行的数据仓库的建模方法非常多，这里主要介绍==范式建模法==，==维度建模法==，==实体建模法==等几种方法，每种方法其实从本质上讲就是从不同的角度看我们业务中的问题，不管从技术层面还是业务层面，其实代表的是哲学上的一种世界观。\n\n\n\n#### 6.1 范式建模法（Third Normal Form 3NF）\n\n~~~\n范式建模法是基于整个关系型数据库的理论基础之上发展而来的，其实是我们在构建数据模型常用的一个方法，主要解决关系型数据库得数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。\n\n从其表达的含义来看，一个符合第三范式的关系必须具有以下三个条件 :\n\n（1）每个属性值唯一，不具有多义性 ;\n（2）每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;\n（3）每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。\n\n~~~\n\n#### 6.2 维度建模法\n\n~~~\n维度建模(dimensional modeling)是专门用于分析型数据库、数据仓库、数据集市建模的方法。维度建模法简单描述就是按照事实表、维度表来构建数仓、集市。\n维度建模从分析决策的需求出发构建模型，为分析需求服务，因此它重点关注用户如何更快速地完成需求分析，同时具有较好的大规模复杂查询的相应性能。\n\n~~~\n\n* 维度表\n\n~~~\n维度表示你要对数据进行分析时所用的一个量,比如你要分析产品销售情况, \n你可以选择按类别来进行分析,或按区域来分析。\n\n通常来说维度表信息比较固定，且数据量小\n~~~\n\n- 事实表\n\n```\n表示对分析主题的度量。\n事实表包含了与各维度表相关联的外键，并通过join方式与维度表关联。事实表的度量通常是数值类型，且记录数会不断增加，表规模迅速增长。\n\n消费事实表：Prod_id(引用商品维度表), TimeKey(引用时间维度表), Place_id(引用地点维度表), Unit(销售量)。\n```\n\n~~~\n总的说来，在数据仓库中不需要严格遵守规范化设计原则。因为数据仓库的主导功能就是面向分析，以查询为主，不涉及数据更新操作。事实表的设计是以能够正确记录历史信息为准则，维度表的设计是以能够以合适的角度来聚合主题内容为准则\n~~~\n\n\n\n##### 6.2.1 维度建模三种模式\n\n基于事实表和维表就可以构建出多种多维模型，包括星形模型、雪花模型和星座模型。\n\n维度建模法最被人广泛知晓的名字就是星型模式。\n\n\n\n* ==星型模式==\n\n  ~~~\n  星形模式(Star Schema)是最常用的维度建模方式。星型模式是以事实表为\n  中心，所有的维度表直接连接在事实表上，像星星一样。\n  星形模式的维度建模由一个事实表和一组维表成，且具有以下特点：\n  a. 维表只和事实表关联，维表之间没有关联；\n  b. 每个维表主键为单列，且该主键放置在事实表中，作为两边连接的外键；\n  c. 以事实表为核心，维表围绕核心呈星形分布；\n  \n  ~~~\n\n  ![星型模型](img/星型模型.png)\n\n\n\n* ==雪花模式==\n\n~~~\n雪花模式是对星形模式的扩展。雪花模式的维度表可以拥有其他维度表的，虽然这种模型相比星型更规范一些，但是由于这种模型不太容易理解，维护成本比较高，而且性能方面需要关联多层维表，性能也比星型模型要低。所以一般不是很常用。\n~~~\n\n![雪花模型](img/雪花模型.png)\n\n\n\n* ==星座模式==\n\n~~~\n星座模式是星型模式延伸而来，星型模式是基于一张事实表的，而星座模式是基于多张事实表的，而且共享维度信息。\n\n前面介绍的两种维度建模方法都是多维表对应单事实表，但在很多时候维度空间内的事实表不止一个，而一个维表也可能被多个事实表用到。在业务发展后期，绝大部分维度建模都采用的是星座模式。\n~~~\n\n![星座模型](img/星座模型.png)\n\n\n\n#### 6.3 实体建模法\n\n~~~\n实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。\n\n从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。\n\n那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。\n~~~\n\n参考文档：<http://www.uml.org.cn/sjjmck/201810163.asp>\n\n\n\n\n\n### 7、数据仓库架构\n\n\n\n![](img/数据仓库架构图.png)\n\n* ==数据采集==\n\n~~~\n数据采集层的任务就是把数据从各种数据源中采集和存储到数据存储上，期间有可能会做一些ETL操作。\n\n数据源种类可以有多种：\n日志：所占份额最大，存储在备份服务器上\n业务数据库：如Mysql、Oracle\n来自HTTP/FTP的数据：合作伙伴提供的接口\n其他数据源：如Excel等需要手工录入的数据\n~~~\n\n\n\n* ==数据存储与分析==\n\n~~~\nHDFS是大数据环境下数据仓库/数据平台最完美的数据存储解决方案。\n\n离线数据分析与计算，也就是对实时性要求不高的部分，Hive是不错的选择。\n使用Hadoop框架自然而然也提供了MapReduce接口，如果真的很乐意开发Java，或者对SQL不熟，那么也可以使用MapReduce来做分析与计算。\nSpark性能比MapReduce好很多，同时使用SparkSQL操作Hive。\n~~~\n\n\n\n* ==数据共享==\n\n~~~\n　　前面使用Hive、MR、Spark、SparkSQL分析和计算的结果，还是在HDFS上，但大多业务和应用不可能直接从HDFS上获取数据，那么就需要一个数据共享的地方，使得各业务和产品能方便的获取数据。\n\n　　这里的数据共享，其实指的是前面数据分析与计算后的结果存放的地方，其实就是关系型数据库和NOSQL数据库。\n~~~\n\n\n\n* ==数据应用==\n\n~~~\n报表：报表所使用的数据，一般也是已经统计汇总好的，存放于数据共享层。\n接口：接口的数据都是直接查询数据共享层即可得到。\n即席查询：即席查询通常是现有的报表和数据共享层的数据并不能满足需求，需要从数据存储层直接查询。一般都是通过直接操作SQL得到。\n~~~\n\n## 二、Hive中数据倾斜\n\n### 1、什么是数据倾斜\n\n~~~\n由于数据分布不均匀，造成数据大量的集中到一点，造成数据热点\n~~~\n\n\n\n### 2、数据倾斜的现象\n\n~~~\n在执行任务的时候，任务进度长时间维持在99%左右，查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。\n\n单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。最长时长远大于平均时长。\n~~~\n\n### 3、数据倾斜的情况\n\n![450330742](img/450330742.png)\n\n### 4、数据倾斜的原因\n\n~~~\n1)、key分布不均匀\n\n2)、业务数据本身的特性\n\n3)、建表时考虑不周\n\n4)、某些SQL语句本身就有数据倾斜\n~~~\n\n\n\n### 5、数据倾斜的解决方案\n\n#### 5.1 map端聚合\n\n~~~sql\n--Map 端部分聚合，相当于Combiner\nhive.map.aggr = true；\n--有数据倾斜的时候进行负载均衡\nhive.groupby.skewindata=true；\n\n--有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。\n~~~\n\n#### 5.2 SQL语句调节\n\n* 如何Join\n\n  ~~~\n  关于驱动表的取，用join key分布最均匀的表作为驱动表\n  做好列裁剪和filter操作，以达到两表做join的时候，数据量相对变小的效果。\n  ~~~\n\n* 大小表Join\n\n  ~~~\n  使用map join让小的维度表（1000条以下的记录条数） 先进内存。在map端完成reduce.\n  ~~~\n\n* 大表Join大表\n\n  ~~~\n  把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。\n  ~~~\n\n* count distinct大量相同特殊值\n\n  ~~~\n  count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。\n  ~~~\n\n* group by维度过小\n\n  ~~~\n  采用sum() group by的方式来替换count(distinct)完成计算。\n  ~~~\n\n* 特殊情况特殊处理\n\n  ~~~\n  在业务逻辑优化效果的不大情况下，一些时候是可以将倾斜的数据单独拿出来处理。最后union回去\n  ~~~\n\n#### 5.3 典型的业务场景\n\n* 空值产生的数据倾斜\n\n  * 场景\n\n    ~~~\n    如日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和 用户表中的user_id 关联，会碰到数据倾斜的问题。\n    ~~~\n\n  * 解决办法\n\n  ~~~sql\n  --user_id为空的不参与关联\n  \n  select * from log a\n    join users b\n    on a.user_id is not null\n    and a.user_id = b.user_id\n  union all\n  select * from log a\n    where a.user_id is null;\n    \n    \n    \n  --赋与空值分新的key值\n  select *\n    from log a\n    left outer join users b\n    on case when a.user_id is null then concat(‘hive’,rand()) else a.user_id end = b.user_id;\n    \n  ~~~\n\n* 不同数据类型关联产生数据倾斜\n\n  * 场景\n\n  ~~~\n  用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。\n  ~~~\n\n  * 解决办法\n\n    * 把数字类型转换成字符串类型\n\n    ~~~sql\n    select * from users a\n      left outer join logs b\n      on a.usr_id = cast(b.user_id as string);\n     \n    ~~~\n\n","tags":["hive","数据仓库","数据倾斜"]},{"title":"hive企业综合案例实战","url":"/2019/11/08/it/project/hive企业综合案例实战/","content":"\n# hive的综合案例实战\n\n## 1、需求描述\n\n统计youtube影音视频网站的常规指标，各种TopN指标：\n\n--统计视频观看数Top10\n\n--统计视频类别热度Top10\n\n--统计视频观看数Top20所属类别\n\n--统计视频观看数Top50所关联视频的所属类别Rank\n\n--统计每个类别中的视频热度Top10\n\n--统计每个类别中视频流量Top10\n\n--统计上传视频最多的用户Top10以及他们上传的视频\n\n--统计每个类别视频观看数Top10\n\n## 2、项目表字段\n\n### 1、数据结构\n\n1．视频表\n\n| 字段          | 备注       | 详细描述               |\n| ------------- | ---------- | ---------------------- |\n| video id      | 视频唯一id | 11位字符串             |\n| uploader      | 视频上传者 | 上传视频的用户名String |\n| age           | 视频年龄   | 视频在平台上的整数天   |\n| category      | 视频类别   | 上传视频指定的视频分类 |\n| length        | 视频长度   | 整形数字标识的视频长度 |\n| views         | 观看次数   | 视频被浏览的次数       |\n| rate          | 视频评分   | 满分5分                |\n| ratings       | 流量       | 视频的流量，整型数字   |\n| conments      | 评论数     | 一个视频的整数评论数   |\n| related   ids | 相关视频id | 相关视频的id，最多20个 |\n\n\n\n2．用户表\n\n| 字段     | 备注         | 字段类型 |\n| -------- | ------------ | -------- |\n| uploader | 上传者用户名 | string   |\n| videos   | 上传视频数   | int      |\n| friends  | 朋友数量     | int      |\n\n## 3、ETL原始数据清洗\n\n通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用“\\t”进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用“&”分割，同时去掉两边空格，多个相关视频id也使用“&”进行分割。\n\n三件事情\n\n1. 长度不够9的删掉\n\n2. 视频类别删掉空格\n\n3. 该相关视频的分割符\n\n创建maven工程，并导入jar包\n\n```\n<repositories>\n        <repository>\n            <id>cloudera</id>\n            <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n        </repository>\n    </repositories>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.6.0-mr1-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n            <version>2.6.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-hdfs</artifactId>\n            <version>2.6.0-cdh5.14.2</version>\n        </dependency>\n\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-mapreduce-client-core</artifactId>\n            <version>2.6.0-cdh5.14.2</version>\n        </dependency>\n        <!-- https://mvnrepository.com/artifact/junit/junit -->\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.11</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.testng</groupId>\n            <artifactId>testng</artifactId>\n            <version>RELEASE</version>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.38</version>\n            <scope>compile</scope>\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                    <encoding>UTF-8</encoding>\n                    <!--    <verbal>true</verbal>-->\n                </configuration>\n            </plugin>\n\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>2.4.3</version>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <minimizeJar>true</minimizeJar>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n            <!--  <plugin>\n                  <artifactId>maven-assembly-plugin </artifactId>\n                  <configuration>\n                      <descriptorRefs>\n                          <descriptorRef>jar-with-dependencies</descriptorRef>\n                      </descriptorRefs>\n                      <archive>\n                          <manifest>\n                              <mainClass></mainClass>\n                          </manifest>\n                      </archive>\n                  </configuration>\n                  <executions>\n                      <execution>\n                          <id>make-assembly</id>\n                          <phase>package</phase>\n                          <goals>\n                              <goal>single</goal>\n                          </goals>\n                      </execution>\n                  </executions>\n              </plugin>-->\n        </plugins>\n    </build>\n```\n\n1、代码开发：ETLUtil\n\n```\npublic class VideoUtil {\n    /**\n     * 对我们的数据进行清洗的工作，\n     * 数据切割，如果长度小于9 直接丢掉\n     * 视频类别中间空格 去掉\n     * 关联视频，使用 &  进行分割\n     * @param line\n     * @return\n     * FM1KUDE3C3k  renetto\t736\tNews & Politics\t1063\t9062\t4.57\t525\t488\tLnMvSxl0o0A&IKMtzNuKQso&Bq8ubu7WHkY&Su0VTfwia1w&0SNRfquDfZs&C72NVoPsRGw\n     */\n    public  static String washDatas(String line){\n        if(null == line || \"\".equals(line)) {\n            return null;\n        }\n        //判断数据的长度，如果小于9，直接丢掉\n        String[] split = line.split(\"\\t\");\n        if(split.length <9){\n            return null;\n        }\n        //将视频类别空格进行去掉\n        split[3] =  split[3].replace(\" \",\"\");\n        StringBuilder builder = new StringBuilder();\n        for(int i =0;i<split.length;i++){\n            if(i <9){\n                //这里面是前面八个字段\n                builder.append(split[i]).append(\"\\t\");\n            }else if(i >=9  && i < split.length -1){\n                builder.append(split[i]).append(\"&\");\n            }else if( i == split.length -1){\n                builder.append(split[i]);\n            }\n        }\n        return  builder.toString();\n    }\n}\n```\n\n\n\n2、代码开发：ETLMapper\n\n```\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport java.io.IOException;\n\npublic class VideoMapper extends Mapper<LongWritable,Text,Text,NullWritable> {\n    private Text  key2 ;\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n        key2 = new Text();\n    }\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        String s = VideoUtils.washDatas(value.toString());\n        if(null != s ){\n            key2.set(s);\n            context.write(key2,NullWritable.get());\n        }\n    }\n}\n```\n\n\n\n3、代码开发：ETLRunner\n\n```\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\npublic class VideoMain extends Configured implements Tool {\n    @Override\n    public int run(String[] args) throws Exception {\n        Job job = Job.getInstance(super.getConf(), \"washDatas\");\n        job.setJarByClass(VideoMain.class);\n        job.setInputFormatClass(TextInputFormat.class);\n        TextInputFormat.addInputPath(job,new Path(args[0]));\n\n        job.setMapperClass(VideoMapper.class);\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(NullWritable.class);\n        job.setOutputFormatClass(TextOutputFormat.class);\n        TextOutputFormat.setOutputPath(job,new Path(args[1]));\n        //注意，我们这里没有自定义reducer，会使用默认的一个reducer类\n        job.setNumReduceTasks(7);\n        boolean b = job.waitForCompletion(true);\n        return b?0:1;\n    }\n    public static void main(String[] args) throws Exception {\n        int run = ToolRunner.run(new Configuration(), new VideoMain(), args);\n        System.exit(run);\n    }\n}\n```\n\n## 4、项目建表并加载数据\n\n### 1、创建表\n\n创建表：youtubevideo_ori，youtubevideo_user_ori，\n\n创建表：youtubevideo_orc，youtubevideo_user_orc\n\nyoutubevideo_ori：\n\n开启分桶表功能\n\n```\nset hive.enforce.bucketing=true;\nset mapreduce.job.reduces=-1;\n\ncreate database youtube;\nuse youtube;\ncreate table youtubevideo_ori(\n    videoId string, \n    uploader string, \n    age int, \n    category array<string>, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedId array<string>)\nrow format delimited \nfields terminated by \"\\t\"\ncollection items terminated by \"&\"\nstored as textfile;\n```\n\nyoutubevideo_user_ori：\n\n```\ncreate table youtubevideo_user_ori(\n    uploader string,\n    videos int,\n    friends int)\nclustered by (uploader) into 24 buckets\nrow format delimited \nfields terminated by \"\\t\" \nstored as textfile;\n\n```\n\n然后把原始数据插入到orc表中\n\nyoutubevideo_orc：\n\n```\ncreate table youtubevideo_orc(\n    videoId string, \n    uploader string, \n    age int, \n    category array<string>, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int,\n    relatedId array<string>)\nclustered by (uploader) into 8 buckets \nrow format delimited fields terminated by \"\\t\" \ncollection items terminated by \"&\" \nstored as orc;\n```\n\nyoutubevideo_user_orc：\n\n```\ncreate table youtubevideo_user_orc(\n    uploader string,\n    videos int,\n    friends int)\nclustered by (uploader) into 24 buckets \nrow format delimited \nfields terminated by \"\\t\" \nstored as orc;\n\n```\n\n### 2、导入ETL之后的数据\n\nyoutubevideo_ori：\n\n```\nload data inpath \"/youtubevideo/output/video/2008/0222\" into table youtubevideo_ori;\n```\n\nyoutubevideo_user_ori：\n\n```\nload data inpath \"/youtubevideo/user/2008/0903\" into table youtubevideo_user_ori;\n```\n\n### 3、向ORC表插入数据\n\nyoutubevideo_orc：\n\n```\ninsert overwrite table youtubevideo_orc select * from youtubevideo_ori;\n```\n\nyoutubevideo_user_orc：\n\n```\ninsert into table youtubevideo_user_orc select * from youtubevideo_user_ori;\n```\n\n## 5、业务分析\n\n### 1、统计视频观看数Top10\n\n思路：使用order by按照views字段做一个全局排序即可，同时我们设置只显示前10条。\n\n最终代码：\n\n```sql\nselect \n    videoId, \n    uploader, \n    age, \n    category, \n    length, \n    views, \n    rate, \n    ratings, \n    comments \nfrom \n    youtubevideo_orc \norder by \n    views \ndesc limit \n    10;\n\n```\n\n\n\n### 2、统计视频类别热度Top10\n\n思路：\n\n1) 即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。\n\n2) 我们需要按照类别group by聚合，然后count组内的videoId个数即可。\n\n3) 因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。\n\n4) 最后按照热度排序，显示前10条。\n\n最终代码：\n\n```sql\nselect \n    category_name as category, \n    count(t1.videoId) as hot \nfrom (\n    select \n        videoId,\n        category_name \n    from \n        youtubevideo_orc lateral view explode(category) t_catetory as category_name) t1 \ngroup by \n    t1.category_name \norder by \n    hot \ndesc limit \n    10;\n\n```\n\n### 3、统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数\n\n思路：\n\n1) 先找到观看数最高的20个视频所属条目的所有信息，降序排列\n\n2) 把这20条信息中的category分裂出来(列转行)\n\n3) 最后查询视频分类名称和该分类下有多少个Top20的视频\n\n最终代码：\n\n```sql\nselect \n    category_name as category, \n    count(t2.videoId) as hot_with_views \nfrom (\n    select \n        videoId, \n        category_name \n    from (\n        select \n            * \n        from \n            youtubevideo_orc \n        order by \n            views \n        desc limit \n            20) t1 lateral view explode(category) t_catetory as category_name) t2 \ngroup by \n    category_name \norder by \n    hot_with_views \ndesc;\n```\n\n### 4、 统计视频观看数Top50所关联视频的所属类别Rank\n\n思路：\n\n1)       查询出观看数最多的前50个视频的所有信息(当然包含了每个视频对应的关联视频)，记为临时表t1\n\nt1：观看数前50的视频\n\n```sql\nselect \n    * \nfrom \n    youtubevideo_orc \norder by \n    views \ndesc limit \n    50;\n```\n\n2)       将找到的50条视频信息的相关视频relatedId列转行，记为临时表t2\n\nt2：将相关视频的id进行列转行操作\n\n```\nselect \n    explode(relatedId) as videoId \nfrom \n\tt1;\n\n\n```\n\n3)       将相关视频的id和youtubevideo_orc表进行inner join操作\n\nt5：得到两列数据，一列是category，一列是之前查询出来的相关视频id\n\n```\n(select \n    distinct(t2.videoId), \n    t3.category \nfrom \n    t2\ninner join \n    youtubevideo_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name;\n\n\n```\n\n4) 按照视频类别进行分组，统计每组视频个数，然后排行\n\n最终代码：\n\n```\nselect \n    category_name as category, \n    count(t5.videoId) as hot \nfrom (\n    select \n        videoId, \n        category_name \n    from (\n        select \n            distinct(t2.videoId), \n            t3.category \n        from (\n            select \n                explode(relatedId) as videoId \n            from (\n                select \n                    * \n                from \n                    youtubevideo_orc \n                order by \n                    views \n                desc limit \n                    50) t1) t2 \n        inner join \n            youtubevideo_orc t3 on t2.videoId = t3.videoId) t4 lateral view explode(category) t_catetory as category_name) t5\ngroup by \n    category_name \norder by \n    hot \ndesc;\n\n\n```\n\n\n\n### 5、统计每个类别中的视频热度Top10，以Music为例\n\n思路：\n\n1) 要想统计Music类别中的视频热度Top10，需要先找到Music类别，那么就需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。\n\n2) 向category展开的表中插入数据。\n\n3) 统计对应类别（Music）中的视频热度。\n\n最终代码：\n\n创建表类别表：\n\n```\ncreate table youtubevideo_category(\n    videoId string, \n    uploader string, \n    age int, \n    categoryId string, \n    length int, \n    views int, \n    rate float, \n    ratings int, \n    comments int, \n    relatedId array<string>)\nrow format delimited \nfields terminated by \"\\t\" \ncollection items terminated by \"&\" \nstored as orc;\n\n\n```\n\n向类别表中插入数据：\n\n```\ninsert into table youtubevideo_category  \n    select \n        videoId,\n        uploader,\n        age,\n        categoryId,\n        length,\n        views,\n        rate,\n        ratings,\n        comments,\n        relatedId \n    from \n        youtubevideo_orc lateral view explode(category) catetory as categoryId;\n\n```\n\n统计Music类别的Top10（也可以统计其他）\n\n```\nselect \n    videoId, \n    views\nfrom \n    youtubevideo_category \nwhere \n    categoryId = \"Music\" \norder by \n    views \ndesc limit\n    10;\n\n```\n\n\n\n### 6、 统计每个类别中视频流量Top10，以Music为例\n\n思路：\n\n1) 创建视频类别展开表（categoryId列转行后的表）\n\n2) 按照ratings排序即可\n\n最终代码：\n\n```\nselect videoid,views,ratings \nfrom youtubevideo_category \nwhere categoryid = \"Music\" order by ratings desc limit 10;\n\n```\n\n### 7、 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频\n\n思路：\n\n1) 先找到上传视频最多的10个用户的用户信息\n\n```sql\nselect \n    * \nfrom \n    youtubevideo_user_orc \norder by \n    videos \ndesc limit \n    10;\n```\n\n2) 通过uploader字段与youtubevideo_orc表进行join，得到的信息按照views观看次数进行排序即可。\n\n最终代码：\n\n```\nselect \n    t2.videoId, \n    t2.views,\n    t2.ratings,\n    t1.videos,\n    t1.friends \nfrom (\n    select \n        * \n    from \n        youtubevideo_user_orc \n    order by \n        videos desc \n    limit \n        10) t1 \njoin \n    youtubevideo_orc t2\non \n    t1.uploader = t2.uploader \norder by \n    views desc \nlimit \n    20;\n```\n\n### 8、统计每个类别视频观看数Top10\n\n思路：\n\n1) 先得到categoryId展开的表数据\n\n2) 子查询按照categoryId进行分区，然后分区内排序，并生成递增数字，该递增数字这一列起名为rank列\n\n3) 通过子查询产生的临时表，查询rank值小于等于10的数据行即可。\n\n最终代码：\n\n```\nselect \n    t1.* \nfrom (\n    select \n        videoId,\n        categoryId,\n        views,\n        row_number() over(partition by categoryId order by views desc) rank from youtubevideo_category) t1 \nwhere \n    rank <= 10;\n\n```\n\n\n\n","tags":["hadoop","项目练习","hive"]},{"title":"Hadoop数据分析之Hive（三）","url":"/2019/11/05/it/hive/Hadoop数据分析之Hive（三）/","content":"\n### 1. hive表的文件存储格式\n\nHive支持的存储数的格式主要有；TEXTFILE（行式存储） 、SEQUENCEFILE(行式存储)、ORC（列式存储）、PARQUET（列式存储）。\n\n#### 1、列式存储和行式存储\n\n![img](img/clip_image002.jpg)\n\n上图左边为逻辑表，右边第一个为行式存储，第二个为列式存储。\n\n**行存储的特点：** 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。select  *  \n\n**列存储的特点：** 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。  select   某些字段效率更高\n\nTEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；\n\nORC和PARQUET是基于列式存储的。\n\n#### 2 、TEXTFILE格式\n\n默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。\n\n#### 3 、ORC格式\n\nOrc (Optimized Row Columnar)是hive 0.11版里引入的新的存储格式。\n\n可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB->250MB，这样能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data,Row Data,Stripe Footer：\n\n![img](img/clip_image003.png)\n\n一个orc文件可以分为若干个Stripe\n\n一个stripe可以分为三个部分\n\nindexData：某些列的索引数据\n\nrowData :真正的数据存储\n\nStripFooter：stripe的元数据信息\n\n   1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引只是记录某行的各字段在Row Data中的offset。\n\n​    2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。\n\n​    3）Stripe Footer：存的是各个stripe的元数据信息\n\n每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。\n\n#### 4 、PARQUET格式\n\nParquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。\n\nParquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。\n\n通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。\n\n![Parquet文件格式](img/clip_image005.jpg)\n\n上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。\n\n\n\n#### 5 主流文件存储格式对比实验\n\n从存储文件的压缩比和查询速度两个角度对比。\n\n**存储文件的压缩比测试：**\n\n测试数据 参见log.data\n\n##### 1）TextFile\n\n（1）创建表，存储数据格式为TEXTFILE\n\n```sql\nuse kfly;\ncreate table log_text (\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nSTORED AS TEXTFILE ;\n```\n\n（2）向表中加载数据\n\n```sql\nload data local inpath '/kkb/install/hivedatas/log.data' into table log_text ;\n```\n\n \n\n（3）查看表中数据大小，大小为18.1M\n\n```sql\ndfs -du -h /user/hive/warehouse/myhive.db/log_text;\n18.1 M  /user/hive/warehouse/log_text/log.data\n```\n\n##### 2）ORC\n\n（1）创建表，存储数据格式为ORC\n\n```sql\nas\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nSTORED AS orc ;\n```\n\n（2）向表中加载数据\n\n```sql\ninsert into table log_orc select * from log_text ;\n```\n\n（3）查看表中数据大小\n\n```sql\ndfs -du -h /user/hive/warehouse/myhive.db/log_orc;\n\n2.8 M  /user/hive/warehouse/log_orc/123456_0\n```\n\norc这种存储格式，默认使用了zlib压缩方式来对数据进行压缩，所以数据会变成了2.8M，非常小\n\n##### 3）Parquet\n\n（1）创建表，存储数据格式为parquet\n\n```sql\ncreate table log_parquet(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nSTORED AS PARQUET ;  \n```\n\n（2）向表中加载数据\n\n```sql\ninsert into table log_parquet select * from log_text ;\n```\n\n（3）查看表中数据大小\n\n```sql\ndfs -du -h /user/hive/warehouse/myhive.db/log_parquet;\n\n13.1 M  /user/hive/warehouse/log_parquet/123456_0\n\n```\n\n存储文件的压缩比总结：\n\n```sql\nORC >  Parquet >  textFile\n\n```\n\n**存储文件的查询速度测试：**\n\n```sql\n1）TextFile\nhive (default)> select count(*) from log_text;\n_c0\n100000\nTime taken: 21.54 seconds, Fetched: 1 row(s)  \n\n2）ORC\nhive (default)> select count(*) from log_orc;\n_c0\n100000\nTime taken: 20.867 seconds, Fetched: 1 row(s)  \n\n3）Parquet\nhive (default)> select count(*) from log_parquet; \n_c0\n100000\nTime taken: 22.922 seconds, Fetched: 1 row(s)\n\n存储文件的查询速度总结：\nORC > TextFile > Parquet\n```\n\n### 2、存储和压缩结合\n\n官网：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC\n\nORC存储方式的压缩：\n\n| Key                      | Default    | Notes                                                        |\n| ------------------------ | ---------- | ------------------------------------------------------------ |\n| orc.compress             | ZLIB       | high level   compression (one of NONE, ZLIB, SNAPPY)         |\n| orc.compress.size        | 262,144    | number of bytes in   each compression chunk                  |\n| orc.stripe.size          | 67,108,864 | number of bytes in   each stripe                             |\n| orc.row.index.stride     | 10,000     | number of rows   between index entries (must be >= 1000)     |\n| orc.create.index         | true       | whether to create row   indexes                              |\n| orc.bloom.filter.columns | \"\"         | comma separated list of column names for which bloom filter   should be created |\n| orc.bloom.filter.fpp     | 0.05       | false positive probability for bloom filter (must >0.0 and   <1.0) |\n\n#### 1）创建一个非压缩的的ORC存储方式\n\n（1）建表语句\n\n```sql\ncreate table log_orc_none(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nSTORED AS orc tblproperties (\"orc.compress\"=\"NONE\");\n\n```\n\n（2）插入数据\n\n```sql\ninsert into table log_orc_none select * from log_text ;\n\n```\n\n（3）查看插入后数据\n\n```sql\ndfs -du -h /user/hive/warehouse/myhive.db/log_orc_none;\n\n7.7 M  /user/hive/warehouse/log_orc_none/123456_0\n\n```\n\n#### 2）创建一个SNAPPY压缩的ORC存储方式\n\n（1）建表语句\n\n```sql\ncreate table log_orc_snappy(\ntrack_time string,\nurl string,\nsession_id string,\nreferer string,\nip string,\nend_user_id string,\ncity_id string\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\nSTORED AS orc tblproperties (\"orc.compress\"=\"SNAPPY\");\n\n```\n\n（2）插入数据\n\n```sql\ninsert into table log_orc_snappy select * from log_text ;\n```\n\n（3）查看插入后数据\n\n```sql\ndfs -du -h /user/hive/warehouse/myhive.db/log_orc_snappy ;\n3.8 M  /user/hive/warehouse/log_orc_snappy/123456_0\n```\n\n3）上一节中默认创建的ORC存储方式，导入数据后的大小为\n\n```sql\n2.8 M  /user/hive/warehouse/log_orc/123456_0\n```\n\n比Snappy压缩的还小。原因是orc存储文件默认采用ZLIB压缩。比snappy压缩的小。\n\n4）存储方式和压缩总结：\n\n​        在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy。\n\n\n\n### 3. hive的SerDe\n\n#### 1 hive的SerDe是什么\n\n​\tSerde是 ==Serializer/Deserializer==的简写。hive使用Serde进行行对象的序列与反序列化。最后实现把文件内容映射到 hive 表中的字段数据类型。\n\n​\t为了更好的阐述使用 SerDe 的场景，我们需要了解一下 Hive 是如何读数据的(类似于 HDFS 中数据的读写操作)：\n\n```sql\nHDFS files –> InputFileFormat –> <key, value> –> Deserializer –> Row object\n\nRow object –> Serializer –> <key, value> –> OutputFileFormat –> HDFS files\n```\n\n\n\n#### 2 hive的SerDe 类型\n\n- Hive 中内置==org.apache.hadoop.hive.serde2== 库，内部封装了很多不同的SerDe类型。\n- hive创建表时， 通过自定义的SerDe或使用Hive内置的SerDe类型指定数据的序列化和反序列化方式。\n\n```sql\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name \n[(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] \n[CLUSTERED BY (col_name, col_name, ...) \n[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] \n[ROW FORMAT row_format] \n[STORED AS file_format] \n[LOCATION hdfs_path]\n\n\n```\n\n- 如上创建表语句， 使用==row format 参数说明SerDe的类型。==\n\n- 你可以创建表时使用用户**自定义的Serde或者native Serde**， **如果 ROW FORMAT没有指定或者指定了 ROW FORMAT DELIMITED就会使用native Serde**。\n- [Hive SerDes](https://cwiki.apache.org/confluence/display/Hive/SerDe): \n  - Avro (Hive 0.9.1 and later) \n  - ORC (Hive 0.11 and later) \n  - RegEx \n  - Thrift \n  - Parquet (Hive 0.13 and later) \n  - CSV (Hive 0.14 and later) \n  - MultiDelimitSerDe \n\n#### 3  企业实战\n\n##### 1 通过MultiDelimitSerDe 解决多字符分割场景\n\n- 1、创建表\n\n```sql\nuse kfly;\ncreate  table kfly_mul (id String, name string)\nrow format serde 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe'\nWITH SERDEPROPERTIES (\"field.delim\"=\"##\");\n\n\n```\n\n- 2、准备数据 t1.txt\n\n```sql\ncd /kkb/install/hivedatas\nvim t1.txt\n\n\n1##xiaoming\n2##xiaowang\n3##xiaozhang\n\n```\n\n- 3、加载数据\n\n```sql\nload data local inpath '/kkb/install/hivedatas/t1.txt' into table t1;\n\n```\n\n- 4、查询数据\n\n```sql\n0: jdbc:hive2://node1:10000> select * from t1;\n+--------+------------+--+\n| t1.id  |  t1.name   |\n+--------+------------+--+\n| 1      | xiaoming   |\n| 2      | xiaowang   |\n| 3      | xiaozhang  |\n+--------+------------+--+\n\n```\n\n##### 2 通过RegexSerDe 解决多字符分割场景\n\n- 1、创建表\n\n```sql\ncreate  table t2(id int, name string)\nrow format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' \nWITH SERDEPROPERTIES (\"input.regex\" = \"^(.*)\\\\#\\\\#(.*)$\");\n\n```\n\n- 2、准备数据 t1.txt\n\n```sql\n1##xiaoming\n2##xiaowang\n3##xiaozhang\n\n```\n\n- 3、加载数据\n\n```sql\nload data local inpath '/kkb/install/hivedatas/t1.txt' into table t2;\n\n```\n\n- 4、查询数据\n\n```sql\n0: jdbc:hive2://node1:10000> select * from t2;\n+--------+------------+--+\n| t2.id  |  t2.name   |\n+--------+------------+--+\n| 1      | xiaoming   |\n| 2      | xiaowang   |\n| 3      | xiaozhang  |\n+--------+------------+--+\n\n```\n\n\n- \n\n### 4. hive的企业级调优\n\n#### 1、Fetch抓取\n\n- Fetch抓取是指，==Hive中对某些情况的查询可以不必使用MapReduce计算==\n\n  - 例如：select * from score;\n  - 在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台\n\n- 在hive-default.xml.template文件中 ==hive.fetch.task.conversion默认是more==，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。\n\n- 案例实操\n\n  - 把 hive.fetch.task.conversion设置成**==none==**，然后执行查询语句，都会执行mapreduce程序\n\n  ```sql\n  set hive.fetch.task.fen=none;\n  select * from score;\n  select s_id from score;\n  select s_id from score limit 3;\n  \n  ```\n\n  - 把hive.fetch.task.conversion设置成==**more**==，然后执行查询语句，如下查询方式都不会执行mapreduce程序。\n\n  ```sql\n  set hive.fetch.task.conversion=more;\n  select * from score;\n  select s_id from score;\n  select s_id from score limit 3;\n  \n  ```\n\n#### 2、本地模式\n\n- 在Hive客户端测试时，默认情况下是启用hadoop的job模式,把任务提交到集群中运行，这样会导致计算非常缓慢；\n\n- Hive可以通过本地模式在单台机器上处理任务。对于小数据集，执行时间可以明显被缩短。\n\n- 案例实操\n\n  ```sql\n  --开启本地模式，并执行查询语句\n  set hive.exec.mode.local.auto=true;  //开启本地mr\n  \n  --设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，\n  --默认为134217728，即128M\n  set hive.exec.mode.local.auto.inputbytes.max=50000000;\n  \n  --设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，\n  --默认为4\n  set hive.exec.mode.local.auto.input.files.max=5;\n  \n  \n  --执行查询的sql语句\n  select * from student cluster by s_id;\n  ```\n\n\n\n```sql\n--关闭本地运行模式\nset hive.exec.mode.local.auto=false;\nselect * from student cluster by s_id;\n```\n\n\n\n#### 3、表的优化\n\n##### 1 小表、大表 join\n\n- 将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。\n\n  ```sql\n  select  count(distinct s_id)  from score;\n  \n  select count(s_id) from score group by s_id; 在map端进行聚合，效率更高\n  ```\n\n- 实际测试发现：新版的hive已经对小表 join 大表和大表 join 小表进行了优化。小表放在左边和右边已经没有明显区别。\n\n- 多个表关联时，最好分拆成小段，避免大sql（无法控制中间Job）\n\n##### 2 大表 join 大表\n\n- 1．空 key 过滤\n\n  - 有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。\n\n  - 此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。\n\n  - 测试环境准备：\n\n    ```sql\n    use myhive;\n    create table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n    \n    create table nullidtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n    \n    create table jointable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n    \n    load data local inpath '/kkb/install/hivedatas/hive_big_table/*' into table ori; \n    load data local inpath '/kkb/install/hivedatas/hive_have_null_id/*' into table nullidtable;\n    \n    ```\n\n    过滤空key与不过滤空key的结果比较\n\n    ```sql\n    不过滤：\n    INSERT OVERWRITE TABLE jointable\n    SELECT a.* FROM nullidtable a JOIN ori b ON a.id = b.id;\n    结果：\n    No rows affected (152.135 seconds)\n    \n    过滤：\n    INSERT OVERWRITE TABLE jointable\n    SELECT a.* FROM (SELECT * FROM nullidtable WHERE id IS NOT NULL ) a JOIN ori b ON a.id = b.id;\n    结果：\n    No rows affected (141.585 seconds)\n    ```\n\n- 2、空 key 转换\n\n  - 有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在 join 的结果中，此时我们可以表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上。\n\n    不随机分布：\n\n    ```sql\n    set hive.exec.reducers.bytes.per.reducer=32123456;\n    set mapreduce.job.reduces=7;\n    INSERT OVERWRITE TABLE jointable\n    SELECT a.*\n    FROM nullidtable a\n    LEFT JOIN ori b ON CASE WHEN a.id IS NULL THEN 'hive' ELSE a.id END = b.id;\n    No rows affected (41.668 seconds)  \n    \n    ```\n\n    **结果：这样的后果就是所有为null值的id全部都变成了相同的字符串，及其容易造成数据的倾斜（所有的key相同，相同key的数据会到同一个reduce当中去）**\n\n    **为了解决这种情况，我们可以通过hive的rand函数，随记的给每一个为空的id赋上一个随机值，这样就不会造成数据倾斜**\t\t\n\n  ​\t\t随机分布：\n\n  ```sql\n  set hive.exec.reducers.bytes.per.reducer=32123456;\n  set mapreduce.job.reduces=7;\n  INSERT OVERWRITE TABLE jointable\n  SELECT a.*\n  FROM nullidtable a\n  LEFT JOIN ori b ON CASE WHEN a.id IS NULL THEN concat('hive', rand()) ELSE a.id END = b.id;\n  \n  No rows affected (42.594 seconds)              \n  ```\n\n\n\n##### 3、大表join小表与小表join大表实测\n\n需求：测试大表JOIN小表和小表JOIN大表的效率 （新的版本当中已经没有区别了，旧的版本当中需要使用小表）\n\n（1）建大表、小表和JOIN后表的语句\n\n```sql\ncreate table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\ncreate table smalltable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\ncreate table jointable2(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\n```\n\n（2）分别向大表和小表中导入数据\n\n```sql\nhive (default)> load data local inpath '/kkb/install/hivedatas/big_data' into table bigtable;\n\nhive (default)>load data local inpath '/kkb/install/hivedatas/small_data' into table smalltable;\n```\n\n##### 3 map  join \n\n- 如果不指定MapJoin 或者不符合 MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在map端进行join，避免reducer处理。\n\n- 1、开启MapJoin参数设置\n\n  ```sql\n   --默认为true\n  set hive.auto.convert.join = true;\n  ```\n\n- 2、大表小表的阈值设置（默认25M一下认为是小表）\n\n```sql\nset hive.mapjoin.smalltable.filesize=26214400;\n\n```\n\n- 3、MapJoin工作机制\n\n![xxx](img/xxx-1570506631515.jpg)\n\n首先是Task A，它是一个Local Task（在客户端本地执行的Task），负责扫描小表b的数据，将其转换成一个HashTable的数据结构，并写入本地的文件中，之后将该文件加载到DistributeCache中。\n\n接下来是Task B，该任务是一个没有Reduce的MR，启动MapTasks扫描大表a,在Map阶段，根据a的每一条记录去和DistributeCache中b表对应的HashTable关联，并直接输出结果。\n\n由于MapJoin没有Reduce，所以由Map直接输出结果文件，有多少个Map Task，就有多少个结果文件。\n\n**案例实操：**\n\n（1）开启Mapjoin功能\n\n```sql\nset hive.auto.convert.join = true; 默认为true\n```\n\n（2）执行小表JOIN大表语句\n\n```sql\nINSERT OVERWRITE TABLE jointable2\nSELECT b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nFROM smalltable s\nJOIN bigtable  b\nON s.id = b.id;\n\nTime taken: 31.814 seconds\n```\n\n（3）执行大表JOIN小表语句\n\n```shell\nINSERT OVERWRITE TABLE jointable2\nSELECT b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nFROM bigtable  b\nJOIN smalltable  s\nON s.id = b.id;\n\nTime taken: 28.46 seconds\n```\n\n\n\n##### 4 group By\n\n- 默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。\n\n- 并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。\n\n- 开启Map端聚合参数设置\n\n  ```sql\n  --是否在Map端进行聚合，默认为True\n  set hive.map.aggr = true;\n  --在Map端进行聚合操作的条目数目\n  set hive.groupby.mapaggr.checkinterval = 100000;\n  --有数据倾斜的时候进行负载均衡（默认是false）\n  set hive.groupby.skewindata = true;\n  \n  当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。\n  ```\n\n\n\n##### 5 count(distinct) \n\n- 数据量小的时候无所谓，数据量大的情况下，由于count distinct 操作需要用一个reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般count distinct使用先group by 再count的方式替换\n\n  环境准备：\n\n  \n\n  ```sql\n  create table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n  \n  load data local inpath '/kkb/install/hivedatas/data/100万条大表数据（id除以10取整）/bigtable' into table bigtable;\n  \n  \n  --每个reduce任务处理的数据量 默认256000000（256M）\n   set hive.exec.reducers.bytes.per.reducer=32123456;\n   \n   select  count(distinct ip )  from log_text;\n   \n   转换成\n   set hive.exec.reducers.bytes.per.reducer=32123456;\n   select count(ip) from (select ip from log_text group by ip) t;\n   \n   \n   虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。\n  ```\n\n##### 6 笛卡尔积\n\n- 尽量避免笛卡尔积，即避免join的时候不加on条件，或者无效的on条件\n- Hive只能使用1个reducer来完成笛卡尔积。\n\n#### 4、使用分区剪裁、列剪裁\n\n- 尽可能早地过滤掉尽可能多的数据量，避免大量数据流入外层SQL。\n- **列剪裁**\n  - 只获取需要的列的数据，减少数据输入。\n- **分区裁剪**\n  - 分区在hive实质上是目录，分区裁剪可以方便直接地过滤掉大部分数据。\n  - 尽量使用分区过滤，少用select  *\n\n\n\n​\t环境准备：\n\n```sql\ncreate table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\ncreate table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\nload data local inpath '/home/admin/softwares/data/加递增id的原始数据/ori' into table ori;\n\nload data local inpath '/home/admin/softwares/data/100万条大表数据（id除以10取整）/bigtable' into table bigtable;\n\n```\n\n先关联再Where：\n\n```sql\nSELECT a.id\nFROM bigtable a\nLEFT JOIN ori b ON a.id = b.id\nWHERE b.id <= 10;\n\n```\n\n正确的写法是写在ON后面：先Where再关联\n\n```sql\nSELECT a.id\nFROM ori a\nLEFT JOIN bigtable b ON (a.id <= 10 AND a.id = b.id);\n\n```\n\n或者直接写成子查询：\n\n```sql\nSELECT a.id\nFROM bigtable a\nRIGHT JOIN (SELECT id\nFROM ori\nWHERE id <= 10\n) b ON a.id = b.id;\n\n```\n\n\n\n#### 5、并行执行\n\n- 把一个sql语句中没有相互依赖的阶段并行去运行。提高集群资源利用率\n\n```sql\n--开启并行执行\nset hive.exec.parallel=true;\n--同一个sql允许最大并行度，默认为8。\nset hive.exec.parallel.thread.number=16;\n```\n\n\n\n#### 6、严格模式\n\n- Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。\n\n- 通过设置属性hive.mapred.mode值为默认是非严格模式**nonstrict** 。开启严格模式需要修改hive.mapred.mode值为**strict**，开启严格模式可以禁止3种类型的查询。\n\n  ```sql\n  --设置非严格模式（默认）\n  set hive.mapred.mode=nonstrict;\n  \n  --设置严格模式\n  set hive.mapred.mode=strict;\n  ```\n\n\n\n- （1）对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行\n\n  ```sql\n  --设置严格模式下 执行sql语句报错； 非严格模式下是可以的\n  select * from order_partition；\n  \n  异常信息：Error: Error while compiling statement: FAILED: SemanticException [Error 10041]: No partition predicate found for Alias \"order_partition\" Table \"order_partition\" \n  ```\n\n- （2）对于使用了order by语句的查询，要求必须使用limit语句\n\n  ```sql\n  --设置严格模式下 执行sql语句报错； 非严格模式下是可以的\n  select * from order_partition where month='2019-03' order by order_price; \n  \n  异常信息：Error: Error while compiling statement: FAILED: SemanticException 1:61 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'order_price'\n  ```\n\n- （3）限制笛卡尔积的查询\n\n  - 严格模式下，避免出现笛卡尔积的查询\n\n\n\n#### 7、JVM重用\n\n- JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。\n\n  Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。\n\n  ```xml\n  <property>\n    <name>mapreduce.job.jvm.numtasks</name>\n    <value>10</value>\n    <description>How many tasks to run per jvm. If set to -1, there is\n    no limit. \n    </description>\n  </property>\n  \n  ```\n\n  我们也可以在hive当中通过\n\n  ```sql\n   set  mapred.job.reuse.jvm.num.tasks=10;\n  ```\n\n  这个设置来设置我们的jvm重用\n\n  这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。\n\n#### 8、推测执行\n\n- 在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。\n\n  设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置\n\n```xml\n<property>\n  <name>mapreduce.map.speculative</name>\n  <value>true</value>\n  <description>If true, then multiple instances of some map tasks \n               may be executed in parallel.</description>\n</property>\n\n<property>\n  <name>mapreduce.reduce.speculative</name>\n  <value>true</value>\n  <description>If true, then multiple instances of some reduce tasks \n               may be executed in parallel.</description>\n</property>\n\n```\n\n不过hive本身也提供了配置项来控制reduce-side的推测执行：\n\n```xml\n  <property>\n    <name>hive.mapred.reduce.tasks.speculative.execution</name>\n    <value>true</value>\n    <description>Whether speculative execution for reducers should be turned on. </description>\n  </property>\n\n```\n\n关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。\n\n\n\n#### 9、压缩\n\n​\t参见数据的压缩\n\n- Hive表中间数据压缩\n\n  ```shell\n  #设置为true为激活中间数据压缩功能，默认是false，没有开启\n  set hive.exec.compress.intermediate=true;\n  #设置中间数据的压缩算法\n  set mapred.map.output.compression.codec= org.apache.hadoop.io.compress.SnappyCodec;\n  \n  ```\n\n- Hive表最终输出结果压缩\n\n  ```shell\n  set hive.exec.compress.output=true;\n  set mapred.output.compression.codec= \n  org.apache.hadoop.io.compress.SnappyCodec;\n  ```\n\n#### 10、使用EXPLAIN（执行计划）\n\n查看hql执行计划\n\n\n\n#### 11、数据倾斜\n\n##### 1 合理设置Map数\n\n- 1)  通常情况下，作业会通过input的目录产生一个或者多个map任务。\n\n  ```sql\n  主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。\n  \n  举例：\n  a)  假设input目录下有1个文件a，大小为780M，那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数。\n  b) 假设input目录下有3个文件a，b，c大小分别为10m，20m，150m，那么hadoop会分隔成4个块（10m，20m，128m，22m），从而产生4个map数。即，如果文件大于块大小(128m)，那么会拆分，如果小于块大小，则把该文件当成一个块。\n  \n  ```\n\n- 2） 是不是map数越多越好？\n\n  ```shell\n    答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。\n  ```\n\n- 3） 是不是保证每个map处理接近128m的文件块，就高枕无忧了？\n\n  ```shell\n  答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。\n  \n  针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；\n  \n  ```\n\n\n\n##### 2 小文件合并\n\n- 在map执行前合并小文件，减少map数：\n\n- CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）\n\n  ```sql\n  set mapred.max.split.size=112345600;\n  set mapred.min.split.size.per.node=112345600;\n  set mapred.min.split.size.per.rack=112345600;\n  set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n  \n  ```\n\n  这个参数表示执行前进行小文件合并，前面三个参数确定合并文件块的大小，大于文件块大小128m的，按照128m来分隔，小于128m，大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），进行合并。\n\n##### 3 复杂文件增加Map数\n\n- 当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。\n\n- 增加map的方法为\n\n  - 根据 ==computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))==公式\n  - ==调整maxSize最大值==。让maxSize最大值低于blocksize就可以增加map的个数。\n\n  ```shell\n  mapreduce.input.fileinputformat.split.minsize=1 默认值为1\n  \n  mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue 默认值Long.MAXValue因此，默认情况下，切片大小=blocksize \n  \n  maxsize（切片最大值): 参数如果调到比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。\n  \n  minsize(切片最小值): 参数调的比blockSize大，则可以让切片变得比blocksize还大。\n  \n  ```\n\n  - 例如\n\n  ```sql\n  --设置maxsize大小为10M，也就是说一个fileSplit的大小为10M\n  set mapreduce.input.fileinputformat.split.maxsize=10485760;\n  ```\n\n\n\n##### 4 合理设置Reduce数\n\n- 1、调整reduce个数方法一\n\n  - 1）每个Reduce处理的数据量默认是256MB\n\n    ```sql\n    set hive.exec.reducers.bytes.per.reducer=256000000;\n    ```\n\n  - 2) 每个任务最大的reduce数，默认为1009\n\n    ```sql\n    set hive.exec.reducers.max=1009;\n    ```\n\n  - 3) 计算reducer数的公式\n\n    ```shell\n    N=min(参数2，总输入数据量/参数1)\n    ```\n\n- 2、调整reduce个数方法二\n\n  ```sql\n  --设置每一个job中reduce个数\n  set mapreduce.job.reduces=3;\n  ```\n\n\n\n- 3、reduce个数并不是越多越好\n\n  - 过多的启动和初始化reduce也会消耗时间和资源；\n\n  - 同时过多的reduce会生成很多个文件，也有可能出现小文件问题\n\n","tags":["hadoop","hive","调优"]},{"title":"hadoop之数据分析Hive（二）","url":"/2019/11/03/it/hive/hadoop之数据分析Hive（二）/","content":"\n### 1、hive的参数传递\n\n#### 1、Hive命令行\n\nhive [-hiveconf x=y]* [<-i filename>]* [<-f filename>|<-e query-string>] [-S]\n\n说明：\n\n1、   -i 从文件初始化HQL。\n\n2、   -e从命令行执行指定的HQL \n\n3、   -f 执行HQL脚本 \n\n4、   -v 输出执行的HQL语句到控制台 \n\n5、   -p <port> connect to Hive Server on port number \n\n6、   -hiveconf x=y Use this to set hive/hadoop configuration variables.  设置hive运行时候的参数配置\n\n#### 2、Hive参数配置方式\n\nHive参数大全：\n\nhttps://cwiki.apache.org/confluence/display/Hive/Configuration+Properties\n\n开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么设定的参数没有起作用？这通常是错误的设定方式导致的。\n\n**对于一般参数，有以下三种设定方式：**\n\n```\n配置文件  hive-site.xml\n\n命令行参数  启动hive客户端的时候可以设置参数\n\n参数声明   进入客户单以后设置的一些参数  set  \n```\n\n**配置文件**：Hive的配置文件包括\n\n 用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml \n\n默认配置文件：$HIVE_CONF_DIR/hive-default.xml \n\n用户自定义配置会覆盖默认配置。\n\n另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。\n\n**命令行参数**：启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如：\n\n```shell\nbin/hive -hiveconf hive.root.logger=INFO,console\n```\n\n这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效。\n\n**参数声明**：可以在HQL中使用SET关键字设定参数，例如：\n\n```\nset mapred.reduce.tasks=100;\n```\n\n这一设定的作用域也是session级的。\n\n上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。\n\n```\n参数声明  >   命令行参数   >  配置文件参数（hive）\n```\n\n#### 3、使用变量传递参数\n\n实际工作当中，我们一般都是将hive的hql语法开发完成之后，就写入到一个脚本里面去，然后定时的通过命令 hive  -f  去执行hive的语法即可，然后通过定义变量来传递参数到hive的脚本当中去，那么我们接下来就来看看如何使用hive来传递参数。\n\nhive0.9以及之前的版本是不支持传参的\n hive1.0版本之后支持  hive -f 传递参数\n\n在hive当中我们一般可以使用hivevar或者hiveconf来进行参数的传递\n\n##### hiveconf使用说明\n\nhiveconf用于定义HIVE执行上下文的属性(配置参数)，可覆盖覆盖hive-site.xml（hive-default.xml）中的参数值，如用户执行目录、日志打印级别、执行队列等。例如我们可以使用hiveconf来覆盖我们的hive属性配置，\n\nhiveconf变量取值必须要使用hiveconf作为前缀参数，具体格式如下:\n\n```sql\n${hiveconf:key} \nbin/hive --hiveconf \"mapred.job.queue.name=root.default\"\n```\n\n##### hivevar使用说明\n\nhivevar用于定义HIVE运行时的变量替换，类似于JAVA中的“PreparedStatement”，与\\${key}配合使用或者与 ${hivevar:key}\n\n对于hivevar取值可以不使用前缀hivevar，具体格式如下：\n\n```sql\n使用前缀:\n ${hivevar:key}\n不使用前缀:\n ${key}\n--hivevar  name=zhangsan    ${hivevar:name}  \n也可以这样取值  ${name}\n```\n\n##### define使用说明\n\n```sql\ndefine与hivevar用途完全一样，还有一种简写“-d\nbin/hive --hiveconf \"mapred.job.queue.name=root.default\" -d my=\"201809\" --database mydb\n执行SQL\nselect * from mydb where concat(year, month) = ${my} limit 10;\n```\n\n##### hiveconf与hivevar使用实战\n\n需求：hive当中执行以下hql语句，并将参数全部都传递进去\n\n```sql\nselect * from student left join score on student.s_id = score.s_id where score.month = '201807' and score.s_score > 80 and score.c_id = 03;\n```\n\n###### 第一步：创建student表并加载数据\n\n```sql\nhive (myhive)> create external table student\n(s_id string,s_name string,s_birth string , s_sex string ) row format delimited\nfields terminated by '\\t';\n\nhive (myhive)> load data local inpath '/kkb/install/hivedatas/student.csv' overwrite into table student;\n\n```\n\n###### 第二步：定义hive脚本\n\n开发hql脚本，并使用hiveconf和hivevar进行参数穿肚\n\nnode03执行以下命令定义hql脚本\n\n```sql\ncd /kkb/instal/hivedatas\n\nvim hivevariable.hql\nuse myhive;\nselect * from student left join score on student.s_id = score.s_id where score.month = ${hiveconf:month} and score.s_score > ${hivevar:s_score} and score.c_id = ${c_id};   \n```\n\n###### 第三步：调用hive脚本并传递参数\n\nnode03执行以下命令并\n\n```sql\n[root@node03 hive-1.1.0-cdh5.14.2]# bin/hive --hiveconf month=201807 --hivevar s_score=80 --hivevar c_id=03  -f /kkb/install/hivedatas/hivevariable.hql\n```\n\n### 2、hive的常用函数介绍\n\n#### 系统内置函数\n\n```\n1．查看系统自带的函数\nhive> show functions;\n2．显示自带的函数的用法\nhive> desc function upper;\n3．详细显示自带的函数的用法\nhive> desc function extended upper;\n```\n\n\n\n#### 1、数值计算\n\n##### 1、取整函数: round \n\n**语法**: round(double a)\n **返回值**: BIGINT\n **说明**: 返回double类型的整数值部分 （遵循四舍五入）\n\n```\nhive> select round(3.1415926) from tableName;\n3\nhive> select round(3.5) from tableName;\n4\nhive> create table tableName as select round(9542.158) from tableName;\n\n```\n\n\n\n##### 2、指定精度取整函数: round \n\n**语法**: round(double a, int d)\n **返回值**: DOUBLE\n **说明**: 返回指定精度d的double类型\n\n```\nhive> select round(3.1415926,4) from tableName;\n3.1416\n\n```\n\n##### 3、向下取整函数: floor \n\n**语法**: floor(double a)\n **返回值**: BIGINT\n **说明**: 返回等于或者小于该double变量的最大的整数\n\n```\nhive> select floor(3.1415926) from tableName;\n3\nhive> select floor(25) from tableName;\n25\n\n\n```\n\n##### 4、向上取整函数: ceil \n\n**语法**: ceil(double a)\n **返回值**: BIGINT\n **说明**: 返回等于或者大于该double变量的最小的整数\n\n```\nhive> select ceil(3.1415926) from tableName;\n4\nhive> select ceil(46) from tableName;\n46\n\n\n```\n\n##### 5、向上取整函数: ceiling \n\n**语法**: ceiling(double a)\n **返回值**: BIGINT\n **说明**: 与ceil功能相同\n\n\n\n```\nhive> select ceiling(3.1415926) from tableName;\n4\nhive> select ceiling(46) from tableName;\n46\n\n```\n\n##### 6、取随机数函数: rand \n\n**语法**: rand(),rand(int seed)\n **返回值**: double\n **说明**: 返回一个0到1范围内的随机数。如果指定种子seed，则会等到一个稳定的随机数序列\n\n```\nhive> select rand() from tableName;\n0.5577432776034763\nhive> select rand() from tableName;\n0.6638336467363424\nhive> select rand(100) from tableName;\n0.7220096548596434\nhive> select rand(100) from tableName;\n0.7220096548596434\n```\n\n\n\n#### 2、日期函数\n\n##### 1、UNIX时间戳转日期函数: from_unixtime  \n\n**语法**: from_unixtime(bigint unixtime[, string format])\n **返回值**: string\n **说明**: 转化UNIX时间戳（从1970-01-01 00:00:00 UTC到指定时间的秒数）到当前时区的时间格式\n\n```sql\nhive> select from_unixtime(1323308943,'yyyyMMdd') from tableName;\n20111208\n\n\n```\n\n##### 2、获取当前UNIX时间戳函数: unix_timestamp\n\n**语法**: unix_timestamp()\n **返回值**: bigint\n **说明**: 获得当前时区的UNIX时间戳\n\n```sql\nhive> select unix_timestamp() from tableName;\n1323309615\n\n```\n\n##### 3、日期转UNIX时间戳函数: unix_timestamp \n\n**语法**: unix_timestamp(string date)\n **返回值**: bigint\n **说明**: 转换格式为\"yyyy-MM-dd HH:mm:ss\"的日期到UNIX时间戳。如果转化失败，则返回0。\n\n```   sql\nhive> select unix_timestamp('2011-12-07 13:01:03') from tableName;\n1323234063\n```\n\n##### 4、指定格式日期转UNIX时间戳函数: unix_timestamp \n\n**语法**: unix_timestamp(string date, string pattern)\n **返回值**: bigint\n **说明**: 转换pattern格式的日期到UNIX时间戳。如果转化失败，则返回0。\n\n```sql\nhive> select unix_timestamp('20111207 13:01:03','yyyyMMdd HH:mm:ss') from tableName;\n1323234063\n\n```\n\n##### 5、日期时间转日期函数: to_date  \n\n**语法**: to_date(string timestamp)\n **返回值**: string\n **说明**: 返回日期时间字段中的日期部分。\n\n```sql\nhive> select to_date('2011-12-08 10:03:01') from tableName;\n2011-12-08\n```\n\n##### 6、日期转年函数: year \n\n**语法**: year(string date)\n **返回值**: int\n **说明**: 返回日期中的年。\n\n```sql\nhive> select year('2011-12-08 10:03:01') from tableName;\n2011\nhive> select year('2012-12-08') from tableName;\n2012\n\n\n```\n\n##### 7、日期转月函数: month \n\n**语法**: month (string date)\n **返回值**: int\n **说明**: 返回日期中的月份。\n\n```sql\nhive> select month('2011-12-08 10:03:01') from tableName;\n12\nhive> select month('2011-08-08') from tableName;\n8\n\n\n```\n\n##### 8、日期转天函数: day \n\n**语法**: day (string date)\n **返回值**: int\n **说明**: 返回日期中的天。\n\n```sql\nhive> select day('2011-12-08 10:03:01') from tableName;\n8\nhive> select day('2011-12-24') from tableName;\n24\n\n\n```\n\n##### 9、日期转小时函数: hour \n\n**语法**: hour (string date)\n **返回值**: int\n **说明**: 返回日期中的小时。\n\n```sql\nhive> select hour('2011-12-08 10:03:01') from tableName;\n10\n\n```\n\n##### 10、日期转分钟函数: minute\n\n**语法**: minute (string date)\n **返回值**: int\n **说明**: 返回日期中的分钟。\n\n```sql\nhive> select minute('2011-12-08 10:03:01') from tableName;\n3\n\nhive> select second('2011-12-08 10:03:01') from tableName;\n1\n```\n\n\n\n##### 12、日期转周函数: weekofyear\n\n**语法**: weekofyear (string date)\n **返回值**: int\n **说明**: 返回日期在当前的周数。\n\n```sql\nhive> select weekofyear('2011-12-08 10:03:01') from tableName;\n49\n\n```\n\n##### 13、日期比较函数: datediff \n\n**语法**: datediff(string enddate, string startdate)\n **返回值**: int\n **说明**: 返回结束日期减去开始日期的天数。\n\n```sql\nhive> select datediff('2012-12-08','2012-05-09') from tableName;\n213\n\n```\n\n##### 14、日期增加函数: date_add \n\n**语法**: date_add(string startdate, int days)\n **返回值**: string\n **说明**: 返回开始日期startdate增加days天后的日期。\n\n```\nhive> select date_add('2012-12-08',10) from tableName;\n2012-12-18\n\n```\n\n##### 15、日期减少函数: date_sub \n\n**语法**: date_sub (string startdate, int days)\n **返回值**: string\n **说明**: 返回开始日期startdate减少days天后的日期。\n\n```\nhive> select date_sub('2012-12-08',10) from tableName;\n2012-11-28\n\n```\n\n\n\n#### 3、条件函数\n\n##### 1、If函数: if \n\n**语法**: if(boolean testCondition, T valueTrue, T valueFalseOrNull)\n **返回值**: T\n **说明**: 当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull\n\n```\nhive> select if(1=2,100,200) from tableName;\n200\nhive> select if(1=1,100,200) from tableName;\n100\n\n\n```\n\n##### 2、非空查找函数: COALESCE\n\n**语法**: COALESCE(T v1, T v2, …)\n **返回值**: T\n **说明**: 返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL\n\n```\nhive> select COALESCE(null,'100','50') from tableName;\n100\n\n\n```\n\n##### 3、条件判断函数：CASE \n\n**语法**: CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END\n **返回值**: T\n **说明**：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f\n\n```\nhive> Select case 100 when 50 then 'tom' when 100 then 'mary' else 'tim' end from tableName;\nmary\nhive> Select case 200 when 50 then 'tom' when 100 then 'mary' else 'tim' end from tableName;\ntim\n\n\n```\n\n##### 4、条件判断函数：CASE  \n\n**语法**: CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END\n **返回值**: T\n **说明**：如果a为TRUE,则返回b；如果c为TRUE，则返回d；否则返回e\n\n```\nhive> select case when 1=2 then 'tom' when 2=2 then 'mary' else 'tim' end from tableName;\nmary\nhive> select case when 1=1 then 'tom' when 2=2 then 'mary' else 'tim' end from tableName;\ntom\n\n\n```\n\n\n\n#### 4、字符串函数\n\n##### 1、字符串长度函数：length\n\n**语法**: length(string A)\n **返回值**: int\n **说明**：返回字符串A的长度\n\n```\nhive> select length('abcedfg') from tableName;\n\n```\n\n##### 2、字符串反转函数：reverse\n\n**语法**: reverse(string A)\n **返回值**: string\n **说明**：返回字符串A的反转结果\n\n```\nhive> select reverse('abcedfg') from tableName;\ngfdecba\n```\n\n##### 3、字符串连接函数：concat\n\n**语法**: concat(string A, string B…)\n **返回值**: string\n **说明**：返回输入字符串连接后的结果，支持任意个输入字符串\n\n```\nhive> select concat('abc','def','gh') from tableName;\nabcdefgh\n\n```\n\n##### 4、字符串连接并指定字符串分隔符：concat_ws\n\n**语法**: concat_ws(string SEP, string A, string B…)\n **返回值**: string\n **说明**：返回输入字符串连接后的结果，SEP表示各个字符串间的分隔符\n\n```\nhive> select concat_ws(',','abc','def','gh')from tableName;\nabc,def,gh\n\n```\n\n##### 5、字符串截取函数：substr\n\n**语法**: substr(string A, int start),substring(string A, int start)\n **返回值**: string\n **说明**：返回字符串A从start位置到结尾的字符串\n\n```\nhive> select substr('abcde',3) from tableName;\ncde\nhive> select substring('abcde',3) from tableName;\ncde\nhive>  select substr('abcde',-1) from tableName;  （和ORACLE相同）\ne\n\n```\n\n\n\n##### 6、字符串截取函数：substr,substring \n\n**语法**: substr(string A, int start, int len),substring(string A, int start, int len)\n **返回值**: string\n **说明**：返回字符串A从start位置开始，长度为len的字符串\n\n```\nhive> select substr('abcde',3,2) from tableName;\ncd\nhive> select substring('abcde',3,2) from tableName;\ncd\nhive>select substring('abcde',-2,2) from tableName;\nde\n\n```\n\n\n\n##### 7、字符串转大写函数：upper,ucase  \n\n**语法**: upper(string A) ucase(string A)\n **返回值**: string\n **说明**：返回字符串A的大写格式\n\n```\nhive> select upper('abSEd') from tableName;\nABSED\nhive> select ucase('abSEd') from tableName;\nABSED\n\n```\n\n##### 8、字符串转小写函数：lower,lcase  \n\n**语法**: lower(string A) lcase(string A)\n **返回值**: string\n **说明**：返回字符串A的小写格式\n\n```\nhive> select lower('abSEd') from tableName;\nabsed\nhive> select lcase('abSEd') from tableName;\nabsed\n\n```\n\n\n\n##### 9、去空格函数：trim \n\n**语法**: trim(string A)\n **返回值**: string\n **说明**：去除字符串两边的空格\n\n```\nhive> select trim(' abc ') from tableName;\nabc\n\n```\n\n##### 10、url解析函数  parse_url\n\n**语法**:\nparse_url(string urlString, string partToExtract [, string keyToExtract])\n**返回值**: string\n**说明**：返回URL中指定的部分。partToExtract的有效值为：HOST, PATH,\nQUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO.\n\n```\nhive> select parse_url\n('https://www.tableName.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST') \nfrom tableName;\nwww.tableName.com \nhive> select parse_url\n('https://www.tableName.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k1')\n from tableName;\nv1\n\n```\n\n\n\n##### 11、json解析  get_json_object \n\n**语法**: get_json_object(string json_string, string path)\n **返回值**: string\n **说明**：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。\n\n```\nhive> select  get_json_object('{\"store\":{\"fruit\":\\[{\"weight\":8,\"type\":\"apple\"},{\"weight\":9,\"type\":\"pear\"}], \"bicycle\":{\"price\":19.95,\"color\":\"red\"} },\"email\":\"amy@only_for_json_udf_test.net\",\"owner\":\"amy\"}','$.owner') from tableName;\n\n```\n\n\n\n##### 12、重复字符串函数：repeat \n\n**语法**: repeat(string str, int n)\n **返回值**: string\n **说明**：返回重复n次后的str字符串\n\n```\nhive> select repeat('abc',5) from tableName;\nabcabcabcabcabc\n\n```\n\n##### 13、分割字符串函数: split   \n\n**语法**: split(string str, string pat)\n **返回值**: array\n **说明**: 按照pat字符串分割str，会返回分割后的字符串数组\n\n```\nhive> select split('abtcdtef','t') from tableName;\n[\"ab\",\"cd\",\"ef\"]\n\n```\n\n#### 5、集合统计函数\n\n##### 1、个数统计函数: count  \n\n**语法**: count(*), count(expr), count(DISTINCT expr[, expr_.])\n**返回值**：Int\n\n**说明**: count(*)统计检索出的行的个数，包括NULL值的行；count(expr)返回指定字段的非空值的个数；count(DISTINCT\nexpr[, expr_.])返回指定字段的不同的非空值的个数\n\n```\nhive> select count(*) from tableName;\n20\nhive> select count(distinct t) from tableName;\n10\n\n\n```\n\n##### 2、总和统计函数: sum \n\n**语法**: sum(col), sum(DISTINCT col)\n **返回值**: double\n **说明**: sum(col)统计结果集中col的相加的结果；sum(DISTINCT col)统计结果中col不同值相加的结果\n\n```\nhive> select sum(t) from tableName;\n100\nhive> select sum(distinct t) from tableName;\n70\n\n\n```\n\n##### 3、平均值统计函数: avg   \n\n**语法**: avg(col), avg(DISTINCT col)\n **返回值**: double\n **说明**: avg(col)统计结果集中col的平均值；avg(DISTINCT col)统计结果中col不同值相加的平均值\n\n```\nhive> select avg(t) from tableName;\n50\nhive> select avg (distinct t) from tableName;\n30\n\n\n```\n\n##### 4、最小值统计函数: min \n\n**语法**: min(col)\n **返回值**: double\n **说明**: 统计结果集中col字段的最小值\n\n```\nhive> select min(t) from tableName;\n20\n\n\n```\n\n##### 5、最大值统计函数: max  \n\n**语法**: maxcol)\n **返回值**: double\n **说明**: 统计结果集中col字段的最大值\n\n```\nhive> select max(t) from tableName;\n120\n\n```\n\n#### 6、复合类型构建函数\n\n##### 1、Map类型构建: map  \n\n**语法**: map (key1, value1, key2, value2, …)\n **说明**：根据输入的key和value对构建map类型\n\n```\ncreate table score_map(name string, score map<string,int>)\nrow format delimited fields terminated by '\\t' \ncollection items terminated by ',' map keys terminated by ':';\n\n创建数据内容如下并加载数据\ncd /kkb/install/hivedatas/\nvim score_map.txt\n\nzhangsan\t数学:80,语文:89,英语:95\nlisi\t语文:60,数学:80,英语:99\n\n加载数据到hive表当中去\nload data local inpath '/kkb/install/hivedatas/score_map.txt' overwrite into table score_map;\n\nmap结构数据访问：\n获取所有的value：\nselect name,map_values(score) from score_map;\n\n获取所有的key：\nselect name,map_keys(score) from score_map;\n\n按照key来进行获取value值\nselect name,score[\"数学\"]  from score_map;\n\n查看map元素个数\nselect name,size(score) from score_map;\n\n```\n\n##### 2、Struct类型构建: struct\n\n**语法**: struct(val1, val2, val3, …)\n **说明**：根据输入的参数构建结构体struct类型，似于C语言中的结构体，内部数据通过X.X来获取，假设我们的数据格式是这样的，电影ABC，有1254人评价过，打分为7.4分\n\n```\n创建struct表\nhive> create table movie_score( name string,  info struct<number:int,score:float> )row format delimited fields terminated by \"\\t\"  collection items terminated by \":\"; \n\n加载数据\ncd /kkb/install/hivedatas/\nvim struct.txt\n\nABC\t1254:7.4  \nDEF\t256:4.9  \nXYZ\t456:5.4\n\n加载数据\nload data local inpath '/kkb/install/hivedatas/struct.txt' overwrite into table movie_score;\n\n\nhive当中查询数据\nhive> select * from movie_score;  \nhive> select info.number,info.score from movie_score;  \nOK  \n1254    7.4  \n256     4.9  \n456     5.4  \n\n```\n\n##### 3、array类型构建: array\n\n**语法**: array(val1, val2, …)\n **说明**：根据输入的参数构建数组array类型\n\n```sql\nhive> create table  person(name string,work_locations array<string>)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'\nCOLLECTION ITEMS TERMINATED BY ',';\n\n加载数据到person表当中去\ncd /kkb/install/hivedatas/\nvim person.txt\n\n数据内容格式如下\nbiansutao\tbeijing,shanghai,tianjin,hangzhou\nlinan\tchangchu,chengdu,wuhan\n\n加载数据\nhive > load  data local inpath '/kkb/install/hivedatas/person.txt' overwrite into table person;\n\n查询所有数据数据\nhive > select * from person;\n\n按照下表索引进行查询\nhive > select work_locations[0] from person;\n\n查询所有集合数据\nhive  > select work_locations from person; \n\n查询元素个数\nhive >  select size(work_locations) from person;   \n```\n\n#### 7、复杂类型长度统计函数\n\n##### 1.Map类型长度函数: size(Map<k .V>)\n\n**语法**: size(Map<k .V>)\n **返回值**: int\n **说明**: 返回map类型的长度\n\n```\nhive> select size(t) from map_table2;\n2\n```\n\n##### 2.array类型长度函数: size(Array<T>)\n\n**语法**: size(Array<T>)\n **返回值**: int\n **说明**: 返回array类型的长度\n\n```\nhive> select size(t) from arr_table2;\n4\n\n\n```\n\n##### 3.类型转换函数  \n\n**类型转换函数**: cast\n **语法**: cast(expr as <type>)\n **返回值**: Expected \"=\" to follow \"type\"\n **说明**: 返回转换后的数据类型\n\n```\nhive> select cast('1' as bigint) from tableName;\n1\n\n```\n\n#### 8、hive当中的lateral view 与 explode以及reflect和分析函数\n\n##### 1、使用explode函数将hive表中的Map和Array字段数据进行拆分\n\nlateral view用于和split、explode等UDTF一起使用的，能将一行数据拆分成多行数据，在此基础上可以对拆分的数据进行聚合，lateral view首先为原始表的每行调用UDTF，UDTF会把一行拆分成一行或者多行，lateral view在把结果组合，产生一个支持别名表的虚拟表。\n其中explode还可以用于将hive一列中复杂的array或者map结构拆分成多行\n\n```\n需求：现在有数据格式如下\nzhangsan\tchild1,child2,child3,child4\tk1:v1,k2:v2\nlisi\tchild5,child6,child7,child8\t k3:v3,k4:v4\n\n字段之间使用\\t分割，需求将所有的child进行拆开成为一列\n \n+----------+--+\n| mychild  |\n+----------+--+\n| child1   |\n| child2   |\n| child3   |\n| child4   |\n| child5   |\n| child6   |\n| child7   |\n| child8   |\n+----------+--+\n\n将map的key和value也进行拆开，成为如下结果\n\n+-----------+-------------+--+\n| mymapkey  | mymapvalue  |\n+-----------+-------------+--+\n| k1        | v1          |\n| k2        | v2          |\n| k3        | v3          |\n| k4        | v4          |\n+-----------+-------------+--+\n```\n\n###### 第一步：创建hive数据库\n\n创建hive数据库\n\n```\nhive (default)> create database hive_explode;\nhive (default)> use hive_explode;\n```\n\n###### 第二步：创建hive表，然后使用explode拆分map和array\n\n```\nhive (hive_explode)> create  table hive_explode.t3(name string,children array<string>,address Map<string,string>) row format delimited fields terminated by '\\t'  collection items    terminated by ','  map keys terminated by ':' stored as textFile;\n```\n\n###### 第三步：加载数据\n\nnode03执行以下命令创建表数据文件\n\n```\ncd  /kkb/install/hivedatas/\n\nvim maparray\n数据内容格式如下\n\nzhangsan\tchild1,child2,child3,child4\tk1:v1,k2:v2\nlisi\tchild5,child6,child7,child8\tk3:v3,k4:v4\n```\n\nhive表当中加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kkb/install/hivedatas/maparray' into table hive_explode.t3;\n\n```\n\n###### 第四步：使用explode将hive当中数据拆开\n\n将array当中的数据拆分开\n\n```\nhive (hive_explode)> SELECT explode(children) AS myChild FROM hive_explode.t3;\n\n```\n\n将map当中的数据拆分开\n\n```\nhive (hive_explode)> SELECT explode(address) AS (myMapKey, myMapValue) FROM hive_explode.t3;\n\n```\n\n##### 2、使用explode拆分json字符串\n\n需求：现在有一些数据格式如下：\n\n```\na:shandong,b:beijing,c:hebei|1,2,3,4,5,6,7,8,9|[{\"source\":\"7fresh\",\"monthSales\":4900,\"userCount\":1900,\"score\":\"9.9\"},{\"source\":\"jd\",\"monthSales\":2090,\"userCount\":78981,\"score\":\"9.8\"},{\"source\":\"jdmart\",\"monthSales\":6987,\"userCount\":1600,\"score\":\"9.0\"}]\n\n```\n\n其中字段与字段之间的分隔符是 | \n\n我们要解析得到所有的monthSales对应的值为以下这一列（行转列）\n\n```\n4900\n2090\n6987\n\n```\n\n###### 第一步：创建hive表\n\n```\nhive (hive_explode)> create table hive_explode.explode_lateral_view  (area string, goods_id string, sale_info string)  ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS textfile;\n```\n\n###### 第二步：准备数据并加载数据\n\n准备数据如下\n\n```\ncd /kkb/install/hivedatas\nvim explode_json\n\na:shandong,b:beijing,c:hebei|1,2,3,4,5,6,7,8,9|[{\"source\":\"7fresh\",\"monthSales\":4900,\"userCount\":1900,\"score\":\"9.9\"},{\"source\":\"jd\",\"monthSales\":2090,\"userCount\":78981,\"score\":\"9.8\"},{\"source\":\"jdmart\",\"monthSales\":6987,\"userCount\":1600,\"score\":\"9.0\"}]\n\n \n\n```\n\n加载数据到hive表当中去\n\n```\nhive (hive_explode)> load data local inpath '/kkb/install/hivedatas/explode_json' overwrite into table hive_explode.explode_lateral_view;\n```\n\n###### 第三步：使用explode拆分Array\n\n```\nhive (hive_explode)> select explode(split(goods_id,',')) as goods_id from hive_explode.explode_lateral_view;\n```\n\n \n\n###### 第四步：使用explode拆解Map\n\n```\nhive (hive_explode)> select explode(split(area,',')) as area from hive_explode.explode_lateral_view;\n```\n\n\n\n###### 第五步：拆解json字段\n\n```\nhive (hive_explode)> select explode(split(regexp_replace(regexp_replace(sale_info,'\\\\[\\\\{',''),'}]',''),'},\\\\{')) as  sale_info from hive_explode.explode_lateral_view;\n```\n\n \n\n然后我们想用get_json_object来获取key为monthSales的数据：\n\n```\nhive (hive_explode)> select get_json_object(explode(split(regexp_replace(regexp_replace(sale_info,'\\\\[\\\\{',''),'}]',''),'},\\\\{')),'$.monthSales') as  sale_info from hive_explode.explode_lateral_view;\n\n\n然后出现异常FAILED: SemanticException [Error 10081]: UDTF's are not supported outside the SELECT clause, nor nested in expressions\nUDTF explode不能写在别的函数内\n如果你这么写，想查两个字段，select explode(split(area,',')) as area,good_id from explode_lateral_view;\n会报错FAILED: SemanticException 1:40 Only a single expression in the SELECT clause is supported with UDTF's. Error encountered near token 'good_id'\n使用UDTF的时候，只支持一个字段，这时候就需要LATERAL VIEW出场了\n```\n\n##### 3、配合LATERAL  VIEW使用\n\n配合lateral view查询多个字段\n\n```\nhive (hive_explode)> select goods_id2,sale_info from explode_lateral_view LATERAL VIEW explode(split(goods_id,','))goods as goods_id2;\n\n```\n\n其中LATERAL VIEW explode(split(goods_id,','))goods相当于一个虚拟表，与原表explode_lateral_view笛卡尔积关联。\n\n也可以多重使用\n\n```\nhive (hive_explode)> select goods_id2,sale_info,area2 from explode_lateral_view  LATERAL VIEW explode(split(goods_id,','))goods as goods_id2 LATERAL VIEW explode(split(area,','))area as area2;\n\n```\n\n也是三个表笛卡尔积的结果\n\n最终，我们可以通过下面的句子，把这个json格式的一行数据，完全转换成二维表的方式展现\n\n```\nhive (hive_explode)> select get_json_object(concat('{',sale_info_1,'}'),'$.source') as source, get_json_object(concat('{',sale_info_1,'}'),'$.monthSales') as monthSales, get_json_object(concat('{',sale_info_1,'}'),'$.userCount') as monthSales,  get_json_object(concat('{',sale_info_1,'}'),'$.score') as monthSales from explode_lateral_view   LATERAL VIEW explode(split(regexp_replace(regexp_replace(sale_info,'\\\\[\\\\{',''),'}]',''),'},\\\\{'))sale_info as sale_info_1;\n\n```\n\n总结：\n\nLateral View通常和UDTF一起出现，为了解决UDTF不允许在select字段的问题。 \n Multiple Lateral View可以实现类似笛卡尔乘积。 \n Outer关键字可以把不输出的UDTF的空结果，输出成NULL，防止丢失数据。\n\n\n\n#### 9、列转行\n\n##### 1．相关函数说明\n\nCONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;\n\nCONCAT_WS(separator, str1, str2,...)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;\n\nCOLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。\n\n##### 2．数据准备\n\n表6-6 数据准备\n\n| name   | constellation | blood_type |\n| ------ | ------------- | ---------- |\n| 孙悟空 | 白羊座        | A          |\n| 老王   | 射手座        | A          |\n| 宋宋   | 白羊座        | B          |\n| 猪八戒 | 白羊座        | A          |\n| 冰冰   | 射手座        | A          |\n\n##### 3．需求\n\n把星座和血型一样的人归类到一起。结果如下：\n\n```\n射手座,A            老王|冰冰\n白羊座,A            孙悟空|猪八戒\n白羊座,B            宋宋\n\n```\n\n\n\n##### 4．创建本地constellation.txt，导入数据\n\nnode03服务器执行以下命令创建文件，注意数据使用\\t进行分割\n\n```\ncd /kkb/install/hivedatas\nvim constellation.txt\n```\n\n```\n孙悟空\t白羊座\tA\n老王\t射手座\tA\n宋宋\t白羊座\tB       \n猪八戒\t白羊座\tA\n凤姐\t射手座\tA\n```\n\n##### 5．创建hive表并导入数据\n\n创建hive表并加载数据\n\n```\nhive (hive_explode)> create table person_info(  name string,  constellation string,  blood_type string)  row format delimited fields terminated by \"\\t\";\n```\n\n加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kkb/install/hivedatas/constellation.txt' into table person_info;\n```\n\n##### 6．按需求查询数据\n\n```\nhive (hive_explode)> select t1.base, concat_ws('|', collect_set(t1.name)) name from    (select name, concat(constellation, \",\" , blood_type) base from person_info) t1 group by  t1.base;\n```\n\n\n\n#### 10、行转列\n\n##### 1．函数说明\n\nEXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。\n\nLATERAL VIEW\n\n用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias\n\n解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。\n\n##### 2．数据准备\n\n数据内容如下，字段之间都是使用\\t进行分割\n\n```\ncd /kkb/install/hivedatas\n\nvim movie.txt\n《疑犯追踪》\t悬疑,动作,科幻,剧情\n《Lie to me》\t悬疑,警匪,动作,心理,剧情\n《战狼2》\t战争,动作,灾难\n```\n\n\n\n##### 3．需求\n\n将电影分类中的数组数据展开。结果如下：\n\n```\n《疑犯追踪》\t悬疑\n《疑犯追踪》\t动作\n《疑犯追踪》\t科幻\n《疑犯追踪》\t剧情\n《Lie to me》\t悬疑\n《Lie to me》\t警匪\n《Lie to me》\t动作\n《Lie to me》\t心理\n《Lie to me》\t剧情\n《战狼2》\t战争\n《战狼2》\t动作\n《战狼2》\t灾难\n```\n\n\n\n##### 4．创建hive表并导入数据\n\n创建hive表\n\n```\nhive (hive_explode)> create table movie_info(movie string, category array<string>) row format delimited fields terminated by \"\\t\" collection items terminated by \",\";\n```\n\n加载数据\n\n```\nload data local inpath \"/kkb/install/hivedatas/movie.txt\" into table movie_info;\n```\n\n \n\n##### 5．按需求查询数据\n\n```\nhive (hive_explode)>  select movie, category_name  from  movie_info lateral view explode(category) table_tmp as category_name;\n```\n\n#### 11、reflect函数\n\nreflect函数可以支持在sql中调用java中的自带函数，秒杀一切udf函数。\n\n##### 使用java.lang.Math当中的Max求两列中最大值\n\n创建hive表\n\n```\nhive (hive_explode)>  create table test_udf(col1 int,col2 int) row format delimited fields terminated by ',';\n```\n\n准备数据并加载数据\n\n```\ncd /kkb/install/hivedatas\n\nvim test_udf\n\n1,2\n4,3\n6,4\n7,5\n5,6\n```\n\n加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kkb/install/hivedatas/test_udf' overwrite into table test_udf;\n```\n\n使用java.lang.Math当中的Max求两列当中的最大值\n\n```\nhive (hive_explode)> select reflect(\"java.lang.Math\",\"max\",col1,col2) from test_udf;\n```\n\n##### 不同记录执行不同的java内置函数\n\n创建hive表\n\n```\nhive (hive_explode)> create table test_udf2(class_name string,method_name string,col1 int , col2 int) row format delimited fields terminated by ',';\n```\n\n准备数据\n\n```\ncd /export/servers/hivedatas\n\nvim test_udf2\n\njava.lang.Math,min,1,2\njava.lang.Math,max,2,3\n\n```\n\n加载数据\n\n```\nhive (hive_explode)> load data local inpath '/kkb/install/hivedatas/test_udf2' overwrite into table test_udf2;\n```\n\n执行查询\n\n```\nhive (hive_explode)> select reflect(class_name,method_name,col1,col2) from test_udf2;\n```\n\n##### 判断是否为数字\n\n使用apache commons中的函数，commons下的jar已经包含在hadoop的classpath中，所以可以直接使用。\n\n使用方式如下：\n\n```\nhive (hive_explode)> select reflect(\"org.apache.commons.lang.math.NumberUtils\",\"isNumber\",\"123\");\n```\n\n\n\n#### 12、hive当中的分析函数—分组求topN\n\n##### 1、分析函数的作用介绍\n\n对于一些比较复杂的数据求取过程，我们可能就要用到分析函数，分析函数主要用于分组求topN，或者求取百分比，或者进行数据的切片等等，我们都可以使用分析函数来解决\n\n\n\n##### 2、常用的分析函数介绍\n\n1、ROW_NUMBER()：\n\n从1开始，按照顺序，生成分组内记录的序列,比如，按照pv降序排列，生成分组内每天的pv名次,ROW_NUMBER()的应用场景非常多，再比如，获取分组内排序第一的记录;获取一个session中的第一条refer等。 \n\n2、RANK() ：\n\n生成数据项在分组中的排名，排名相等会在名次中留下空位 \n\n3、DENSE_RANK() ：\n\n生成数据项在分组中的排名，排名相等会在名次中不会留下空位 \n\n4、CUME_DIST ：\n\n小于等于当前值的行数/分组内总行数。比如，统计小于等于当前薪水的人数，所占总人数的比例 \n\n5、PERCENT_RANK ：\n\n分组内当前行的RANK值/分组内总行数\n\n6、NTILE(n) ：\n\n用于将分组数据按照顺序切分成n片，返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。NTILE不支持ROWS BETWEEN，比如 NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)。\n\n##### 3、需求描述\n\n现有数据内容格式如下，分别对应三个字段，cookieid，createtime ，pv，求取每个cookie访问pv前三名的数据记录，其实就是分组求topN，求取每组当中的前三个值\n\n```\ncookie1,2015-04-10,1\ncookie1,2015-04-11,5\ncookie1,2015-04-12,7\ncookie1,2015-04-13,3\ncookie1,2015-04-14,2\ncookie1,2015-04-15,4\ncookie1,2015-04-16,4\ncookie2,2015-04-10,2\ncookie2,2015-04-11,3\ncookie2,2015-04-12,5\ncookie2,2015-04-13,6\ncookie2,2015-04-14,3\ncookie2,2015-04-15,9\ncookie2,2015-04-16,7\n```\n\n###### 第一步：创建数据库表\n\n在hive当中创建数据库表\n\n```\nCREATE EXTERNAL TABLE cookie_pv (\ncookieid string,\ncreatetime string, \npv INT\n) ROW FORMAT DELIMITED \nFIELDS TERMINATED BY ',' ;\n```\n\n###### 第二步：准备数据并加载\n\nnode03执行以下命令，创建数据，并加载到hive表当中去\n\n```\ncd /kkb/install/hivedatas\nvim cookiepv.txt\n\ncookie1,2015-04-10,1\ncookie1,2015-04-11,5\ncookie1,2015-04-12,7\ncookie1,2015-04-13,3\ncookie1,2015-04-14,2\ncookie1,2015-04-15,4\ncookie1,2015-04-16,4\ncookie2,2015-04-10,2\ncookie2,2015-04-11,3\ncookie2,2015-04-12,5\ncookie2,2015-04-13,6\ncookie2,2015-04-14,3\ncookie2,2015-04-15,9\ncookie2,2015-04-16,7\n```\n\n加载数据到hive表当中去\n\n```\nload  data  local inpath '/kkb/install/hivedatas/cookiepv.txt'  overwrite into table  cookie_pv \n```\n\n###### 第三步：使用分析函数来求取每个cookie访问PV的前三条记录\n\n```sql\nSELECT * FROM (\n\tSELECT \n  cookieid,\n  createtime,\n  pv,\n  RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn1,\n  DENSE_RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn2,\n  ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY pv DESC) AS rn3 \n  FROM cookie_pv) temp \nWHERE temp.rn1 <=  3 ;\n```\n\n\n\n\n\n#### 13、hive自定义函数\n\n##### 1、自定义函数的基本介绍\n\n1）Hive 自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。\n\n2）当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。\n\n3）根据用户自定义函数类别分为以下三种：\n\n        （1）UDF（User-Defined-Function）\n    \n                一进一出\n    \n        （2）UDAF（User-Defined Aggregation Function）\n    \n                聚集函数，多进一出\n    \n                类似于：count/max/min\n    \n        （3）UDTF（User-Defined Table-Generating Functions）\n    \n                一进多出\n    \n                如lateral view explode()\n\n4）官方文档地址\n\nhttps://cwiki.apache.org/confluence/display/Hive/HivePlugins\n\n5）编程步骤：\n\n        （1）继承org.apache.hadoop.hive.ql.UDF\n    \n        （2）需要实现evaluate函数；evaluate函数支持重载；\n\n6）注意事项\n\n        （1）UDF必须要有返回类型，可以返回null，但是返回类型不能为void；\n    \n        （2）UDF中常用Text/LongWritable等类型，不推荐使用java类型；\n\n \n\n\n\n\n#####  2、自定义函数开发\n\n###### 第一步：创建maven java 工程，并导入jar包\n\n```\n<repositories>\n    <repository>\n        <id>cloudera</id>\n <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n    </repository>\n</repositories>\n<dependencies>\n    <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-common</artifactId>\n        <version>2.6.0-cdh5.14.2</version>\n    </dependency>\n    <dependency>\n        <groupId>org.apache.hive</groupId>\n        <artifactId>hive-exec</artifactId>\n        <version>1.1.0-cdh5.14.2</version>\n    </dependency>\n</dependencies>\n<build>\n<plugins>\n    <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>3.0</version>\n        <configuration>\n            <source>1.8</source>\n            <target>1.8</target>\n            <encoding>UTF-8</encoding>\n        </configuration>\n    </plugin>\n     <plugin>\n         <groupId>org.apache.maven.plugins</groupId>\n         <artifactId>maven-shade-plugin</artifactId>\n         <version>2.2</version>\n         <executions>\n             <execution>\n                 <phase>package</phase>\n                 <goals>\n                     <goal>shade</goal>\n                 </goals>\n                 <configuration>\n                     <filters>\n                         <filter>\n                             <artifact>*:*</artifact>\n                             <excludes>\n                                 <exclude>META-INF/*.SF</exclude>\n                                 <exclude>META-INF/*.DSA</exclude>\n                                 <exclude>META-INF/*/RSA</exclude>\n                             </excludes>\n                         </filter>\n                     </filters>\n                 </configuration>\n             </execution>\n         </executions>\n     </plugin>\n</plugins>\n</build>\n```\n\n \n\n \n\n###### 第二步：开发java类继承UDF，并重载evaluate 方法\n\n```\npublic class MyUDF extends UDF {\n     public Text evaluate(final Text s) {\n         if (null == s) {\n             return null;\n         }\n         //**返回大写字母         return new Text(s.toString().toUpperCase());\n     }\n }\n```\n\n###### 第三步：将我们的项目打包，并上传到hive的lib目录下\n\n使用maven的package进行打包，将我们打包好的jar包上传到node03服务器的/kkb/install/hive-1.1.0-cdh5.14.2/lib 这个路径下\n\n###### 第四步：添加我们的jar包\n\n重命名我们的jar包名称\n\n```\ncd /kkb/install/hive-1.1.0-cdh5.14.2/lib\nmv original-day_hive_udf-1.0-SNAPSHOT.jar udf.jar\n```\n\nhive的客户端添加我们的jar包\n\n```\n0: jdbc:hive2://node03:10000> add jar /kkb/install/hive-1.1.0-cdh5.14.2/lib/udf.jar;\n```\n\n \n\n###### 第五步：设置函数与我们的自定义函数关联\n\n```\n0: jdbc:hive2://node03:10000> create temporary function tolowercase as 'com.kkb.udf.MyUDF';\n```\n\n###### 第六步：使用自定义函数\n\n```\n0: jdbc:hive2://node03:10000>select tolowercase('abc');\n```\n\n \n\nhive当中如何创建永久函数\n\n在hive当中添加临时函数，需要我们每次进入hive客户端的时候都需要添加以下，退出hive客户端临时函数就会失效，那么我们也可以创建永久函数来让其不会失效\n\n创建永久函数\n\n```\n1、指定数据库，将我们的函数创建到指定的数据库下面\n0: jdbc:hive2://node03:10000>use myhive;\n\n2、使用add jar添加我们的jar包到hive当中来\n0: jdbc:hive2://node03:10000>add jar /kkb/install/hive-1.1.0-cdh5.14.2/lib/udf.jar;\n\n3、查看我们添加的所有的jar包\n0: jdbc:hive2://node03:10000>list  jars;\n\n4、创建永久函数，与我们的函数进行关联\n0: jdbc:hive2://node03:10000>create  function myuppercase as 'com.kkb.udf.MyUDF';\n\n5、查看我们的永久函数\n0: jdbc:hive2://node03:10000>show functions like 'my*';\n\n6、使用永久函数\n0: jdbc:hive2://node03:10000>select myhive.myuppercase('helloworld');\n\n7、删除永久函数\n0: jdbc:hive2://node03:10000>drop function myhive.myuppercase;\n\n8、查看函数\n show functions like 'my*';\n```\n\n\n\n### 3. hive表的数据压缩 \n\n#### 1、数据的压缩说明\n\n- 压缩模式评价\n  - 可使用以下三种标准对压缩方式进行评价\n    - 1、压缩比：压缩比越高，压缩后文件越小，所以压缩比越高越好\n    - 2、压缩时间：越快越好\n    - 3、已经压缩的格式文件是否可以再分割：可以分割的格式允许单一文件由多个Mapper程序处理，可以更好的并行化\n\n- 常见压缩格式\n\n| 压缩方式 | 压缩比 | 压缩速度 | 解压缩速度 | 是否可分割 |\n| :------: | :----: | :------: | :--------: | :--------: |\n|   gzip   | 13.4%  | 21 MB/s  |  118 MB/s  |     否     |\n|  bzip2   | 13.2%  | 2.4MB/s  |  9.5MB/s   |     是     |\n|   lzo    | 20.5%  | 135 MB/s |  410 MB/s  |     是     |\n|  snappy  | 22.2%  | 172 MB/s |  409 MB/s  |     否     |\n\n- Hadoop编码/解码器方式\n\n| 压缩格式 |             对应的编码/解码器              |\n| :------: | :----------------------------------------: |\n| DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |\n|   Gzip   |  org.apache.hadoop.io.compress.GzipCodec   |\n|  BZip2   |  org.apache.hadoop.io.compress.BZip2Codec  |\n|   LZO    |     com.hadoop.compress.lzo.LzopCodec      |\n|  Snappy  | org.apache.hadoop.io.compress.SnappyCodec  |\n\n \t压缩性能的比较\n\n| 压缩算法 | 原始文件大小 | 压缩文件大小 | 压缩速度 | 解压速度 |\n| -------- | ------------ | ------------ | -------- | -------- |\n| gzip     | 8.3GB        | 1.8GB        | 17.5MB/s | 58MB/s   |\n| bzip2    | 8.3GB        | 1.1GB        | 2.4MB/s  | 9.5MB/s  |\n| LZO      | 8.3GB        | 2.9GB        | 49.3MB/s | 74.6MB/s |\n\nhttp://google.github.io/snappy/\n\nOn a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.\n\n#### 2、压缩配置参数\n\n要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：\n\n| 参数                                                 | 默认值                                                       | 阶段        | 建议                                         |\n| ---------------------------------------------------- | ------------------------------------------------------------ | ----------- | -------------------------------------------- |\n| io.compression.codecs      （在core-site.xml中配置） | org.apache.hadoop.io.compress.DefaultCodec,   org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,   org.apache.hadoop.io.compress.Lz4Codec | 输入压缩    | Hadoop使用文件扩展名判断是否支持某种编解码器 |\n| mapreduce.map.output.compress                        | false                                                        | mapper输出  | 这个参数设为true启用压缩                     |\n| mapreduce.map.output.compress.codec                  | org.apache.hadoop.io.compress.DefaultCodec                   | mapper输出  | 使用LZO、LZ4或snappy编解码器在此阶段压缩数据 |\n| mapreduce.output.fileoutputformat.compress           | false                                                        | reducer输出 | 这个参数设为true启用压缩                     |\n| mapreduce.output.fileoutputformat.compress.codec     | org.apache.hadoop.io.compress. DefaultCodec                  | reducer输出 | 使用标准工具或者编解码器，如gzip和bzip2      |\n| mapreduce.output.fileoutputformat.compress.type      | RECORD                                                       | reducer输出 | SequenceFile输出使用的压缩类型：NONE和BLOCK  |\n\n#### 3、开启Map输出阶段压缩\n\n开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：\n\n**案例实操：**\n\n```\n1）开启hive中间传输数据压缩功能\nhive (default)>set hive.exec.compress.intermediate=true;\n\n2）开启mapreduce中map输出压缩功能\nhive (default)>set mapreduce.map.output.compress=true;\n\n3）设置mapreduce中map输出数据的压缩方式\nhive (default)>set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec;\n\n4）执行查询语句\n   select count(1) from score;\n```\n\n\n\n#### 4、 开启Reduce输出阶段压缩\n\n当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。\n\n**案例实操：**\n\n```\n1）开启hive最终输出数据压缩功能\nhive (default)>set hive.exec.compress.output=true;\n\n2）开启mapreduce最终输出数据压缩\nhive (default)>set mapreduce.output.fileoutputformat.compress=true;\n\n3）设置mapreduce最终数据输出压缩方式\nhive (default)> set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;\n\n4）设置mapreduce最终数据输出压缩为块压缩\nhive (default)>set mapreduce.output.fileoutputformat.compress.type=BLOCK;\n\n5）测试一下输出结果是否是压缩文件\ninsert overwrite local directory '/kkb/install/hivedatas/snappy' select * from score distribute by s_id sort by s_id desc;\n```\n\n","tags":["hadoop","hive","function"]},{"title":"Hadoop之数据分析Hive（一）","url":"/2019/11/01/it/hive/Hadoop之数据分析Hive（一）/","content":"\n### 1.数据仓库的基本概念\n\n#### 1.数据仓库的基本概念\n\n英文名称为Data Warehouse，可简写为DW或DWH。数据仓库的目的是构建面向分析的集成化数据环境，为企业提供决策支持（Decision Support）。它出于分析性报告和决策支持目的而创建。\n\n数据仓库本身并不“生产”任何数据，同时自身也不需要“消费”任何的数据，数据来源于外部，并且开放给外部应用，这也是为什么叫“仓库”，而不叫“工厂”的原因。\n\n#### 2.数据仓库的主要特征\n\n数据仓库是面向主题的（Subject-Oriented）、集成的（Integrated）、非易失的（Non-Volatile）和时变的（Time-Variant ）数据集合，用以支持管理决策。\n\n#### 3. 数据仓库与数据库区别 \n\n数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别。 \n\n操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing，），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。 \n\n分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持管理决策。\n\n首先要明白，数据仓库的出现，并不是要取代数据库。\n\n数据库是面向事务的设计，数据仓库是面向主题设计的。\n\n数据库一般存储业务数据，数据仓库存储的一般是历史数据。\n\n数据库设计是尽量避免冗余，一般针对某一业务应用进行设计，比如一张简单的User表，记录用户名、密码等简单数据即可，符合业务应用，但是不符合分析。数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计。\n\n数据库是为捕获数据而设计，数据仓库是为分析数据而设计。\n\n以银行业务为例。数据库是事务系统的数据平台，客户在银行做的每笔交易都会写入数据库，被记录下来，这里，可以简单地理解为用数据库记账。数据仓库是分析系统的数据平台，它从事务系统获取数据，并做汇总、加工，为决策者提供决策的依据。比如，某银行某分行一个月发生多少交易，该分行当前存款余额是多少。如果存款又多，消费交易又多，那么该地区就有必要设立ATM了。 \n\n显然，银行的交易量是巨大的，通常以百万甚至千万次来计算。事务系统是实时的，这就要求时效性，客户存一笔钱需要几十秒是无法忍受的，这就要求数据库只能存储很短一段时间的数据。而分析系统是事后的，它要提供关注时间段内所有的有效数据。这些数据是海量的，汇总计算起来也要慢一些，但是，只要能够提供有效的分析数据就达到目的了。 \n\n数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它决不是所谓的“大型数据库”。\n\n#### 4.数据仓库分层架构\n\n按照数据流入流出的过程，数据仓库架构可分为三层——**源数据**、**数据仓库**、**数据应用。**                            \n\n数据仓库的数据来源于不同的源数据，并提供多样的数据应用，数据自下而上流入数据仓库后向上层开放应用，而数据仓库只是中间集成化数据管理的一个平台。\n\n源数据层（ODS）：此层数据无任何更改，直接沿用外围系统数据结构和数据，不对外开放；为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。\n\n数据仓库层（DW）：也称为细节层，DW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。\n\n数据应用层（DA或APP）：前端应用直接读取的数据源；根据报表、专题分析需求而计算生成的数据。\n\n数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。\n\n为什么要对数据仓库分层？\n\n用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。\n\n通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。\n\n### 2. Hive是什么\n\n#### 1 hive的概念\n\nHive是基于Hadoop的一个数据仓库工具，==可以将结构化的数据文件映射为一张数据库表==，并提供类SQL查询功能。其本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据的存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更进一步可以说hive就是一个MapReduce的客户端\n\n![](/Users/dingchuangshi/Downloads/hive_day01_课前资料/hive_day01课程设计.assets/Snipaste_2019-07-10_23-23-31.png)\n\n\n\n#### 2 Hive与数据库的区别\n\n![](/Users/dingchuangshi/Downloads/hive_day01_课前资料/hive_day01课程设计.assets/2018040319335283.png)\n\n* Hive 具有 SQL 数据库的外表，但应用场景完全不同。\n* ==Hive 只适合用来做海量离线数据统计分析，也就是数据仓库==。\n\n\n\n#### 3 Hive的优缺点\n\n\n\n* ==优点==\n  * **操作接口采用类SQL语法**，提供快速开发的能力（简单、容易上手）。\n\n  * **避免了去写MapReduce**，减少开发人员的学习成本。\n\n  * **Hive支持用户自定义函数**，用户可以根据自己的需求来实现自己的函数。\n\n* ==缺点==\n  * **Hive 不支持记录级别的增删改操作**\n  * **Hive 的查询延迟很严重**\n    * hadoop jar  xxxx.jar  xxx.class /input /output\n      * 进行任务的划分，然后进行计算资源的申请\n      * map 0%  reduce 0%\n      * map 10%  reduce 0%\n  * **Hive 不支持事务**\n\n\n\n#### 4 Hive架构原理\n\n\n\n![](assets/2019-07-11_11-08-35.png)\n\n* 1、用户接口：Client\n\n- CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）\n\n* 2、元数据：Metastore\n  * 元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；\n\n    * 默认存储在自带的derby数据库中，==推荐使用MySQL存储Metastore==\n\n* 3、Hadoop集群\n  * 使用HDFS进行存储，使用MapReduce进行计算。\n\n* 4、Driver：驱动器\n  * 解析器（SQL Parser） \n    * 将SQL字符串转换成抽象语法树AST\n    * 对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误\n  * 编译器（Physical Plan）：将AST编译生成逻辑执行计划\n  * 优化器（Query Optimizer）：对逻辑执行计划进行优化\n  * 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说默认就是mapreduce任务\n\n\n\n![hive1](assets/hive1.png)\n\n\n\n\n\n### 3. Hive的安装部署\n\n​\t[hive安装部署](https://kfly.top/2019/11/26/hadoop/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/)\t\n\n### 4. hive的交互方式 \n\n* ==先启动hadoop集群和mysql服务==\n\n#### 1 Hive交互shell\n\n~~~shell\ncd /kkb/install/hive-1.1.0-cdh5.14.2\nbin/hive\n~~~\n\n#### 2 Hive JDBC服务\n\n* 启动hiveserver2服务\n\n  * 前台启动\n\n    ~~~shell\n    cd /kkb/install/hive-1.1.0-cdh5.14.2\n    bin/hive --service hiveserver2\n    ~~~\n\n  * 后台启动\n\n  ~~~shell\n  cd /kkb/install/hive-1.1.0-cdh5.14.2\n  nohup  bin/hive --service hiveserver2  &\n  ~~~\n\n* beeline连接hiveserver2\n\n  重新开启一个会话窗口，然后使用beeline连接hive\n\n  ~~~shell\n  cd /kkb/install/hive-1.1.0-cdh5.14.2\n  bin/beeline\n  beeline> !connect jdbc:hive2://node03:10000\n  ~~~\n\n\n#### 3  Hive的命令\n\n* hive  ==-e== sql语句\n  * 使用 –e  参数来直接执行hql的语句\n\n~~~\ncd /kkb/install/hive-1.1.0-cdh5.14.2/\nbin/hive -e \"show databases\"\n~~~\n\n* hive  ==-f==  sql文件\n\n  * 使用 –f  参数执行包含hql语句的文件\n\n  * node03执行以下命令准备hive执行脚本\n\n  * ```\n    cd /kkb/install/\n    vim hive.sql\n    \n    文件内容如下\n    create database if not exists myhive;\n    \n    通过以下命令来执行我们的hive脚本\n    cd /kkb/install/hive-1.1.0-cdh5.14.2/\n    bin/hive -f /kkb/install/hive.sql \n    ```\n\n### 5、Hive的数据类型\n\n#### 1 基本数据类型\n\n\n\n|    类型名称    |              描述               |    举例    |\n| :------------: | :-----------------------------: | :--------: |\n|    boolean     |           true/false            |    true    |\n|    tinyint     |        1字节的有符号整数        |     1      |\n|    smallint    |        2字节的有符号整数        |     1      |\n|  ==**int**==   |        4字节的有符号整数        |     1      |\n| **==bigint==** |        8字节的有符号整数        |     1      |\n|     float      |        4字节单精度浮点数        |    1.0     |\n| **==double==** |        8字节单精度浮点数        |    1.0     |\n| **==string==** |        字符串(不设长度)         |   “abc”    |\n|    varchar     | 字符串（1-65355长度，超长截断） |   “abc”    |\n|   timestamp    |             时间戳              | 1563157873 |\n|      date      |              日期               |  20190715  |\n\n\n\n#### 2 复合数据类型\n\n| 类型名称 |                         描述                          |       举例        |\n| :------: | :---------------------------------------------------: | :---------------: |\n|  array   | 一组有序的字段，字段类型必须相同 array(元素1，元素2)  |  Array（1,2,3）   |\n|   map    |           一组无序的键值对 map(k1,v1,k2,v2)           | Map(‘a’,1,'b',2)  |\n|  struct  | 一组命名的字段，字段类型可以不同 struct(元素1，元素2) | Struct('a',1,2,0) |\n\n* array字段的元素访问方式：\n\n  * 下标获取元素，下标从0开始\n\n    * 获取第一个元素\n\n      * array[0]\n\n* map字段的元素访问方式\n\n  * 通过键获取值\n\n    * 获取a这个key对应的value\n\n      * map['a']\n\n\n* struct字段的元素获取方式\n  * 定义一个字段c的类型为struct{a int;b string}\n    * 获取a和b的值\n      * 使用c.a 和c.b 获取其中的元素值\n        * 这里可以把这种类型看成是一个对象\n\n~~~sql\ncreate table complex(\n         col1 array<int>,\n         col2 map<string,int>,\n         col3 struct<a:string,b:int,c:double>\n)\n\n~~~\n\n\n\n### 6、Hive的DDL操作\n\n#### 1 hive的数据库DDL操作\n\n##### 1、创建数据库\n\n```sql\nhive > create database db_hive;\n# 或者\nhive > create database if not exists db_hive;\n```\n\n- 数据库在HDFS上的默认存储路径是==/user/hive/warehouse/*.db==\n\n\n##### 2、显示所有数据库\n\n```sql\n  hive> show databases;\n```\n\n##### 3、查询数据库\t\n\n```sql\nhive> show databases like 'db_hive*';\n```\n\n##### 4、查看数据库详情\n\n```sql\nhive> desc database db_hive;\n```\n\n##### 5、显示数据库详细信息\n\n```sql\nhive> desc database extended db_hive;\n```\n\n##### 6、切换当前数据库\n\n```sql\nhive > use db_hive;\n```\n\n##### 7、删除数据库\n\n```sql\n#删除为空的数据库\nhive> drop database db_hive;\n\n#如果删除的数据库不存在，最好采用if exists 判断数据库是否存在\nhive> drop database if exists db_hive;\n\n#如果数据库中有表存在，这里需要使用cascade强制删除数据库\nhive> drop database if exists db_hive cascade;\n```\n\n#### 6.2 hive的表DDL操作\n\n##### 1 、建表语法介绍\n\n```sql\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name \n[(col_name data_type [COMMENT col_comment], ...)] \n[COMMENT table_comment] \n[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] 分区\n[CLUSTERED BY (col_name, col_name, ...) 分桶\n[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] \n[ROW FORMAT row_format]  row format delimited fields terminated by “分隔符”\n[STORED AS file_format] \n[LOCATION hdfs_path]\n```\n\n官网地址：<https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL>\n\n##### 2 、字段解释说明\n\n- create table \n\n  - 创建一个指定名字的表\n- EXTERNAL  \n\n  - 创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），指定表的数据保存在哪里\n- COMMENT\n\n  - 为表和列添加注释\n- PARTITIONED BY\n\n  - 创建分区表\n- CLUSTERED BY\n\n  - 创建分桶表\n- SORTED BY\n\n  - 按照字段排序（一般不常用）\n- ROW FORMAT\n  - 指定每一行中字段的分隔符\n\n    - row format delimited fields terminated by ‘\\t’\n\n* STORED AS\n  * 指定存储文件类型\n    - 常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）\n    - 如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE\n\n- LOCATION \n\n  - 指定表在HDFS上的存储位置。\n\n##### 3、 创建内部表\n\n- 1、==直接建表==\n  - 使用标准的建表语句\n\n```sql\nuse myhive;\ncreate table stu(id int,name string);\n\n可以通过insert  into  向hive表当中插入数据，但是不建议工作当中这么做\ninsert  into stu(id,name) values(1,\"zhangsan\");\nselect * from  stu;\n```\n\n- 2、==查询建表法==\n  - 通过AS 查询语句完成建表：将子查询的结果存在新表里，有数据 \n\n```sql\ncreate table if not exists myhive.stu1 as select id, name from stu;\n```\n\n- 3、==like建表法==\n  - 根据已经存在的表结构创建表\n\n```sql\ncreate table if not exists myhive.stu2 like stu;\n```\n\n- 4、查询表的类型\n\n```sql\nhive > desc formatted myhive.stu;\n```\n\n![2019-07-12_14-33-00](/Users/dingchuangshi/Downloads/hive_day01_课前资料/hive_day01课程设计.assets/2019-07-12_14-33-00-1563156555810.png)\n\n\n\n创建内部表并指定字段之间的分隔符，指定文件的存储格式，以及数据存放的位置\n\n```\ncreate  table if not exists myhive.stu3(id int ,name string)\nrow format delimited fields terminated by '\\t' stored as textfile location       '/user/stu2';\n```\n\n##### 4、 创建外部表\n\n外部表因为是指定其他的hdfs路径的数据加载到表当中来，所以hive表会认为自己不完全独占这份数据，所以删除hive表的时候，数据仍然存放在hdfs当中，不会删掉\n\n```\ncreate external table myhive.teacher (t_id string,t_name string) row format delimited fields terminated by '\\t';\n```\n\n- 创建外部表的时候需要加上**==external==** 关键字\n- location字段可以指定，也可以不指定\n  - 指定就是数据存放的具体目录\n  - 不指定就是使用默认目录 ==/user/hive/warehouse==\n\n![2019-07-12_14-51-53](assets/2019-07-12_14-51-53.png)\n\n\n\n向外部表当中加载数据：\n\n我们前面已经看到过通过insert的方式向内部表当中插入数据，外部表也可以通过insert的方式进行插入数据，只不过insert的方式，我们一般都不推荐，实际工作当中我们都是使用load的方式来加载数据到内部表或者外部表\n\nload数据可以从本地文件系统加载或者也可以从hdfs上面的数据进行加载\n\n- 从本地文件系统加载数据到teacher表当中去，==将我们附件当汇总的数据资料都上传到node03服务器的/kkb/install/hivedatas路径下面去==\n\n  ```\n  mkdir -p /kkb/install/hivedatas\n  #将数据都上传到/kkb/install/hivedatas路径下，然后在hive客户端下执行以下操作\n  load data local inpath '/kkb/install/hivedatas/teacher.csv' into table myhive.teacher;\n  ```\n\n- 从hdfs上面加载文件到teacher表里面去(将teacher.csv文件上传到==hdfs的/kkb/hdfsload/hivedatas==路径下)\n\n  ```shell\n  cd /kkb/install/hivedatas\n  hdfs dfs -mkdir -p /kkb/hdfsload/hivedatas\n  hdfs dfs -put teacher.csv /kkb/hdfsload/hivedatas\n  # 在hive的客户端当中执行\n  load  data  inpath  '/kkb/hdfsload/hivedatas'  overwrite into table myhive.teacher;\n  ```\n\n##### 5、 内部表与外部表的互相转换\n\n- 1、内部表转换为外部表\n\n```sql\n#将stu内部表改为外部表\nalter table stu set tblproperties('EXTERNAL'='TRUE');\n```\n\n- 2、外部表转换为内部表\n\n```sql\n#把emp外部表改为内部表\nalter table teacher set tblproperties('EXTERNAL'='FALSE');\n```\n\n##### 6、 内部表与外部表的区别\n\n- 1、建表语法的区别\n  - 外部表在创建的时候需要加上==external==关键字\n\n- 2、删除表之后的区别\n  - 内部表删除后，表的元数据和真实数据都被删除了\n- 外部表删除后，仅仅只是把该表的元数据删除了，真实数据还在，后期还是可以恢复出来\n\n##### 7、内部表与外部表的使用时机\n\n​\t内部表由于删除表的时候会同步删除HDFS的数据文件，所以确定如果一个表仅仅是你独占使用，其他人不适用的时候就可以创建内部表，如果一个表的文件数据，其他人也要使用，那么就创建外部表\n\n一般外部表都是用在数据仓库的ODS层\n\n内部表都是用在数据仓库的DW层\n\n##### 8、hive的分区表\n\n如果hive当中所有的数据都存入到一个文件夹下面，那么在使用MR计算程序的时候，读取一整个目录下面的所有文件来进行计算，就会变得特别慢，因为数据量太大了，实际工作当中一般都是计算前一天的数据，所以我们只需要将前一天的数据挑出来放到一个文件夹下面即可，专门去计算前一天的数据。这样就可以使用hive当中的分区表，通过分文件夹的形式，将每一天的数据都分成为一个文件夹，然后我们计算数据的时候，通过指定前一天的文件夹即可只计算前一天的数据。\n\n在大数据中，最常用的一种思想就是分治，我们可以把大的文件切割划分成一个个的小的文件，这样每次操作一个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每天，或者每小时进行切分成一个个的小的文件，这样去操作小的文件就会容易得多了\n\n\n\n![2019-07-15_11-35-37](assets/2019-07-15_11-35-37.png)\n\n\n\n```\n在文件系统上建立文件夹，把表的数据放在不同文件夹下面，加快查询速度。\n```\n\n创建分区表语法\n\n```\nhive (myhive)> create table score(s_id string,c_id string, s_score int) partitioned by (month string) row format delimited fields terminated by '\\t';\n```\n\n创建一个表带多个分区\n\n```\nhive (myhive)> create table score2 (s_id string,c_id string, s_score int) partitioned by (year string,month string,day string) row format delimited fields terminated by '\\t';\n```\n\n 加载数据到分区表当中去\n\n```\n hive (myhive)>load data  local inpath '/kkb/install/hivedatas/score.csv' into table score partition  (month='201806');\n```\n\n加载数据到多分区表当中去\n\n```\nhive (myhive)> load data local inpath '/kkb/install/hivedatas/score.csv' into table score2 partition(year='2018',month='06',day='01');\n```\n\n查看分区\n\n```\nhive (myhive)> show  partitions  score;\n```\n\n添加一个分区\n\n```\nhive (myhive)> alter table score add partition(month='201805');\n```\n\n同时添加多个分区\n\n```\nhive (myhive)> alter table score add partition(month='201804') partition(month = '201803');\n```\n\n注意：添加分区之后就可以在hdfs文件系统当中看到表下面多了一个文件夹\n\n删除分区\n\n```\nhive (myhive)> alter table score drop partition(month = '201806');\n```\n\n\n\n外部分区表综合练习：\n\n需求描述：现在有一个文件score.csv文件，里面有三个字段，分别是s_id string, c_id string,s_score int，字段都是使用 \\t进行分割，存放在集群的这个目录下/scoredatas/day=20180607，这个文件每天都会生成，存放到对应的日期文件夹下面去，文件别人也需要公用，不能移动。需求，创建hive对应的表，并将数据加载到表中，进行数据统计分析，且删除表之后，数据不能删除\n\n需求实现:\n\n数据准备:\n\nnode03执行以下命令，将数据上传到hdfs上面去\n\n将我们的score.csv上传到node03服务器的/kkb/install/hivedatas目录下，然后将score.csv文件上传到/kkb/install/hivedatas目录下去\n\n```\ncd /kkb/install/hivedatas/\nhdfs dfs -mkdir -p /scoredatas/day=20180607\nhdfs dfs -put score.csv /scoredatas/day=20180607/\n```\n\n创建外部分区表，并指定文件数据存放目录\n\n```\nhive (myhive)> create external table score4(s_id string, c_id string,s_score int) partitioned by (day string) row format delimited fields terminated by '\\t' location '/scoredatas';\n```\n\n进行表的修复,说白了就是建立我们表与我们数据文件之间的一个关系映射\n\n```\nhive (myhive)> msck  repair   table  score4;\n```\n\n修复成功之后即可看到数据已经全部加载到表当中去了\n\n##### 9、hive的分桶表\n\n![](assets/2019-07-16_17-01-51.png)\n\n- 分桶是相对分区进行更细粒度的划分。\n\n- ==分桶将整个数据内容安装某列属性值取hash值进行区分，具有相同hash值的数据进入到同一个文件中==\n\n  - 比如按照name属性分为3个桶，就是对name属性值的hash值对3取摸，按照取模结果对数据分桶。\n    - 取模结果为==0==的数据记录存放到一个文件\n    - 取模结果为==1==的数据记录存放到一个文件\n    - 取模结果为==2==的数据记录存放到一个文件\n    - 取模结果为==3==的数据记录存放到一个文件\n\n- **==作用==**\n\n  - 1、取样sampling更高效。没有分区的话需要扫描整个数据集。\n  - 2、提升某些查询操作效率，例如map side join\n\n- 案例演示\n\n  - 1、创建分桶表\n\n    - 在创建分桶表之前要执行的命令\n      - ==set hive.enforce.bucketing=true;==  开启对分桶表的支持\n      - ==set mapreduce.job.reduces=4;==      设置与桶相同的reduce个数（默认只有一个reduce）\n\n    ```sql\n    # 进入hive客户端然后执行以下命令\n    use myhive;\n    set mapreduce.job.reduces=4;  \n    set hive.enforce.bucketing=true; \n    --分桶表\n    create table myhive.user_buckets_demo(id int, name string)\n    clustered by(id) \n    into 4 buckets \n    row format delimited fields terminated by '\\t';\n    \n    --普通表\n    create table user_demo(id int, name string)\n    row format delimited fields terminated by '\\t';\n    \n    ```\n\n  - 2、准备数据文件 buckets.txt\n\n    ```\n    #在linux当中执行以下命令\n    cd /kkb/install/hivedatas/\n    vim user_bucket.txt\n    \n    1\tlaowang1\n    2\tlaowang2\n    3\tlaowang3\n    4\tlaowang4\n    5\tlaowang5\n    6\tlaowang6\n    7\tlaowang7\n    8\tlaowang8\n    9\tlaowang9\n    10\tlaowang10\n    ```\n\n  ```\n  - 3、加载数据到普通表 user_demo 中\n  \n  load data local inpath '/kkb/install/hivedatas/user_bucket.txt'  overwrite into table user_demo; \n  \n  \n  ```\n\n```sql\n  #在hive客户端当中加载数据\nload data local inpath '/kkb/install/hivedatas/user_bucket.txt' into table user_demo;\n```\n\n  - 4、加载数据到桶表user_buckets_demo中\n\n  ```sql\n  insert into table user_buckets_demo select * from user_demo;\n  ```\n\n- 5、hdfs上查看表的数据目录\n\n  ![](assets/2019-07-16_16-30-09.png)\n\n\n\n- 6、抽样查询桶表的数据\n  - tablesample抽样语句，语法：tablesample(bucket  x  out  of  y)\n    - x表示从第几个桶开始取数据\n    - y表示桶数的倍数，一共需要从 ==桶数/y==  个桶中取数据\n\n```sql\nselect * from user_buckets_demo tablesample(bucket 1 out of 2)\n\n-- 需要的总桶数=4/2=2个\n-- 先从第1个桶中取出数据\n-- 再从第1+2=3个桶中取出数据\n```\n\n### 7、Hive修改表结构 \n\n修改表名称语法\n\n```\nalter table  old_table_name  rename to  new_table_name;\n```\n\n#### 7.1 修改表的名称\n\n~~~sql\nhive> alter table stu3 rename to stu4;\n~~~\n\n#### 7.2 表的结构信息\n\n~~~sql\nhive> desc stu4;\n\nhive> desc formatted stu4;\n\n~~~\n\n#### 7.3 增加/修改/替换列信息\n\n* 增加列\n\n~~~sql\nhive> alter table stu4 add columns(address string);\n\n~~~\n\n* 修改列\n\n~~~sql\nhive> alter table stu4 change column address address_id int;\n\n~~~\n\n### 8. Hive数据导入\n\n#### 8.1、直接向表中插入数据（强烈不推荐使用）\n\n```sql\nhive (myhive)> create table score3 like score;\nhive (myhive)> insert into table score3 partition(month ='201807') values ('001','002','100');\n\n```\n\n#### 8.2、通过load方式加载数据（必须掌握）\n\n语法：\n\n```sql\n hive> load data [local] inpath 'dataPath' overwrite | into table student [partition (partcol1=val1,…)]; \n\n```\n\n通过load方式加载数据\n\n```sql\nhive (myhive)> load data local inpath '/kkb/install/hivedatas/score.csv' overwrite into table score3 partition(month='201806');\n\n```\n\n#### 8.3、通过查询方式加载数据（必须掌握）\n\n通过查询方式加载数据\n\n```sql\nhive (myhive)> create table score5 like score;\nhive (myhive)> insert overwrite table score5 partition(month = '201806') select s_id,c_id,s_score from score;\n\n```\n\n#### 8.4、查询语句中创建表并加载数据（as select）\n\n将查询的结果保存到一张表当中去\n\n```\nhive (myhive)> create table score6 as select * from score;\n\n```\n\n#### 8.5、创建表时通过location指定加载数据路径\n\n1）创建表，并指定在hdfs上的位置\n\n```sql\nhive (myhive)> create external table score7 (s_id string,c_id string,s_score int) row format delimited fields terminated by '\\t' location '/myscore7';\n\n```\n\n2）上传数据到hdfs上，我们也可以直接在hive客户端下面通过dfs命令来进行操作hdfs的数据\n\n```\nhive (myhive)>  dfs -mkdir -p /myscore7;\nhive (myhive)> dfs -put /kkb/install/hivedatas/score.csv /myscore7;\n\n```\n\n3）查询数据\n\n```\nhive (myhive)> select * from score7;\n\n```\n\n#### 8.6、export导出与import 导入 hive表数据（内部表操作）\n\n```\nhive (myhive)> create table teacher2 like teacher;\nhive (myhive)> export table teacher to  '/kkb/teacher';\nhive (myhive)> import table teacher2 from '/kkb/teacher';\n\n```\n\n### 9、Hive数据导出\n\n#### 9.1 insert 导出\n\n* 1、将查询的结果导出到本地\n\n~~~sql\ninsert overwrite local directory '/kkb/install/hivedatas/stu' select * from stu;\n\n~~~\n\n* 2、将查询的结果格式化导出到本地\n\n~~~sql\ninsert overwrite local directory '/kkb/install/hivedatas/stu2' row format delimited fields terminated by  ',' select * from stu;\n\n~~~\n\n* 3、将查询的结果导出到HDFS上==(没有local)==\n\n~~~sql\ninsert overwrite  directory '/kkb/hivedatas/stu'  row format delimited fields terminated by  ','  select * from stu;\n\n~~~\n\n#### 9.2、 Hive Shell 命令导出\n\n* 基本语法：\n\n  * hive -e \"sql语句\" >   file\n  * hive -f  sql文件   >    file\n\n  ~~~shell\n  bin/hive -e 'select * from myhive.stu;' > /kkb/install/hivedatas/student1.txt\n  \n  ~~~\n\n\n\n#### 9.3、export导出到HDFS上\n\n~~~sql\nexport table  myhive.stu to '/kkb/install/hivedatas/stuexport';\n\n~~~\n\n\n\n### 10、hive的静态分区和动态分区\n\n#### 10.1 静态分区\n\n- 表的分区字段的值需要开发人员手动给定\n\n  - 1、创建分区表\n\n  ~~~sql\n  use myhive;\n  create table order_partition(\n  order_number string,\n  order_price  double,\n  order_time string\n  )\n  partitioned BY(month string)\n  row format delimited fields terminated by '\\t';\n  \n  ~~~\n\n~~~\n  \n- 10.2、准备数据\torder.txt内容如下\n  \ncd /kkb/install/hivedatas\nvim order.txt \n10001\t100\t2019-03-02\n10002\t200\t2019-03-02\n10003\t300\t2019-03-02\n10004\t400\t2019-03-03\n10005\t500\t2019-03-03\n10006\t600\t2019-03-03\n10007\t700\t2019-03-04\n10008\t800\t2019-03-04\n10009\t900\t2019-03-04\n\n~~~\n\n\n\n  - 3、加载数据到分区表\n\n  ~~~sql\nload data local inpath '/kkb/install/hivedatas/order.txt' overwrite into table order_partition partition(month='2019-03');\n\n  ~~~\n\n  - 4、查询结果数据\t\n\n  ~~~sql\nselect * from order_partition where month='2019-03';\n结果为：\n  \n10001   100.0   2019-03-02      2019-03\n10002   200.0   2019-03-02      2019-03\n10003   300.0   2019-03-02      2019-03\n10004   400.0   2019-03-03      2019-03\n10005   500.0   2019-03-03      2019-03\n10006   600.0   2019-03-03      2019-03\n10007   700.0   2019-03-04      2019-03\n10008   800.0   2019-03-04      2019-03\n10009   900.0   2019-03-04      2019-03\n\n  ~~~\n\n\n\n\n#### .2 动态分区\n\n- 按照需求实现把数据自动导入到表的不同分区中，==不需要手动指定==\n\n  - **需求：按照不同部门作为分区导数据到目标表** \n\n    1、创建表\n\n    ~~~sql\n    --创建普通表\n    create table t_order(\n        order_number string,\n        order_price  double, \n        order_time   string\n    )row format delimited fields terminated by '\\t';\n    \n    --创建目标分区表\n    create table order_dynamic_partition(\n        order_number string,\n        order_price  double    \n    )partitioned BY(order_time string)\n    row format delimited fields terminated by '\\t';\n    \n    \n    ~~~\n\n    2、准备数据\t order_created.txt内容如下\n\n    ~~~sql\n    cd /kkb/install/hivedatas\n    vim order_partition.txt\n    \n    10001\t100\t2019-03-02 \n    10002\t200\t2019-03-02\n    10003\t300\t2019-03-02\n    10004\t400\t2019-03-03\n    10005\t500\t2019-03-03\n    10006\t600\t2019-03-03\n    10007\t700\t2019-03-04\n    10008\t800\t2019-03-04\n    10009\t900\t2019-03-04\n    \n    ~~~\n\n    3、向普通表t_order加载数据\n\n    ```\n    load data local inpath '/kkb/install/hivedatas/order_partition.txt' overwrite into table t_order;\n    \n    ```\n\n    4、动态加载数据到分区表中\n\n    ```\n    要想进行动态分区，需要设置参数\n    //开启动态分区功能\n    hive> set hive.exec.dynamic.partition=true; \n    //设置hive为非严格模式\n    hive> set hive.exec.dynamic.partition.mode=nonstrict; \n    hive> insert into table order_dynamic_partition partition(order_time) select order_number,order_price,order_time from t_order;\n    \n    ```\n\n    5、查看分区\n\n    ```\n    bin/hive>  show partitions order_dynamic_partition;\n    \n    ```\n\n    ![1569313506031](assets/1569313506031.png)\n\n### 11、hive的基本查询语法\n\n#### 1. 基本查询\n\n- 注意\n  - SQL 语言==大小写不敏感==\n  - SQL 可以写在一行或者多行\n  - ==关键字不能被缩写也不能分行==\n  - 各子句一般要分行写\n  - 使用缩进提高语句的可读性\n\n\n\n##### 1.1 全表和特定列查询\n\n- 全表查询\n\n```sql\nselect * from stu;\n\n```\n\n- 选择特定列查询\n\n```sql\nselect id,name from stu;\n\n```\n\n##### 1.2 列起别名\n\n- 重命名一个列\n\n  - 紧跟列名，也可以在列名和别名之间加入关键字 ‘as’ \n\n- 案例实操\n\n  ```sql\n  select id,name as stuName from stu;\n  \n  ```\n\n\n\n1.3 常用函数\n\n- 1．求总行数（count）\n\n```sql\n select count(*) cnt from score;\n\n```\n\n- 2、求分数的最大值（max）\n\n```sql\nselect max(s_score) from score;\n\n```\n\n- 3、求分数的最小值（min）\n\n```sql\nselect min(s_score) from score;\n\n```\n\n- 4、求分数的总和（sum）\n\n```sql\nselect sum(s_score) from score;\n\n```\n\n- 5、求分数的平均值（avg）\n\n```sql\nselect avg(s_score) from score;\n\n```\n\n\n\n##### 1.4 limit 语句\n\n- 典型的查询会返回多行数据。limit子句用于限制返回的行数。\n\n```sql\n select  * from score limit 5;\n\n```\n\n\n\n##### 1.5 where 语句\n\n- 1、使用 where 子句，将不满足条件的行过滤掉\n- 2、==where 子句紧随from子句==\n- 3、案例实操\n\n```sql\nselect  * from score where s_score > 60;\n\n```\n\n\n\n##### 1.6 算术运算符\n\n\n\n| 运算符 | 描述           |\n| ------ | -------------- |\n| A+B    | A和B 相加      |\n| A-B    | A减去B         |\n| A*B    | A和B 相乘      |\n| A/B    | A除以B         |\n| A%B    | A对B取余       |\n| A&B    | A和B按位取与   |\n| A\\|B   | A和B按位取或   |\n| A^B    | A和B按位取异或 |\n| ~A     | A按位取反      |\n\n\n\n##### 1.7 比较运算符\n\n\n\n|         操作符          | 支持的数据类型 |                             描述                             |\n| :---------------------: | :------------: | :----------------------------------------------------------: |\n|           A=B           |  基本数据类型  |             如果A等于B则返回true，反之返回false              |\n|          A<=>B          |  基本数据类型  | 如果A和B都为NULL，则返回true，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL |\n|       A<>B, A!=B        |  基本数据类型  | A或者B为NULL则返回NULL；如果A不等于B，则返回true，反之返回false |\n|           A<B           |  基本数据类型  | A或者B为NULL，则返回NULL；如果A小于B，则返回true，反之返回false |\n|          A<=B           |  基本数据类型  | A或者B为NULL，则返回NULL；如果A小于等于B，则返回true，反之返回false |\n|           A>B           |  基本数据类型  | A或者B为NULL，则返回NULL；如果A大于B，则返回true，反之返回false |\n|          A>=B           |  基本数据类型  | A或者B为NULL，则返回NULL；如果A大于等于B，则返回true，反之返回false |\n| A [NOT] BETWEEN B AND C |  基本数据类型  | 如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为true，反之为false。如果使用NOT关键字则可达到相反的效果。 |\n|        A IS NULL        |  所有数据类型  |           如果A等于NULL，则返回true，反之返回false           |\n|      A IS NOT NULL      |  所有数据类型  |          如果A不等于NULL，则返回true，反之返回false          |\n|    IN(数值1, 数值2)     |  所有数据类型  |                  使用 IN运算显示列表中的值                   |\n|     A [NOT] LIKE B      |  STRING 类型   | B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回true；反之返回false。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。like不是正则，而是通配符 |\n|  A RLIKE B, A REGEXP B  |  STRING 类型   | B是一个正则表达式，如果A与其匹配，则返回true；反之返回false。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。 |\n\n\n\n##### 1.8 逻辑运算符\n\n\n\n|  操作符  |  操作  |                   描述                    |\n| :------: | :----: | :---------------------------------------: |\n| A AND  B | 逻辑并 |    如果A和B都是true则为true，否则false    |\n| A  OR  B | 逻辑或 | 如果A或B或两者都是true则为true，否则false |\n|  NOT  A  | 逻辑否 |      如果A为false则为true,否则false       |\n\n\n\n#### 2. 分组 \n\n##### 2.1 Group By 语句\n\n​\tGroup By 语句通常会和==聚合函数==一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。\n\n- 案例实操：\n\n  - （1）计算每个学生的平均分数\n\n  ```sql\n  select s_id,avg(s_score) from score group by s_id;\n  \n  ```\n\n  - （2）计算每个学生最高的分数\n\n  ```sql\n  select s_id,max(s_score) from score group by s_id;\n  \n  ```\n\n\n\n##### 2.2 Having语句\n\n- having 与 where 不同点\n\n  - where针对==表中的列发挥作用==，查询数据；==having针对查询结果中的列==发挥作用，筛选数据\n  - where后面==不能写分组函数==，而having后面可以==使用分组函数==\n  - having只用于group by分组统计语句\n\n- 案例实操\n\n  - 求每个学生的平均分数\n\n  ```sql\n  select s_id,avg(s_score) from score group by s_id;\n  \n  ```\n\n  - 求每个学生平均分数大于60的人\n\n  ```sql\n  select s_id,avg(s_score) as avgScore from score group by s_id having avgScore > 60;\n  \n  ```\n\n#### 3. join语句\n\n##### 11.3.1 等值 join\n\n- Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。\n\n- 案例实操\n\n  - 根据学生和成绩表，查询学生姓名对应的成绩\n\n  ```sql\n  select * from stu left join score on stu.id = score.s_id;\n  \n  ```\n\n\n\n##### 11.3.2 表的别名\n\n- 好处\n\n  - 使用别名可以简化查询。\n  - 使用表名前缀可以提高执行效率。\n\n- 案例实操\n\n  - 合并老师与课程表\n\n  ```sql\n  #hive当中创建course表并加载数据\n  create table course (c_id string,c_name string,t_id string) row format delimited fields terminated by '\\t';\n  load data local inpath '/kkb/install/hivedatas/course.csv' overwrite into table course;\n  \n  select * from teacher t join course c on t.t_id = c.t_id;\n  \n  ```\n\n##### 11.3.3 内连接 inner join\n\n- 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。\n  - join默认是inner  join\n- 案例实操\n\n```sql\nselect * from teacher t inner join course c  on t.t_id = c.t_id;\n\n```\n\n\n\n##### 11.3.4 左外连接 left outer join\n\n- 左外连接：join操作符==左边表中==符合where子句的所有记录将会被返回。\n\n- 案例实操\n\n  - 查询老师对应的课程\n\n  ```sql\n   select * from teacher t left outer join course c  on t.t_id = c.t_id;\n  \n  ```\n\n\n\n##### 3.5 右外连接 right outer join\n\n- 右外连接：join操作符==右边表中==符合where子句的所有记录将会被返回。\n\n- 案例实操\n\n  ```sql\n   select * from teacher t right outer join course c  on t.t_id = c.t_id;\n  \n  ```\n\n##### 11.3.6 满外连接 full outer join\n\n- 满外连接：将会返回==所有表中==符合where语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用null值替代。\n\n- 案例实操\n\n  ```sql\n  select * from teacher t full outer join course c  on t.t_id = c.t_id;\n  \n  ```\n\n##### 11.3.7 多表连接 \n\n- **多个表使用join进行连接**\n\n- 注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。\n\n- 案例实操\n\n  - 多表连接查询，查询老师对应的课程，以及对应的分数，对应的学生\n\n  ```sql\n  select * from teacher t left join course c on t.t_id = c.t_id \n  left join score s on c.c_id = s.c_id \n  left join stu on s.s_id = stu.id;\n  \n  ```\n\n#### 11.4. 排序\n\n##### 11.4.1 order by 全局排序\n\n- order by 说明\n\n  - 全局排序，只有一个reduce\n  - 使用 ORDER BY 子句排序\n    - asc ( ascend)\n      - 升序 (默认)\n    - desc (descend)\n      - 降序\n  - order by 子句在select语句的结尾\n\n- 案例实操\n\n  - 查询学生的成绩，并按照分数降序排列\n\n  ```sql\n  select * from score  s order by s_score desc ;\n  \n  ```\n\n##### 11.4.2 按照别名排序\n\n- 按照学生分数的平均值排序\n\n```sql\nselect s_id,avg(s_score) avgscore  from score group by s_id order by avgscore desc; \n\n```\n\n##### 11.4.3 每个MapReduce内部排序（Sort By）局部排序\n\n- sort by：每个reducer内部进行排序，对全局结果集来说不是排序。\n\n  1、设置reduce个数\n\n  ```\n  set mapreduce.job.reduces=3;\n  \n  ```\n\n  2、查看reduce的个数\n\n  ```sql\n  set mapreduce.job.reduces;\n  \n  ```\n\n  3、查询成绩按照成绩降序排列\n\n  ```sql\n   select * from score s sort by s.s_score;\n  \n  ```\n\n  4、将查询结果导入到文件中（按照成绩降序排列）\n\n  ```sql\n  insert overwrite local directory '/kkb/install/hivedatas/sort' select * from score s sort by s.s_score;\n  \n  ```\n\n\n\n##### 11.4.4 distribute by 分区排序\n\n- distribute by：类似MR中partition，==采集hash算法，在map端将查询的结果中hash值相同的结果分发到对应的reduce文件中==。结合sort by使用。\n\n- 注意\n\n  - Hive要求 **distribute by** 语句要写在 **sort by** 语句之前。\n\n- 案例实操\n\n  - 先按照学生 sid 进行分区，再按照学生成绩进行排序\n\n    - 设置reduce的个数\n\n    ```sql\n    set mapreduce.job.reduces=3;\n    \n    ```\n\n    - 通过distribute by  进行数据的分区,，将不同的sid 划分到对应的reduce当中去\n\n    ```sql\n    insert overwrite local directory '/kkb/install/hivedatas/distribute' select * from score distribute by s_id sort by s_score;\n    \n    ```\n\n\n\n##### 11.4.5 cluster by\n\n- 当distribute by和sort by字段相同时，可以使用cluster by方式\n\n- 除了distribute by 的功能外，还会对该字段进行排序，所以cluster by = distribute by + sort by\n\n  ```sql\n  --以下两种写法等价\n  \n  insert overwrite local directory '/kkb/install/hivedatas/distribute_sort' select * from score distribute  by s_score sort  by s_score;\n  \n  \n  insert overwrite local directory '/kkb/install/hivedatas/cluster' select * from score  cluster by s_score;\n  \n  \n  ```\n\n\n\n### 12、hive客户端jdbc操作\n\n#### 第一步：启动hiveserver2的服务端\n\nnode03执行以下命令启动hiveserver2的服务端\n\n```\ncd /kkb/install/hive-1.1.0-cdh5.14.2/\nnohup bin/hive --service hiveserver2 2>&1 &\n\n```\n\n#### 第二步：引入依赖\n\n```xml\n   <repositories>\n        <repository>\n            <id>cloudera</id>\n            <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>\n        </repository>\n    </repositories>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-exec</artifactId>\n            <version>1.1.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-jdbc</artifactId>\n            <version>1.1.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hive</groupId>\n            <artifactId>hive-cli</artifactId>\n            <version>1.1.0-cdh5.14.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n            <version>2.6.0-cdh5.14.2</version>\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.0</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                    <encoding>UTF-8</encoding>\n                    <!--    <verbal>true</verbal>-->\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n```\n\n#### 第三步：代码开发\n\n```java\nimport java.sql.*;\n\npublic class HiveJDBC {\n    private static String url=\"jdbc:hive2://192.168.52.120:10000/myhive\";\n    public static void main(String[] args) throws Exception {\n        Class.forName(\"org.apache.hive.jdbc.HiveDriver\");\n        //获取数据库连接\n        Connection connection = DriverManager.getConnection(url, \"hadoop\",\"\");\n        //定义查询的sql语句\n        String sql=\"select * from stu\";\n        try {\n            PreparedStatement ps = connection.prepareStatement(sql);\n            ResultSet rs = ps.executeQuery();\n            while (rs.next()){\n                //获取id字段值\n                int id = rs.getInt(1);\n                //获取deptid字段\n                String name = rs.getString(2);\n                System.out.println(id+\"\\t\"+name);\n            }\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n    }\n}\n\n```\n\n\n\n### 、hive的可视化工具dbeaver介绍以及使用\n\n#### 1、dbeaver的基本介绍\n\ndbeaver是一个图形化的界面工具，专门用于与各种数据库的集成，通过dbeaver我们可以与各种数据库进行集成通过图形化界面的方式来操作我们的数据库与数据库表，类似于我们的sqlyog或者navicate\n\n\n\n#### 2、dbeaver的下载安装\n\nhttps://github.com/dbeaver/dbeaver/releases\n\n我们可以直接从github上面下载我们需要的对应的安装包即可\n\n#### 3、dbeaver的安装与使用\n\n这里我们使用的版本是6.15这个版本，下载zip的压缩包，直接解压就可以使用，然后双击dbeaver.exe即可启动\n\n##### 第一步：双击dbeaver.exe然后启动dbeaver图形化界面\n\n![1569407489837](assets/1569407489837.png)\n\n\n\n\n\n##### 第二步：配置我们的主机名与端口号\n\n\n\n![20181113110722636](assets/20181113110722636.png)\n\n![1569407697691](assets/1569407697691.png)\n\n\n\n\n\n\n\n\n\n**注意：⚠️**\n\n企业hive可视化工具：Hub\n\n","tags":["hive"]},{"title":"zookeeper分布式协调框架-hadoop高可用方案","url":"/2019/10/28/it/zookeeper/zookeeper分布式协调框架-hadoop高可用方案/","content":"\n# 集群规划hadoop高可用搭建\n\n>  说明：\n>\n>  - 集群共5个节点，主机名分别是node01、node02、node03、node04、node05\n>\n>  - 初始启动集群\n>    - node01上运行active namenode即主namenode；node02上运行standby namenode即从namenode\n>    - node04上运行主resourcemanager；node05上运行从resourcemanager\n\n- 每个节点运行的进程如下表\n\n| 机器名 | 运行进程                                                    |\n| ------ | ----------------------------------------------------------- |\n| node01 | NameNode/zkfc/Zookeeper/Journalnode/DataNode/NodeManager    |\n| node02 | NameNode/zkfc/Zookeeper/Journalnode/DataNode/NodeManager    |\n| node03 | Zookeeper/Journalnode/DataNode/NodeManager/JobHistoryServer |\n| node04 | ResourceManager                                             |\n| node05 | ResourceManager                                             |\n\n\n\n# Hadoop HA搭建\n\n## 1. 虚拟机环境准备\n\n- 准备**5台**虚拟机\n- 在做五节点hadoop HA集群搭建之前，要求先完成**每台**虚拟机的**基本环境准备**\n  - 每个节点都要做好“在node01上开始解压hadoop的tar.gz包之前的环境配置”\n  - 主要包括如下步骤（三节点Hadoop集群搭建时已讲解过，不再赘述）\n    - windows|mac安装VMWare虚拟化软件\n    - VMWare下安装CenoOS7\n    - 虚拟机关闭防火墙\n    - 禁用selinux\n    - 配置虚拟网卡\n    - 配置虚拟机网络\n    - 安装JDK\n    - 配置时间同步\n    - 修改主机名\n    - 修改ip地址\n    - 修改/etc/hosts\n    - 各节点免密钥登陆\n    - 重启虚拟机\n\n\n\n## 2. 安装ZooKeeper集群\n\n> Hadoop高可用集群需要使用ZooKeeper集群做分布式协调；所以先安装ZooKeeper集群\n\n- 在node01、node02、node03上安装ZooKeeper集群（详见三节点ZooKeeper集群搭建，不再赘述）\n\n\n\n## 3. 五节点Hadoop HA搭建\n\n> **注意：**\n>\n> ①3.1到3.8在**node01**上操作\n>\n> ②**此文档使用<font color=red>普通用户</font>操作，如hadoop**\n>\n> ③**hadoop安装到用户主目录下，如/kkb/install**\n>\n> <font color=red>**请根据自己的实际情况修改**</font>\n\n\n\n### 3.1 解压hadoop压缩包\n\n- hadoop压缩包hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz上传到node01的/kkb/soft路径中\n\n- 解压hadoop压缩包到/kkb/install\n\n```shell\n#解压hadoop压缩包到/kkb/install\n[hadoop@node01 ~]$ cd\n[hadoop@node01 ~]$ cd /kkb/soft/\n[hadoop@node01 soft]$ tar -xzvf hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz -C /kkb/install/\n```\n\n\n### 3.2 修改hadoop-env.sh\n\n- 进入hadoop配置文件路径$HADOOP_HOME/etc/hadoop\n\n```shell\n[hadoop@node01 soft]$ cd /kkb/install/hadoop-2.6.0-cdh5.14.2/\n[hadoop@node01 hadoop-2.6.0-cdh5.14.2]$ cd etc/hadoop/\n```\n\n- 修改hadoop-env.sh，修改JAVA_HOME值为jdk解压路径；保存退出\n\n```shell\nexport JAVA_HOME=/kkb/install/jdk1.8.0_141\n```\n\n> 注意：JAVA_HOME值修改为<font color=red>**自己jdk的实际目录**</font>\n\n### 3.3 修改core-site.xml\n\n> **注意：**\n>\n> **情况一：值/kkb/install/hadoop-2.6.0-cdh5.14.2/tmp根据实际情况修改**\n>\n> **情况二：值node01:2181,node02:2181,node03:2181根据实际情况修改，修改成安装了zookeeper的虚拟机的主机名**\n\n```xml\n<configuration>\n\t<!-- 指定hdfs的nameservice id为ns1 -->\n\t<property>\n\t\t<name>fs.defaultFS</name>\n\t\t<value>hdfs://ns1</value>\n\t</property>\n\t<!-- 指定hadoop临时文件存储的基目录 -->\n\t<property>\n\t\t<name>hadoop.tmp.dir</name>\n\t\t<value>/kkb/install/hadoop-2.6.0-cdh5.14.2/tmp</value>\n\t</property>\n\t<!-- 指定zookeeper地址，ZKFailoverController使用 -->\n\t<property>\n\t\t<name>ha.zookeeper.quorum</name>\n\t\t<value>node01:2181,node02:2181,node03:2181</value>\n\t</property>\n</configuration>\n```\n\n### 3.4 修改hdfs-site.xml\n\n> **注意：**\n>\n> **情况一：属性值qjournal://node01:8485;node02:8485;node03:8485/ns1中的主机名，修改成实际安装zookeeper的虚拟机的主机名**\n>\n> **情况二：属性值/kkb/install/hadoop-2.6.0-cdh5.14.2/journal中”/kkb/install/hadoop-2.6.0-cdh5.14.2”替换成实际hadoop文件夹的路径**\n>\n> **情况三：属性值/home/hadoop/.ssh/id_rsa中/home/hadoop根据实际情况替换**\n\n```xml\n<configuration>\n\t<!--指定hdfs的nameservice列表，多个之前逗号分隔；此处只有一个ns1，需要和core-site.xml中的保持一致 -->\n\t<property>\n\t\t<name>dfs.nameservices</name>\n\t\t<value>ns1</value>\n\t</property>\n\t<!-- ns1下面有两个NameNode，分别是nn1，nn2 -->\n\t<property>\n\t\t<name>dfs.ha.namenodes.ns1</name>\n\t\t<value>nn1,nn2</value>\n\t</property>\n\t<!-- nn1的RPC通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.ns1.nn1</name>\n\t\t<value>node01:8020</value>\n\t</property>\n\t<!-- nn1的http通信地址,web访问地址 -->\n\t<property>\n\t\t<name>dfs.namenode.http-address.ns1.nn1</name>\n\t\t<value>node01:50070</value>\n\t</property>\n\t<!-- nn2的RPC通信地址 -->\n\t<property>\n\t\t<name>dfs.namenode.rpc-address.ns1.nn2</name>\n\t\t<value>node02:8020</value>\n\t</property>\n\t<!-- nn2的http通信地址,web访问地址 -->\n\t<property>\n\t\t<name>dfs.namenode.http-address.ns1.nn2</name>\n\t\t<value>node02:50070</value>\n\t</property>\n\t<!-- 指定NameNode的元数据在JournalNode上的存放位置 -->\n\t<property>\n\t\t<name>dfs.namenode.shared.edits.dir</name>\n\t\t<value>qjournal://node01:8485;node02:8485;node03:8485/ns1</value>\n\t</property>\n\t<!-- 指定JournalNode在本地磁盘存放数据的位置 -->\n\t<property>\n\t\t<name>dfs.journalnode.edits.dir</name>\n\t\t<value>/kkb/install/hadoop-2.6.0-cdh5.14.2/journal</value>\n\t</property>\n\t<!-- 开启NameNode失败自动切换 -->\n\t<property>\n\t\t<name>dfs.ha.automatic-failover.enabled</name>\n\t\t<value>true</value>\n\t</property>\n\t<!-- 此类决定哪个namenode是active，切换active和standby -->\n\t<property>\n\t\t<name>dfs.client.failover.proxy.provider.ns1</name>\n\t\t<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n\t</property>\n\t<!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行-->\n\t<property>\n\t\t<name>dfs.ha.fencing.methods</name>\n\t\t<value>\n\t\tsshfence\n\t\tshell(/bin/true)\n\t\t</value>\n\t</property>\n\t<!-- 使用sshfence隔离机制时需要ssh免密登陆到目标机器 -->\n\t<property>\n\t\t<name>dfs.ha.fencing.ssh.private-key-files</name>\n\t\t<value>/home/hadoop/.ssh/id_rsa</value>\n\t</property>\n\t<!-- 配置sshfence隔离机制超时时间 -->\n\t<property>\n\t\t<name>dfs.ha.fencing.ssh.connect-timeout</name>\n\t\t<value>30000</value>\n\t</property>\n</configuration>\n```\n\n### 3.5 修改mapred-site.xml\n\n- 重命名文件\n\n```shell\n[hadoop@node01 hadoop]$ mv mapred-site.xml.template mapred-site.xml\n```\n\n- 修改mapred-site.xml\n\n```xml\n<configuration>\n\t<!-- 指定运行mr job的运行时框架为yarn -->\n\t<property>\n\t\t<name>mapreduce.framework.name</name>\n\t\t<value>yarn</value>\n\t</property>\n    <!-- MapReduce JobHistory Server IPC host:port -->\n\t<property>\n\t\t<name>mapreduce.jobhistory.address</name>\n\t\t<value>node03:10020</value>\n\t</property>\n\t<!-- MapReduce JobHistory Server Web UI host:port -->\n\t<property>\n\t\t<name>mapreduce.jobhistory.webapp.address</name>\n\t\t<value>node03:19888</value>\n\t</property>\n</configuration>\n```\n\n### 3.6 修改yarn-site.xml\n\n> **注意：**\n>\n> **情况一：属性yarn.resourcemanager.hostname.rm1的值node04根据实际情况替换**\n>\n> **情况二：属性yarn.resourcemanager.hostname.rm2的值node05根据实际情况替换**\n>\n> **情况三：属性值node01:2181,node02:2181,node03:2181根据实际情况替换；替换成实际安装zookeeper的虚拟机的主机名**\n\n```xml\n<configuration>\n    <!-- 是否启用日志聚合.应用程序完成后,日志汇总收集每个容器的日志,这些日志移动到文件系统,例如HDFS. -->\n\t<!-- 用户可以通过配置\"yarn.nodemanager.remote-app-log-dir\"、\"yarn.nodemanager.remote-app-log-dir-suffix\"来确定日志移动到的位置 -->\n\t<!-- 用户可以通过应用程序时间服务器访问日志 -->\n\t<!-- 启用日志聚合功能，应用程序完成后，收集各个节点的日志到一起便于查看 -->\n\t<property>\n\t\t\t<name>yarn.log-aggregation-enable</name>\n\t\t\t<value>true</value>\n\t</property>\n\t<!-- 开启RM高可靠 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.ha.enabled</name>\n\t\t<value>true</value>\n\t</property>\n\t<!-- 指定RM的cluster id为yrc，意为yarn cluster -->\n\t<property>\n\t\t<name>yarn.resourcemanager.cluster-id</name>\n\t\t<value>yrc</value>\n\t</property>\n\t<!-- 指定RM的名字 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.ha.rm-ids</name>\n\t\t<value>rm1,rm2</value>\n\t</property>\n\t<!-- 指定第一个RM的地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.hostname.rm1</name>\n\t\t<value>node04</value>\n\t</property>\n    <!-- 指定第二个RM的地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.hostname.rm2</name>\n\t\t<value>node05</value>\n\t</property>\n    <!-- 配置第一台机器的resourceManager通信地址 -->\n\t<!--客户端通过该地址向RM提交对应用程序操作-->\n\t<property>\n\t\t<name>yarn.resourcemanager.address.rm1</name>\n\t\t<value>node04:8032</value>\n\t</property>\n\t<!--向RM调度资源地址--> \n\t<property>\n\t\t<name>yarn.resourcemanager.scheduler.address.rm1</name>\n\t\t<value>node04:8030</value>\n\t</property>\n\t<!--NodeManager通过该地址交换信息-->\n\t<property>\n\t\t<name>yarn.resourcemanager.resource-tracker.address.rm1</name>\n\t\t<value>node04:8031</value>\n\t</property>\n\t<!--管理员通过该地址向RM发送管理命令-->\n\t<property>\n\t\t<name>yarn.resourcemanager.admin.address.rm1</name>\n\t\t<value>node04:8033</value>\n\t</property>\n\t<!--RM HTTP访问地址,查看集群信息-->\n\t<property>\n\t\t<name>yarn.resourcemanager.webapp.address.rm1</name>\n\t\t<value>node04:8088</value>\n\t</property>\n\t<!-- 配置第二台机器的resourceManager通信地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.address.rm2</name>\n\t\t<value>node05:8032</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.scheduler.address.rm2</name>\n\t\t<value>node05:8030</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.resource-tracker.address.rm2</name>\n\t\t<value>node05:8031</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.admin.address.rm2</name>\n\t\t<value>node05:8033</value>\n\t</property>\n\t<property>\n\t\t<name>yarn.resourcemanager.webapp.address.rm2</name>\n\t\t<value>node05:8088</value>\n\t</property>\n    <!--开启resourcemanager自动恢复功能-->\n\t<property>\n\t\t<name>yarn.resourcemanager.recovery.enabled</name>\n\t\t<value>true</value>\n\t</property>\t\n    <!--在node4上配置rm1,在node5上配置rm2,注意：一般都喜欢把配置好的文件远程复制到其它机器上，但这个在YARN的另一个机器上一定要修改，其他机器上不配置此项-->\n\t<!--\n    <property>       \n\t\t<name>yarn.resourcemanager.ha.id</name>\n\t\t<value>rm1</value>\n\t   <description>If we want to launch more than one RM in single node, we need this configuration</description>\n\t</property>\n\t-->\n\t<!--用于持久存储的类。尝试开启-->\n\t<property>\n\t\t<name>yarn.resourcemanager.store.class</name>\n\t\t<!-- 基于zookeeper的实现 -->\n\t\t<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\n\t</property>\n    <!-- 单个任务可申请最少内存，默认1024MB -->\n\t<property>\n\t\t<name>yarn.scheduler.minimum-allocation-mb</name>\n\t\t<value>512</value>\n\t</property>\n\t<!--多长时间聚合删除一次日志 此处-->\n\t<property>\n\t\t<name>yarn.log-aggregation.retain-seconds</name>\n\t\t<value>2592000</value><!--30 day-->\n\t</property>\n\t<!--时间在几秒钟内保留用户日志。只适用于如果日志聚合是禁用的-->\n\t<property>\n\t\t<name>yarn.nodemanager.log.retain-seconds</name>\n\t\t<value>604800</value><!--7 day-->\n\t</property>\n\t<!-- 指定zk集群地址 -->\n\t<property>\n\t\t<name>yarn.resourcemanager.zk-address</name>\n\t\t<value>node01:2181,node02:2181,node03:2181</value>\n\t</property>\n    <!-- 逗号隔开的服务列表，列表名称应该只包含a-zA-Z0-9_,不能以数字开始-->\n\t<property>\n\t\t<name>yarn.nodemanager.aux-services</name>\n\t\t<value>mapreduce_shuffle</value>\n\t</property>\n</configuration>\n```\n\n### 3.7 修改slaves\n\n> node01、node02、node03上运行了datanode、nodemanager，所以修改slaves内容**替换**为：\n\n```shell\nnode01\nnode02\nnode03\n\n```\n\n### 3.8 远程拷贝hadoop文件夹\n\n> 拷贝到node02~node05\n\n```shell\n[hadoop@node01 hadoop]$ scp -r /kkb/install/hadoop-2.6.0-cdh5.14.2/ node02:/kkb/install/\n[hadoop@node01 hadoop]$ scp -r /kkb/install/hadoop-2.6.0-cdh5.14.2/ node03:/kkb/install/\n[hadoop@node01 hadoop]$ scp -r /kkb/install/hadoop-2.6.0-cdh5.14.2/ node04:/kkb/install/\n[hadoop@node01 hadoop]$ scp -r /kkb/install/hadoop-2.6.0-cdh5.14.2/ node05:/kkb/install/\n\n```\n\n### 3.9 修改两个RM的yarn-site.xml\n\n- 在**node04**上，找到属性`yarn.resourcemanager.ha.id`去除注释①、②\n\n```shell\n[hadoop@node04 ~]$ cd /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop\n[hadoop@node04 hadoop]$ vim yarn-site.xml \n\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909232016.png)\n\n- 在**node05**上\n  - 找到属性`yarn.resourcemanager.ha.id`去除注释**①、②**\n  - **③**修改成rm2\n\n```shell\n[hadoop@node05 ~]$ cd /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/\n[hadoop@node05 hadoop]$ vim yarn-site.xml\n\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909232022.png)\n\n- 修改后，结果如下\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909232024.png)\n\n### 3.10 配置环境变量\n\n- **node01到node05<font color='red'>五个节点都配置环境变量</font>**\n\n```shell\n#将hadoop添加到环境变量中\nvim /etc/profile\n\n```\n\n- 添加内容如下（注意：若HADOOP_HOME已经存在，则修改）：\n\n```shell\nexport HADOOP_HOME=/kkb/install/hadoop-2.6.0-cdh5.14.2/\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n\n```\n\n- 编译文件，使新增环境变量生效\n\n```shell\nsource /etc/profile\n\n```\n\n## 4. 启动与初始化hadoop集群\n\n>  **注意：**严格按照下面的步骤 先检查各台hadoop环境变量是否设置好\n\n### 4.1 启动zookeeper集群\n\n>  注意：根据zookeeper实际安装情况，启动zookeeper\n\n分别在node01、node02、node03上启动zookeeper\n\n```shell\nzkServer.sh start\n\n```\n\n#查看状态：一个为leader，另外两个为follower\n\n```shell\nzkServer.sh status\n\n```\n\n### 4.2 启动HDFS\n\n#### 4.2.1 格式化ZK\n\n> 在**node01**上执行即可\n>\n> - 集群有两个namenode，分别在node01、node02上\n>\n> - 每个namenode对应一个zkfc进程；\n>\n> - 在主namenode node01上格式化zkfc\n\n```shell\nhdfs zkfc -formatZK\n\n```\n\n#### 4.2.2 启动journalnode\n\n- 在**node01**上执行\n  - 会启动node01、node02、node03上的journalnode\n  - 因为使用的是hadoop-daemon**s**.sh\n\n```shell\nhadoop-daemons.sh start journalnode\n\n```\n\n- 运行jps命令检验，node01、node02、node03上多了JournalNode进程\n\n####  4.2.3 格式化HDFS\n\n- 在node01上执行\n- 根据集群规划node01、node02上运行namenode；所以<font color='red'>**只在主namenode节点**</font>即node01上执行命令:\n  - 此命令慎用；只在集群搭建（初始化）时使用一次；\n  - 一旦再次使用，会将HDFS上之前的数据格式化删除掉\n\n```shell\nhdfs namenode -format\n\n```\n\n- 下图表示格式化成功\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909241056.png)\n\n#### 4.2.4 初始化元数据、启动主NN\n\n- node01上执行（主namenode）\n\n```shell\nhdfs namenode -initializeSharedEdits -force\n#启动HDFS\nstart-dfs.sh\n\n```\n\n#### 4.2.5 同步元数据信息、启动从NN\n\n- **node02**上执行（从namenode）\n- 同步元数据信息，并且设置node02上namenode为standBy状态\n\n```shell\nhdfs namenode -bootstrapStandby\nhadoop-daemon.sh start namenode\n\n```\n\n#### 4.2.5 JPS查看进程\n\n- node01上\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909241118.png)\n\n- node02上\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909241119.png)\n\n- node03上\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201909241120.png)\n\n### 4.3 启动YARN\n\n#### 4.6.1 **主resourcemanager**\n\n- **node04**上执行（**<font color='red'>主resourcemanager</font>**）\n  - 把namenode和resourcemanager部署在不同节点，是因为性能问题，因为他们都要占用大量资源\n  - <font color='red'>坑</font>：在node04上启动yarn之前，先依次从node04 ssh远程连接到node01、node02、node03、node04、node05；因为初次ssh时，需要交互，输入yes，回车\n\n```shell\nstart-yarn.sh\n\n```\n\n#### 4.6.2 从resourcemanager\n\n- 在<font color='red'>从resourcemanager</font>即**node05**上启动rm\n\n```shell\nyarn-daemon.sh start resourcemanager\n\n```\n\n#### 4.6.3 查看resourceManager状态\n\n- node04上，它的resourcemanager的Id是rm1\n\n```shell\nyarn rmadmin -getServiceState rm1\n\n```\n\n- node05上，它的resourcemanager的Id是rm2\n\n```shell\nyarn rmadmin -getServiceState rm2\n\n```\n\n### 4.4 启动JobHistory\n\n- **node03**上执行\n\n```shell\nmr-jobhistory-daemon.sh start historyserver\n\n```\n\n\n\n## 5. 验证集群是否可用\n\n### 5.1 验证HDFS HA\n\n#### 5.1.1 访问WEB UI\n\n> node01、node02一主一备\n\n```html\nhttp://node01:50070\n\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271415.png)\n\n```\nhttp://node02:50070\n\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271416.png)\n\n#### 5.1.2 模拟主备切换\n\n- 在主namenode节点，运行\n\n```shell\nhadoop-daemon.sh stop namenode\n\n```\n\n- 访问之前为\"备namenode\"的WEB UI；发现状态更新为active\n\n- 或者使用命令查看状态\n\n```shell\nhdfs haadmin -getServiceState nn2\n\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271417.png)\n\n- 启动刚才手动停掉的namenode\n\n```shell\nhadoop-daemon.sh start namenode\n\n```\n\n- 访问它的WEB UI，发现状态更新为standby\n\n- 或者使用命令查看状态\n\n```\nhdfs haadmin -getServiceState nn1\n\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271419.png)\n\n### 5.2 验证Yarn HA\n\n> node04、node05主备切换\n\n#### 5.2.1 访问WEB UI\n\n- node04浏览器访问\n\n```\nhttp://node04:8088/cluster/cluster\n\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271519.png)\n\n- node05浏览器访问\n\n```\nhttp://node05:8088/cluster/cluster\n\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271520.png)\n\n#### 5.2.2 模拟主备切换\n\n- 在主resourcemanager节点，运行\n\n```shell\nyarn-daemon.sh stop resourcemanager\n\n```\n\n- 访问之前为\"备resourcemanager\"的WEB UI；发现状态更新为active\n\n- 或者命令查看状态\n\n```shell\nyarn rmadmin -getServiceState rm2\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271524.png)\n\n- 启动刚才手动停掉的resourcemanager\n\n```shell\nyarn-daemon.sh start resourcemanager\n```\n\n- 访问它的WEB UI，发现状态更新为standby\n\n- 或者命令查看状态\n\n```shell\nyarn rmadmin -getServiceState rm1\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架-hadoop高可用方案/assets/Image201907271526.png)\n\n#### 5.2.3 运行MR示例\n\n- 运行一下hadoop示例中的WordCount程序：\n\n```shell\nhadoop fs -put /kkb/install/hadoop-2.6.0-cdh5.14.2/LICENSE.txt /\nhadoop jar /kkb/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar wordcount /LICENSE.txt /w0727\n\n```\n\n\n\n## 6. 集群常用命令\n\n### 6.1 关闭Hadoop HA集群\n\n> 正确指令执行顺序如下\n\n- 主namenode上运行\n\n```shell\nstop-dfs.sh\n```\n\n- 主resoucemanager上运行\n\n```shell\nstop-yarn.sh\n```\n\n- 从resoucemanager上运行\n\n```shell\nyarn-daemon.sh stop resourcemanager\n```\n\n- 关闭zookeeper集群；每个zk服务器运行\n\n```shell\nzkServer.sh stop\n```\n\n### 6.2 常用命令\n\n- 单独启动namenode\n\n```shell\nhadoop-daemon.sh start namenode\n```\n\n- 单独启动datanode\n\n```shell\nhadoop-daemon.sh start datanode\n```\n\n- 单独启动journalnode\n\n```shell\nhadoop-daemon.sh start journalnode\n```\n\n- 启动zookeeper\n\n```shell\n./zkServer.sh start\n```\n\n- 启动hdfs\n\n```shell\nstart-dfs.sh\n```\n\n- 启动yarn\n\n```shell\nstart-yarn.sh\n```\n\n- 单独启动resorucemanager\n\n```shell\nyarn-daemon.sh start resouremanger\n```\n\n- 查看namenode状态（namenode1）\n\n```shell\nhdfs haadmin -getServiceState nn1\n```\n\n- 查看resourcemanager状态（resourcemanager2）\n\n```shell\nyarn rmadmin -getServiceState rm2\n```\n\n","tags":["hadoop","环境搭建","zookeeper","zookeeper ha"]},{"title":"zookeeper分布式协调框架","url":"/2019/10/27/it/zookeeper/zookeeper分布式协调框架/","content":"\n# zookeeper分布式协调框架\n\n\n\n## 1. 为什么要用ZooKeeper\n\n- 分布式框架多个独立的程序协同工作比较复杂\n  - 开发人员容易花较多的精力实现如何使多个程序协同工作的逻辑\n  - 导致没有时间更好的思考实现程序本身的逻辑\n  - 或者开发人员对程序间的协同工作关注不够，造成协调问题\n  - 且这个分布式框架中协同工作的逻辑是共性的需求\n- ZooKeeper简单易用，能够很好的解决分布式框架在运行中，出现的各种协调问题。比如集群master主备切换、节点的上下线感知、统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等等\n\n\n\n## 2. 什么是ZooKeeper\n\n- 是Google的Chubby的一个开源实现版\n- ZooKeeper\n  - 一个分布式的，开源的，用于分布式应用程序的协调服务（service）\n  - 主从架构\n- Zookeeper 作为一个分布式的服务框架\n  - 主要用来解决分布式集群中应用系统的一致性问题\n  - 它能提供基于类似于文件系统的**目录节点树**方式的数据存储，\n  - Zookeeper 作用主要是用来维护和监控存储的数据的状态变化，通过监控这些数据状态的变化，从而达到基于数据的集群管理\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201906091839.png)\n\n\n\n## 3. ZooKeeper应用初体验\n\n> 从下图观察：ZooKeeper集群目前有两种角色：leader、follower；\n>\n> ZooKeeper集群也是主从架构的：leader为主；follower为从\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/zkservice.jpg)\n\n> 通过客户端操作ZooKeeper集群，有两种类型的客户端\n>\n> ①命令行zkCli\n>\n> ②Java编程\n\n### 3.1 zkCli命令行\n\n- 集群命令（**每个节点运行此命令**）\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201906101409.png)\n\n```shell\n# 启动ZooKeeper集群；在ZooKeeper集群中的每个节点执行此命令\n${ZK_HOME}/bin/zkServer.sh start\n# 停止ZooKeeper集群（每个节点执行以下命令）\n${ZK_HOME}/bin/zkServer.sh stop\n# 查看集群状态（每个节点执行此命令）\n${ZK_HOME}/bin/zkServer.sh status\n```\n\n- 客户端连接zkServer服务器\n\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201906101413.png)\n\n```shell\n# 使用ZooKeeper自带的脚本，连接ZooKeeper的服务器\nzkCli.sh -server node01:2181,node02:2181,node03:2181\n```\n\n> -server选项后指定参数node01:2181,node02:2181,node03:2181\n>\n> 客户端随机的连接三个服务器中的一个\n\n- 客户端发出对ZooKeeper集群的读写请求\n\n  - ZooKeeper集群中有类似于linux文件系统的一个简版的文件系统；目录结构也是树状结构（目录树）\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201910231127.png)\n\n  - 重要技巧：<font color='red'>不会就喊**help**</font>\n\n  - 还记得其它框架中help的使用吗？\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201905291936.png)\n\n- 常用命令\n\n```shell\n#查看ZooKeeper根目录/下的文件列表\nls /\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201910231148.png)\n\n```shell\n#创建节点，并指定数据\ncreate /kkb\tkkb\n```\n\n```shell\n#获得某节点的数据\nget /kkb\n```\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201910231151.png)\n\n```shell\n#修改节点的数据\nset /kkb kkb01\n\n#删除节点\ndelete /kkb\n```\n\n\n\n### 3.2 Java API编程\n\n> IDE可以是eclipse，或IDEA；此处以IDEA演示\n>\n> 编程分两类：原生API编程；curator编程\n\n- [Curator官网](< http://curator.apache.org/ >)\n- Curator编程\n  - Curator对ZooKeeper的api做了封装，提供简单易用的api；\n  - 它的风格是Curator链式编程\n  - 参考《使用curator做zk编程》\n\n#### 3.2.1 常用api接口\n\n ```java\n\n\t\t/**\n     * 连接参数\n     */\n    private static final String ZK_ADDRESS = \"node01:2181,node02:2181,node03:2181\";\n    private static final String ZK_PATH = \"/zk_test\";\n    /**\n     * 初始化，建立连接\n     */\n    public static void init() {\n        //重试连接策略，失败重试次数；每次休眠5000毫秒\n        //RetryPolicy policy = new ExponentialBackoffRetry(3000, 3);\n        RetryNTimes retryPolicy = new RetryNTimes(10, 5000);\n\n        // 1.设置客户端参数，参数1：指定连接的服务器集端口列表；参数2：重试策略\n        client = CuratorFrameworkFactory.newClient(ZK_ADDRESS, retryPolicy);\n        //启动客户端，连接到zk集群\n        client.start();\n\n        System.out.println(\"zk client start successfully!\");\n    }\n\n\t\t/**\n     * 创建永久节点\n     * @throws Exception\n     */\n    public static void createPersistentZNode() throws Exception {\n        String zNodeData = \"火辣的\";\n\n        ///a/b/c\n        client.create().\n                creatingParentsIfNeeded().\n                withMode(CreateMode.PERSISTENT).\n                forPath(\"/kfly/top/orchid\", zNodeData.getBytes());\n    }\n\n// 查询节点列表\nclient.getChildren().forPath(\"/\")\n  \n// 删除节点\nclient.delete().forPath(ZK_PATH);\n\n// 查询节点数据\nclient.getData().forPath(ZK_PATH)\n  \n// 修改节点数据\nclient.setData().forPath(ZK_PATH, data2.getBytes())\n  \n  \n    /**\n     * 监听ZNode， 一次监听，多次生效（zk client命令行，一次监听，生效一次）\n     */\n    public static void watchZNode() throws Exception {\n        //设置节点的cache\n        TreeCache treeCache = new TreeCache(client, \"/zk_test\");\n        //设置监听器和处理过程\n        treeCache.getListenable().addListener(new TreeCacheListener() {\n            @Override\n            public void childEvent(CuratorFramework client, TreeCacheEvent event) throws Exception {\n                ChildData data = event.getData();\n                if(data !=null){\n                    switch (event.getType()) {\n                        case NODE_ADDED: // 新增\n                            break;\n                        case NODE_REMOVED: // 删除\n                            break;\n                        case NODE_UPDATED: // 修改\n                            break;\n                        default:\n                            break;\n                    }\n                }\n            }\n        });\n        //开始监听\n        treeCache.start();\n        Thread.sleep(60000);\n        //关闭cache\n        treeCache.close();\n    }\n\n ```\n\n\n\n## 4.基本概念和操作（25分钟）\n\n> 分布式通信有几种方式\n>\n> 1、直接通过网络连接的方式进行通信；\n>\n> 2、通过共享存储的方式，来进行通信或数据的传输\n>\n> ZooKeeper使用第二种方式，提供分布式协调服务\n\n### 4.1 ZooKeeper数据结构\n\n> ZooKeeper主要由以下三个部分实现\n\n**ZooKeeper=①简版文件系统(Znode)+②原语+③通知机制(Watcher)。**\n\n- ZK文件系统\n  - 基于类似于文件系统的**目录节点树**方式的数据存储\n- 原语\n  - 可简单理解成ZooKeeper的基本的命令\n- Watcher（监听器）\n\n![img](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/fcfaaf51f3deb48f36625a57fa1f3a292df57834.jpg)\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201909181739.png)\n\n\n\n### **4.2 数据节点**ZNode\n\n**4.2.1 什么是ZNode**\n\n- ZNode 分为四类：\n\n|            | 持久节点  | 临时节点     |\n| ---------- | --------- | ------------ |\n| 非有序节点 | create    | create -e    |\n| 有序节点   | create -s | create -s -e |\n\n**4.2.2 持久节点**\n\n- 类比，文件夹\n\n```shell\n# 创建节点/zk_test，并设置数据my_data\ncreate /zk_test my_data\n# 持久节点，只有显示的调用命令，才能删除永久节点\ndelete /zk_test\n```\n\n**4.2.3 临时节点**\n\n- 临时节点的生命周期跟客户端**会话**session绑定，一旦会话失效，临时节点被删除。\n\n```shell\n# client1上创建临时节点\ncreate -e /tmp tmpdata\n\n# client2上查看client1创建的临时节点\nls /\n\n# client1断开连接\nclose\n\n# client2上观察现象，发现临时节点被自动删除\nls /\n```\n\n**4.2.4 有序节点**\n\n- ZNode也可以设置为**有序节点**\n\n- 为什么设计临时节点？\n\n- 防止多个不同的客户端在同一目录下，创建同名ZNode，由于重名，导致创建失败\n\n- 如何创建临时节点\n\n  - 命令行使用-s选项：create -s /kkb kkb\n\n  - Curator编程，可添加一个特殊的属性：CreateMode.EPHEMERAL\n\n    ```java\n    \n        /**\n         * 创建临时节点\n         * @throws Exception\n         */\n        public static void createEphemeralZNode() throws Exception {\n            // 创建临时节点\n            String zNodeData2 = \"kfly\";\n            client.create().\n                    creatingParentsIfNeeded().\n              \t\t\t// 创建临时节点 EPHEMERAL\n                    withMode(CreateMode.EPHEMERAL).\n                  \n              forPath(\"/top/ding\", zNodeData2.getBytes());\n            TimeUnit.SECONDS.sleep(10);\n        }\n    ```\n    \n    \n\n- 一旦节点被标记上这个属性，那么在这个节点被创建时，ZooKeeper 就会自动在其节点后面追加上一个整型数字\n\n  - 这个整数是一个由父节点维护的自增数字。\n  - 提供了创建唯一名字的ZNode的方式\n\n  ```shell\n  # 创建持久、有序节点\n  create -s /test01 test01-data\n  # Created /test010000000009\n  \n  ```\n\n\n\n\n### 4.3 会话（Session)\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/ZooKeeper2.png)\n\n**4.4.1 什么是会话** \n\n- 客户端要对ZooKeeper集群进行读写操作，得先与某一ZooKeeper服务器建立TCP长连接；此TCP长连接称为建立一个会话Session。\n\n- 每个会话有超时时间：SessionTimeout\n  - 当客户端与集群建立会话后，如果超过SessionTimeout时间，两者间没有通信，会话超时\n\n**4.4.2 会话的特点**\n\n- 客户端打开一个Session中的请求以FIFO（先进先出）的顺序执行；\n  - 如客户端client01与集群建立会话后，先发出一个create请求，再发出一个get请求；\n  - 那么在执行时，会先执行create，再执行get\n- 若打开两个Session，无法保证Session间，请求FIFO执行；只能保证一个session中请求的FIFO\n\n**4.4.3 会话的生命周期**\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201905311514.png)\n\n- 会话的生命周期\n  - 未建立连接\n  - 正在连接\n  - 已连接\n  - 关闭连接\n\n### **4.4 请求**\n\n- 读写请求\n  - 通过客户端向ZooKeeper集群中写数据\n  - 通过客户端从ZooKeeper集群中读数据\n\n![ZooKeeper官网架构图](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/zkservice.jpg)\n\n### 4.5 事务zxid\n\n- 事务\n  - 客户端的写请求，会对ZooKeeper中的数据做出更改；如增删改的操作\n  - 每次写请求，会生成一次事务\n  - 每个事务有一个全局唯一的事务ID，用 ZXID 表示；全局自增\n\n- 事务特点\n  - ACID：\n  - 原子性atomicity | 一致性consistency | 隔离性isolation | 持久性durability\n\n- ZXID结构：\n  - 通常是一个64位的数字。由**epoch+counter**组成\n  - epoch、counter各32位\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201906140813.png)\n\n### 4.6 Watcher监视与通知 \n\n**4.6.1 为什么要有Watcher**\n\n- 问：客户端如何获取ZooKeeper服务器上的最新数据？\n\n  - **方式一**轮询：ZooKeeper以远程服务的方式，被客户端访问；客户端以轮询的方式获得znode数据，效率会比较低（代价比较大）\n\n  ![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201905291811.png)\n\n  - **方式二**基于通知的机制：\n    - 客户端在znode上注册一个Watcher监视器\n    - 当znode上数据出现变化，watcher监测到此变化，通知客户端\n\n  ![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201905291818.png)\n\n- 对比，那种好？\n\n**4.6.2 什么是Watcher?**\n\n- 客户端在服务器端，注册的事件监听器；\n- watcher用于监听znode上的某些事件\n  - 比如znode数据修改、节点增删等；\n  - 当监听到事件后，watcher会触发通知客户端\n\n**4.6.3 如何设置Watcher**\n\n> 注意：**Watcher是一个<font color='red'>单次触发的操作</font>**\n\n- 可以设置watcher的命令如下：\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201905291977.png)\n\n- **示例1**\n\n```shell\n#ls path [watch]\n#node01 上执行\nls /zk_test watch\n\n#node02 上执行\ncreate /zk_test/dir01 dir01-data\n\n#观察node-01上变化\n[zk: node-01:2181,node-02:2181,node-03:2181(CONNECTED) 87] \nWATCHER::\n\nWatchedEvent state:SyncConnected type:NodeChildrenChanged path:/zk_test\n\n```\n\n图示：\n\n- client1上执行步骤1\n\n- client2上执行步骤2；\n\n- client1上观察现象3\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201905311334.png)\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201905311335.png)\n\n- **示例2**\n\n```shell\n#监控节点数据的变化；\n#node02上\nget /zk_test watch\n\n#node03上\nset /zk_test \"junk01\"\n#观察node2上cli的输出，检测到变化\n\n```\n\n- **示例3**：节点上下线监控\n\n  - **原理：**\n\n    1. 节点1（client1）创建临时节点\n    2. 节点2（client2）在临时节点，注册监听器watcher\n    3. 当client1与zk集群断开连接，临时节点会被删除\n    4. watcher发送消息，通知client2，临时节点被删除的事件\n\n  - **用到的zk特性：**\n\n    ​\tWatcher+临时节点\n\n  - **好处：**\n\n    ​\t通过这种方式，检测和被检测系统不需要直接关联（如client1与client2），而是通过ZK上的某个节点进行关联，大大减少了系统**耦合**。\n\n  - **实现：**\n\n    client1操作\n\n    ```shell\n    # 创建临时节点\n    create -e /zk_tmp tmp-data\n    \n    ```\n\n    client2操作\n\n    ```shell\n    # 在/zk_tmp注册监听器\n    ls /zk_tmp watch\n    \n    ```\n\n    client1操作\n\n    ```shell\n    # 模拟节点下线\n    close\n    \n    ```\n\n    观察client2\n\n    ```shell\n    WATCHER::\n    \n    WatchedEvent state:SyncConnected type:NodeDeleted path:/zk_tmp\n    \n    ```\n\n  - **图示：**\n\n    client1：\n\n    ![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201905311401.png)\n\n    client2：\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201905311402.png)\n\n\n\n\n\n## 5. ZooKeeper工作原理\n\n- ZooKeeper使用原子广播协议叫做Zab(ZooKeeper Automic Broadcast)协议\n- Zab协议有两种模式\n  - **恢复模式（选主）**：因为ZooKeeper也是主从架构；当ZooKeeper集群没有主的角色leader时，从众多服务器中选举leader时，处于此模式\n  - **广播模式（同步）**：当集群有了leader后，客户端向ZooKeeper集群读写数据时，集群处于此模式\n- 为了保证事务的顺序一致性，ZooKeeper采用了递增的事务id号（zxid）来标识事务，所有提议（proposal）都有zxid\n\n\n\n## 6. ZooKeeper应用场景\n\n- ZooKeeper应用场景\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/20170221224856838.png)\n\n1. NameNode使用ZooKeeper实现高可用.\n\n2. Yarn ResourceManager使用ZooKeeper实现高可用.\n\n3. 利用ZooKeeper对HBase集群做高可用配置\n\n4. kafka使用ZooKeeper\n\n   - 保存消息消费信息比如offset.\n   \n   - 用于检测崩溃\n   \n- 主题topic发现\n  \n   - 保持主题的生产和消费状态\n   \n## 7. ACL访问控制列表\n\n### 7.1 为什么要用ACL\n\nzk做为分布式架构中的重要中间件，通常会在上面以节点的方式存储一些关键信息，默认情况下，所有应用都可以读写任何节点，在复杂的应用中，这不太安全，ZK通过ACL机制来解决访问权限问题\n\n### 7.2 什么是ACL\n\nACL(Access Control List)可以设置某些客户端，对zookeeper服务器上节点的权限，如增删改查等\n\n### 7.3 ACL种类\n\nZooKeeper 采用 ACL（Access Control Lists）策略来进行权限控制。ZooKeeper 定义了如下5种权限。\n\n- CREATE: 创建**子节点**的权限。\n\n- READ: 获取节点数据和子节点列表的权限。\n\n- WRITE：更新节点数据的权限。\n\n- DELETE: 删除**子节点**的权限。\n\n- ADMIN: 设置节点ACL的权限。\n\n>  注意：CREATE 和 DELETE 都是针对子节点的权限控制。\n\n### 7.4 如何设置ACL\n\n1. 五种权限简称\n\n   ```shell\n   CREATE -> 增 -> c\n   READ -> 查 -> r\n   WRITE -> 改 -> w\n   DELETE -> 删 -> d\n   ADMIN -> 管理 -> a\n   这5种权限简写为**crwda**\n   ```\n\n2. 鉴权模式\n\n```shell\n- world：默认方式，相当于全世界都能访问\n- auth：代表已经认证通过的用户(cli中可以通过addauth digest user:pwd 来添加当前上下文中的授权用户)\n- digest：即用户名:密码这种方式认证，这也是业务系统中最常用的\n- ip：使用Ip地址认证\n```\n\n3. 演示auth方式\n\n```shell\n# 1）增加一个认证用户\n# addauth digest 用户名:密码明文\naddauth digest kkb:kkb\n\n# 2）设置权限\n# setAcl /path auth:用户名:密码明文:权限\nsetAcl /zk_test auth:kkb:kkb:rw\n\n# 3）查看ACL设置\ngetAcl /zk_test\n```\n\n\n\n## 8. HDFS HA方案\n\n### 8.1 ZooKeeper监听器\n\n- 关于ZooKeeper监听器有三个重要的逻辑：\n\n   - **注册**：客户端向ZooKeeper集群注册监听器\n\n   - **监听事件**：监听器负责监听特定的事件\n\n   - **回调函数**：当监听器监听到事件的发生后，调用注册监听器时定义的回调函数\n\n### 8.2 类比举例\n\n- 为了便于理解，举例：旅客住店无房可住的情况\n\n   - 一哥们去酒店办理入住，但是被告知目前无空房\n   - 这哥们告诉客服：你给我记住了，帮我留意一下有没有空出的房间，如果有，及时通知我（**类似注册监听器，监听特定事件**）\n   - 将近12点，有房客退房，有空闲的房间（**事件**）\n   - 客服发现有空房（**监听到事件**）\n   - 及时通知这哥们\n   - 这哥们收到通知后，**做一些事**，比如马上从附近酒吧赶回酒店（**调用回调函数**）\n   \n   ![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201910242342.png)\n   \n\n### 8.3 HDFS HA原理\n\n> 关键逻辑：\n>\n> ①监听器：**注册、监听事件、回调函数**\n>\n> ②共享存储：JournalNode\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201905211519.png)   \n\n- 在Hadoop 1.x版本，HDFS集群的NameNode一直存在单点故障问题：\n  - 集群只存在一个NameNode节点，它维护了HDFS所有的元数据信息\n  - 当该节点所在服务器宕机或者服务不可用，整个HDFS集群处于不可用状态\n  \n- Hadoop 2.x版本提出了高可用 (High Availability, HA) 解决方案\n  \n> HDFS HA方案，主要分两部分：\n  >\n> ①元数据同步\n  >\n  > ②主备切换\n\n- 元数据同步\n- 在同一个HDFS集群，运行两个互为主备的NameNode节点。\n  - 一台为主Namenode节点，处于Active状态，一台为备NameNode节点，处于Standby状态。\n  - 其中只有Active NameNode对外提供读写服务，Standby NameNode会根据Active NameNode的状态变化，在必要时**切换**成Active状态。\n  - **JournalNode集群**\n    - 在主备切换过程中，新的Active NameNode必须确保与原Active NamNode元数据同步完成，才能对外提供服务\n    - 所以用JournalNode集群作为共享存储系统；\n    - 当客户端对HDFS做操作，会在Active NameNode中edits.log文件中作日志记录，同时日志记录也会写入JournalNode集群；负责存储HDFS新产生的元数据\n    - 当有新数据写入JournalNode集群时，Standby NameNode能监听到此情况，将新数据同步过来\n    - Active NameNode(写入)和Standby NameNode(读取)实现元数据同步\n    - 另外，所有datanode会向两个主备namenode做block report\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201909200732.png)\n\n- <font color='blue'>②主备切换</font>\n\n- **ZKFC涉及角色**\n\n  - 每个NameNode节点上各有一个ZKFC进程\n  - ZKFC即ZKFailoverController，作为独立进程存在，负责控制NameNode的主备切换\n  - ZKFC会监控NameNode的健康状况，当发现Active NameNode异常时，通过Zookeeper集群进行namenode主备选举，完成Active和Standby状态的切换\n    - ZKFC在启动时，同时会初始化HealthMonitor和ActiveStandbyElector服务\n    - ZKFC同时会向HealthMonitor和ActiveStandbyElector注册相应的回调方法（如上图的①回调、②回调）\n    - **HealthMonitor**定时调用NameNode的HAServiceProtocol RPC接口(monitorHealth和getServiceStatus)，监控NameNode的健康状态并向ZKFC反馈\n    - **ActiveStandbyElector**接收ZKFC的选举请求，通过Zookeeper自动完成namenode主备选举\n    - 选举完成后回调ZKFC的主备切换方法对NameNode进行Active和Standby状态的切换\n  \n- **主备选举过程：**两个ZKFC通过各自ActiveStandbyElector发起NameNode的主备选举，这个过程利用Zookeeper的写一致性和临时节点机制实现\n\n  - 当发起一次**主备**选举时，ActiveStandbyElector会尝试在Zookeeper创建临时节点`/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock`，Zookeeper的写一致性保证最终只会有一个ActiveStandbyElector创建成功\n- ActiveStandbyElector从ZooKeeper获得选举结果\n  \n  - 创建成功的 ActiveStandbyElector回调ZKFC的回调方法②，将对应的NameNode切换为Active NameNode状态\n- 而创建失败的ActiveStandbyElector回调ZKFC的回调方法②，将对应的NameNode切换为Standby NameNode状态\n  \n- 不管是否选举成功，所有ActiveStandbyElector都会在临时节点ActiveStandbyElectorLock上注册一个Watcher监听器，来监听这个节点的状态变化事件\n  \n  - 如果Active NameNode对应的HealthMonitor检测到NameNode状态异常时，通知对应ZKFC\n- ZKFC会调用 ActiveStandbyElector 方法，删除在Zookeeper上创建的临时节点ActiveStandbyElectorLock\n  \n  - 此时，Standby NameNode的ActiveStandbyElector注册的Watcher就会监听到此节点的 NodeDeleted事件。\n- 收到这个事件后，此ActiveStandbyElector发起主备选举，成功创建临时节点ActiveStandbyElectorLock，如果创建成功，则Standby NameNode被选举为Active NameNode（过程同上）\n  \n- **如何防止脑裂**\n\n  - 脑裂\n\n    在分布式系统中双主现象又称为脑裂，由于Zookeeper的“假死”、长时间的垃圾回收或其它原因都可能导致双Active NameNode现象，此时两个NameNode都可以对外提供服务，无法保证数据一致性\n\n  - 隔离\n\n    对于生产环境，这种情况的出现是毁灭性的，必须通过自带的**隔离（Fencing）**机制预防此类情况\n\n  - 原理\n    - ActiveStandbyElector成功创建ActiveStandbyElectorLock临时节点后，会创建另一个ActiveBreadCrumb持久节点\n  \n    - ActiveBreadCrumb持久节点保存了Active NameNode的地址信息\n  \n    - 当Active NameNode在正常的状态下断开Zookeeper Session，会一并删除临时节点ActiveStandbyElectorLock、持久节点ActiveBreadCrumb\n  \n    - 但是如果ActiveStandbyElector在异常的状态下关闭Zookeeper Session，那么持久节点ActiveBreadCrumb会保留下来（此时有可能由于active NameNode与ZooKeeper通信不畅导致，所以此NameNode**还处于active状态**）\n  \n    - 当另一个NameNode要由standy变成active状态时，会发现上一个Active NameNode遗留下来的ActiveBreadCrumb节点，那么会回调ZKFailoverController的方法对旧的Active NameNode进行fencing\n  \n      ①首先ZKFC会尝试调用旧Active NameNode的HAServiceProtocol RPC接口的transitionToStandby方法，看能否将其状态切换为Standby\n  \n      ②如果transitionToStandby方法切换状态失败，那么就需要执行Hadoop自带的隔离措施，Hadoop目前主要提供两种隔离措施：\n      sshfence：SSH to the Active NameNode and kill the process；\n      shellfence：run an arbitrary shell command to fence the Active NameNode\n  \n      ③只有成功地fencing之后，选主成功的ActiveStandbyElector才会回调ZKFC的becomeActive方法将对应的NameNode切换为Active，开始对外提供服务\n  \n\n\n\n>前情回顾：\n>\n>- ZooKeeper使用原子广播协议Zab(ZooKeeper Automic Broadcast)，保证分布式一致性\n>- 协议Zab协议有两种模式，它们分别是\n>  - ①**恢复模式（选主）**：因为ZooKeeper也是主从架构；当ZooKeeper集群没有主的角色leader时，从众多服务器中选举leader时，处于此模式；主要处理内部矛盾，我们称之为**安其内**\n>  - ②**广播模式（同步）**：当集群有了leader后，客户端向ZooKeeper集群读写数据时，集群处于此模式；主要处理外部矛盾，我们称之为**攘其外**\n>- 事务\n>  - 为了保证事务的顺序一致性，ZooKeeper采用了递增的事务id号（zxid）来标识事务，所有提议（proposal）都有zxid\n>  - 每次事务的提交，必须符合quorum多数派\n\n## 9. ZooKeeper读写\n\n### 9.1 ZooKeeper集群架构图\n\n- ZooKeeper集群也是主从架构\n  - 主角色：leader\n  - 从角色：follower或observer；统称为learner\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/zkservice.jpg)\n\n\n\n> 客户端与ZK集群交互，主要分两大类操作\n\n### 9.2 读操作\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201910251149.png)\n\n- 常见的读取操作，如ls /查看目录；get /zktest查询ZNode数据\n\n- 读操作\n\n  - 客户端先与某个ZK服务器建立Session\n\n  - 然后，直接从此ZK服务器读取数据，并返回客户端即可\n\n  - 关闭Session\n\n### 9.3 写操作\n\n- 写操作比较复杂；为了便于理解，先举个生活中的例子：去银行存钱\n  - 银行柜台共有5个桂圆姐姐，编程从①到⑤，其中③是**领导leader**\n  - 有两个客户\n  - 客户①找到桂圆①，说：昨天少给我存了1000万，现在需要给我加进去\n  - 桂圆①说，对不起先生，我没有这么大的权限，请你稍等一下，我向领导**leader**③汇报一下\n  - 领导③收到消息后，为了做出英明的决策，要征询下属的意见(**proposal**)①②④⑤\n  - 只要有**过半数quorum**（5/2+1=3，包括leader自己）同意，则leader做出决定(**commit**)，同意此事\n  - leader告知所有下属follower，你们都记下此事生效\n  - 桂圆①答复客户①，说已经给您账号里加了1000万\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201906121126.png)\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image2019061212537.png)\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201910251203.png)\n\n- 客户端写操作\n  \n  - ①客户端向zk集群写入数据，如create /kkb；与一个follower建立Session连接，从节点follower01\n  \n  - ②follower将写请求转发给leader\n  \n  - ③leader收到消息后，发出**proposal提案**（创建/kkb），每个follower先**记录下**要创建/kkb\n  \n  - ④超过**半数quorum**（包括leader自己）同意提案，则leader提交**commit提案**，leader本地创建/kkb节点ZNode\n  \n  - ⑤leader通知所有follower，也commit提案；follower各自在本地创建/kkb\n  \n  - ⑥follower01响应client\n  \n    \n  \n\n## 10.  ZooKeeper服务\n\n### 10.1 **架构问题**\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/zkservice.jpg)\n\n- leader很重要？\n- 如果没有leader怎么办？\n  - 开始选举新的leader\n\n- **ZooKeeper服务器四种状态：**\n    - looking：服务器处于寻找Leader群首的状态\n\n    - leading：服务器作为群首时的状态\n\n    - following：服务器作为follower跟随者时的状态\n\n    - observing：服务器作为观察者时的状态\n\n\n\n> leader选举分**两种情况**\n>\n> - 全新集群leader选举\n>\n> - 非全新集群leader选举\n\n### 10.2 全新集群leader选举\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201906130749.png)\n\n  - 以3台机器组成的ZooKeeper集群为例 \n\n  - 原则：集群中过**半数**（多数派quorum）Server启动后，才能选举出Leader；\n\n      - 此处quorum数是多少？3/2+1=2\n      - 即quorum=集群服务器数除以2，再加1\n\n  - 理解leader选举前，先了解几个概念\n\n        - 选举过程中，每个server需发出投票；投票信息**vote信息**结构为(sid, zxid)\n\n            全新集群，server1~3初始投票信息分别为：\n      \n            ​\tserver1 ->  **(1, 0)**\n          ​\t​server2 ->  **(2, 0)**\n          ​\tserver3 ->  **(3, 0)**\n        \n    - **leader选举公式**：\n    \n      ​\tserver1 vote信息 (sid1,zxid1)\n    \n      ​\tserver2 vote信息 (sid2,zxid2)\n    \n      ​\t**①zxid大的server胜出；**\n    \n      ​\t**②若zxid相等，再根据判断sid判断，sid大的胜出**\n  \n  - 选举leader流程：\n\n    > 假设按照ZK1、ZK2、ZK3的依次启动\n    \n    - 启动ZK1后，投票给自己，vote信息(1,0)，没有过半数，选举不出leader\n    \n    - 再启动ZK2；ZK1和ZK2票投给自己及其他服务器；ZK1的投票为(1, 0)，ZK2的投票为(2, 0)\n    \n    - 处理投票。每个server将收到的多个投票做处理\n      - 如ZK1投给自己的票(1,0)与ZK2传过来的票(2,0)比较；\n      - 利用leader选举公式，因为zxid都为0，相等；所以判断sid最大值；2>1；ZK1更新自己的投票为(2, 0)\n      - ZK2也是如此逻辑，ZK2更新自己的投票为(2,0)\n    \n    - 再次发起投票\n      - ZK1、ZK2上的投票都是(2,0)\n      - 发起投票后，ZK1上有一个自己的票(2,0)和一票来自ZK2的票(2,0)，这两票都选ZK2为leader\n      - ZK2上有一个自己的票(2,0)和一票来自ZK1的票(2,0)，这两票都选ZK2为leader\n      - 统计投票。server统计投票信息，是否有半数server投同一个服务器为leader；\n        - ZK2当选2票；多数\n      - 改变服务器状态。确定Leader后，各服务器更新自己的状态\n        - 更改ZK2状态从looking到leading，为Leader\n        - 更改ZK1状态从looking到following，为Follower\n    \n    - 当K3启动时，发现已有Leader，不再选举，直接从LOOKING改为FOLLOWING\n\n### 10.3 非全新集群leader选举\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201906131101.png)\n\n- 选举原理同上比较zxid、sid\n- 不再赘述\n\n\n\n## 11. ZAB算法\n\n### 11.1 仲裁quorum\n\n- 什么是仲裁quorum？\n\n  - 发起proposal时，只要多数派同意，即可生效\n\n- 为什么要仲裁？\n\n  - 多数据派不需要所有的服务器都响应，proposal就能生效\n  - 且能提高集群的响应速度\n\n- quorum数如何选择？\n\n  -    **集群节点数 / 2 + 1**\n  - 如3节点的集群：quorum数=3/2+1=2\n\n### 11.2 网络分区、脑裂\n\n  - 网络分区：网络通信故障，集群被分成了2部分\n\n  - 脑裂：\n\n    - 原leader处于一个分区；\n    - 另外一个分区选举出新的leader \n    - 集群出现2个leader\n\n### 11.3 ZAB算法\n\n> [raft算法动图地址](<http://thesecretlivesofdata.com/raft/#replication>)\n\n- **ZAB与RAFT相似，区别如下：**\n\n  1、zab心跳从follower到leader；raft相反\n\n  2、zab任期叫epoch\n\n- 一下以RAFT算法动图为例，分析ZAB算法\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/脑裂.gif)\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/脑裂-1560463867696.png)\n\n\n\n### 11.4 ZooKeeper服务器个数\n\n- 仲裁模式下，服务器个数最好为奇数个。**why?**\n\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201906131311.png)\n\n  - 5节点的比6节点的集群\n      - 容灾能力一样，\n      - quorum小，响应快\n\n\n\n## 12. ZooKeeper工作原理\n\n### 12.1写操作流程图\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/ZooKeeper3.png)\n\n1. 在Client向Follwer发出一个写的请求\n2. Follwer把请求发送给Leader\n3. Leader接收到以后开始发起投票并通知Follwer进行投票\n4. Follwer把投票结果发送给Leader\n5. Leader将结果汇总，如果多数同意，则开始写入同时把写入操作通知给Follwer，然后commit\n6. Follwer把请求结果返回给Client\n\n### 12.2 ZooKeeper状态同步\n\n完成leader选举后，zk就进入ZooKeeper之间状态同步过程\n\n1. leader构建NEWLEADER封包，包含leader中最大的zxid值；广播给其它follower\n2. follower收到后，如果自己的最大zxid小于leader的，则需要与leader状态同步；否则不需要\n3. leader给需要同步的每个follower创建LearnerHandler线程，负责数据同步请求\n4. leader主线程等待LearnHandler线程处理结果\n5. 只有多数follower完成同步，leader才开始对外服务，响应写请求\n6. LearnerHandler线程处理逻辑\n   1. 接收follower封包FOLLOWERINFO，包含此follower最大zxid（代称f-max-zxid）\n   2. f-max-zxid与leader最大zxid（代称l-max-zxid）比较\n   3. 若相等，说明当前follower是最新的\n   4. 另外，若在判断期间，有没有新提交的proposal\n      1. 如果有那么会发送DIFF封包将有差异的数据同步过去.同时将follower没有的数据逐个发送COMMIT封包给follower要求记录下来.\n      2. 如果follower数据id更大,那么会发送TRUNC封包告知截除多余数据.\n      3. 如果这一阶段内没有提交的提议值,直接发送SNAP封包将快照同步发送给follower.\n   5. 以上消息完毕之后,发送UPTODATE封包告知follower当前数据就是最新的了\n   6. 再次发送NEWLEADER封包宣称自己是leader,等待follower的响应.\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201906140856.png)\n\n\n\n## 分布式锁\n\n![](https://kfly.top/picture/kfly-top/zookeeper分布式协调框架/assets/Image201906121639.png)\n\n```shell\ncreate -s -e /locker/node_ ndata\n```\n","tags":["zookeeper","zookeeper ha"]},{"title":"Hadoop架构原理之Yarn","url":"/2019/10/21/it/hadoop/Hadoop架构原理之Yarn/","content":"\n# Hadoop架构原理之Yarn\n\n\n\n## 1. YARN介绍\n\n![img](assets/a19a61bc-9378-3e38-944a-899a09f37908.jpg)\n\n- Apache Hadoop YARN(Yet Another Resource Negotiator)是Hadoop的子项目，为分离Hadoop2.0资源管理和计算组件而引入\n- YRAN具有足够的通用性，可以支持其它的分布式计算模式\n\n![img](assets/99b59921-9a97-3199-8c39-d3b77dfdceaf.jpg)\n\n\n\n## 2. YARN架构\n\n- 类似HDFS，YARN也是经典的**主从（master/slave）架构**\n  - YARN服务由一个ResourceManager（RM）和多个NodeManager（NM）构成\n  - ResourceManager为主节点（master）\n  - NodeManager为从节点（slave）\n\n![yarn的体系结构](assets/Figure3Architecture-of-YARN.png)\n\n\n\n\n\n- ApplicationMaster可以在容器内运行任何类型的任务。例如，MapReduce ApplicationMaster请求容器启动map或reduce任务，而Giraph ApplicationMaster请求容器运行Giraph任务。\n\n| 组件名                 | 作用                                                         |\n| :--------------------- | ------------------------------------------------------------ |\n| **ApplicationManager** | 相当于这个Application的监护人和管理者，负责监控、管理这个Application的所有Attempt在cluster中各个节点上的具体运行，同时负责向Yarn ResourceManager申请资源、返还资源等； |\n| **NodeManager**        | 是Slave上一个独立运行的进程，负责上报节点的状态(磁盘，内存，cpu等使用信息)； |\n| **Container**          | 是yarn中分配资源的一个单位，包涵内存、CPU等等资源，YARN以Container为单位分配资源； |\n\nResourceManager 负责对各个 NodeManager 上资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的 ApplicationMaster，它负责向 ResourceManager 申请资源，并要求 NodeManger 启动可以占用一定资源的任务。由于不同的 ApplicationMaster 被分布到不同的节点上，因此它们之间不会相互影响。\n\nClient 向 ResourceManager 提交的每一个应用程序都必须有一个 ApplicationMaster，它经过 ResourceManager 分配资源后，运行于某一个 Slave 节点的 Container 中，具体做事情的 Task，同样也运行与某一个 Slave 节点的 Container 中。\n\n### 2.1 **ResourceManager**\n\n- RM是一个全局的资源管理器，集群只有一个\n  - 负责整个系统的资源管理和分配\n  - 包括处理客户端请求\n  - 启动/监控 ApplicationMaster\n  - 监控 NodeManager、资源的分配与调度\n- 它主要由两个组件构成：\n  - 调度器（Scheduler）\n  - 应用程序管理器（Applications Manager，ASM）\n\n- 调度器\n  - 调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。\n  - 需要注意的是，该调度器是一个“纯调度器”\n    - 它不从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。\n    - 调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。\n\n- 应用程序管理器\n  - 应用程序管理器主要负责管理整个系统中所有应用程序\n  - 接收job的提交请求\n  - 为应用分配第一个 Container 来运行 ApplicationMaster，包括应用程序提交、与调度器协商资源以启动 ApplicationMaster、监控 ApplicationMaster 运行状态并在失败时重新启动它等\n\n### 2.2 **NodeManager**\n\n![nodemanager架构](assets/20190103113256851.png)\n\n- NodeManager 是一个 slave 服务，整个集群有多个\n\n- NodeManager ：\n  - 它负责接收 ResourceManager 的资源分配请求，分配具体的 Container 给应用。\n  - 负责监控并报告 Container 使用信息给 ResourceManager。\n\n- 功能：\n\n  - NodeManager 本节点上的资源使用情况和各个 Container 的运行状态（cpu和内存等资源）\n  - 接收及处理来自 ResourceManager 的命令请求，分配 Container 给应用的某个任务；\n  - 定时地向RM汇报以确保整个集群平稳运行，RM 通过收集每个 NodeManager 的报告信息来追踪整个集群健康状态的，而 NodeManager 负责监控自身的健康状态；\n  - 处理来自 ApplicationMaster 的请求；\n  - 管理着所在节点每个 Container 的生命周期；\n  - 管理每个节点上的日志；\n\n  - 当一个节点启动时，它会向 ResourceManager 进行注册并告知 ResourceManager 自己有多少资源可用。\n  - 在运行期，通过 NodeManager 和 ResourceManager 协同工作，这些信息会不断被更新并保障整个集群发挥出最佳状态。\n\n  - NodeManager 只负责管理自身的 Container，它并不知道运行在它上面应用的信息。负责管理应用信息的组件是 ApplicationMaster\n\n### 2.3 Container\n\n- Container 是 YARN 中的资源抽象\n  - 它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等\n  - 当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。\n  - YARN 会为每个任务分配一个 Container，且该任务只能使用该 Container 中描述的资源。\n\n- Container 和集群NodeManager节点的关系是：\n  - 一个NodeManager节点可运行多个 Container\n  - 但一个 Container 不会跨节点。\n  - 任何一个 job 或 application 必须运行在一个或多个 Container 中\n  - 在 Yarn 框架中，ResourceManager 只负责告诉 ApplicationMaster 哪些 Containers 可以用\n  - ApplicationMaster 还需要去找 NodeManager 请求分配具体的 Container。\n\n- 需要注意的是\n  - Container 是一个动态资源划分单位，是根据应用程序的需求动态生成的\n  - 目前为止，YARN 仅支持 CPU 和内存两种资源，且使用了轻量级资源隔离机制 Cgroups 进行资源隔离。\n\n- 功能：\n  - 对task环境的抽象；\n\n  - 描述一系列信息；\n\n  - 任务运行资源的集合（cpu、内存、io等）；\n\n  - 任务运行环境\n\n### 2.4 **ApplicationMaster**\n\n- 功能：\n  - 数据切分；\n  - 为应用程序申请资源并进一步分配给内部任务（TASK）；\n  - 任务监控与容错；\n  - 负责协调来自ResourceManager的资源，并通过NodeManager监视容器的执行和资源使用情况。\n\n- ApplicationMaster 与 ResourceManager 之间的通信\n  - 是整个 Yarn 应用从提交到运行的最核心部分，是 Yarn 对整个集群进行动态资源管理的根本步骤\n  - Yarn 的动态性，就是来源于多个Application 的 ApplicationMaster 动态地和 ResourceManager 进行沟通，不断地申请、释放、再申请、再释放资源的过程。\n\n### 2.5 Resource Request\n\n[引用连接](https://www.jianshu.com/p/f50e85bdb9ce)\n\n- Yarn的设计目标\n  - 允许我们的各种应用以共享、安全、多租户的形式使用整个集群。\n  - 并且，为了保证集群资源调度和数据访问的高效性，Yarn还必须能够感知整个集群拓扑结构。\n\n- 为了实现这些目标，ResourceManager的调度器Scheduler为应用程序的资源请求定义了一些灵活的协议，**Resource Request**和**Container**。\n  - 一个应用先向ApplicationMaster发送一个满足自己需求的资源请求\n  - 然后ApplicationMaster把这个资源请求以resource-request的形式发送给ResourceManager的Scheduler\n  - Scheduler再在这个原始的resource-request中返回分配到的资源描述Container。\n\n- 每个ResourceRequest可看做一个可序列化Java对象，包含的字段信息如下：\n\n```xml\n<!--\n- resource-name：资源名称，现阶段指的是资源所在的host和rack，后期可能还会支持虚拟机或者更复杂的网络结构\n- priority：资源的优先级\n- resource-requirement：资源的具体需求，现阶段指内存和cpu需求的数量\n- number-of-containers：满足需求的Container的集合\n-->\n<resource-name, priority, resource-requirement, number-of-containers>\n```\n\n### 2.6 JobHistoryServer \n\n- 作业历史服务\n\n  - 记录在yarn中调度的作业历史运行情况情况 ,\n\n  - 通过命令启动\n\n    ```shell\n    mr-jobhistory-daemon.sh start historyserver\n    ```\n\n  - 在集群中的数据节点机器上单独使用命令启动直接启动即可,\n\n  - 启动成功后会出现JobHistoryServer进程(使用jps命令查看，下面会有介绍) ,\n\n  - 并且可以从19888端口进行查看日志详细信息\n\n    ```\n    node01:19888\n    ```\n\n    点击链接，查看job日志\n\n    ![](assets/Image201910202320.png)\n\n- 如果没有启动jobhistoryserver，无法查看应用的日志\n\n![1563002086000](assets/1563002086000.png)\n\n- 打开如下图界面，在下图中点击History，页面会进行一次跳转\n\n![1563002731472](assets/1563002731472.png)\n\n- 点击History之后 跳转后的页面如下图是空白的，因为没有启动jobhistoryserver\n\n![1563002773601](assets/1563002773601.png)\n\n- jobhistoryserver启动后，在此运行MR程序，如wordcount\n\n![1563004024903](assets/1563004024903.png)\n\n- 点击History连接，跳转一个赞新的页面\n  - TaskType中列举的map和reduce，Total表示此次运行的mapreduce程序执行所需要的map和reduce的任务数\n\n![1563004057197](assets/1563004057197.png)\n\n- 点击TaskType列中Map连接\n\n![1563004490476](assets/1563004490476.png)\n\n- 看到map任务的相关信息比如执行状态,启动时间，完成时间。\n\n![1563004598290](assets/1563004598290.png)\n\n- 可以使用同样的方式我们查看reduce任务执行的详细信息，这里不再赘述.\n\n- jobhistoryserver就是进行作业运行过程中历史运行信息的记录，方便我们对作业进行分析.\n\n### 2.7 Timeline Server \n\n- 用来写日志服务数据 , 一般来写与第三方结合的日志服务数据(比如spark等)\n- 它是对jobhistoryserver功能的有效补充，jobhistoryserver只能对mapreduce类型的作业信息进行记录\n- 它记录除了jobhistoryserver能够进行对作业运行过程中信息进行记录之外\n- 还记录更细粒度的信息，比如任务在哪个队列中运行，运行任务时设置的用户是哪个用户。\n\n- 根据官网的解释jobhistoryserver只能记录mapreduce应用程序的记录，timelineserver功能更强大,但不是替代jobhistory两者是功能间的互补关系.\n\n![1563006522419](assets/1563006522419.png)\n\n- [官网教程](<http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/TimelineServer.html>)\n\n\n\n## 3. YARN应用运行原理\n\n![yarn架构图](assets/yarn_architecture.gif)\n\n\n### 3.1 YARN应用提交过程\n\n- Application在Yarn中的执行过程，整个执行过程可以总结为三步：\n\n  - 应用程序提交\n  - 启动应用的ApplicationMaster实例\n  - ApplicationMaster 实例管理应用程序的执行\n\n- **具体提交过程为：**\n\n  ![](assets/Image201909161351.png)\n\n  - 客户端程序向 ResourceManager 提交应用，并请求一个 ApplicationMaster 实例；\n  - ResourceManager 找到一个可以运行一个 Container 的 NodeManager，并在这个 Container 中启动 ApplicationMaster 实例；\n  - ApplicationMaster 向 ResourceManager 进行注册，注册之后客户端就可以查询 ResourceManager 获得自己 ApplicationMaster 的详细信息，以后就可以和自己的 ApplicationMaster 直接交互了（这个时候，客户端主动和 ApplicationMaster 交流，应用先向 ApplicationMaster 发送一个满足自己需求的资源请求）；\n  - ApplicationMaster 根据 resource-request协议 向 ResourceManager 发送 resource-request请求；\n  - 当 Container 被成功分配后，ApplicationMaster 通过向 NodeManager 发送 **container-launch-specification**信息 来启动Container，container-launch-specification信息包含了能够让Container 和 ApplicationMaster 交流所需要的资料；\n  - 应用程序的代码以 task 形式在启动的 Container 中运行，并把运行的进度、状态等信息通过 **application-specific**协议 发送给ApplicationMaster；\n  - 在应用程序运行期间，提交应用的客户端主动和 ApplicationMaster 交流获得应用的运行状态、进度更新等信息，交流协议也是 **application-specific**协议；\n  - 应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster 向 ResourceManager 取消注册然后关闭，用到所有的 Container 也归还给系统。\n\n- **精简版的：**\n\n  - 步骤1：用户将应用程序提交到 ResourceManager 上；\n  - 步骤2：ResourceManager为应用程序 ApplicationMaster 申请资源，并与某个 NodeManager 通信启动第一个 Container，以启动ApplicationMaster；\n  - 步骤3：ApplicationMaster 与 ResourceManager 注册进行通信，为内部要执行的任务申请资源，一旦得到资源后，将于 NodeManager 通信，以启动对应的 Task；\n  - 步骤4：所有任务运行完成后，ApplicationMaster 向 ResourceManager 注销，整个应用程序运行结束。\n\n### 3.2 MapReduce on YARN\n\n![img](assets/820234-20160604233916133-2026396104.jpg)\n\n- 提交作业\n\n  - ①程序打成jar包，在客户端运行hadoop jar命令，提交job到集群运行\n  - job.waitForCompletion(true)中调用Job的submit()，此方法中调用JobSubmitter的submitJobInternal()方法；\n    - ②submitClient.getNewJobID()向resourcemanager请求一个MR作业id\n    - 检查输出目录：如果没有指定输出目录或者目录已经存在，则报错\n    - 计算作业分片；若无法计算分片，也会报错\n    - ③运行作业的相关资源，如作业的jar包、配置文件、输入分片，被上传到HDFS上一个以作业ID命名的目录（jar包副本默认为10，运行作业的任务，如map任务、reduce任务时，可从这10个副本读取jar包）\n    - ④调用resourcemanager的submitApplication()提交作业\n  - 客户端**每秒**查询一下作业的进度（map 50% reduce 0%），进度如有变化，则在控制台打印进度报告；\n  - 作业如果成功执行完成，则打印相关的计数器\n  - 但如果失败，在控制台打印导致作业失败的原因（要学会查看日志，定位问题，分析问题，解决问题）\n\n- **初始化作业**\n\n  - 当ResourceManager(一下简称RM)收到了submitApplication()方法的调用通知后，请求传递给RM的scheduler（调度器）；调度器分配container（容器）\n  - ⑤a RM与指定的NodeManager通信，通知NodeManager启动容器；NodeManager收到通知后，创建占据特定资源的container；\n  - ⑤b 然后在container中运行MRAppMaster进程\n  - ⑥MRAppMaster需要接受任务（各map任务、reduce任务的）的进度、完成报告，所以appMaster需要创建多个簿记对象，记录这些信息\n  - ⑦从HDFS获得client计算出的输入分片split\n    - 每个分片split创建一个map任务\n    - 通过 mapreduce.job.reduces 属性值(编程时，jog.setNumReduceTasks()指定)，知道当前MR要创建多少个reduce任务\n    - 每个任务(map、reduce)有task id\n\n- **Task 任务分配**\n\n  - 如果小作业，appMaster会以uberized的方式运行此MR作业；appMaster会决定在它的JVM中顺序此MR的任务；\n\n    - 原因是，若每个任务运行在一个单独的JVM时，都需要单独启动JVM，分配资源（内存、CPU），需要时间；多个JVM中的任务再在各自的JVM中并行运行\n\n    - 若将所有任务在appMaster的JVM中顺序执行的话，更高效，那么appMaster就会这么做 ，任务作为uber任务运行\n\n    - 小作业判断依据：①小于10个map任务；②只有一个reduce任务；③MR输入大小小于一个HDFS块大小\n\n    - 如何开启uber?设置属性 mapreduce.job.ubertask.enable 值为true\n\n      ```java\n      configuration.set(\"mapreduce.job.ubertask.enable\", \"true\");\n      ```\n\n    - 在运行任何task之前，appMaster调用setupJob()方法，创建OutputCommitter，创建作业的最终输出目录（一般为HDFS上的目录）及任务输出的临时目录（如map任务的中间结果输出目录）\n\n  - ⑧若作业不以uber任务方式运行，那么appMaster会为作业中的每一个任务（map任务、reduce任务）向RM请求container\n\n    - 由于reduce任务在进入排序阶段之前，所有的map任务必须执行完成；所以，为map任务申请容器要优先于为reduce任务申请容器\n    - 5%的map任务执行完成后，才开始为reduce任务申请容器\n    - 为map任务申请容器时，遵循数据本地化，调度器尽量将容器调度在map任务的输入分片所在的节点上（移动计算，不移动数据）\n\n    - reduce任务能在集群任意计算节点运行\n    - 默认情况下，为每个map任务、reduce任务分配1G内存、1个虚拟内核，由属性决定mapreduce.map.memory.mb、mapreduce.reduce.memory.mb、mapreduce.map.cpu.vcores、mapreduce.reduce.reduce.cpu.vcores\n\n- **Task 任务执行**\n\n  - 当调度器为当前任务分配了一个NodeManager（暂且称之为NM01）的容器，并将此信息传递给appMaster后；appMaster与NM01通信，告知NM01启动一个容器，并此容器占据特定的资源量（内存、CPU）\n  - NM01收到消息后，启动容器，此容器占据指定的资源量\n  - 容器中运行YarnChild，由YarnChild运行当前任务（map、reduce）\n  - ⑩在容器中运行任务之前，先将运行任务需要的资源拉取到本地，如作业的JAR文件、配置文件、分布式缓存中的文件\n\n- **作业运行进度与状态更新**\n\n  - 作业job以及它的每个task都有状态（running、successfully completed、failed），当前任务的运行进度、作业计数器\n  - 任务在运行期间，每隔3秒向appMaster汇报执行进度、状态（包括计数器）\n  - appMaster汇总目前运行的所有任务的上报的结果\n  - 客户端每个1秒，轮询访问appMaster获得作业执行的最新状态，若有改变，则在控制台打印出来\n\n- 完成作业\n\n  - appMaster收到最后一个任务完成的报告后，将作业状态设置为成功\n  - 客户端轮询appMaster查询进度时，发现作业执行成功，程序从waitForCompletion()退出\n  - 作业的所有统计信息打印在控制台\n  - appMaster及运行任务的容器，清理中间的输出结果\n  - 作业信息被历史服务器保存，留待以后用户查询\n\n  \n\n\n### 3.3 yarn应用生命周期\n\n- RM: Resource Manager\n- AM: Application Master\n- NM: Node Manager\n\n1. Client向RM提交应用，包括AM程序及启动AM的命令。\n\n2. RM为AM分配第一个容器，并与对应的NM通信，令其在容器上启动应用的AM。\n\n3. AM启动时向RM注册，允许Client向RM获取AM信息然后直接和AM通信。\n\n4. AM通过资源请求协议，为应用协商容器资源。\n\n5. 如容器分配成功，AM要求NM在容器中启动应用，应用启动后可以和AM独立通信。\n\n6. 应用程序在容器中执行，并向AM汇报。\n\n7. 在应用执行期间，Client和AM通信获取应用状态。\n\n8. 应用执行完成，AM向RM注销并关闭，释放资源。\n\n   **申请资源->启动appMaster->申请运行任务的container->分发Task->运行Task->Task结束->回收container**\n\n\n\n## 4. 如何使用YARN\n\n### 4.1 配置文件\n\n```xml\n<!-- $HADOOP_HOME/etc/hadoop/mapred-site.xml -->\n<configuration>\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n</configuration>\n```\n\n```xml\n<!-- $HADOOP_HOME/etc/hadoop/yarn-site.xml -->\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n</configuration>\n```\n\n### 4.2 YARN启动停止\n\n- 启动 ResourceManager 和 NodeManager （以下分别简称RM、NM）\n\n```shell\n#主节点运行命令\n$HADOOP_HOME/sbin/start-yarn.sh\n```\n\n- 停止 RM 和 NM \n\n```shell\n#主节点运行命令\n$HADOOP_HOME/sbin/stop-yarn.sh\n```\n\n- 若RM没有启动起来，可以单独启动\n\n```shell\n#若RM没有启动，在主节点运行命令\n$HADOOP_HOME/sbin/yarn-daemon.sh start resouremanager\n#相反，可单独关闭\n$HADOOP_HOME/sbin/yarn-daemon.sh stop resouremanager\n\n```\n\n- 若NM没有启动起来，可以单独启动\n\n```shell\n#若NM没有启动，在相应节点运行命令\n$HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager\n#相反，可单独关闭\n$HADOOP_HOME/sbin/yarn-daemon.sh stop nodemanager\n\n```\n\n### 4.3 YARN常用命令\n\n**4.3.1 YARN命令列表**\n\n![](assets/Image201907162219.png)\n\n**4.3.2 yarn application命令**\n\n![](assets/Image201907162224.png)\n\n```shell\n#1.查看正在运行的任务\nyarn application -list\n\n```\n\n```shell\n#2.杀掉正在运行任务\nyarn application -kill 任务id\n\n```\n\n```shell\n#3.查看节点列表\nyarn node -list\n\n```\n\n![](assets/Image201907162252.png)\n\n```shell\n#4.查看节点状况；所有端口号与上图中端口号要一致（随机分配）\nyarn node -status node-03:45568\n\n```\n\n![](assets/Image201907171511.png)\n\n```shell\n#5.查看yarn依赖jar的环境变量\nyarn classpath\n\n```\n\n\n\n## 5. YARN调度器\n\n- 试想一下，你现在所在的公司有**一个**hadoop的集群。但是A项目组经常做一些定时的BI报表，B项目组则经常使用一些软件做一些临时需求。那么他们肯定会遇到同时提交任务的场景，这个时候到底如何分配资源满足这两个任务呢？是先执行A的任务，再执行B的任务，还是同时跑两个？\n\n- 在Yarn框架中，调度器是一块很重要的内容。有了合适的调度规则，就可以保证多个应用可以在同一时间有条不紊的工作。最原始的调度规则就是FIFO，即按照用户提交任务的时间来决定哪个任务先执行，先提交的先执行。但是这样很可能一个大任务独占资源，其他的资源需要不断的等待。也可能一堆小任务占用资源，大任务一直无法得到适当的资源，造成饥饿。所以FIFO虽然很简单，但是并不能满足我们的需求。\n\n- 理想情况下，yarn应用发出的资源请求应该立刻给予满足。然而现实中的资源有限，在一个繁忙的集群上，一个应用经常需要等待才能得到所需的资源。yarn调度器的工作就是根据既定的策略为应用分配资源。调度通常是一个难题，并且**没有一个所谓的“最好”的策略**，这也是为什么yarn提供了多重调度器和可配置策略供我们选择的原因。\n\n**yarn分为一级调度管理和二级调度管理**\n一级调度管理(更近底层,更接近于操作资源, 更偏向于应用层和底层结合)\n    计算资源管理(cpu,内存等,计算复杂消耗的cpu多)\n    App生命周期管理\n二级调度管理(自己代码的算法等, 更偏向于应用层)\n    App内部的计算模型管理\n    多样化的计算模型\n\n### 5.1 调度器\n\n- 在YARN中有三种调度器可以选择：FIFO Scheduler ，Capacity Scheduler，FairS cheduler\n\n![三种调度模型](assets/20180912140209122.png)\n\n### 5.2 FIFO Scheduler\n\n- FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。\n\n- FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。在共享集群中，更适合采用Capacity Scheduler或Fair Scheduler，这两个调度器都允许大任务和小任务在提交的同时获得一定的系统资源。\n\n- 上图展示了这几个调度器的区别，从图中可以看出，在FIFO 调度器中，小任务会被大任务阻塞。\n\n### 5.3 Capacity Scheduler\n\n- CDH版本默认使用Fair Scheduler公平调度器\n\n![](assets/Image201909241610.png)\n\n- 观察yarn web界面；使用的是fair scheduler\n\n![](assets/Image201909241637.png)\n\n- 若要使用capacity scheduler，需要修改yarn-site.xml文件；node01上\n\n  ```xml\n  <property>\n  \t<name>yarn.resourcemanager.scheduler.class</name>\n  \t<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>\n  </property>\n  \n  ```\n\n- 并分发到各节点\n\n  ```shell\n  [hadoop@node01 hadoop]$ pwd\n  /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop\n  [hadoop@node01 hadoop]$ scp yarn-site.xml node02:$PWD\n  [hadoop@node01 hadoop]$ scp yarn-site.xml node03:$PWD\n  \n  ```\n\n- 重启yarn\n\n  ```shell\n  [hadoop@node01 hadoop]$ stop-yarn.sh\n  [hadoop@node01 hadoop]$ start-yarn.sh\n  \n  ```\n\n- 而对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间\n\n- 如何配置容量调度器\n\n  - 队列层级结构如下\n\n    ```\n    root \n    ├── prod \n    └── dev \n    \t├── spark \n    \t└── hdp\n    \n    ```\n\n  - 主节点上，将$HADOOP_HOME/etc/hadoop/中的对应capacity-scheduler.xml配置文件备份到其它目录\n\n  - 目录$HADOOP_HOME/etc/hadoop/中建立一个新的capacity-scheduler.xml；内容如下\n\n    ```xml\n    <?xml version=\"1.0\" encoding=\"utf-8\"?>\n    \n    <configuration> \n      <property> \n        <name>yarn.scheduler.capacity.root.queues</name>  \n        <value>prod,dev</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.queues</name>  \n        <value>hdp,spark</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.prod.capacity</name>  \n        <value>40</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.capacity</name>  \n        <value>60</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.maximum-capacity</name>  \n        <value>75</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.hdp.capacity</name>  \n        <value>50</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.spark.capacity</name>  \n        <value>50</value> \n      </property> \n    </configuration>\n    \n    ```\n\n  - 将此xml文件，远程拷贝到相同目录下\n\n  - 将应用放置在哪个队列中，取决于应用本身。\n\n    例如MR，可以通过设置属性**mapreduce.job.queuename**指定相应队列。以WordCount为例，如下\n\n    如果指定的队列不存在，则发生错误。如果不指定，默认使用\"default\"队列，如下图\n\n![](assets/Image201907171118.png)\n\n![](assets/Image201907171138.png)\n\n- 动态更新配置：容量调度器的配置在运行时，可以随时重新加载，调整资源分配参数；你需要编辑conf/capacity-scheduler.xml 并在yarn主节点运行命令让配置文件生效\n  - 另外，除非重启resourcemanager，否则队列只能添加不能删除；但允许关闭\n\n```shell\n[hadoop@node01 hadoop]$ yarn rmadmin -refreshQueues\n\n```\n\n- 程序打包，提交集群运行\n\n```shell\n[hadoop@node01 target]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.wordcount.WordCountMain /README.txt /w24\n\n```\n\n![](assets/capacity scheduler.gif)\n\n\n\n### 5.4 Fair Scheduler\n\n![](assets/Image201907171437 (38).png)\n\n- Apache Hadoop默认使用Capacity Scheduler容量调度器\n\n- CDH版本默认使用Fair Scheduler公平调度器\n\n\n\n- 若要用Fair Scheduler的话，需要配置yarn-site.xml，将属性\"yarn.resourcemanager.scheduler.class\"的值修改成\"org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler\"，如下\n\n```xml\n<property>\n\t<name>yarn.resourcemanager.scheduler.class</name>\n\t<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>\n</property>\n\n```\n\n![](assets/Image201907171501.png)\n\n- 注意：同样，集群中所有yarn-site.xml文件要同步更新\n\n- 在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如下图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。\n\n- 需要注意的是，在下图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成.\n\n- 支持资源抢占\n\n在yarn-site.xml中设置yarn.scheduler.fair.preemption为true\n\n- 可通过一个名为fair-scheduler.xml文件对公平调度器进行配置\n- 此文件可放置在${HADOOP_HOME}/etc/hadoop/目录下\n- 当没有设置此配置文件时，每个应用放置在以当前用户名命名的队列中\n- 队列是用户提交第一个应用时动态创建的\n\n![](assets/Image201909161716.png)\n\n\n\n## 6. YARN应用状态\n\n我们在yarn 的web ui上能够看到yarn 应用程序分为如下几个状态:\n\n- NEW -----新建状态\n- NEW_SAVING-----新建保存状态\n- SUBMITTED-----提交状态\n- ACCEPTED-----接受状态\n- RUNNING-----运行状态\n- FINISHED-----完成状态\n- FAILED-----失败状态\n- KILLED-----杀掉状态\n\n![](assets/1558703612265.png)\n\n\n\n# 7、拓展点、未来计划、行业趋势（5分钟）\n\n1. [查看官网capacity scheduler内容](<https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html>)\n2. [capacity scheduler参考资料](<https://blog.csdn.net/u014589856/article/details/78119504>)\n3. [官网查看fair scheduler内容](<https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/FairScheduler.html>)\n4. 《Hadoop权威指南 第4版》\n   - 4.3 YARN中的调度\n   - 7.1 剖析MapReduce运行机制\n","tags":["hadoop","Yarn"]},{"title":"MapReduce编程（三）","url":"/2019/10/21/it/hadoop/MapReduce编程（三）/","content":"\n# MapReduce编程模型\n\n### 1. 自定义OutputFormat\n\n#### 1.1 需求\n\n- 现在有一些订单的评论数据，要将订单的好评与其它级别的评论（中评、差评）进行区分开来，将最终的数据分开到不同的文件夹下面去\n\n- 数据第九个字段表示评分等级：0 好评，1 中评，2 差评\n\n  ![](assets/Image201909111129.png)\n\n#### 1.2 逻辑分析\n\n- 程序的关键点是在一个mapreduce程序中，根据数据的不同(好评的评级不同)，输出两类结果到不同**目录**\n- 这类灵活的输出，需求通过自定义OutputFormat来实现\n\n#### 1.3 实现要点\n\n- 在mapreduce中访问外部资源\n- 自定义OutputFormat类，覆写getRecordWriter()方法\n- 自定义RecordWriter类，覆写具体输出数据的方法write()\n\n#### 1.4 MR代码\n\n- 自定义OutputFormat\n\n```java\npackage com.kaikeba.hadoop.outputformat;\n\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.RecordWriter;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n *\n * 本例使用框架默认的Reducer，它将Mapper输入的kv对，原样输出；所以reduce输出的kv类型分别是Text, NullWritable\n * 自定义OutputFormat的类，泛型表示reduce输出的键值对类型；要保持一致;\n * map--(kv)-->reduce--(kv)-->OutputFormat\n */\npublic class MyOutPutFormat extends FileOutputFormat<Text, NullWritable> {\n\n    /**\n     * 两个输出文件;\n     * good用于保存好评文件；其它评级保存到bad中\n     * 根据实际情况修改path;node01及端口号8020\n     */\n    String bad = \"hdfs://node01:8020/outputformat/bad/r.txt\";\n    String good = \"hdfs://node01:8020/outputformat/good/r.txt\";\n\n    /**\n     *\n     * @param context\n     * @return\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    public RecordWriter<Text, NullWritable> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException {\n        //获得文件系统对象\n        FileSystem fs = FileSystem.get(context.getConfiguration());\n        //两个输出文件路径\n        Path badPath = new Path(bad);\n        Path goodPath = new Path(good);\n        FSDataOutputStream badOut = fs.create(badPath);\n        FSDataOutputStream goodOut = fs.create(goodPath);\n        return new MyRecordWriter(badOut,goodOut);\n    }\n\n    /**\n     * 泛型表示reduce输出的键值对类型；要保持一致\n     */\n    static class MyRecordWriter extends RecordWriter<Text, NullWritable>{\n\n        FSDataOutputStream badOut = null;\n        FSDataOutputStream goodOut = null;\n\n        public MyRecordWriter(FSDataOutputStream badOut, FSDataOutputStream goodOut) {\n            this.badOut = badOut;\n            this.goodOut = goodOut;\n        }\n\n        /**\n         * 自定义输出kv对逻辑\n         * @param key\n         * @param value\n         * @throws IOException\n         * @throws InterruptedException\n         */\n        @Override\n        public void write(Text key, NullWritable value) throws IOException, InterruptedException {\n            if (key.toString().split(\"\\t\")[9].equals(\"0\")){//好评\n                goodOut.write(key.toString().getBytes());\n                goodOut.write(\"\\r\\n\".getBytes());\n            }else{//其它评级\n                badOut.write(key.toString().getBytes());\n                badOut.write(\"\\r\\n\".getBytes());\n            }\n        }\n\n        /**\n         * 关闭流\n         * @param context\n         * @throws IOException\n         * @throws InterruptedException\n         */\n        @Override\n        public void close(TaskAttemptContext context) throws IOException, InterruptedException {\n            if(goodOut !=null){\n                goodOut.close();\n            }\n            if(badOut !=null){\n                badOut.close();\n            }\n        }\n    }\n}\n```\n\n- main方法\n\n```java\npackage com.kaikeba.hadoop.outputformat;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\nimport java.io.IOException;\n\npublic class MyOwnOutputFormatMain extends Configured implements Tool {\n\n    public int run(String[] args) throws Exception {\n        Configuration conf = super.getConf();\n        Job job = Job.getInstance(conf, MyOwnOutputFormatMain.class.getSimpleName());\n        job.setJarByClass(MyOwnOutputFormatMain.class);\n\n        //默认项，可以省略或者写出也可以\n        //job.setInputFormatClass(TextInputFormat.class);\n        //设置输入文件\n        TextInputFormat.addInputPath(job, new Path(args[0]));\n        job.setMapperClass(MyMapper.class);\n        //job.setMapOutputKeyClass(Text.class);\n        //job.setMapOutputValueClass(NullWritable.class);\n\n        //设置自定义的输出类\n        job.setOutputFormatClass(MyOutPutFormat.class);\n        //设置一个输出目录，这个目录会输出一个success的成功标志的文件\n        MyOutPutFormat.setOutputPath(job, new Path(args[1]));\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        //默认项，即默认有一个reduce任务，所以可以省略\n//        job.setNumReduceTasks(1);\n//        //Reducer将输入的键值对原样输出\n//        job.setReducerClass(Reducer.class);\n\n        boolean b = job.waitForCompletion(true);\n        return b ? 0: 1;\n    }\n\n    /**\n     *\n     * Mapper输出的key、value类型\n     * 文件每行的内容作为输出的key，对应Text类型\n     * 输出的value为null，对应NullWritable\n     */\n    public static class MyMapper extends Mapper<LongWritable, Text, Text, NullWritable> {\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            //把当前行内容作为key输出；value为null\n            context.write(value, NullWritable.get());\n        }\n    }\n\n    /**\n     *\n     * @param args /ordercomment.csv /ofo\n     * @throws Exception\n     */\n    public static void main(String[] args) throws Exception {\n        Configuration configuration = new Configuration();\n        ToolRunner.run(configuration, new MyOwnOutputFormatMain(), args);\n    }\n}\n```\n\n#### 1.5 总结\n\n- 自定义outputformat\n  - 泛型与reduce输出的键值对类型保持一致\n  - 覆写getRecordWriter()方法\n- 自定义RecordWriter\n  - 泛型与reduce输出的键值对类型保持一致\n  - 覆写具体输出数据的方法write()、close()\n\n- main方法\n  - job.setOutputFormatClass使用自定义在输出类\n\n\n\n### 2. 二次排序\n\n#### 2.1 需求\n\n- 数据：有一个简单的关于员工工资的记录文件salary.txt\n\n  - 每条记录如下，有3个字段，分别表示name、age、salary\n\n  - nancy\t22\t8000\n\n    ![](assets/Image201910181039.png)\n\n- 使用MR处理记录，实现结果中\n\n  - 按照工资从高到低的降序排序\n  - 若工资相同，则按年龄升序排序\n\n#### 2.2 逻辑分析\n\n- 利用MR中key具有可比较的特点\n\n- MapReduce中，根据key进行分区、排序、分组\n\n- 有些MR的输出的key可以直接使用hadoop框架的可序列化可比较类型表示，如Text、IntWritable等等，而这些类型本身是可比较的；如IntWritable默认升序排序\n\n  ![](assets/Image201909111209.png)\n\n- 但有时，使用MR编程，输出的key，若使用hadoop自带的key类型无法满足需求\n\n  - 此时，需要自定义的key类型（包含的是非单一信息，如此例包含工资、年龄）；\n  - 并且也得是**<font color='red'>可序列化、可比较的</font>**\n\n- 需要自定义key，定义排序规则\n\n  - 实现：按照人的salary降序排序，若相同，则再按age升序排序；若salary、age相同，则放入同一组\n\n#### 2.3 MR代码\n\n- 详见工程代码\n- 自定义key类型Person类\n\n```java\npackage com.kaikeba.hadoop.secondarysort;\n\nimport org.apache.hadoop.io.WritableComparable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n//根据输入文件格式，定义JavaBean，作为MR时，Map的输出key类型；要求此类可序列化、可比较\npublic class Person implements WritableComparable<Person> {\n    private String name;\n    private int age;\n    private int salary;\n\n    public Person() {\n    }\n\n    public Person(String name, int age, int salary) {\n        //super();\n        this.name = name;\n        this.age = age;\n        this.salary = salary;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    public void setAge(int age) {\n        this.age = age;\n    }\n\n    public int getSalary() {\n        return salary;\n    }\n\n    public void setSalary(int salary) {\n        this.salary = salary;\n    }\n\n    @Override\n    public String toString() {\n        return this.salary + \"  \" + this.age + \"    \" + this.name;\n    }\n\n    //两个Person对象的比较规则：①先比较salary，高的排序在前；②若相同，age小的在前\n    public int compareTo(Person other) {\n        int compareResult= this.salary - other.salary;\n        if(compareResult != 0) {//若两个人工资不同\n            //工资降序排序；即工资高的排在前边\n            return -compareResult;\n        } else {//若工资相同\n            //年龄升序排序；即年龄小的排在前边\n            return this.age - other.age;\n        }\n    }\n\n    //序列化，将NewKey转化成使用流传送的二进制\n    public void write(DataOutput dataOutput) throws IOException {\n        //注意：①使用正确的write方法；②记住此时的序列化的顺序，name、age、salary\n        dataOutput.writeUTF(name);\n        dataOutput.writeInt(age);\n        dataOutput.writeInt(salary);\n    }\n\n    //使用in读字段的顺序，要与write方法中写的顺序保持一致：name、age、salary\n    public void readFields(DataInput dataInput) throws IOException {\n        //read string\n        //注意：①使用正确的read方法；②读取顺序与write()中序列化的顺序保持一致\n        this.name = dataInput.readUTF();\n        this.age = dataInput.readInt();\n        this.salary = dataInput.readInt();\n    }\n}\n```\n\n- main类、mapper、reducer\n\n```java\npackage com.kaikeba.hadoop.secondarysort;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\nimport java.net.URI;\n\npublic class SecondarySort {\n\n\t/**\n\t *\n\t * @param args /salary.txt /secondarysort\n\t * @throws Exception\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\t\tConfiguration configuration = new Configuration();\n\t\t//configuration.set(\"mapreduce.job.jar\",\"/home/bruce/project/kkbhdp01/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n\t\tJob job = Job.getInstance(configuration, SecondarySort.class.getSimpleName());\n\n\t\tFileSystem fileSystem = FileSystem.get(URI.create(args[1]), configuration);\n\t\t//生产中慎用\n\t\tif (fileSystem.exists(new Path(args[1]))) {\n\t\t\tfileSystem.delete(new Path(args[1]), true);\n\t\t}\n\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tjob.setMapperClass(MyMap.class);\n\t\t//由于mapper与reducer输出的kv类型分别相同，所以，下两行可以省略\n//\t\tjob.setMapOutputKeyClass(Person.class);\n//\t\tjob.setMapOutputValueClass(NullWritable.class);\n\t\t\n\t\t//设置reduce的个数;默认为1\n\t\t//job.setNumReduceTasks(1);\n\n\t\tjob.setReducerClass(MyReduce.class);\n\t\tjob.setOutputKeyClass(Person.class);\n\t\tjob.setOutputValueClass(NullWritable.class);\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n\t\tjob.waitForCompletion(true);\n\n\t}\n\n\t//MyMap的输出key用自定义的Person类型；输出的value为null\n\tpublic static class MyMap extends Mapper<LongWritable, Text, Person, NullWritable> {\n\t\t@Override\n\t\tprotected void map(LongWritable key, Text value, Context context)\n\t\t\t\tthrows IOException, InterruptedException {\n\n\t\t\tString[] fields = value.toString().split(\"\\t\");\n\t\t\tString name = fields[0];\n\t\t\tint age = Integer.parseInt(fields[1]);\n\t\t\tint salary = Integer.parseInt(fields[2]);\n\t\t\t//在自定义类中进行比较\n\t\t\tPerson person = new Person(name, age, salary);\n\t\t\t//person对象作为输出的key\n\t\t\tcontext.write(person, NullWritable.get());\n\t\t}\n\t}\n\n\tpublic static class MyReduce extends Reducer<Person, NullWritable, Person, NullWritable> {\n\t\t@Override\n\t\tprotected void reduce(Person key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {\n\t\t\t//输入的kv对，原样输出\n\t\t\tcontext.write(key, NullWritable.get());\n\t\t}\n\t}\n}\n```\n\n#### 2.4 总结\n\n- 如果MR时，key的排序规则比较复杂，比如需要根据字段1排序，若字段1相同，则需要根据字段2排序...，此时，可以使用自定义key实现\n- 将自定义的key作为MR中，map输出的key的类型（reduce输入的类型）\n- 自定义的key\n  - 实现WritableComparable接口\n  - 实现compareTo比较方法\n  - 实现write序列化方法\n  - 实现readFields反序列化方法\n\n\n\n### 3. 自定义分组求topN\n\n#### 3.1 需求\n\n- 现有一个淘宝用户订单历史记录文件；每条记录有6个字段，分别表示\n\n  - userid、datetime、title商品标题、unitPrice商品单价、purchaseNum购买量、productId商品ID\n\n  ![](assets/Image201909111241.png)\n\n- 现使用MR编程，求出每个用户、每个月消费金额最多的两笔订单，花了多少钱\n\n  - 所以得相同用户、同一个年月的数据，分到同一组\n\n#### 3.2 逻辑分析\n\n- 根据文件格式，自定义JavaBean类OrderBean\n  - 实现WritableComparable接口\n  - 包含6个字段分别对应文件中的6个字段\n  - 重点实现compareTo方法\n    - 先比较userid是否相等；若不相等，则userid升序排序\n    - 若相等，比较两个Bean的日期是否相等；若不相等，则日期升序排序\n    - 若相等，再比较总开销，降序排序\n  - 实现序列化方法write()\n  - 实现反序列化方法readFields()\n- 自定义分区类\n  - 继承Partitioner类\n  - getPartiton()实现，userid相同的，处于同一个分区\n- 自定义Mapper类\n  - 输出key是当前记录对应的Bean对象\n  - 输出的value对应当前下单的总开销\n- 自定义分组类\n  - 决定userid相同、日期（年月）相同的记录，分到同一组中，调用一次reduce()\n- 自定义Reduce类\n  - reduce()中求出当前一组数据中，开销头两笔的信息\n- main方法\n  - job.setMapperClass\n  - job.setPartitionerClass\n  - job.setReducerClass\n  - job.setGroupingComparatorClass\n\n#### 3.3 MR代码\n\n> 详细代码见代码工程\n\n- OrderBean\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.WritableComparable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n//实现WritableComparable接口\npublic class OrderBean implements WritableComparable<OrderBean> {\n\n    //用户ID\n    private String userid;\n    //年月\n    //year+month -> 201408\n    private String datetime;\n    //标题\n    private String title;\n    //单价\n    private double unitPrice;\n    //购买量\n    private int purchaseNum;\n    //商品ID\n    private String produceId;\n\n    public OrderBean() {\n    }\n\n    public OrderBean(String userid, String datetime, String title, double unitPrice, int purchaseNum, String produceId) {\n        super();\n        this.userid = userid;\n        this.datetime = datetime;\n        this.title = title;\n        this.unitPrice = unitPrice;\n        this.purchaseNum = purchaseNum;\n        this.produceId = produceId;\n    }\n\n    //key的比较规则\n    public int compareTo(OrderBean other) {\n        //OrderBean作为MR中的key；如果对象中的userid相同，即ret1为0；就表示两个对象是同一个用户\n        int ret1 = this.userid.compareTo(other.userid);\n\n        if (ret1 == 0) {\n            //如果userid相同，比较年月\n            String thisYearMonth = this.getDatetime();\n            String otherYearMonth = other.getDatetime();\n            int ret2 = thisYearMonth.compareTo(otherYearMonth);\n\n            if(ret2 == 0) {//若datetime相同\n                //如果userid、年月都相同，比较单笔订单的总开销\n                Double thisTotalPrice = this.getPurchaseNum()*this.getUnitPrice();\n                Double oTotalPrice = other.getPurchaseNum()*other.getUnitPrice();\n                //总花销降序排序；即总花销高的排在前边\n                return -thisTotalPrice.compareTo(oTotalPrice);\n            } else {\n                //若datatime不同，按照datetime升序排序\n                return ret2;\n            }\n        } else {\n            //按照userid升序排序\n            return ret1;\n        }\n    }\n\n    /**\n     * 序列化\n     * @param dataOutput\n     * @throws IOException\n     */\n    public void write(DataOutput dataOutput) throws IOException {\n        dataOutput.writeUTF(userid);\n        dataOutput.writeUTF(datetime);\n        dataOutput.writeUTF(title);\n        dataOutput.writeDouble(unitPrice);\n        dataOutput.writeInt(purchaseNum);\n        dataOutput.writeUTF(produceId);\n    }\n\n    /**\n     * 反序列化\n     * @param dataInput\n     * @throws IOException\n     */\n    public void readFields(DataInput dataInput) throws IOException {\n        this.userid = dataInput.readUTF();\n        this.datetime = dataInput.readUTF();\n        this.title = dataInput.readUTF();\n        this.unitPrice = dataInput.readDouble();\n        this.purchaseNum = dataInput.readInt();\n        this.produceId = dataInput.readUTF();\n    }\n\n    /**\n     * 使用默认分区器，那么userid相同的，落入同一分区；\n     * 另外一个方案：此处不覆写hashCode方法，而是自定义分区器，getPartition方法中，对OrderBean的userid求hashCode值%reduce任务数\n     * @return\n     */\n//    @Override\n//    public int hashCode() {\n//        return this.userid.hashCode();\n//    }\n\n    @Override\n    public String toString() {\n        return \"OrderBean{\" +\n                \"userid='\" + userid + '\\'' +\n                \", datetime='\" + datetime + '\\'' +\n                \", title='\" + title + '\\'' +\n                \", unitPrice=\" + unitPrice +\n                \", purchaseNum=\" + purchaseNum +\n                \", produceId='\" + produceId + '\\'' +\n                '}';\n    }\n\n    public String getUserid() {\n        return userid;\n    }\n\n    public void setUserid(String userid) {\n        this.userid = userid;\n    }\n\n    public String getDatetime() {\n        return datetime;\n    }\n\n    public void setDatetime(String datetime) {\n        this.datetime = datetime;\n    }\n\n    public String getTitle() {\n        return title;\n    }\n\n    public void setTitle(String title) {\n        this.title = title;\n    }\n\n    public double getUnitPrice() {\n        return unitPrice;\n    }\n\n    public void setUnitPrice(double unitPrice) {\n        this.unitPrice = unitPrice;\n    }\n\n    public int getPurchaseNum() {\n        return purchaseNum;\n    }\n\n    public void setPurchaseNum(int purchaseNum) {\n        this.purchaseNum = purchaseNum;\n    }\n\n    public String getProduceId() {\n        return produceId;\n    }\n\n    public void setProduceId(String produceId) {\n        this.produceId = produceId;\n    }\n}\n\n```\n\n- MyPartitioner\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\n//mapper的输出key类型是自定义的key类型OrderBean；输出value类型是单笔订单的总开销double -> DoubleWritable\npublic class MyPartitioner extends Partitioner<OrderBean, DoubleWritable> {\n    @Override\n    public int getPartition(OrderBean orderBean, DoubleWritable doubleWritable, int numReduceTasks) {\n        //userid相同的，落入同一分区\n        return (orderBean.getUserid().hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n}\n\n```\n\n- MyMapper\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * 输出kv，分别是OrderBean、用户每次下单的总开销\n */\npublic class MyMapper extends Mapper<LongWritable, Text, OrderBean, DoubleWritable> {\n    DoubleWritable valueOut = new DoubleWritable();\n    DateUtils dateUtils = new DateUtils();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        //13764633023     2014-12-01 02:20:42.000 全视目Allseelook 原宿风暴显色美瞳彩色隐形艺术眼镜1片 拍2包邮    33.6    2       18067781305\n        String record = value.toString();\n        String[] fields = record.split(\"\\t\");\n        if(fields.length == 6) {\n            String userid = fields[0];\n            String datetime = fields[1];\n            String yearMonth = dateUtils.getYearMonthString(datetime);\n            String title = fields[2];\n            double unitPrice = Double.parseDouble(fields[3]);\n            int purchaseNum = Integer.parseInt(fields[4]);\n            String produceId = fields[5];\n\n            //生成OrderBean对象\n            OrderBean orderBean = new OrderBean(userid, yearMonth, title, unitPrice, purchaseNum, produceId);\n\n            //此订单的总开销\n            double totalPrice = unitPrice * purchaseNum;\n            valueOut.set(totalPrice);\n\n            context.write(orderBean, valueOut);\n        }\n    }\n}\n\n```\n\n- MyReducer\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\n//输出的key为userid拼接上年月的字符串，对应Text；输出的value对应单笔订单的金额\npublic class MyReducer extends Reducer<OrderBean, DoubleWritable, Text, DoubleWritable> {\n    /**\n     * ①由于自定义分组逻辑，相同用户、相同年月的订单是一组，调用一次reduce()；\n     * ②由于自定义的key类OrderBean中，比较规则compareTo规定，相同用户、相同年月的订单，按总金额降序排序\n     * 所以取出头两笔，就实现需求\n     * @param key\n     * @param values\n     * @param context\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    protected void reduce(OrderBean key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException {\n        //求每个用户、每个月、消费金额最多的两笔多少钱\n        int num = 0;\n        for(DoubleWritable value: values) {\n            if(num < 2) {\n                String keyOut = key.getUserid() + \"  \" + key.getDatetime();\n                context.write(new Text(keyOut), value);\n                num++;\n            } else {\n                break;\n            }\n        }\n\n    }\n}\n```\n\n- MyGroup\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\n\n//自定义分组类：reduce端调用reduce()前，对数据做分组；每组数据调用一次reduce()\npublic class MyGroup extends WritableComparator {\n\n    public MyGroup() {\n        //第一个参数表示key class\n        super(OrderBean.class, true);\n    }\n\n  \t// 注意： 分组实现的方法是这个\n  \t// compare（Object a,Object b） 这个方法不可以\n    //分组逻辑\n    @Override\n    public int compare(WritableComparable a, WritableComparable b) {\n        //userid相同，且同一月的分成一组\n        OrderBean aOrderBean = (OrderBean)a;\n        OrderBean bOrderBean = (OrderBean)b;\n\n        String aUserId = aOrderBean.getUserid();\n        String bUserId = bOrderBean.getUserid();\n\n        //userid、年、月相同的，作为一组\n        int ret1 = aUserId.compareTo(bUserId);\n        if(ret1 == 0) {//同一用户\n            //年月也相同返回0，在同一组；\n            return aOrderBean.getDatetime().compareTo(bOrderBean.getDatetime());\n        } else {\n            return ret1;\n        }\n    }\n}\n\n```\n\n- CustomGroupingMain\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport com.kaikeba.hadoop.wordcount.WordCountMain;\nimport com.kaikeba.hadoop.wordcount.WordCountMap;\nimport com.kaikeba.hadoop.wordcount.WordCountReduce;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\nimport java.io.IOException;\n\npublic class CustomGroupingMain extends Configured implements Tool {\n\n    ///tmall-201412-test.csv /cgo\n    public static void main(String[] args) throws Exception {\n        int exitCode = ToolRunner.run(new CustomGroupingMain(), args);\n        System.exit(exitCode);\n    }\n\n    @Override\n    public int run(String[] args) throws Exception {\n        //判断以下，输入参数是否是两个，分别表示输入路径、输出路径\n        if (args.length != 2 || args == null) {\n            System.out.println(\"please input Path!\");\n            System.exit(0);\n        }\n\n        Configuration configuration = new Configuration();\n        //告诉程序，要运行的jar包在哪\n        //configuration.set(\"mapreduce.job.jar\",\"/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n\n        //调用getInstance方法，生成job实例\n        Job job = Job.getInstance(configuration, CustomGroupingMain.class.getSimpleName());\n\n        //设置jar包，参数是包含main方法的类\n        job.setJarByClass(CustomGroupingMain.class);\n\n        //通过job设置输入/输出格式\n        //MR的默认输入格式是TextInputFormat，所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n        //设置输入/输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        //设置处理Map阶段的自定义的类\n        job.setMapperClass(MyMapper.class);\n        //设置map combine类，减少网路传出量\n        //job.setCombinerClass(MyReducer.class);\n        job.setPartitionerClass(MyPartitioner.class);\n        //设置处理Reduce阶段的自定义的类\n        job.setReducerClass(MyReducer.class);\n        job.setGroupingComparatorClass(MyGroup.class);\n\n        //如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n        job.setMapOutputKeyClass(OrderBean.class);\n        job.setMapOutputValueClass(DoubleWritable.class);\n\n        //设置reduce task最终输出key/value的类型\n        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(DoubleWritable.class);\n\n        // 提交作业\n        return job.waitForCompletion(true) ? 0 : 1;\n    }\n}\n\n```\n\n- DateUtils\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport java.time.LocalDateTime;\nimport java.time.format.DateTimeFormatter;\n\npublic class DateUtils {\n\n    public static void main(String[] args) {\n        //test1\n//        String str1 = \"13764633024  2014-10-01 02:20:42.000\";\n//        String str2 = \"13764633023  2014-11-01 02:20:42.000\";\n//        System.out.println(str1.compareTo(str2));\n\n        //test2\n//        String datetime = \"2014-12-01 02:20:42.000\";\n//        LocalDateTime localDateTime = parseDateTime(datetime);\n//        int year = localDateTime.getYear();\n//        int month = localDateTime.getMonthValue();\n//        int day = localDateTime.getDayOfMonth();\n//        System.out.println(\"year-> \" + year + \"; month -> \" + month + \"; day -> \" + day);\n\n        //test3\n//        String datetime = \"2014-12-01 02:20:42.000\";\n//        System.out.println(getYearMonthString(datetime));\n    }\n\n    public LocalDateTime parseDateTime(String dateTime) {\n        DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\");\n        LocalDateTime localDateTime = LocalDateTime.parse(dateTime, formatter);\n        return localDateTime;\n    }\n\n    //日期格式转换工具类：将2014-12-14 20:42:14.000转换成201412\n    public String getYearMonthString(String dateTime) {\n        DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\");\n        LocalDateTime localDateTime = LocalDateTime.parse(dateTime, formatter);\n        int year = localDateTime.getYear();\n        int month = localDateTime.getMonthValue();\n        return year + \"\" + month;\n    }\n\n\n\n}\n\n```\n\n\n\n#### 3.4 总结\n\n- 要实现自定义分组逻辑\n  - 一般会自定义JavaBean，作为map输出的key\n    - 实现其中的compareTo方法，设置key的比较逻辑\n    - 实现序列化方法write()\n    - 实现反序列化方法readFields()\n  - 自定义mapper类、reducer类\n  - 自定义partition类，getPartition方法，决定哪些key落入哪些分区\n  - 自定义group分组类，决定reduce阶段，哪些kv对，落入同一组，调用一次reduce()\n  - 写main方法，设置自定义的类\n    - job.setMapperClass\n    - job.setPartitionerClass\n    - job.setReducerClass\n    - job.setGroupingComparatorClass\n\n\n\n### 4. MapReduce数据倾斜(20分钟)\n\n- 什么是数据倾斜？\n  - 数据中不可避免地会出现离群值（outlier），并导致数据倾斜。这些离群值会显著地拖慢MapReduce的执行。\n- 常见的数据倾斜有以下几类：\n  - 数据频率倾斜——某一个区域的数据量要远远大于其他区域。比如某一个key对应的键值对远远大于其他键的键值对。\n  - 数据大小倾斜——部分记录的大小远远大于平均值。\n\n- 在map端和reduce端都有可能发生数据倾斜\n  - 在map端的数据倾斜可以考虑使用combine\n  - 在reduce端的数据倾斜常常来源于MapReduce的默认分区器\n\n- 数据倾斜会导致map和reduce的任务执行时间大为延长，也会让需要缓存数据集的操作消耗更多的内存资源\n\n#### 4.1 如何诊断是否存在数据倾斜（10分钟）\n\n2. 如何诊断哪些键存在数据倾斜？\n   - 发现倾斜数据之后，有必要诊断造成数据倾斜的那些键。有一个简便方法就是在代码里实现追踪每个键的**最大值**。\n   - 为了减少追踪量，可以设置数据量阀值，只追踪那些数据量大于阀值的键，并输出到日志中。实现代码如下\n   - 运行作业后就可以从日志中判断发生倾斜的键以及倾斜程度；跟踪倾斜数据是了解数据的重要一步，也是设计MapReduce作业的重要基础\n   \n```java\n   package com.kaikeba.hadoop.dataskew;\n   \n   import org.apache.hadoop.io.IntWritable;\n   import org.apache.hadoop.io.Text;\n   import org.apache.hadoop.mapreduce.Reducer;\n   import org.apache.log4j.Logger;\n   \n   import java.io.IOException;\n   \n   public class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {\n   \n       private int maxValueThreshold;\n   \n       //日志类\n       private static final Logger LOGGER = Logger.getLogger(WordCountReduce.class);\n   \n       @Override\n       protected void setup(Context context) throws IOException, InterruptedException {\n   \n           //一个键达到多少后，会做数据倾斜记录\n           maxValueThreshold = 10000;\n       }\n   \n       /*\n               (hello, 1)\n               (hello, 1)\n               (hello, 1)\n               ...\n               (spark, 1)\n   \n               key: hello\n               value: List(1, 1, 1)\n           */\n       public void reduce(Text key, Iterable<IntWritable> values,\n                             Context context) throws IOException, InterruptedException {\n           int sum = 0;\n           //用于记录键出现的次数\n           int i = 0;\n   \n           for (IntWritable count : values) {\n               sum += count.get();\n               i++;\n           }\n   \n           //如果当前键超过10000个，则打印日志\n           if(i > maxValueThreshold) {\n               LOGGER.info(\"Received \" + i + \" values for key \" + key);\n           }\n   \n           context.write(key, new IntWritable(sum));// 输出最终结果\n       };\n   }\n```\n\n\n\n\n#### 4.2 减缓数据倾斜\n\n- Reduce数据倾斜一般是指map的输出数据中存在数据频率倾斜的状况，即部分输出键的数据量远远大于其它的输出键\n\n- 如何减小reduce端数据倾斜的性能损失？常用方式有：\n  - 一、自定义分区\n\n    - 基于输出键的背景知识进行自定义分区。\n\n    - 例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的一部分reduce实例。而将其他的都发送给剩余的reduce实例。\n\n  - 二、Combine\n\n    - 使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。\n    - combine的目的就是聚合并精简数据。\n\n  - 三、抽样和范围分区\n\n    - Hadoop默认的分区器是HashPartitioner，基于map输出键的哈希值分区。这仅在数据分布比较均匀时比较好。**在有数据倾斜时就很有问题**。\n\n    - 使用分区器需要首先了解数据的特性。**TotalOrderPartitioner**中，可以通过对原始数据进行抽样得到的结果集来**预设分区边界值**。\n    - TotalOrderPartitioner中的范围分区器可以通过预设的分区边界值进行分区。因此它也可以很好地用在矫正数据中的部分键的数据倾斜问题。\n\n  - 四、数据大小倾斜的自定义策略\n\n    - 在map端或reduce端的数据大小倾斜都会对缓存造成较大的影响，乃至导致OutOfMemoryError异常。处理这种情况并不容易。可以参考以下方法。\n\n    - 设置mapreduce.input.linerecordreader.line.maxlength来限制RecordReader读取的最大长度。\n    - RecordReader在TextInputFormat和KeyValueTextInputFormat类中使用。默认长度没有上限。\n\n\n\n### 5. MR调优\n\n- 见后续文章\n\n\n\n\n\n### 6. 抽样、范围分区\n\n#### 6.1 数据\n\n   - 数据：气象站气象数据，来源美国国家气候数据中心（NCDC）（1900-2000年数据，每年一个文件）\n\n        - 气候数据record的格式如下\n\n\n![](assets/Image201907151554.png)\n\n#### 6.2 需求\n\n- 对气象数据，按照气温进行排序（气温符合正太分布）\n\n#### 6.3 实现方案\n\n- 三种实现思路\n\n  - 方案一：\n    - 设置一个分区，即一个reduce任务；在一个reduce中对结果进行排序；\n    - 失去了MR框架并行计算的优势\n  - 方案二：\n    - 自定义分区，人为指定各温度区间的记录，落入哪个分区；如分区温度边界值分别是-15、0、20，共4个分区\n    - 但由于对整个数据集的气温分布不了解，可能某些分区的数据量大，其它的分区小，数据倾斜\n  - 方案三：\n    - 通过对键空间采样\n    - 只查看一小部分键，获得键的近似分布（好温度的近似分布）\n    - 进而据此结果创建分区，实现尽可能的均匀的划分数据集；\n    - Hadoop内置了采样器；InputSampler\n\n#### 6.4 MR代码\n\n> 分两大步\n\n- 一、先将数据按气温对天气数据集排序。结果存储为sequencefile文件，气温作为输出键，数据行作为输出值\n\n- 代码\n\n  > 此代码处理原始日志文件\n  >\n  > 结果用SequenceFile格式存储；\n  >\n  > 温度作为SequenceFile的key；记录作为value\n\n``` java\npackage com.kaikeba.hadoop.totalorder;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n * 此代码处理原始日志文件 1901\n * 结果用SequenceFile格式存储；\n * 温度作为SequenceFile的key；记录作为value\n */\npublic class SortDataPreprocessor {\n\n  //输出的key\\value分别是气温、记录\n  static class CleanerMapper extends Mapper<LongWritable, Text, IntWritable, Text> {\n  \n    private NcdcRecordParser parser = new NcdcRecordParser();\n    private IntWritable temperature = new IntWritable();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context)\n        throws IOException, InterruptedException {\n      //0029029070999991901010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n      parser.parse(value);\n      if (parser.isValidTemperature()) {//是否是有效的记录\n        temperature.set(parser.getAirTemperature());\n        context.write(temperature, value);\n      }\n    }\n  }\n\n\n  //两个参数：/ncdc/input /ncdc/sfoutput\n  public static void main(String[] args) throws Exception {\n\n    if (args.length != 2) {\n      System.out.println(\"<input> <output>\");\n    }\n\n    Configuration conf = new Configuration();\n\n    Job job = Job.getInstance(conf, SortDataPreprocessor.class.getSimpleName());\n    job.setJarByClass(SortDataPreprocessor.class);\n    //\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n    job.setMapperClass(CleanerMapper.class);\n    //最终输出的键、值类型\n    job.setOutputKeyClass(IntWritable.class);\n    job.setOutputValueClass(Text.class);\n    //reduce个数为0\n    job.setNumReduceTasks(0);\n    //以sequencefile的格式输出\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    //开启job输出压缩功能\n    //方案一\n    conf.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n    conf.set(\"mapreduce.output.fileoutputformat.compress.type\",\"RECORD\");\n    //指定job输出使用的压缩算法\n    conf.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n\n    //方案二\n    //设置sequencefile的压缩、压缩算法、sequencefile文件压缩格式block\n    //SequenceFileOutputFormat.setCompressOutput(job, true);\n    //SequenceFileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);\n    //SequenceFileOutputFormat.setOutputCompressorClass(job, SnappyCodec.class);\n    //SequenceFileOutputFormat.setOutputCompressionType(job, SequenceFile.CompressionType.BLOCK);\n\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n```\n\n- 二、全局排序\n\n  > 使用全排序分区器TotalOrderPartitioner\n  >\n\n```java\npackage com.kaikeba.hadoop.totalorder;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.filecache.DistributedCache;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.partition.InputSampler;\nimport org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner;\n\nimport java.net.URI;\n\n/**\n * 使用TotalOrderPartitioner全局排序一个SequenceFile文件的内容；\n * 此文件是SortDataPreprocessor的输出文件；\n * key是IntWritble，气象记录中的温度\n */\npublic class SortByTemperatureUsingTotalOrderPartitioner{\n\n  /**\n   * 两个参数：/ncdc/sfoutput /ncdc/totalorder\n   * 第一个参数是SortDataPreprocessor的输出文件\n   */\n  public static void main(String[] args) throws Exception {\n    if (args.length != 2) {\n      System.out.println(\"<input> <output>\");\n    }\n\n    Configuration conf = new Configuration();\n\n    Job job = Job.getInstance(conf, SortByTemperatureUsingTotalOrderPartitioner.class.getSimpleName());\n    job.setJarByClass(SortByTemperatureUsingTotalOrderPartitioner.class);\n\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n    //输入文件是SequenceFile\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    //Hadoop提供的方法来实现全局排序，要求Mapper的输入、输出的key必须保持类型一致\n    job.setOutputKeyClass(IntWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    //分区器：全局排序分区器\n    job.setPartitionerClass(TotalOrderPartitioner.class);\n\n    //分了3个区；且分区i-1中的key小于i分区中所有的键\n    job.setNumReduceTasks(3);\n\n    /**\n     * 随机采样器从所有的分片中采样\n     * 每一个参数：采样率；\n     * 第二个参数：总的采样数\n     * 第三个参数：采样的最大分区数；\n     * 只要numSamples和maxSplitSampled（第二、第三参数）任一条件满足，则停止采样\n     */\n    InputSampler.Sampler<IntWritable, Text> sampler =\n            new InputSampler.RandomSampler<IntWritable, Text>(0.1, 5000, 10);\n//    TotalOrderPartitioner.setPartitionFile();\n    /**\n     * 存储定义分区的键；即整个数据集中温度的大致分布情况；\n     * 由TotalOrderPartitioner读取，作为全排序的分区依据，让每个分区中的数据量近似\n     */\n    InputSampler.writePartitionFile(job, sampler);\n\n    //根据上边的SequenceFile文件（包含键的近似分布情况），创建分区\n    String partitionFile = TotalOrderPartitioner.getPartitionFile(job.getConfiguration());\n    URI partitionUri = new URI(partitionFile);\n\n//    JobConf jobConf = new JobConf();\n\n    //与所有map任务共享此文件，添加到分布式缓存中\n    DistributedCache.addCacheFile(partitionUri, job.getConfiguration());\n//    job.addCacheFile(partitionUri);\n\n    //方案一：输出的文件RECORD级别，使用BZip2Codec进行压缩\n    conf.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n    conf.set(\"mapreduce.output.fileoutputformat.compress.type\",\"RECORD\");\n    //指定job输出使用的压缩算法\n    conf.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n\n    //方案二\n    //SequenceFileOutputFormat.setCompressOutput(job, true);\n    //SequenceFileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);\n    //SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);\n\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n\n```\n\n#### 6.5 总结\n\n- 对大量数据进行全局排序\n\n  - 先使用InputSampler.Sampler采样器，对整个key空间进行采样，得到key的近似分布\n\n  - 保存到key分布情况文件中\n\n  - 使用TotalOrderPartitioner，利用上边的key分布情况文件，进行分区；每个分区的数据量近似，从而防止数据倾斜\n\n\n\n## 注意\n\n1. 描述MR的shuffle全流程（面试）\n2. 搭建MAVEN工程，统计词频，并提交集群运行，查看结果\n3. 利用搜狗数据，找出所有独立的uid并写入HDFS\n4. 利用搜狗数据，找出所有独立的uid出现次数，并写入HDFS，并要求使用Map端的Combine操作\n5. 谈谈什么是数据倾斜，什么情况会造成数据倾斜？（面试）\n6. 对MR数据倾斜，如何解决？（面试）\n  ","tags":["hadoop","MapReduce"]},{"title":"MapReduce编程（二）","url":"/2019/10/20/it/hadoop/MapReduce编程（二）/","content":"\n# MapReduce编程模型（二）\n\n### 1. 自定义分区\n\n#### 1.1 分区原理\n\n- 根据之前讲的shuffle，我们知道在map任务中，从环形缓冲区溢出写磁盘时，会先对kv对数据进行分区操作\n\n- 分区操作是由MR中的分区器负责的\n\n- MapReduce有自带的默认分区器\n\n  - **HashPartitioner**\n  - 关键方法getPartition返回当前键值对的**分区索引**(partition index)\n\n  ```java\n  public class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n  \n    public void configure(JobConf job) {}\n  \n    /** Use {@link Object#hashCode()} to partition. */\n    public int getPartition(K2 key, V2 value, int numReduceTasks) {\n      return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n  }\n  ```\n\n- 环形缓冲区溢出写磁盘前，将每个kv对，作为getPartition()的参数传入；\n\n- 先对键值对中的key求hash值（int类型），与MAX_VALUE按位与；再模上reduce task个数，假设reduce task个数设置为4（可在程序中使用job.setNumReduceTasks(4)指定reduce task个数为4）\n\n  - 那么map任务溢出文件有**4个分区**，分区index分别是0、1、2、3\n  - getPartition()结果有四种：0、1、2、3\n  - 根据计算结果，决定当前kv对，落入哪个分区，如结果是0，则当前kv对落入溢出文件的0分区中\n  - 最终被相应的reduce task通过http获得\n\n![](assets/Image201906280826.png)\n\n![](assets/Image201906272145.png)\n\n- 若是MR默认分区器，不满足需求；可根据业务逻辑，设计自定义分区器，比如实现图上的功能\n\n#### 1.2 默认分区\n\n> 程序执行略\n>\n> 代码详见工程com.kaikeba.hadoop.partitioner包\n\n- MR读取三个文件part1.txt、part2.txt、part3.txt；三个文件放到HDFS目录：/customParttitioner中\n\n  ![](assets/Image201909061640.png)\n  \n- part1.txt内容如下：\n\n  ```\n  Dear Bear River\n  Dear Car\n  ```\n  \n- part2.txt内容如下：\n\n  ```\n  Car Car River\n  Dear Bear\n  ```\n\n- part3.txt内容如下：\n\n  ```\n  Dear Car Bear\n  Car Car\n  ```\n\n- 默认HashPartitioner分区时，查看结果（看代码）\n\n![](assets/Image201906272204.png)\n\n- 运行参数：\n\n```shell\n/customParttitioner /cp01\n```\n\n- 打jar包运行，结果如下：\n\n![](assets/Image201906272210.png)\n\n> 只有part-r-00001、part-r-00003有数据；另外两个没有数据\n>\n> HashPartitioner将Bear分到index=1的分区；将Car|Dear|River分到index=3分区\n\n#### 1.3 自定义分区\n\n**1.3.1** 需求\n\n- 自定义分区，使得文件中，分别以Dear、Bear、River、Car为键的键值对，分别落到index是0、1、2、3的分区中\n\n**1.3.2** 逻辑分析\n\n- 若要实现以上的分区策略，需要自定义分区类\n  - 此类实现Partitioner接口\n  - 在getPartition()中实现分区逻辑\n- main方法中\n  - **设定reduce个数**为4\n  - 设置自定义的分区类，调用job.setPartitionerClass方法\n\n**1.3.3** MR代码\n\n> 完整代码见代码工程\n\n- 自定义分区类如下\n\n```java\npackage com.kaikeba.hadoop.partitioner;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\nimport java.util.HashMap;\n\npublic class CustomPartitioner extends Partitioner<Text, IntWritable> {\n    public static HashMap<String, Integer> dict = new HashMap<String, Integer>();\n\n    //定义每个键对应的分区index，使用map数据结构完成\n    static{\n        dict.put(\"Dear\", 0);\n        dict.put(\"Bear\", 1);\n        dict.put(\"River\", 2);\n        dict.put(\"Car\", 3);\n    }\n\n    public int getPartition(Text text, IntWritable intWritable, int i) {\n        //\n        int partitionIndex = dict.get(text.toString());\n        return partitionIndex;\n    }\n}\n```\n\n\n\n![](assets/Image201906272213.png)\n\n- 运行结果\n\n![](assets/Image201906272217.png)\n\n> 结果满足需求\n\n#### 1.4 总结\n\n- 如果默认分区器不满足业务需求，可以自定义分区器\n  - 自定义分区器的类继承Partitioner类\n  - 覆写getPartition()，在方法中，定义自己的分区策略\n  - 在main()方法中调用job.setPartitionerClass()\n  - main()中设置reduce任务数\n\n\n\n### 2. 自定义Combiner\n\n#### 2.1 需求\n\n- 普通的MR是reduce通过http，取得map任务的分区结果；具体的聚合出结果是在reduce端进行的；\n\n- 以单词计数为例：\n  - 下图中的第一个map任务(map1)，本地磁盘中的结果有5个键值对：(Dear, 1)、(Bear, 1)、(River, 1)、(Dear, 1)、(Car, 1)\n  - 其中，map1中的两个相同的键值对(Dear, 1)、(Dear, 1)，会被第一个reduce任务(reduce1)通过网络拉取到reduce1端\n  - 那么假设map1中(Dear, 1)有1亿个呢？按原思路，map1端需要存储1亿个(Dear, 1)，再将1亿个(Dear, 1)通过网络被reduce1获得，然后再在reduce1端汇总\n  - 这样做map端本地磁盘IO、数据从map端到reduce端传输的网络IO比较大\n  - 那么想，能不能在reduce1从map1拉取1亿个(Dear, 1)之前，在map端就提前先做下reduce汇总，得到结果(Dear, 100000000)，然后再将这个结果（一个键值对）传输到reduce1呢？\n  - 答案是可以的\n  - 我们称之为combine操作\n  \n- map端combine本地聚合（**本质是reduce**）\n\n  ![](assets/Image201906280906.png)\n\n#### 2.2 逻辑分析\n\n- **<font color='red'>注意：</font>**\n\n  - **不论运行多少次Combine操作，都不能影响最终的结果**\n\n  - **并非**所有的mr都适合combine操作，比如求平均值 \n\n    **参考：《并非所有MR都适合combine.txt》**\n\n- 原理图\n\n  > 看原图\n\n![](assets/Image201909091014.png)\n\n- 当每个map任务的环形缓冲区添满80%，开始溢写磁盘文件\n\n- 此过程会分区、每个分区内按键排序、再combine操作（若设置了combine的话）、若设置map输出压缩的话则再压缩\n\n  - 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作；\n  - 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销）\n\n- combine本质上也是reduce；因为自定义的combine类继承自Reducer父类\n\n- map: (K1, V1) -> list(K2, V2)\n\n- combiner: (K2, list(V2)) -> (K2, V2)\n\n- reduce: (K2, list(V2)) -> (K3, V3)\n\n  - reduce函数与combine函数通常是一样的\n  - K3与K2类型相同；\n  - V3与V2类型相同\n  - 即reduce的输入的kv类型分别与输出的kv类型相同\n  \n  \n\n#### 2.3 MR代码\n\n> 对原词频统计代码做修改；\n>\n> 详细代码见代码工程\n\n- WordCountMap、WordCountReduce代码保持不变\n- 唯一需要做的修改是在WordCountMain中，增加job.**setCombinerClass**(WordCountReduce.class);\n- 修改如下：\n\n![](assets/Image201906272006.png)\n\n#### 2.4 小结\n\n- 使用combine时，首先考虑当前MR是否适合combine\n- 总原则是不论使不使用combine不能影响最终的结果\n- 在MR时，发生数据倾斜，且可以使用combine时，可以使用combine缓解数据倾斜\n\n\n\n### 3. MR压缩\n\n#### 3.1 需求\n\n- 作用：在MR中，为了减少磁盘IO及网络IO，可考虑在map端、reduce端设置压缩功能\n- 给“MapReduce编程：用户搜索次数”代码，增加压缩功能\n\n#### 3.2 逻辑分析\n\n- 那么如何设置压缩功能呢？只需在main方法中，给Configuration对象增加如下设置即可\n\n\n```java\n//开启map输出进行压缩的功能\nconfiguration.set(\"mapreduce.map.output.compress\", \"true\");\n//设置map输出的压缩算法是：BZip2Codec，它是hadoop默认支持的压缩算法，且支持切分\nconfiguration.set(\"mapreduce.map.output.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n//开启job输出压缩功能\nconfiguration.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n//指定job输出使用的压缩算法\nconfiguration.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n```\n\n#### 3.3 MR代码\n\n- 给“MapReduce编程：用户搜索次数”代码，增加压缩功能，代码如下\n\n  > 如何打jar包，已演示过，此处不再赘述\n\n```java\npackage com.kaikeba.hadoop.mrcompress;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n * 本MR示例，用于统计每个用户搜索并查看URL链接的次数\n */\npublic class UserSearchCount {\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        //判断以下，输入参数是否是两个，分别表示输入路径、输出路径\n        if (args.length != 2 || args == null) {\n            System.out.println(\"please input Path!\");\n            System.exit(0);\n        }\n\n        Configuration configuration = new Configuration();\n        //configuration.set(\"mapreduce.job.jar\",\"/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n        //开启map输出进行压缩的功能\n        configuration.set(\"mapreduce.map.output.compress\", \"true\");\n        //设置map输出的压缩算法是：BZip2Codec，它是hadoop默认支持的压缩算法，且支持切分\n        configuration.set(\"mapreduce.map.output.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n        //开启job输出压缩功能\n        configuration.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n        //指定job输出使用的压缩算法\n        configuration.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n\n        //调用getInstance方法，生成job实例\n        Job job = Job.getInstance(configuration, UserSearchCount.class.getSimpleName());\n\n        //设置jar包，参数是包含main方法的类\n        job.setJarByClass(UserSearchCount.class);\n\n        //通过job设置输入/输出格式\n        //MR的默认输入格式是TextInputFormat，所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n        //设置输入/输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n//        FileOutputFormat.setCompressOutput(job, true);\n//        FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);\n\n        //设置处理Map阶段的自定义的类\n        job.setMapperClass(SearchCountMapper.class);\n        //设置map combine类，减少网路传出量\n        //job.setCombinerClass(WordCountReduce.class);\n        //设置处理Reduce阶段的自定义的类\n        job.setReducerClass(SearchCountReducer.class);\n\n        //如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n//        job.setMapOutputKeyClass(Text.class);\n//        job.setMapOutputValueClass(IntWritable.class);\n\n        //设置reduce task最终输出key/value的类型\n        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // 提交作业\n        job.waitForCompletion(true);\n    }\n\n    public static class SearchCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n        //定义共用的对象，减少GC压力\n        Text userIdKOut = new Text();\n        IntWritable vOut = new IntWritable(1);\n\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            //获得当前行的数据\n            //样例数据：20111230111645  169796ae819ae8b32668662bb99b6c2d        塘承高速公路规划线路图  1       1       http://auto.ifeng.com/roll/20111212/729164.shtml\n            String line = value.toString();\n\n            //切分，获得各字段组成的数组\n            String[] fields = line.split(\"\\t\");\n\n            //因为要统计每个user搜索并查看URL的次数，所以将userid放到输出key的位置\n            //注意：MR编程中，根据业务需求设计key是很重要的能力\n            String userid = fields[1];\n\n            //设置输出的key的值\n            userIdKOut.set(userid);\n            //输出结果\n            context.write(userIdKOut, vOut);\n        }\n    }\n\n    public static class SearchCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n        //定义共用的对象，减少GC压力\n        IntWritable totalNumVOut = new IntWritable();\n\n        @Override\n        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n            int sum = 0;\n\n            for(IntWritable value: values) {\n                sum += value.get();\n            }\n\n            //设置当前user搜索并查看总次数\n            totalNumVOut.set(sum);\n            context.write(key, totalNumVOut);\n        }\n    }\n}\n```\n\n- 生成jar包，并运行jar包\n\n```shell\n[hadoop@node01 target]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.mrcompress.UserSearchCount /sogou.2w.utf8 /compressed\n```\n\n- 查看结果\n\n  > 可增加数据量，查看使用压缩算法前后的系统各计数器的数据量变化\n\n```shell\n[hadoop@node01 target]$ hadoop fs -ls -h /compressed\n```\n\n![](assets/Image201908241707.png)\n\n#### 3.4 总结\n\n- MR过程中使用压缩可减少数据量，进而减少磁盘IO、网络IO数据量\n- 可设置map端输出的压缩\n- 可设置job最终结果的压缩\n- 通过相应的配置项即可实现\n\n\n\n### 4. 自定义InputFormat\n\n#### 4.1 MapReduce执行过程\n\n![](assets/Image201905211621.png)\n\n- 上图也描述了mapreduce的一个完整的过程；我们主要看map任务是如何从hdfs读取分片数据的部分\n\n  - 涉及3个关键的类\n\n  - ①InputFormat输入格式类\n    \n    ②InputSplit输入分片类：getSplits()\n    \n    - InputFormat输入格式类将输入文件分成一个个分片InputSplit\n    - 每个Map任务对应一个split分片\n    \n    ③RecordReader记录读取器类：createRecordReader()\n    \n    - RecordReader（记录读取器）读取分片数据，一行记录生成一个键值对\n    - 传入map任务的map()方法，调用map()\n    \n    ![](assets/Image201910161117.png)\n    \n    \n\n- 所以，如果需要根据自己的业务情况，自定义输入的话，需要自定义两个类：\n  - InputFormat类\n  - RecordReader类\n\n- 详细流程：\n\n  - 客户端调用InputFormat的**getSplits()**方法，获得输入文件的分片信息\n\n    ![](assets/Image201909111008.png)\n\n  - 针对每个MR job会生成一个相应的app master，负责map\\reduce任务的调度及监控执行情况\n\n  - 将分片信息传递给MR job的app master\n\n  - app master根据分片信息，尽量将map任务尽量调度在split分片数据所在节点（**移动计算不移动数据**）\n\n    ![](assets/Image201909111013.png)\n\n  - 有几个分片，就生成几个map任务\n  \n  - 每个map任务将split分片传递给createRecordReader()方法，生成此分片对应的RecordReader\n  \n  - RecordReader用来读取分片的数据，生成记录的键值对\n  \n    - nextKeyValue()判断是否有下一个键值对，如果有，返回true；否则，返回false\n    - 如果返回true，调用getCurrentKey()获得当前的键\n    - 调用getCurrentValue()获得当前的值\n  \n  - map任务运行过程\n  \n    ![](assets/Image201909111022.png)\n  \n    - map任务运行时，会调用run()\n  \n    - 首先运行一次setup()方法；只在map任务启动时，运行一次；一些初始化的工作可以在setup方法中完成；如要连接数据库之类的操作\n  \n    - while循环，调用context.nextKeyValue()；会委托给RecordRecord的nextKeyValue()，判断是否有下一个键值对\n  \n    - 如果有下一个键值对，调用context.getCurrentKey()、context.getCurrentValue()获得当前的键、值的值（也是调用RecordReader的同名方法）\n  \n      ![](assets/Image201909111045.png)\n  \n    - 作为参数传入map(key, value, context)，调用一次map()\n  \n    - 当读取分片尾，context.nextKeyValue()返回false；退出循环\n  \n    - 调用cleanup()方法，只在map任务结束之前，调用一次；所以，一些回收资源的工作可在此方法中实现，如关闭数据库连接\n\n#### 4.2 需求\n\n- 无论hdfs还是mapreduce，处理小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案\n\n#### 4.3 逻辑分析\n\n- 小文件的优化无非以下几种方式：\n  - 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS(SequenceFile方案)\n  - 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并；可使用**自定义InputFormat**实现\n  - 在mapreduce处理时，可采用**CombineFileInputFormat**提高效率\n- 本例使用第二种方案，自定义输入格式\n\n#### 4.4 MR代码\n\n- 自定义InputFormat\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.JobContext;\n  import org.apache.hadoop.mapreduce.RecordReader;\n  import org.apache.hadoop.mapreduce.TaskAttemptContext;\n  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n  \n  import java.io.IOException;\n  \n  /**\n   * 自定义InputFormat类；\n   * 泛型：\n   *  键：因为不需要使用键，所以设置为NullWritable\n   *  值：值用于保存小文件的内容，此处使用BytesWritable\n   */\n  public class WholeFileInputFormat extends FileInputFormat<NullWritable, BytesWritable> {\n  \n      /**\n       *\n       * 返回false，表示输入文件不可切割\n       * @param context\n       * @param file\n       * @return\n       */\n      @Override\n      protected boolean isSplitable(JobContext context, Path file) {\n          return false;\n      }\n  \n      /**\n       * 生成读取分片split的RecordReader\n       * @param split\n       * @param context\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public RecordReader<NullWritable, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException,InterruptedException {\n          //使用自定义的RecordReader类\n          WholeFileRecordReader reader = new WholeFileRecordReader();\n          //初始化RecordReader\n          reader.initialize(split, context);\n          return reader;\n      }\n  }\n  ```\n\n- 自定义RecordReader\n\n  实现6个相关方法\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.conf.Configuration;\n  import org.apache.hadoop.fs.FSDataInputStream;\n  import org.apache.hadoop.fs.FileSystem;\n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.IOUtils;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.RecordReader;\n  import org.apache.hadoop.mapreduce.TaskAttemptContext;\n  import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n  \n  import java.io.IOException;\n  \n  /**\n   *\n   * RecordReader的核心工作逻辑：\n   * 通过nextKeyValue()方法去读取数据构造将返回的key   value\n   * 通过getCurrentKey 和 getCurrentValue来返回上面构造好的key和value\n   *\n   * @author\n   */\n  public class WholeFileRecordReader extends RecordReader<NullWritable, BytesWritable> {\n  \n      //要读取的分片\n      private FileSplit fileSplit;\n      private Configuration conf;\n  \n      //读取的value数据\n      private BytesWritable value = new BytesWritable();\n      /**\n       *\n       * 标识变量，分片是否已被读取过；因为小文件设置成了不可切分，所以一个小文件只有一个分片；\n       * 而这一个分片的数据，只读取一次，一次读完所有数据\n       * 所以设置此标识\n       */\n      private boolean processed = false;\n  \n      /**\n       * 初始化\n       * @param split\n       * @param context\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public void initialize(InputSplit split, TaskAttemptContext context)\n              throws IOException, InterruptedException {\n          this.fileSplit = (FileSplit) split;\n          this.conf = context.getConfiguration();\n      }\n  \n      /**\n       * 判断是否有下一个键值对。若有，则读取分片中的所有的数据\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public boolean nextKeyValue() throws IOException, InterruptedException {\n          if (!processed) {\n              byte[] contents = new byte[(int) fileSplit.getLength()];\n              Path file = fileSplit.getPath();\n              FileSystem fs = file.getFileSystem(conf);\n              FSDataInputStream in = null;\n              try {\n                  in = fs.open(file);\n                  IOUtils.readFully(in, contents, 0, contents.length);\n                  value.set(contents, 0, contents.length);\n              } finally {\n                  IOUtils.closeStream(in);\n              }\n              processed = true;\n              return true;\n          }\n          return false;\n      }\n  \n      /**\n       * 获得当前的key\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public NullWritable getCurrentKey() throws IOException,\n              InterruptedException {\n          return NullWritable.get();\n      }\n  \n      /**\n       * 获得当前的value\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public BytesWritable getCurrentValue() throws IOException,\n              InterruptedException {\n          return value;\n      }\n  \n      /**\n       * 获得分片读取的百分比；因为如果读取分片数据的话，会一次性的读取完；所以进度要么是1，要么是0\n       * @return\n       * @throws IOException\n       */\n      @Override\n      public float getProgress() throws IOException {\n          //因为一个文件作为一个整体处理，所以，如果processed为true，表示已经处理过了，进度为1；否则为0\n          return processed ? 1.0f : 0.0f;\n      }\n  \n      @Override\n      public void close() throws IOException {\n      }\n  }\n  ```\n\n- main方法\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.conf.Configuration;\n  import org.apache.hadoop.conf.Configured;\n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.io.Text;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.Job;\n  import org.apache.hadoop.mapreduce.Mapper;\n  import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n  import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\n  import org.apache.hadoop.util.Tool;\n  import org.apache.hadoop.util.ToolRunner;\n  \n  import java.io.IOException;\n  \n  /**\n   * 让主类继承Configured类，实现Tool接口\n   * 实现run()方法\n   * 将以前main()方法中的逻辑，放到run()中\n   * 在main()中，调用ToolRunner.run()方法，第一个参数是当前对象；第二个参数是输入、输出\n   */\n  public class SmallFiles2SequenceFile extends Configured implements Tool {\n  \n      /**\n       * 自定义Mapper类\n       * mapper类的输入键值对类型，与自定义InputFormat的输入键值对保持一致\n       * mapper类的输出的键值对类型，分别是文件名、文件内容\n       */\n      static class SequenceFileMapper extends\n              Mapper<NullWritable, BytesWritable, Text, BytesWritable> {\n  \n          private Text filenameKey;\n  \n          /**\n           * 取得文件名\n           * @param context\n           * @throws IOException\n           * @throws InterruptedException\n           */\n          @Override\n          protected void setup(Context context) throws IOException,\n                  InterruptedException {\n              InputSplit split = context.getInputSplit();\n              //获得当前文件路径\n              Path path = ((FileSplit) split).getPath();\n              filenameKey = new Text(path.toString());\n          }\n  \n          @Override\n          protected void map(NullWritable key, BytesWritable value,\n                             Context context) throws IOException, InterruptedException {\n              context.write(filenameKey, value);\n          }\n      }\n  \n      public int run(String[] args) throws Exception {\n          Configuration conf = new Configuration();\n          Job job = Job.getInstance(conf,\"combine small files to sequencefile\");\n          job.setJarByClass(SmallFiles2SequenceFile.class);\n  \n          //设置自定义输入格式\n          job.setInputFormatClass(WholeFileInputFormat.class);\n  \n          WholeFileInputFormat.addInputPath(job,new Path(args[0]));\n          //设置输出格式SequenceFileOutputFormat及输出路径\n          job.setOutputFormatClass(SequenceFileOutputFormat.class);\n          SequenceFileOutputFormat.setOutputPath(job,new Path(args[1]));\n  \n          job.setOutputKeyClass(Text.class);\n          job.setOutputValueClass(BytesWritable.class);\n          job.setMapperClass(SequenceFileMapper.class);\n          return job.waitForCompletion(true) ? 0 : 1;\n      }\n  \n      public static void main(String[] args) throws Exception {\n          int exitCode = ToolRunner.run(new SmallFiles2SequenceFile(),\n                  args);\n          System.exit(exitCode);\n  \n      }\n  }\n  ```\n\n#### 4.5 总结\n\n- 若要自定义InputFormat的话\n  - 需要自定义InputFormat类，并覆写getRecordReader()方法\n  - 自定义RecordReader类，实现方法\n    - initialize()\n    - nextKeyValue()\n    - getCurrentKey()\n    - getCurrentValue()\n    - getProgress()\n    - close()\n\n\n\n## 5、拓展点、未来计划、行业趋势\n\n1. MR中还有一些自带的输入格式，扩展阅读：《Hadoop权威指南 第4版》8.2 输入格式\n\n   ![](assets/Image201909091251.png)\n\n   \n","tags":["hadoop","MapReduce"]},{"title":"MapRedecer编程（一）","url":"/2019/10/15/it/hadoop/MapRedecer编程（一）/","content":"\n# MapReduce编程模型\n\n\n## 一、知识要点\n\n### 1. MapReduce编程模型\n\n- Hadoop架构图\n\n  Hadoop由HDFS分布式存储、**MapReduce分布式计算**、Yarn资源调度三部分组成\n\n![](assets/Image201906191834-1562922704761.png)\n\n- MapReduce是采用一种**分而治之**的思想设计出来的分布式计算框架\n- MapReduce由两个阶段组成：\n  - Map阶段（切分成一个个小的任务）\n  - Reduce阶段（汇总小任务的结果）\n- 那什么是分而治之呢？\n  - 比如一复杂、计算量大、耗时长的的任务，暂且称为“大任务”；\n  - 此时使用单台服务器无法计算或较短时间内计算出结果时，可将此大任务切分成一个个小的任务，小任务分别在不同的服务器上**并行**的执行\n  - 最终再汇总每个小任务的结果\n\n![](assets/Image201906251747.png)\n\n#### 1.1 Map阶段\n\n- map阶段有一个关键的map()函数；\n- 此函数的输入是**键值对**\n- 输出是一系列**键值对**，输出写入**本地磁盘**。\n\n#### 1.2 Reduce阶段\n\n- reduce阶段有一个关键的函数reduce()函数\n\n- 此函数的输入也是键值对（即map的输出（kv对））\n\n- 输出也是一系列键值对，结果最终写入HDFS\n\n#### 1.3 Map&Reduce\n\n![](assets/Image201906251807.png)\n\n\n\n### 2. MapReduce编程示例\n\n- 以**MapReduce的词频统计**为例：统计一批英文文章当中，每个单词出现的总次数\n\n#### 2.1 MapReduce原理图\n\n![](assets/Image201906271715.png)\n\n- Map阶段\n  - 假设MR的输入文件“**Gone With The Wind**”有三个block；block1、block2、block3 \n  - MR编程时，每个block对应一个分片split\n  - 每一个split对应一个map任务（map task）\n  - 如图共3个map任务（map1、map2、map3）；这3个任务的逻辑一样，所以以第一个map任务（map1）为例分析 \n  - map1读取block1的数据；一次读取block1的一行数据；\n    - 产生键值对(key/value)，作为map()的参数传入，调用map()；\n    - 假设当前所读行是第一行\n    - 将当前所读行的行首相对于当前block开始处的字节偏移量作为key（0）\n    - 当前行的内容作为value（Dear Bear River）\n  - map()内\n    - (按需求，写业务代码)，将value当前行内容按空格切分，得到三个单词Dear | Bear | River\n    - 将每个单词变成键值对，输出出去(Dear, 1) | (Bear, 1) | (River, 1)；最终结果写入map任务所在节点的本地磁盘中（内里还有细节，讲到shuffle时，再细细展开）\n    - block的第一行的数据被处理完后，接着处理第二行；逻辑同上\n    - 当map任务将当前block中所有的数据全部处理完后，此map任务即运行结束\n  - 其它的每一个map任务都是如上逻辑，不再赘述\n- Reduce阶段\n  - reduce任务（reduce task）的个数由自己写的程序编程指定，main()内的job.setNumReduceTasks(4)指定reduce任务是4个（reduce1、reduce2、reduce3、reduce4）\n  - 每一个reduce任务的逻辑一下，所以以第一个reduce任务（reduce1）为例分析\n  - map1任务完成后，reduce1通过网络，连接到map1，将map1输出结果中属于reduce1的分区的数据，通过网络获取到reduce1端（拷贝阶段）\n  - 同样也如此连接到map2、map3获取结果\n  - 最终reduce1端获得4个(Dear, 1)键值对；由于key键相同，它们分到同一组；\n  - 4个(Dear, 1)键值对，转换成[Dear, Iterable(1, 1, 1, )]，作为两个参数传入reduce()\n  - 在reduce()内部，计算Dear的总数为4，并将(Dear, 4)作为键值对输出\n  - 每个reduce任务最终输出文件（内里还有细节，讲到shuffle时，再细细展开），文件写入到HDFS\n\n#### 2.2 MR中key的作用\n\n- <font color='red'>**MapReduce编程中，key有特殊的作用**</font>\n\n  - **①数据中，若要针对某个值进行分组、聚合时，需将此值作为MR中的reduce的输入的key**\n\n  - **如当前的词频统计例子，按单词进行分组，每组中对出现次数做聚合（计算总和）；所以需要将每个单词作为reduce输入的key，MapReduce框架自动按照单词分组，进而求出每组即每个单词的总次数**\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201910141101.png)\n\n  - **②另外，key还具有可排序的特性，因为MR中的key类需要实现WritableComparable接口；而此接口又继承Comparable接口（可查看源码）**\n\n  - **MR编程时，要充分利用以上两点；结合实际业务需求，设置合适的key**\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201908221717.png)\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201908221718.png)\n\n\n\n#### 2.4 MR参考代码\n\n**2.4.1 Mapper代码**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * 类Mapper<LongWritable, Text, Text, IntWritable>的四个泛型分别表示\n * map方法的输入的键的类型kin、值的类型vin；输出的键的类型kout、输出的值的类型vout\n * kin指的是当前所读行行首相对于split分片开头的字节偏移量,所以是long类型，对应序列化类型LongWritable\n * vin指的是当前所读行，类型是String，对应序列化类型Text\n * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text\n * vout根据需求，输出值指的是单词的个数，1，类型是int，对应序列化类型是IntWritable\n *\n */\npublic class WordCountMap extends Mapper<LongWritable, Text, Text, IntWritable> {\n\n    /**\n     * 处理分片split中的每一行的数据；针对每行数据，会调用一次map方法\n     * 在一次map方法调用时，从一行数据中，获得一个个单词word，再将每个单词word变成键值对形式(word, 1)输出出去\n     * 输出的值最终写到本地磁盘中\n     * @param key 当前所读行行首相对于split分片开头的字节偏移量\n     * @param value  当前所读行\n     * @param context\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public void map(LongWritable key, Text value, Context context)\n            throws IOException, InterruptedException {\n        //当前行的示例数据(单词间空格分割)：Dear Bear River\n        //取得当前行的数据\n        String line = value.toString();\n        //按照\\t进行分割，得到当前行所有单词\n        String[] words = line.split(\"\\t\");\n\n        for (String word : words) {\n            //将每个单词word变成键值对形式(word, 1)输出出去\n            //同样，输出前，要将kout, vout包装成对应的可序列化类型，如String对应Text，int对应IntWritable\n            context.write(new Text(word), new IntWritable(1));\n        }\n    }\n}\n\n```\n\n**2.4.2 Reducer代码**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\n/**\n *\n * Reducer<Text, IntWritable, Text, IntWritable>的四个泛型分别表示\n * reduce方法的输入的键的类型kin、输入值的类型vin；输出的键的类型kout、输出的值的类型vout\n * 注意：因为map的输出作为reduce的输入，所以此处的kin、vin类型分别与map的输出的键类型、值类型相同\n * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text\n * vout根据需求，输出值指的是每个单词的总个数，类型是int，对应序列化类型是IntWritable\n *\n */\npublic class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {\n    /**\n     *\n     * key相同的一组kv对，会调用一次reduce方法\n     * 如reduce task汇聚了众多的键值对，有key是hello的键值对，也有key是spark的键值对，如下\n     * (hello, 1)\n     * (hello, 1)\n     * (hello, 1)\n     * (hello, 1)\n     * ...\n     * (spark, 1)\n     * (spark, 1)\n     * (spark, 1)\n     *\n     * 其中，key是hello的键值对被分成一组；merge成[hello, Iterable(1,1,1,1)]，调用一次reduce方法\n     * 同样，key是spark的键值对被分成一组；merge成[spark, Iterable(1,1,1)]，再调用一次reduce方法\n     *\n     * @param key 当前组的key\n     * @param values 当前组中，所有value组成的可迭代集和\n     * @param context reduce上下文环境对象\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public void reduce(Text key, Iterable<IntWritable> values,\n                          Context context) throws IOException, InterruptedException {\n        //定义变量，用于累计当前单词出现的次数\n        int sum = 0;\n\n        for (IntWritable count : values) {\n            //从count中获得值，累加到sum中\n            sum += count.get();\n        }\n\n        //将单词、单词次数，分别作为键值对，输出\n        context.write(key, new IntWritable(sum));// 输出最终结果\n    };\n}\n```\n\n**2.4.3 Main程序入口**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport java.io.IOException;\n\n/**\n *\n * MapReduce程序入口\n * 注意：\n *  导包时，不要导错了；\n *  另外，map\\reduce相关的类，使用mapreduce包下的，是新API，如org.apache.hadoop.mapreduce.Job;；\n */\npublic class WordCountMain {\n    //若在IDEA中本地执行MR程序，需要将mapred-site.xml中的mapreduce.framework.name值修改成local\n    //参数 c:/test/README.txt c:/test/wc\n    public static void main(String[] args) throws IOException,\n            ClassNotFoundException, InterruptedException {\n\n        //判断一下，输入参数是否是两个，分别表示输入路径、输出路径\n       if (args.length != 2 || args == null) {\n            System.out.println(\"please input Path!\");\n            System.exit(0);\n        }\n\n        Configuration configuration = new Configuration();\n        //configuration.set(\"mapreduce.framework.name\",\"local\");\n\n\n        //告诉程序，要运行的jar包在哪\n        //configuration.set(\"mapreduce.job.jar\",\"/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n\n        //调用getInstance方法，生成job实例\n        Job job = Job.getInstance(configuration, WordCountMain.class.getSimpleName());\n\n        //设置job的jar包，如果参数指定的类包含在一个jar包中，则此jar包作为job的jar包； 参数class跟主类在一个工程即可；一般设置成主类\n//        job.setJarByClass(WordCountMain.class);\n        job.setJarByClass(WordCountMain.class);\n\n        //通过job设置输入/输出格式\n        //MR的默认输入格式是TextInputFormat，输出格式是TextOutputFormat；所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n        //设置输入/输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        //设置处理Map阶段的自定义的类\n        job.setMapperClass(WordCountMap.class);\n        //设置map combine类，减少网路传出量\n        job.setCombinerClass(WordCountReduce.class);\n        //设置处理Reduce阶段的自定义的类\n        job.setReducerClass(WordCountReduce.class);\n        //注意：如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n//        job.setMapOutputKeyClass(Text.class);\n//        job.setMapOutputValueClass(IntWritable.class);\n\n        //设置reduce task最终输出key/value的类型\n        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // 提交作业\n        job.waitForCompletion(true);\n\n    }\n}\n```\n\n> 程序运行有两种方式，分别是windows本地运行、集群运行，依次演示\n\n\n#### 2.5 集群运行\n\n- 用maven插件打jar包；①点击Maven，②双击package打包\n\n```shell\n[hadoop@node01 ~]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.wordcount.WordCountMain /README.txt /wordcount01\n```\n\n> 说明：\n>\n> com.kaikeba.hadoop-1.0-SNAPSHOT.jar是jar包名\n>\n> com.kaikeba.hadoop.wordcount.WordCountMain是包含main方法的类的全限定名\n>\n> /NOTICE.txt和/wordcount是main方法的两个参数，表示输入路径、输出路径\n\n![](assets/hadoop jar.gif)\n\n- 确认结果\n\n```shell\n[hadoop@node01 ~]$ hadoop fs -ls /wordcount01\n```\n\n![](assets/Image201908221620.png)\n\n#### 2.6 总结\n\n- MR分为两个阶段：map阶段、reduce阶段\n- MR输入的文件有几个block，就会生成几个map任务\n- MR的reduce任务的个数，由程序中编程指定：job.setNumReduceTasks(4)\n- map任务\n  - map任务中map()一次读取block的一行数据，以kv对的形式输入map()\n  - map()的输出作为reduce()的输入\n- reduce任务\n  - reduce任务通过网络将各执行完成的map任务输出结果中，属于自己的数据取过来\n  - key相同的键值对作为一组，调用一次reduce()\n  - reduce任务生成一个结果文件\n  - 文件写入HDFS\n\n\n\n### 3. WEB UI查看结果\n\n#### 3.1 Yarn\n\n> node01是resourcemanager所在节点主机名，根据自己的实际情况修改主机名\n\n浏览器访问url地址：http://node01:8088\n\n![](assets/Image201908221638.png)\n\n#### 3.2 HDFS结果\n\n浏览器输入URL：http://node01:50070\n\n①点击下拉框；②浏览文件系统；③输入根目录，查看hdfs根路径中的内容\n\n![](assets/Image201908221639.png)\n\n\n\n### 4. Shuffle\n\n- shuffle主要指的是map端的输出作为reduce端输入的过程\n\n#### 4.1 shuffle简图\n\n![](assets/Image201905231409.png)\n\n#### 4.2 shuffle细节图\n\n![](assets/Image201906280906.png)\n\n- 分区用到了分区器，默认分区器是HashPartitioner\n\n  源码：\n\n  ```java\n  public class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n  \n    public void configure(JobConf job) {}\n  \n    /** Use {@link Object#hashCode()} to partition. */\n    public int getPartition(K2 key, V2 value,\n                            int numReduceTasks) {\n      return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n  \n  }\n  ```\n\n#### 4.3 map端\n\n  - 每个map任务都有一个对应的环形内存缓冲区；输出是kv对，先写入到环形缓冲区（默认大小100M），当内容占据80%缓冲区空间后，由一个后台线程将缓冲区中的数据溢出写到一个磁盘文件\n  - 在溢出写的过程中，map任务可以继续向环形缓冲区写入数据；但是若写入速度大于溢出写的速度，最终造成100m占满后，map任务会暂停向环形缓冲区中写数据的过程；只执行溢出写的过程；直到环形缓冲区的数据全部溢出写到磁盘，才恢复向缓冲区写入\n  - 后台线程溢写磁盘过程，有以下几个步骤：\n    - 先对每个溢写的kv对做分区；分区的个数由MR程序的reduce任务数决定；默认使用HashPartitioner计算当前kv对属于哪个分区；计算公式：(key.hashCode() & Integer.MAX_VALUE) % numReduceTasks\n    - 每个分区中，根据kv对的key做内存中排序；\n    - 若设置了map端本地聚合combiner，则对每个分区中，排好序的数据做combine操作；\n    - 若设置了对map输出压缩的功能，会对溢写数据压缩\n  - 随着不断的向环形缓冲区中写入数据，会多次触发溢写（每当环形缓冲区写满100m），本地磁盘最终会生成多个溢出文件\n  - 合并溢写文件：在map task完成之前，所有溢出文件会被合并成一个大的溢出文件；且是已分区、已排序的输出文件\n  - 小细节：\n    - 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作；\n    - 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销）\n\n#### 4.4 reduce端\n\n- reduce task会在每个map task运行完成后，通过HTTP获得map task输出中，属于自己的分区数据（许多kv对）\n\n- 如果map输出数据比较小，先保存在reduce的jvm内存中，否则直接写入reduce磁盘\n\n- 一旦内存缓冲区达到阈值（默认0.66）或map输出数的阈值（默认1000），则触发**归并merge**，结果写到本地磁盘\n\n- 若MR编程指定了combine，在归并过程中会执行combine操作\n\n- 随着溢出写的文件的增多，后台线程会将它们合并大的、排好序的文件\n\n- reduce task将所有map task复制完后，将合并磁盘上所有的溢出文件\n\n- 默认一次合并10个\n\n- 最后一批合并，部分数据来自内存，部分来自磁盘上的文件\n\n- 进入“归并、排序、分组阶段”\n\n- 每组数据调用一次reduce方法\n\n- 参考文件《**reduce端merge 排序 分组.txt**》\n\n\n\n#### 4.5 总结\n\n- map端\n  - map()输出结果先写入环形缓冲区\n  - 缓冲区100M；写满80M后，开始溢出写磁盘文件\n  - 此过程中，会进行分区、排序、combine（可选）、压缩（可选）\n  - map任务完成前，会将多个小的溢出文件，合并成一个大的溢出文件（已分区、排序）\n- reduce端\n  - 拷贝阶段：reduce任务通过http将map任务属于自己的分区数据拉取过来\n  - 开始merge及溢出写磁盘文件\n  - 所有map任务的分区全部拷贝过来后，进行阶段合并、排序、分组阶段\n  - 每组数据调用一次reduce()\n  - 结果写入HDFS\n","tags":["hadoop","MapReduce"]},{"title":"HDFS文件系统","url":"/2019/10/14/it/hadoop/HDFS文件系统/","content":"\n#  HDFS分布式文件系统\n\n### 1. HDFS读写流程\n\n#### 1.1 数据写流程\n\n![1557999856839](img/1557999856839.png)\n\n![HDFS写入文件流程](img/HDFS写入文件流程.png)\n\n**1.1 详细流程**\n\n- 创建文件：\t\n\n  - HDFS客户端向HDFS写数据，先调用DistributedFileSystem.create()方法，在HDFS创建新的空文件\n  - RPC（ClientProtocol.create()）远程过程调用NameNode（NameNodeRpcServer）的create()，首先在HDFS目录树指定路径添加新文件\n  - 然后将创建新文件的操作记录在editslog中\n  - NameNode.create方法执行完后，DistributedFileSystem.create()返回FSDataOutputStream，它本质是封装了一个DFSOutputStream对象\n\n- 建立数据流管道：\n\n  - 客户端调用DFSOutputStream.write()写数据\n  - DFSOutputStream调用ClientProtocol.addBlock()，首先向NameNode申请一个空的数据块\n  - addBlock()返回LocatedBlock对象，对象包含当前数据块的所有datanode的位置信息\n  - 根据位置信息，建立数据流管道\n\n- 向数据流管道pipeline中写当前块的数据：\n\n  - 客户端向流管道中写数据，先将数据写入一个检验块chunk中，大小512Byte，写满后，计算chunk的检验和checksum值（4Byte）\n  - 然后将chunk数据本身加上checksum，形成一个带checksum值的chunk（516Byte）\n  - 保存到一个更大一些的结构**packet数据包**中，packet为64kB大小\n- packet写满后，先被写入一个**dataQueue**队列中\n  - packet被从队列中取出，向pipeline中写入，先写入datanode1，再从datanoe1传到datanode2，再从datanode2传到datanode3中\n- 一个packet数据取完后，后被放入到**ackQueue**中等待pipeline关于该packet的ack的反馈\n  - 每个packet都会有ack确认包，逆pipeline（dn3 -> dn2 -> dn1）传回输出流\n- 若packet的ack是SUCCESS成功的，则从ackQueue中，将packet删除；否则，将packet从ackQueue中取出，重新放入dataQueue，重新发送\n  - 如果当前块写完后，文件还有其它块要写，那么再调用addBlock方法（**流程同上**）\n- 文件最后一个block块数据写完后，会再发送一个空的packet，表示当前block写完了，然后关闭pipeline\n  - 所有块写完，close()关闭流\n- ClientProtocol.complete()通知namenode当前文件所有块写完了\n\n**6.1.2 容错**\n\n- 在写的过程中，pipeline中的datanode出现故障（如网络不通），输出流如何恢复\n  - 输出流中ackQueue缓存的所有packet会被重新加入dataQueue\n  - 输出流调用ClientProtocol.updateBlockForPipeline()，为block申请一个新的时间戳，namenode会记录新时间戳\n  - 确保故障datanode即使恢复，但由于其上的block时间戳与namenode记录的新的时间戳不一致，故障datanode上的block进而被删除\n  - 故障的datanode从pipeline中删除\n  - 输出流调用ClientProtocol.getAdditionalDatanode()通知namenode分配新的datanode到数据流pipeline中，并使用新的时间戳建立pipeline\n  - 新添加到pipeline中的datanode，目前还没有存储这个新的block，HDFS客户端通过DataTransferProtocol通知pipeline中的一个datanode复制这个block到新的datanode中\n  - pipeline重建后，输出流调用ClientProtocol.updatePipeline()，更新namenode中的元数据\n  - 故障恢复完毕，完成后续的写入流程\n\n#### 1.2 数据读流程\n\n**1.2.1 基本流程**\n\n![HDFS文件读取流程](img/HDFS文件读取流程.png)\n\n- 1、client端读取HDFS文件，client调用文件系统对象DistributedFileSystem的open方法\n- 2、返回FSDataInputStream对象（对DFSInputStream的包装）\n- 3、构造DFSInputStream对象时，调用namenode的getBlockLocations方法，获得file的开始若干block（如blk1, blk2, blk3, blk4）的存储datanode（以下简称dn）列表；针对每个block的dn列表，会根据网络拓扑做排序，离client近的排在前；\n- 4、调用DFSInputStream的read方法，先读取blk1的数据，与client最近的datanode建立连接，读取数据\n- 5、读取完后，关闭与dn建立的流\n- 6、读取下一个block，如blk2的数据（重复步骤4、5、6）\n- 7、这一批block读取完后，再读取下一批block的数据（重复3、4、5、6、7）\n- 8、完成文件数据读取后，调用FSDataInputStream的close方法\n\n**1.2.2 容错**\n\n- 情况一：读取block过程中，client与datanode通信中断\n\n  - client与存储此block的第二个datandoe建立连接，读取数据\n  - 记录此有问题的datanode，不会再从它上读取数据\n\n- 情况二：client读取block，发现block数据有问题\n  -  client读取block数据时，同时会读取到block的校验和，若client针对读取过来的block数据，计算检验和，其值与读取过来的校验和不一样，说明block数据损坏\n  -  client从存储此block副本的其它datanode上读取block数据（也会计算校验和）\n  -  同时，client会告知namenode此情况；\n\n\n\n\n\n### 2. Hadoop HA高可用\n\n#### 2.1 HDFS高可用原理\n\n![](img/Image201905211519.png)\n\n- 对于HDFS ，NN存储元数据在内存中，并负责管理文件系统的命名空间和客户端对HDFS的读写请求。但是，如果只存在一个NN，一旦发生“单点故障”，会使整个系统失效。\n- 虽然有个SNN，但是它并不是NN的热备份\n- 因为SNN无法提供“热备份”功能，在NN故障时，无法立即切换到SNN对外提供服务，即HDFS处于停服状态。\n- HDFS2.x采用了HA（High Availability高可用）架构。\n  - 在HA集群中，可设置两个NN，一个处于“活跃（Active）”状态，另一个处于“待命（Standby）”状态。\n  - 由zookeeper确保一主一备（讲zookeeper时具体展开）\n  - 处于Active状态的NN负责响应所有客户端的请求，处于Standby状态的NN作为热备份节点，保证与active的NN的元数据同步\n  - Active节点发生故障时，zookeeper集群会发现此情况，通知Standby节点立即切换到活跃状态对外提供服务\n  - 确保集群一直处于可用状态\n- 如何热备份元数据：\n  - Standby NN是Active NN的“热备份”，因此Active NN的状态信息必须实时同步到StandbyNN。\n  - 可借助一个共享存储系统来实现状态同步，如NFS(NetworkFile System)、QJM(Quorum Journal Manager)或者Zookeeper。\n  - Active NN将更新数据写入到共享存储系统，Standby NN一直监听该系统，一旦发现有新的数据写入，就立即从公共存储系统中读取这些数据并加载到Standby NN自己内存中，从而保证元数据与Active NN状态一致。\n- 块报告：\n  - NN保存了数据块到实际存储位置的映射信息，为了实现故障时的快速切换，必须保证StandbyNN中也包含最新的块映射信息\n  - 因此需要给所有DN配置Active和Standby两个NN的地址，把块的位置和心跳信息同时发送到两个NN上。\n\n### 3. Hadoop联邦\n\n#### 3.1 为什么需要联邦\n\n- 虽然HDFS HA解决了“单点故障”问题，但HDFS在扩展性、整体性能和隔离性方面仍有问题\n  - 系统扩展性方面，元数据存储在NN内存中，受限于内存上限（每个文件、目录、block占用约150字节）\n  - 整体性能方面，吞吐量受单个NN的影响\n  - 隔离性方面，一个程序可能会影响其他程序的运行，如果一个程序消耗过多资源会导致其他程序无法顺利运行\n  - HDFS HA本质上还是单名称节点\n\n#### 3.2 联邦\n\n![](img/Image201909041239.png)\n\n\n- HDFS联邦可以解决以上三个问题\n  - HDFS联邦中，设计了多个命名空间；每个命名空间有一个NN或一主一备两个NN，使得HDFS的命名服务能够水平扩展\n  - 这些NN分别进行各自命名空间namespace和块的管理，相互独立，不需要彼此协调\n  - 每个DN要向集群中所有的NN注册，并周期性的向所有NN发送心跳信息和块信息，报告自己的状态\n  - HDFS联邦每个相互独立的NN对应一个独立的命名空间\n  - 每一个命名空间管理属于自己的一组块，这些属于同一命名空间的块对应一个“块池”的概念。\n  - 每个DN会为所有块池提供块的存储，块池中的各个块实际上是存储在不同DN中的\n\n#### 3.3 扩展\n\n[联邦-官网](<https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/Federation.html>)\n\n\n\n\n\n### 4. 文件压缩\n\n#### 4.1 压缩算法\n\n- 文件压缩好处：\n\n  - 减少数据所占用的磁盘空间\n  - 加快数据在磁盘、网络上的IO\n\n- 常用压缩格式\n\n  | 压缩格式 | UNIX工具 | 算      法 | 文件扩展名 | 可分割 |\n  | -------- | -------- | ---------- | ---------- | ------ |\n  | DEFLATE  | 无       | DEFLATE    | .deflate   | No     |\n  | gzip     | gzip     | DEFLATE    | .gz        | No     |\n  | zip      | zip      | DEFLATE    | .zip       | YES    |\n  | bzip     | bzip2    | bzip2      | .bz2       | YES    |\n  | LZO      | lzop     | LZO        | .lzo       | No     |\n  | Snappy   | 无       | Snappy     | .snappy    | No     |\n\n- Hadoop的压缩实现类；均实现CompressionCodec接口\n\n  | 压缩格式 | 对应的编码/解码器                          |\n  | -------- | ------------------------------------------ |\n  | DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |\n  | gzip     | org.apache.hadoop.io.compress.GzipCodec    |\n  | bzip2    | org.apache.hadoop.io.compress.BZip2Codec   |\n  | LZO      | com.hadoop.compression.lzo.LzopCodec       |\n  | Snappy   | org.apache.hadoop.io.compress.SnappyCodec  |\n\n- 查看集群是否支持本地压缩（所有节点都要确认）\n\n  ```\n  [hadoop@node01 ~]$ hadoop checknative\n  ```\n\n  ![](img/Image201910111114.png)\n\n#### 4.2 编程实践\n\n- 编程：上传压缩过的文件到HDFS\n\n\n```java\n\n    /**\n     * 上传压缩文件到服务器\n     *  传递参数\n     *  args[0] 本地文件路径\n     *  args[1] hdoop文件系统 路径\n     */\n    public static void uploadFileZipToFileSystem(String source,String targetUrl){\n        System.out.println(\"文件地址：\" + source);\n        System.out.println(\"目标服务器：\" + targetUrl);\n        InputStream inputStreamSourceFile = null;\n\n        try {\n            // 获取文件输入流\n            inputStreamSourceFile = new BufferedInputStream(new FileInputStream(source));\n            // HDFS 读写配置文件\n            Configuration configuration = new Configuration();\n            // 压缩类型\n            BZip2Codec codec = new BZip2Codec();\n            codec.setConf(configuration);\n            // 通过url 返回文件系统实例\n            FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);\n            //调用Filesystem的create方法返回的是FSDataOutputStream对象\n            //该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n            // 获取文件系用的输出流\n            OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n            // 对输出流进行压缩\n            CompressionOutputStream compressionOut = codec.createOutputStream(outputStreamTarget);\n            // 将文件输入流，写入输入流\n            IOUtils.copyBytes(inputStreamSourceFile,compressionOut,4069,true);\n            System.out.println(\"上传成功\");\n        } catch (FileNotFoundException e) {\n            System.err.println(e.getMessage());\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n```\n\n- 扩展阅读\n  - 《Hadoop权威指南》 5.2章节 压缩\n  - [HDFS文件压缩](<https://blog.csdn.net/qq_38262266/article/details/79171524>)\n\n\n\n\n\n### 5. 小文件治理\n\n#### 5.1 有没有问题\n\n- NameNode存储着文件系统的元数据，每个文件、目录、块大概有150字节的元数据；\n- 因此文件数量的限制也由NN内存大小决定，如果小文件过多则会造成NN的压力过大\n- 且HDFS能存储的数据总量也会变小\n\n#### 5.2 HAR文件方案（10分钟）\n\n- 本质启动mr程序，所以需要启动yarn\n\n![1558004541101](img/1558004541101.png)\n\n用法：\n\n```sh\narchive -archiveName <NAME>.har -p <parent path> [-r <replication factor>]<src>* <dest>\n```\n\n![](img/Image201909041408.png)\n\n![alt](img/Image201906210960.png)\n\n```shell\n# 创建archive文件；/testhar有两个子目录th1、th2；两个子目录中有若干文件\nhadoop archive -archiveName test.har -p /testhar -r 3 th1 th2 /outhar # 原文件还存在，需手动删除\n\n# 查看archive文件\nhdfs dfs -ls -R har:///outhar/test.har\n\n# 解压archive文件\n# 方式一\nhdfs dfs -cp har:///outhar/test.har/th1 hdfs:/unarchivef # 顺序\nhadoop fs -ls /unarchivef\t\n# 方式二\nhadoop distcp har:///outhar/test.har/th1 hdfs:/unarchivef2 # 并行，启动MR\n```\n\n#### 5.3 Sequence Files方案（*）\n\n- SequenceFile文件，主要由一条条record记录组成；每个record是键值对形式的\n- SequenceFile文件可以作为小文件的存储容器；\n  - 每条record保存一个小文件的内容\n  - 小文件名作为当前record的键；\n  - 小文件的内容作为当前record的值；\n  - 如10000个100KB的小文件，可以编写程序将这些文件放到一个SequenceFile文件。\n- 一个SequenceFile是**可分割**的，所以MapReduce可将文件切分成块，每一块独立操作。\n- 具体结构（如下图）：\n  - 一个SequenceFile首先有一个4字节的header（文件版本号）\n  - 接着是若干record记录\n  - 记录间会随机的插入一些同步点sync marker，用于方便定位到记录边界\n- 不像HAR，SequenceFile**支持压缩**。记录的结构取决于是否启动压缩\n  - 支持两类压缩：\n    - 不压缩NONE，如下图\n    - 压缩RECORD，如下图\n    - 压缩BLOCK，①一次性压缩多条记录；②每一个新块Block开始处都需要插入同步点；如下图\n  - 在大多数情况下，以block（注意：指的是SequenceFile中的block）为单位进行压缩是最好的选择\n  - 因为一个block包含多条记录，利用record间的相似性进行压缩，压缩效率更高\n  - 把已有的数据转存为SequenceFile比较慢。比起先写小文件，再将小文件写入SequenceFile，一个更好的选择是直接将数据写入一个SequenceFile文件，省去小文件作为中间媒介.\n\n![](img/Image201907101934.png)\n\n\n\n![](img/Image201907101935.png)\n\n- 向SequenceFile写入数据\n\n```java\npackage com.kaikeba.hadoop.sequencefile;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.SequenceFile;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.BZip2Codec;\n\nimport java.io.IOException;\nimport java.net.URI;\n\npublic class SequenceFileWriteNewVersion {\n\n    //模拟数据源\n    private static final String[] DATA = {\n            \"The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.\",\n            \"It is designed to scale up from single servers to thousands of machines, each offering local computation and storage.\",\n            \"Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer\",\n            \"o delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.\",\n            \"Hadoop Common: The common utilities that support the other Hadoop modules.\"\n    };\n\n    public static void main(String[] args) throws IOException {\n        //输出路径：要生成的SequenceFile文件名\n        String uri = \"hdfs://node01:9000/writeSequenceFile\";\n\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(URI.create(uri), conf);\n        //向HDFS上的此SequenceFile文件写数据\n        Path path = new Path(uri);\n\n        //因为SequenceFile每个record是键值对的\n        //指定key类型\n        IntWritable key = new IntWritable();\n        //指定value类型\n        Text value = new Text();\n//\n//            FileContext fileContext = FileContext.getFileContext(URI.create(uri));\n//            Class<?> codecClass = Class.forName(\"org.apache.hadoop.io.compress.SnappyCodec\");\n//            CompressionCodec SnappyCodec = (CompressionCodec)ReflectionUtils.newInstance(codecClass, conf);\n//            SequenceFile.Metadata metadata = new SequenceFile.Metadata();\n//            //writer = SequenceFile.createWriter(fs, conf, path, key.getClass(), value.getClass());\n//            writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(path), SequenceFile.Writer.keyClass(IntWritable.class),\n//                                        SequenceFile.Writer.valueClass(Text.class));\n\n        //创建向SequenceFile文件写入数据时的一些选项\n        //要写入的SequenceFile的路径\n        SequenceFile.Writer.Option pathOption       = SequenceFile.Writer.file(path);\n        //record的key类型选项\n        SequenceFile.Writer.Option keyOption        = SequenceFile.Writer.keyClass(IntWritable.class);\n        //record的value类型选项\n        SequenceFile.Writer.Option valueOption      = SequenceFile.Writer.valueClass(Text.class);\n        //SequenceFile压缩方式：NONE | RECORD | BLOCK三选一\n        //方案一：RECORD、不指定压缩算法\n        SequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD);\n        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);\n\n\n        //方案二：BLOCK、不指定压缩算法\n//        SequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.BLOCK);\n//        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);\n\n\n\n        //方案三：使用BLOCK、压缩算法BZip2Codec；压缩耗时间\n        //再加压缩算法\n//        BZip2Codec codec = new BZip2Codec();\n//        codec.setConf(conf);\n//        SequenceFile.Writer.Option compressAlgorithm = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD, codec);\n//        //创建写数据的Writer实例\n//        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressAlgorithm);\n\n\n\n        for (int i = 0; i < 100000; i++) {\n            //分别设置key、value值\n            key.set(100 - i);\n            value.set(DATA[i % DATA.length]);\n            System.out.printf(\"[%s]\\t%s\\t%s\\n\", writer.getLength(), key, value);\n            //在SequenceFile末尾追加内容\n            writer.append(key, value);\n        }\n        //关闭流\n        IOUtils.closeStream(writer);\n    }\n}\n```\n\n- 命令查看SequenceFile内容\n\n```shell\n hadoop fs -text /writeSequenceFile\n```\n\n- 读取SequenceFile文件\n\n```java\npackage com.kaikeba.hadoop.sequencefile;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.SequenceFile;\nimport org.apache.hadoop.io.Writable;\nimport org.apache.hadoop.util.ReflectionUtils;\n\nimport java.io.IOException;\n\npublic class SequenceFileReadNewVersion {\n\n    public static void main(String[] args) throws IOException {\n        //要读的SequenceFile\n        String uri = \"hdfs://node01:9000/writeSequenceFile\";\n        Configuration conf = new Configuration();\n        Path path = new Path(uri);\n\n        //Reader对象\n        SequenceFile.Reader reader = null;\n        try {\n            //读取SequenceFile的Reader的路径选项\n            SequenceFile.Reader.Option pathOption = SequenceFile.Reader.file(path);\n\n            //实例化Reader对象\n            reader = new SequenceFile.Reader(conf, pathOption);\n\n            //根据反射，求出key类型\n            Writable key = (Writable)\n                    ReflectionUtils.newInstance(reader.getKeyClass(), conf);\n            //根据反射，求出value类型\n            Writable value = (Writable)\n                    ReflectionUtils.newInstance(reader.getValueClass(), conf);\n\n            long position = reader.getPosition();\n            System.out.println(position);\n\n            while (reader.next(key, value)) {\n                String syncSeen = reader.syncSeen() ? \"*\" : \"\";\n                System.out.printf(\"[%s%s]\\t%s\\t%s\\n\", position, syncSeen, key, value);\n                position = reader.getPosition(); // beginning of next record\n            }\n        } finally {\n            IOUtils.closeStream(reader);\n        }\n    }\n}\n```\n\n\n\n###  6. 文件快照\n\n####  6.1 什么是快照\n\n- 快照比较常见的应用场景是数据备份，以防一些用户错误或灾难恢复\n- 快照snapshots是HDFS文件系统的，只读的、某时间点的拷贝\n- 可以针对某个目录，或者整个文件系统做快照\n- 创建快照时，block块并不会被拷贝。快照文件中只是记录了block列表和文件大小，**不会做任何数据拷贝**\n\n####  6.2 快照操作\n\n- 允许快照\n\n  允许一个快照目录被创建。如果这个操作成功完成，这个目录就变成snapshottable\n\n  用法：hdfs dfsadmin -allowSnapshot <snapshotDir>\n\n  ```shell\n  hdfs dfsadmin -allowSnapshot /wordcount\n  ```\n\n- 禁用快照\n\n  用法：hdfs dfsadmin -disallowSnapshot <snapshotDir>\n\n  ```shell\n  hdfs dfsadmin -disallowSnapshot /wordcount\n  ```\n\n- 创建快照\n\n  用法：hdfs dfs -createSnapshot <snapshotDir> [<snapshotName>]\n\n  ```shell\n  #注意：先将/wordcount目录变成允许快照的\n  hdfs dfs -createSnapshot /wordcount wcSnapshot\n  ```\n\n- 查看快照\n\n  ```shell\n  hdfs dfs -ls /wordcount/.snapshot\n  \n  ```\n\n  ![](img/Image201909041346.png)\n\n- 重命名快照\n\n  这个操作需要拥有snapshottabl目录所有者权限\n\n  用法：hdfs dfs -renameSnapshot <snapshotDir> <oldName> <newName>\n\n  ```shell\n  hdfs dfs -renameSnapshot /wordcount wcSnapshot newWCSnapshot\n  \n  ```\n\n- 用快照恢复误删除数据\n\n  HFDS的/wordcount目录，文件列表如下\n\n  ![](img/Image201909041356.png)\n\n  误删除/wordcount/edit.xml文件\n\n  ```shell\n  hadoop fs -rm /wordcount/edit.xml\n  \n  ```\n\n  ![](img/Image201909041400.png)\n\n  恢复数据\n\n  ```shell\n  hadoop fs -cp /wordcount/.snapshot/newWCSnapshot/edit.xml /wordcount\n  \n  ```\n\n- 删除快照\n\n  这个操作需要拥有snapshottabl目录所有者权限\n\n  用法：hdfs dfs -deleteSnapshot <snapshotDir> <snapshotName>\n\n  ```shell\n  hdfs dfs -deleteSnapshot /wordcount newWCSnapshot\n  \n  ```\n\n\n\n\n\n\n##  7、拓展点、未来计划、行业趋势\n\n1. HDFS存储地位\n\n2. **block块为什么设置的比较大**\n\n- [磁盘基础知识](<https://www.cnblogs.com/jswang/p/9071847.html>)\t\n\n  - 盘片platter、磁头head、磁道track、扇区sector、柱面cylinder\n  - 为了最小化寻址开销；从磁盘传输数据的时间明显大于定位这个块开始位置所需的时间\n\n- 问：块的大小是不是设置的越大越好呢？\n\n  1、 不是，寻址的时间大概是 100ms，设计一般设置为寻址时间占用十分之一，也就是一秒。 硬盘的传输速录大概是100m/s 一秒大概为100M，最接近100的大小为128M。 \n\n![](img/Image201906211143.png)\n","tags":["hadoop","hdfs"]},{"title":"数据结构与算法（一）","url":"/2018/11/18/it/datastructure/数据结构与算法（一）/","content":"\n## 1. 数据结构基础概念\n\n### 1.1 数据结构定义\n- 数据结构是指相互之间存在着一种或多种关系的数据元素的集合和该集合中数据元素之间的关系组成。记为:Data_Structure=(D,R) \n  - D： 是数据元素的集合\n  - R： 是该集合中所有元素之间的关系的有限集合\n\n-  广义:：数据结构是指数据的存储结构 \n\n- 狭义：在使用数据的过程中，定义的各种特性及限制的抽象出的逻辑结。\n\n  - 数据结构服务于算法，算法依赖于数据结构。\n\n  - 数据结构是静态，是定理，而算法依赖于其相关计算，才构成了我们绚烂多 彩的程序帝国 。\n\n### 1.2 数据结构的组成\n\n<img src=\"assets/image-20191118232701597.png\" alt=\"image-20191118232701597\" style=\"zoom: 40%;\" />\n\n#### 1.2.1 存储结构\n\n- 存储结构:又名物理结构，是数据存储到计算机上的真实结构。 \n- 包含顺序/链式/哈希/索引等 \n\n<img src=\"assets/image-20191118232932344.png\" alt=\"image-20191118232932344\" style=\"zoom: 50%;\" />\n\n#### 1.2.2 逻辑结构\n\n- 逻辑结构: 逻辑结构是数据组合的规则与关系的抽象描 述，一般我们讨论的数据结构通常是指逻辑结构。 \n- 包含树/图等非线性结构与数组/队列/串等线性结构。 \n\n#### 1.2.3  数据算法\n\n- 算法: 算法是针对逻辑结构一系列操作的方法。 \n- 算法在考虑性能的时候，受到存储结构的制约。\n- 核心理念：用最省力最省空间的方法，去促成数据呈现的结果。\n\n### 1.3 算法的复杂度分析\n","tags":["data structure"]},{"title":"大数据概论-HDFS理论基础","url":"/2018/10/10/it/hadoop/Java大数据基础概论/","content":"## 大数据概论\n\n> 概念： 大数据（big data）是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产\n\n| 数据单位 | B    | KB   | MB   | GB   | PE   | PB   | EB   | ZB   | YB   |\n| -------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| 基数     |      | 2    | 2    | 2    | 2    | 2    | 2    | 10   | 10   |\n| 次方     | 0    | 10   | 20   | 30   | 40   | 50   | 60   | 21   | 24   |\n\n### 一、大数据特性\n\n1. 数据量大（Volume） \n2. 类型繁多（Variety） \n3. 价值密度低（Value） \n4. 速度快时效高（Velocity）\n\n### 二、大数据的挑战\n\n1. 存储： 每天几TB、GB的数据增量，并且还在持续的增长中。\n2. 分析： 如何从巨大的数据中挖掘出隐藏的商业价值。\n3. 管理： 如何快速构建并且保证系统的安全、简单可用。\n\n### 三、传统的大数据项目流程\n\n```flow\nst=>start: 开始\ndataCollect=>operation: 数据收集 ： Flume、Kafaka、Scribe\ndataStore=>operation: 数据存储 ： HDFS、HBase、Cassadra\ndataCaculate=>operation: 数据计算 : Mapreduce、Strom、Impala、Spark、Spark Streaming...\n数据计算三大类：\n1、离线处理平台： Spark、Spark Core\n2、交互式处理平台： Spark SQL、Hive 、Impala\n3、流处理平台 ： Strom、Spring Stoeaming 、 Flink\ndataAnalyse=>operation: 分析与挖掘 ： Mahour、R语言、Hive、Pig\ndataEtl=>operation: ETL ： sqoop、DataX\ndataView=>operation: 可视化 ： Echarts.js 、 E3.js、 数据报表系统\ndataActual=>operation: 项目实战\ne=>end: 结束\n\nst->dataCollect->dataStore->dataCaculate->dataAnalyse->dataEtl->dataView->dataActual->e\n\n```\n\n## 分布式文件系统\n\n### 一、Hadoop简介\n\n1. Hadoop架构\n\n   ![Image201906191834](img/Image201906191834.png)\n\n2. Hadoop历史\n\n   ![Image201906202055](img/Image201906202055.png)\n\n### 二、HDFS\n\n- HDFS是Hadoop中的一个存储子模块\n- HDFS (全称Hadoop Distributed File System)，即hadoop的分布式文件系统\n- File System**文件系统**：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读出、修改、转储、删除文件等\n- 当数据集大小超出一台计算机的存储能力时，就有必要将它拆分成若干部分，然后分散到不同的计算机中存储。管理网络中跨多台计算机存储的文件系统称之为**分布式文件系统**（distributed filesystem）\n\n#### 2.1 HDFS特点\n\n**2.1.1 优点：**\n\n- 适合存储大文件，能用来存储管理PB级的数据；不适合存储小文件\n- 处理非结构化数据\n- 流式的访问数据，一次写入、多次读写\n- 运行于廉价的商用机器集群上，成本低\n- 高容错：故障时能继续运行且不让用户察觉到明显的中断\n- 可扩展\n\n![](img/Image201907081216.png)\n\n**2.1.2 局限性**\n\n- 不适合处理低延迟数据访问\n  - HDFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的\n  - 对于低延时的访问需求，HBase是更好的选择\n- 无法高效存储大量的小文件\n  - 小文件会给Hadoop的扩展性和性能带来严重问题\n  - 利用SequenceFile、MapFile等方式归档小文件\n- 不支持多用户写入及任意修改文件\n  - 文件有一个写入者，只能执行追加操作\n  - 不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改\n\n#### 2.2 HDFS常用命令\n\n> HDFS两种命令风格，两种命令效果等同\n>\n> hadoop fs / hdfs dfs\n\n![image-20191010155353956](/Users/dingchuangshi/Library/Application Support/typora-user-images/image-20191010155353956.png)\n\n\n\n1. 如何查看hdfs或hadoop子命令的**帮助信息**，如ls子命令\n\n   ```shell\n   hdfs dfs -help ls\n   hadoop fs -help ls\t#两个命令等价\n   ```\n\n2. **查看**hdfs文件系统中已经存在的文件。对比linux命令ls\n\n   ```shell\n   hdfs dfs -ls /\n   hadoop fs -ls /\n   ```\n\n3. 在hdfs文件系统中创建文件\n\n   ```shell\n   hdfs dfs -touchz /edits.txt\n   ```\n\n4. 向HDFS文件中追加内容\n\n    ```shell\n    hadoop fs -appendToFile edit1.xml /edits.txt #将本地磁盘当前目录的edit1.xml内容追加到HDFS根目录 的edits.txt文件\n    ```\n\n5. 查看HDFS文件内容\n\n    ```shell\n    hdfs dfs -cat /edits.txt\n    ```\n\n6. **从本地路径上传文件至HDFS**\n\n    ````` shell\n    #用法：hdfs dfs -put /本地路径 /hdfs路径\n    hdfs dfs -put hadoop-2.7.3.tar.gz /\n    hdfs dfs -copyFromLocal hadoop-2.7.3.tar.gz /  #根put作用一样\n    hdfs dfs -moveFromLocal hadoop-2.7.3.tar.gz /  #根put作用一样，只不过，源文件被拷贝成功后，会被删除\n    `````\n\n7. **在hdfs文件系统中下载文件**\n\n     ```shell\n     hdfs dfs -get /hdfs路径 /本地路径\n     hdfs dfs -copyToLocal /hdfs路径 /本地路径  #根get作用一样\n     ```\n\n8. 在hdfs文件系统中**创建目录**\n\n     ```shell\n     hdfs dfs -mkdir /shell\n     ```\n\n9. 在hdfs文件系统中**删除**文件\n\n     ```shell\n     hdfs dfs -rm /edits.txt\n     hdfs dfs -rm -r /shell\n     ```\n\n10. 在hdfs文件系统中**修改文件名称**（也可以用来**移动**文件到目录）\n\n     ```shell\n     hdfs dfs -mv /xcall.sh /call.sh\n     hdfs dfs -mv /call.sh /shell\n     ```\n\n11. 在hdfs中拷贝文件到目录\n\n      ```shell\n      hdfs dfs -cp /xrsync.sh /shell\n      ```\n\n12. 递归删除目录\n\n      ```shell\n      hdfs dfs -rmr /shell\n      ```\n\n13. 列出本地文件的内容（默认是hdfs文件系统）\n\n      ```shell\n      hdfs dfs -ls file:///home/bruce/\n      ```\n\n14. 查找文件\n\n      ```shell\n      # linux find命令\n      find . -name 'edit*'\n      \n      # HDFS find命令\n      hadoop fs -find / -name part-r-00000 # 在HDFS根目录中，查找part-r-00000文件\n      ```\n\n\n> 还有许多其他命令，大家可以自己探索一下   \n\n##### 2.2.1 命令行小结\n\n- 输入hadoop fs 或hdfs dfs，回车，查看所有的HDFS命令\n\n- 许多命令与linux命令有很大的相似性，学会举一反三\n- 有用的help，如查看ls命令的使用说明：hadoop fs -help ls\n\n##### 2.2.2 hdfs与getconf结合使用\n\n1. 获取NameNode的节点名称（可能有多个）\n\n      ``````shell\n      hdfs getconf -namenodes\n      ``````\n\n2. 获取hdfs最小块信息\n\n      ``````shell\n      hdfs getconf -confKey dfs.namenode.fs-limits.min-block-size\n      ``````\n\n3. 查找hdfs的NameNode的RPC地址\n\n\t``````shell\n\thdfs getconf -nnRpcAddresses\n\t``````\n\t\n\t\n\n##### 2.2.3 hdfs与dfsadmin结合使用\n\n1. 同样要学会借助帮助信息\n\n      ```shell\n      hdfs dfsadmin -help safemode\n      ```\n\n2. 查看hdfs dfsadmin的帮助信息\n\n      ``````shell\n      hdfs dfsadmin\n      ``````\n\n3. 查看当前的模式\n\n      ``````shell\n      hdfs dfsadmin -safemode get\n      ``````\n\n4. 进入安全模式\n\n  ``````shell\n  hdfs dfsadmin -safemode enter\n  ``````\n\n  \n\n##### 2.2.4 hdfs与fsck结合使用\n\n1. fsck指令**显示HDFS块信息**\n\n\t``````shell\n\thdfs fsck /02-041-0029.mp4 -files -blocks -locations # 查看文件02-041-0029.mp4的块信息\n\t``````\n\t\n\t\n\n##### 2.2.5 其他命令\n\n1. 检查压缩库本地安装情况\n\n      ``````shell\n      hadoop checknative\n      ``````\n\n2. 格式化名称节点（**慎用**，一般只在初次搭建集群，使用一次；格式化成功后，不要再使用）\n\n      ``````shell\n      hadoop namenode -format\n      ``````\n\n3. 执行自定义jar包\n\n   ``````shell\n   hadoop jar YinzhengjieMapReduce-1.0-SNAPSHOT.jar com.kaikeba.hadoop.WordCount /world.txt /out\n   ``````\n\n#### 2.3 HDFS编程\n\n\n- 1.向hdfs中,上传一个文本文件\n\n  ```java\n   /**\n       * 上传文件到服务器\n       *  传递参数\n       *  args[0] 本地文件路径\n       *  args[1] hdoop文件系统 路径\n       */\n      public static void uploadFileToFileSystem(String source,String targetUrl){\n          System.out.println(\"文件地址：\" + source);\n          System.out.println(\"目标服务器：\" + targetUrl);\n          InputStream inputStreamSourceFile = null;\n          try {\n              // 获取文件输入流\n              inputStreamSourceFile = new BufferedInputStream(new FileInputStream(source));\n              // HDFS 读写配置文件\n              Configuration configuration = new Configuration();\n              // 通过url 返回文件系统实例\n              FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);\n              //调用Filesystem的create方法返回的是FSDataOutputStream对象\n              //该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n              // 获取文件系用的输出流\n              OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n              // 将文件输入流，写入输入流\n              IOUtils.copyBytes(inputStreamSourceFile,outputStreamTarget,4069,true);\n              System.out.println(\"上传成功\");\n          } catch (FileNotFoundException e) {\n              System.err.println(e.getMessage());\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n      }\n  \n  ```\n\n- 2.读取hdfs上的文件\n\n```java\n\n    /**\n     * 从文件系统中读取文件\n     * @param source 需要读取的文件\n     * @return 读取文件内容\n     */\n    public static String readFileFromFileSystem(String source){\n        String result = null;\n        try {\n            // HDFS 读写文件配置\n            Configuration configuration = new Configuration();\n            // HDFS文件系统\n            FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n            // 文件输入流，用于读取文件\n            InputStream inputStream = fileSystem.open(new Path(source));\n            BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream));\n            result = readBufferReader(bufferedReader).toString();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return result;\n    }\n    \n    \n    /**\n     * 获取内容\n     * @param br\n     * @return\n     */\n    private static StringBuffer readBufferReader(BufferedReader br) throws IOException {\n        StringBuffer stringBuffer = new StringBuffer();\n        String temp = \"\";\n        while ((temp = br.readLine()) != null){\n            stringBuffer.append(temp);\n        }\n        return stringBuffer;\n    }\n```\n\n\n\n- 3.列出某一个文件夹下的所有文件\n\n```java\n\n    /**\n     * 列出当前目录下所有字文件名称\n     * @param source\n     * @return\n     */\n    public static String listAllFileChildren(String source){\n        StringBuffer stringBuffer = new StringBuffer();\n        try {\n            // HDFS 读写文件配置\n            Configuration configuration = new Configuration();\n            // HDFS文件系统\n            FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n            // recursive 继续深入遍历\n            RemoteIterator<LocatedFileStatus> iterator = fileSystem.listFiles(new Path(source),true);\n            while (iterator.hasNext()){\n                LocatedFileStatus fileStatus = iterator.next();\n                stringBuffer.append(fileStatus.getPath() + \"\\n\");\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return stringBuffer.toString();\n    }\n\n```\n\n\n\n- 4.列出多级目录名称和目录下的文件名称\n\n  ```java\n  \n      /**\n       * 递归列出当前目录下所有目录和文件名称\n       * @param source\n       * @return\n       */\n      public static String listAllChildren(String source){\n          StringBuffer stringBuffer = new StringBuffer();\n          try {\n              // HDFS 读写文件配置\n              Configuration configuration = new Configuration();\n              // HDFS文件系统\n              FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n              list(stringBuffer,fileSystem,new Path(source));\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n          return stringBuffer.toString();\n      }\n  \n      /**\n       * 递归目录和文件\n       * @param stringBuffer  文件目录名称集合\n       * @param fileSystem  hdfs 文件系统\n       * @param source path 路径\n       * @throws IOException\n       */\n      private static void list(StringBuffer stringBuffer,FileSystem fileSystem, Path source) throws IOException {\n          FileStatus[] iterator = fileSystem.listStatus(source);\n          for (FileStatus status:iterator) {\n              stringBuffer.append(status.getPath() + \"\\n\");\n              if(status.isDirectory()){\n                  list(stringBuffer,fileSystem,status.getPath());\n              }\n  \n          }\n      }\n  \n  \n  ```\n\n  \n\n#### 2.4  HDFS架构\n\n![](img/1558073557041.png)\n\n- 大多数分布式框架都是主从架构\n- HDFS也是主从架构Master|Slave或称为管理节点|工作节点\n\n##### 1、NameNode\n\n**1.1 文件系统**\n\n- file system文件系统：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读取、修改、转储、删除文件等\n- 读文件 =>>找到文件 =>> 在哪 + 叫啥？\n- 元数据\n  - 关于文件或目录的描述信息，如文件所在路径、文件名称、文件类型等等，这些信息称为文件的元数据metadata\n- 命名空间\n  - 文件系统中，为了便于管理存储介质上的，给每个目录、目录中的文件、子目录都起了名字，这样形成的层级结构，称之为命名空间\n  - 同一个目录中，不能有同名的文件或目录\n  - 这样通过目录+文件名称的方式能够唯一的定位一个文件\n\n![](img/Image201906211418.png)\n\n**5.1.2 HDFS-NameNode**\n\n- HDFS本质上也是文件系统filesystem，所以它也有元数据metadata；\n- 元数据metadata保存在NameNode**内存**中\n- NameNode作用\n  - HDFS的主节点，负责管理文件系统的命名空间，将HDFS的元数据存储在NameNode节点的内存中\n  - 负责响应客户端对文件的读写请求\n- HDFS元数据\n  - 文件目录树、所有的文件（目录）名称、文件属性（生成时间、副本、权限）、每个文件的块列表、每个block块所在的datanode列表\n\n![](img/Image201909031504.png)\n\n  - 每个文件、目录、block占用大概**150Byte字节的元数据**；所以HDFS适合存储大文件，不适合存储小文件\n\n  - HDFS元数据信息以两种形式保存：①编辑日志**edits log**②命名空间镜像文件**fsimage**\n    - edits log：HDFS编辑日志文件 ，保存客户端对HDFS的所有更改记录，如增、删、重命名文件（目录），这些操作会修改HDFS目录树；\u0010NameNode会在编辑日志edit日志中记录下来；\n    - fsimage：HDFS元数据镜像文件 ，即将namenode内存中的数据落入磁盘生成的文件；保存了文件系统目录树信息以及文件、块、datanode的映射关系，如下图\n\n\n![](img/Image201910091133.png)\n\n> 说明：\n>\n> ①为hdfs-site.xml中属性dfs.namenode.edits.dir的值决定；用于namenode保存edits.log文件\n>\n> ②为hdfs-site.xml中属性dfs.namenode.name.dir的值决定；用于namenode保存fsimage文件\n\n##### 2、DataNode\n\n- DataNode数据节点的作用\n  - 存储block以及block元数据到datanode本地磁盘；此处的元数据包括数据块的长度、块数据的校验和、时间戳\n\n##### 3 SeconddaryNameNode   \n\n- 为什么引入SecondaryNameNode\n\n  - 为什么元数据存储在NameNode在内存中？\n\n  - 这样做有什么问题？如何解决？\n\n  - HDFS编辑日志文件 editlog：在NameNode节点中的编辑日志editlog中，记录下来客户端对HDFS的所有更改的记录，如增、删、重命名文件（目录）；\n\n  - 作用：一旦系统出故障，可以从editlog进行恢复；\n\n  - 但editlog日志大小会随着时间变在越来越大，导致系统重启根据日志恢复的时候会越来越长；\n\n  - 为了避免这种情况，引入**检查点机制checkpoint**，命名空间镜像fsimage就是HDFS元数据的持久性检查点，即将内存中的元数据落磁盘生成的文件；\n\n  - 此时，namenode如果重启，可以将磁盘中的fsimage文件读入内容，将元数据恢复到某一个检查点，然后再执行检查点之后记录的编辑日志，最后完全恢复元数据。\n\n  - 但是依然，随着时间的推移，editlog记录的日志会变多，那么当namenode重启，恢复元数据过程中，会花越来越长的时间执行editlog中的每一个日志；而在namenode元数据恢复期间，HDFS不可用。\n\n  - 为了解决此问题，引入secondarynamenode辅助namenode，用来合并fsimage及editlog\n\n\n\n![](img/Image201906211525.png)\n\n- SecondaryNameNode定期做checkpoint检查点操作\n\n  - 创建检查点checkpoint的两大条件：\n    - SecondaryNameNode每隔1小时创建一个检查点\n    - 另外，Secondary NameNode每1分钟检查一次，从上一检查点开始，edits日志文件中是否已包括100万个事务，如果是，也会创建检查点\n  - Secondary NameNode首先请求原NameNode进行edits的滚动，这样新的编辑操作就能够进入新的文件中\n  - Secondary NameNode通过HTTP GET方式读取原NameNode中的fsimage及edits\n  - Secondary NameNode读取fsimage到内存中，然后执行edits中的每个操作，并创建一个新的统一的fsimage文件\n  - Secondary NameNode通过HTTP PUT方式将新的fsimage发送到原NameNode\n  - 原NameNode用新的fsimage替换旧的fsimage，同时系统会更新fsimage文件到记录检查点的时间。 \n  - 这个过程结束后，NameNode就有了最新的fsimage文件和更小的edits文件\n\n- SecondaryNameNode一般部署在另外一台节点上\n\n  - 因为它需要占用大量的CPU时间\n  - 并需要与namenode一样多的内存，来执行合并操作\n\n- 如何查看edits日志文件\n\n  ```shell\n  hdfs oev -i edits_0000000000000000256-0000000000000000363 -o /home/hadoop/edit1.xml\n  ```\n\n- 如何查看fsimage文件\n\n  ```shell\n  hdfs oiv -p XML -i fsimage_0000000000000092691 -o fsimage.xml  \n  ```\n\n- checkpoint相关属性\n\n  ```properties\n  # 3600秒(即1小时) 每隔1小时创建一个检查点\n  #The number of seconds between two periodic checkpoints\ndfs.namenode.checkpoint.period = 3600\n  \n  # edits日志文件中是否已包括100万个事务，如果是，也会创建检查点\n  # The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless of whether 'dfs.namenode.checkpoint.period' has expired.\n  dfs.namenode.checkpoint.txns = 1000000 \n  \n  # 60(1分钟)  SecondaryNameNode每1分钟检查一次\n  #  The SecondaryNameNode and CheckpointNode will poll the NameNode every 'dfs.namenode.checkpoint.check.period' seconds to query the number of uncheckpointed transactions.\n  dfs.namenode.checkpoint.check.period = 60\n  ```\n  \n  \n\n##### 4 心跳机制\n\n![](img/Image201906211518.png)\n\n**工作原理：**\n\n1. NameNode启动的时候，会开一个ipc server在那里\n2. DataNode启动后向NameNode注册，每隔**3秒钟**向NameNode发送一个“**心跳heartbeat**”\n3. 心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一DataNode，或删除某个数据块\n4. 如果超过**10分钟**NameNode没有收到某个DataNode 的心跳，则认为该DataNode节点不可用\n5. DataNode周期性（**6小时**）的向NameNode上报当前DataNode上的块状态报告BlockReport；块状态报告包含了一个该 Datanode上所有数据块的列表\n\n**心跳的作用：**\n\n1. 通过周期心跳，NameNode可以向DataNode返回指令\n\n2. 可以判断DataNode是否在线\n\n3. 通过BlockReport，NameNode能够知道各DataNode的存储情况，如磁盘利用率、块列表；跟**负载均衡**有关\n\n4. **hadoop集群刚开始启动时，99.9%的block没有达到最小副本数(dfs.namenode.replication.min默认值为1)，集群处于安全模式，涉及BlockReport；**\n\n**心跳相关配置**\n\n- [hdfs-default.xml](<https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml>)\n- 心跳间隔\n\n| 属性                   | 值   | 解释                                               |\n| ---------------------- | ---- | -------------------------------------------------- |\n| dfs.heartbeat.interval | 3    | Determines datanode heartbeat interval in seconds. |\n\n- **block report**\n\n| More Actions属性             | 值               | 解释                                                 |\n| ---------------------------- | ---------------- | ---------------------------------------------------- |\n| dfs.blockreport.intervalMsec | 21600000 (6小时) | Determines block reporting interval in milliseconds. |\n\n- 查看hdfs-default.xml默认配置文件\n\n![](img/Image201907311730.png)\n\n##### 5 负载均衡\n\n- 什么原因会有可能造成不均衡？\n  - 机器与机器之间磁盘利用率不平衡是HDFS集群非常容易出现的情况\n  - 尤其是在DataNode节点出现故障或在现有的集群上增添新的DataNode的时候\n\n- 为什么需要均衡？\n  - 提升集群存储资源利用率\n  - 从存储与计算两方面提高集群性能\n\n- 如何手动负载均衡？\n\n```shell\n$HADOOP_HOME/sbin/start-balancer.sh -t 5%\t# 磁盘利用率最高的节点若比最少的节点，大于5%，触发均衡\n```\n\n##### 6 小结\n\n- NameNode负责存储元数据，存在内存中\n- DataNode负责存储block块及块的元数据\n- SecondaryNameNode主要负责对HDFS元数据做checkpoint操作\n- 集群的心跳机制，让集群中各节点形成一个整体；主节点知道从节点的死活\n- 节点的上下线，导致存储的不均衡，可以手动触发负载均衡\n","tags":["hadoop"]}]