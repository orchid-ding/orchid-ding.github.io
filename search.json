[{"url":"/2019/10/21/Java/ja/","content":"## s#adada\n"},{"title":"Hadoop架构原理之Yarn","url":"/2019/10/21/hadoop/Hadoop架构原理之Yarn/","content":"\n# Hadoop架构原理之Yarn\n\n\n\n## 1. YARN介绍\n\n![img](/Users/dingchuangshi/Downloads/20191021-YARN/assets/a19a61bc-9378-3e38-944a-899a09f37908.jpg)\n\n- Apache Hadoop YARN(Yet Another Resource Negotiator)是Hadoop的子项目，为分离Hadoop2.0资源管理和计算组件而引入\n- YRAN具有足够的通用性，可以支持其它的分布式计算模式\n\n![img](/Users/dingchuangshi/Downloads/20191021-YARN/assets/99b59921-9a97-3199-8c39-d3b77dfdceaf.jpg)\n\n\n\n## 2. YARN架构\n\n- 类似HDFS，YARN也是经典的**主从（master/slave）架构**\n  - YARN服务由一个ResourceManager（RM）和多个NodeManager（NM）构成\n  - ResourceManager为主节点（master）\n  - NodeManager为从节点（slave）\n\n![yarn的体系结构](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Figure3Architecture-of-YARN.png)\n\n\n\n\n\n- ApplicationMaster可以在容器内运行任何类型的任务。例如，MapReduce ApplicationMaster请求容器启动map或reduce任务，而Giraph ApplicationMaster请求容器运行Giraph任务。\n\n| 组件名                 | 作用                                                         |\n| :--------------------- | ------------------------------------------------------------ |\n| **ApplicationManager** | 相当于这个Application的监护人和管理者，负责监控、管理这个Application的所有Attempt在cluster中各个节点上的具体运行，同时负责向Yarn ResourceManager申请资源、返还资源等； |\n| **NodeManager**        | 是Slave上一个独立运行的进程，负责上报节点的状态(磁盘，内存，cpu等使用信息)； |\n| **Container**          | 是yarn中分配资源的一个单位，包涵内存、CPU等等资源，YARN以Container为单位分配资源； |\n\nResourceManager 负责对各个 NodeManager 上资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的 ApplicationMaster，它负责向 ResourceManager 申请资源，并要求 NodeManger 启动可以占用一定资源的任务。由于不同的 ApplicationMaster 被分布到不同的节点上，因此它们之间不会相互影响。\n\nClient 向 ResourceManager 提交的每一个应用程序都必须有一个 ApplicationMaster，它经过 ResourceManager 分配资源后，运行于某一个 Slave 节点的 Container 中，具体做事情的 Task，同样也运行与某一个 Slave 节点的 Container 中。\n\n### 2.1 **ResourceManager**\n\n- RM是一个全局的资源管理器，集群只有一个\n  - 负责整个系统的资源管理和分配\n  - 包括处理客户端请求\n  - 启动/监控 ApplicationMaster\n  - 监控 NodeManager、资源的分配与调度\n- 它主要由两个组件构成：\n  - 调度器（Scheduler）\n  - 应用程序管理器（Applications Manager，ASM）\n\n- 调度器\n  - 调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。\n  - 需要注意的是，该调度器是一个“纯调度器”\n    - 它不从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。\n    - 调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。\n\n- 应用程序管理器\n  - 应用程序管理器主要负责管理整个系统中所有应用程序\n  - 接收job的提交请求\n  - 为应用分配第一个 Container 来运行 ApplicationMaster，包括应用程序提交、与调度器协商资源以启动 ApplicationMaster、监控 ApplicationMaster 运行状态并在失败时重新启动它等\n\n### 2.2 **NodeManager**\n\n![nodemanager架构](/Users/dingchuangshi/Downloads/20191021-YARN/assets/20190103113256851.png)\n\n- NodeManager 是一个 slave 服务，整个集群有多个\n\n- NodeManager ：\n  - 它负责接收 ResourceManager 的资源分配请求，分配具体的 Container 给应用。\n  - 负责监控并报告 Container 使用信息给 ResourceManager。\n\n- 功能：\n\n  - NodeManager 本节点上的资源使用情况和各个 Container 的运行状态（cpu和内存等资源）\n  - 接收及处理来自 ResourceManager 的命令请求，分配 Container 给应用的某个任务；\n  - 定时地向RM汇报以确保整个集群平稳运行，RM 通过收集每个 NodeManager 的报告信息来追踪整个集群健康状态的，而 NodeManager 负责监控自身的健康状态；\n  - 处理来自 ApplicationMaster 的请求；\n  - 管理着所在节点每个 Container 的生命周期；\n  - 管理每个节点上的日志；\n\n  - 当一个节点启动时，它会向 ResourceManager 进行注册并告知 ResourceManager 自己有多少资源可用。\n  - 在运行期，通过 NodeManager 和 ResourceManager 协同工作，这些信息会不断被更新并保障整个集群发挥出最佳状态。\n\n  - NodeManager 只负责管理自身的 Container，它并不知道运行在它上面应用的信息。负责管理应用信息的组件是 ApplicationMaster\n\n### 2.3 Container\n\n- Container 是 YARN 中的资源抽象\n  - 它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等\n  - 当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。\n  - YARN 会为每个任务分配一个 Container，且该任务只能使用该 Container 中描述的资源。\n\n- Container 和集群NodeManager节点的关系是：\n  - 一个NodeManager节点可运行多个 Container\n  - 但一个 Container 不会跨节点。\n  - 任何一个 job 或 application 必须运行在一个或多个 Container 中\n  - 在 Yarn 框架中，ResourceManager 只负责告诉 ApplicationMaster 哪些 Containers 可以用\n  - ApplicationMaster 还需要去找 NodeManager 请求分配具体的 Container。\n\n- 需要注意的是\n  - Container 是一个动态资源划分单位，是根据应用程序的需求动态生成的\n  - 目前为止，YARN 仅支持 CPU 和内存两种资源，且使用了轻量级资源隔离机制 Cgroups 进行资源隔离。\n\n- 功能：\n  - 对task环境的抽象；\n\n  - 描述一系列信息；\n\n  - 任务运行资源的集合（cpu、内存、io等）；\n\n  - 任务运行环境\n\n### 2.4 **ApplicationMaster**\n\n- 功能：\n  - 数据切分；\n  - 为应用程序申请资源并进一步分配给内部任务（TASK）；\n  - 任务监控与容错；\n  - 负责协调来自ResourceManager的资源，并通过NodeManager监视容器的执行和资源使用情况。\n\n- ApplicationMaster 与 ResourceManager 之间的通信\n  - 是整个 Yarn 应用从提交到运行的最核心部分，是 Yarn 对整个集群进行动态资源管理的根本步骤\n  - Yarn 的动态性，就是来源于多个Application 的 ApplicationMaster 动态地和 ResourceManager 进行沟通，不断地申请、释放、再申请、再释放资源的过程。\n\n### 2.5 Resource Request\n\n[引用连接](https://www.jianshu.com/p/f50e85bdb9ce)\n\n- Yarn的设计目标\n  - 允许我们的各种应用以共享、安全、多租户的形式使用整个集群。\n  - 并且，为了保证集群资源调度和数据访问的高效性，Yarn还必须能够感知整个集群拓扑结构。\n\n- 为了实现这些目标，ResourceManager的调度器Scheduler为应用程序的资源请求定义了一些灵活的协议，**Resource Request**和**Container**。\n  - 一个应用先向ApplicationMaster发送一个满足自己需求的资源请求\n  - 然后ApplicationMaster把这个资源请求以resource-request的形式发送给ResourceManager的Scheduler\n  - Scheduler再在这个原始的resource-request中返回分配到的资源描述Container。\n\n- 每个ResourceRequest可看做一个可序列化Java对象，包含的字段信息如下：\n\n```xml\n<!--\n- resource-name：资源名称，现阶段指的是资源所在的host和rack，后期可能还会支持虚拟机或者更复杂的网络结构\n- priority：资源的优先级\n- resource-requirement：资源的具体需求，现阶段指内存和cpu需求的数量\n- number-of-containers：满足需求的Container的集合\n-->\n<resource-name, priority, resource-requirement, number-of-containers>\n```\n\n### 2.6 JobHistoryServer \n\n- 作业历史服务\n\n  - 记录在yarn中调度的作业历史运行情况情况 ,\n\n  - 通过命令启动\n\n    ```shell\n    mr-jobhistory-daemon.sh start historyserver\n    ```\n\n  - 在集群中的数据节点机器上单独使用命令启动直接启动即可,\n\n  - 启动成功后会出现JobHistoryServer进程(使用jps命令查看，下面会有介绍) ,\n\n  - 并且可以从19888端口进行查看日志详细信息\n\n    ```\n    node01:19888\n    ```\n\n    点击链接，查看job日志\n\n    ![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201910202320.png)\n\n- 如果没有启动jobhistoryserver，无法查看应用的日志\n\n![1563002086000](/Users/dingchuangshi/Downloads/20191021-YARN/assets/1563002086000.png)\n\n- 打开如下图界面，在下图中点击History，页面会进行一次跳转\n\n![1563002731472](/Users/dingchuangshi/Downloads/20191021-YARN/assets/1563002731472.png)\n\n- 点击History之后 跳转后的页面如下图是空白的，因为没有启动jobhistoryserver\n\n![1563002773601](/Users/dingchuangshi/Downloads/20191021-YARN/assets/1563002773601.png)\n\n- jobhistoryserver启动后，在此运行MR程序，如wordcount\n\n![1563004024903](/Users/dingchuangshi/Downloads/20191021-YARN/assets/1563004024903.png)\n\n- 点击History连接，跳转一个赞新的页面\n  - TaskType中列举的map和reduce，Total表示此次运行的mapreduce程序执行所需要的map和reduce的任务数\n\n![1563004057197](/Users/dingchuangshi/Downloads/20191021-YARN/assets/1563004057197.png)\n\n- 点击TaskType列中Map连接\n\n![1563004490476](/Users/dingchuangshi/Downloads/20191021-YARN/assets/1563004490476.png)\n\n- 看到map任务的相关信息比如执行状态,启动时间，完成时间。\n\n![1563004598290](/Users/dingchuangshi/Downloads/20191021-YARN/assets/1563004598290.png)\n\n- 可以使用同样的方式我们查看reduce任务执行的详细信息，这里不再赘述.\n\n- jobhistoryserver就是进行作业运行过程中历史运行信息的记录，方便我们对作业进行分析.\n\n### 2.7 Timeline Server \n\n- 用来写日志服务数据 , 一般来写与第三方结合的日志服务数据(比如spark等)\n- 它是对jobhistoryserver功能的有效补充，jobhistoryserver只能对mapreduce类型的作业信息进行记录\n- 它记录除了jobhistoryserver能够进行对作业运行过程中信息进行记录之外\n- 还记录更细粒度的信息，比如任务在哪个队列中运行，运行任务时设置的用户是哪个用户。\n\n- 根据官网的解释jobhistoryserver只能记录mapreduce应用程序的记录，timelineserver功能更强大,但不是替代jobhistory两者是功能间的互补关系.\n\n![1563006522419](/Users/dingchuangshi/Downloads/20191021-YARN/assets/1563006522419.png)\n\n- [官网教程](<http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/TimelineServer.html>)\n\n\n\n## 3. YARN应用运行原理\n\n![yarn架构图](/Users/dingchuangshi/Downloads/20191021-YARN/assets/yarn_architecture.gif)\n\n\n### 3.1 YARN应用提交过程\n\n- Application在Yarn中的执行过程，整个执行过程可以总结为三步：\n\n  - 应用程序提交\n  - 启动应用的ApplicationMaster实例\n  - ApplicationMaster 实例管理应用程序的执行\n\n- **具体提交过程为：**\n\n  ![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201909161351.png)\n\n  - 客户端程序向 ResourceManager 提交应用，并请求一个 ApplicationMaster 实例；\n  - ResourceManager 找到一个可以运行一个 Container 的 NodeManager，并在这个 Container 中启动 ApplicationMaster 实例；\n  - ApplicationMaster 向 ResourceManager 进行注册，注册之后客户端就可以查询 ResourceManager 获得自己 ApplicationMaster 的详细信息，以后就可以和自己的 ApplicationMaster 直接交互了（这个时候，客户端主动和 ApplicationMaster 交流，应用先向 ApplicationMaster 发送一个满足自己需求的资源请求）；\n  - ApplicationMaster 根据 resource-request协议 向 ResourceManager 发送 resource-request请求；\n  - 当 Container 被成功分配后，ApplicationMaster 通过向 NodeManager 发送 **container-launch-specification**信息 来启动Container，container-launch-specification信息包含了能够让Container 和 ApplicationMaster 交流所需要的资料；\n  - 应用程序的代码以 task 形式在启动的 Container 中运行，并把运行的进度、状态等信息通过 **application-specific**协议 发送给ApplicationMaster；\n  - 在应用程序运行期间，提交应用的客户端主动和 ApplicationMaster 交流获得应用的运行状态、进度更新等信息，交流协议也是 **application-specific**协议；\n  - 应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster 向 ResourceManager 取消注册然后关闭，用到所有的 Container 也归还给系统。\n\n- **精简版的：**\n\n  - 步骤1：用户将应用程序提交到 ResourceManager 上；\n  - 步骤2：ResourceManager为应用程序 ApplicationMaster 申请资源，并与某个 NodeManager 通信启动第一个 Container，以启动ApplicationMaster；\n  - 步骤3：ApplicationMaster 与 ResourceManager 注册进行通信，为内部要执行的任务申请资源，一旦得到资源后，将于 NodeManager 通信，以启动对应的 Task；\n  - 步骤4：所有任务运行完成后，ApplicationMaster 向 ResourceManager 注销，整个应用程序运行结束。\n\n### 3.2 MapReduce on YARN\n\n![img](/Users/dingchuangshi/Downloads/20191021-YARN/assets/820234-20160604233916133-2026396104.jpg)\n\n- 提交作业\n\n  - ①程序打成jar包，在客户端运行hadoop jar命令，提交job到集群运行\n  - job.waitForCompletion(true)中调用Job的submit()，此方法中调用JobSubmitter的submitJobInternal()方法；\n    - ②submitClient.getNewJobID()向resourcemanager请求一个MR作业id\n    - 检查输出目录：如果没有指定输出目录或者目录已经存在，则报错\n    - 计算作业分片；若无法计算分片，也会报错\n    - ③运行作业的相关资源，如作业的jar包、配置文件、输入分片，被上传到HDFS上一个以作业ID命名的目录（jar包副本默认为10，运行作业的任务，如map任务、reduce任务时，可从这10个副本读取jar包）\n    - ④调用resourcemanager的submitApplication()提交作业\n  - 客户端**每秒**查询一下作业的进度（map 50% reduce 0%），进度如有变化，则在控制台打印进度报告；\n  - 作业如果成功执行完成，则打印相关的计数器\n  - 但如果失败，在控制台打印导致作业失败的原因（要学会查看日志，定位问题，分析问题，解决问题）\n\n- **初始化作业**\n\n  - 当ResourceManager(一下简称RM)收到了submitApplication()方法的调用通知后，请求传递给RM的scheduler（调度器）；调度器分配container（容器）\n  - ⑤a RM与指定的NodeManager通信，通知NodeManager启动容器；NodeManager收到通知后，创建占据特定资源的container；\n  - ⑤b 然后在container中运行MRAppMaster进程\n  - ⑥MRAppMaster需要接受任务（各map任务、reduce任务的）的进度、完成报告，所以appMaster需要创建多个簿记对象，记录这些信息\n  - ⑦从HDFS获得client计算出的输入分片split\n    - 每个分片split创建一个map任务\n    - 通过 mapreduce.job.reduces 属性值(编程时，jog.setNumReduceTasks()指定)，知道当前MR要创建多少个reduce任务\n    - 每个任务(map、reduce)有task id\n\n- **Task 任务分配**\n\n  - 如果小作业，appMaster会以uberized的方式运行此MR作业；appMaster会决定在它的JVM中顺序此MR的任务；\n\n    - 原因是，若每个任务运行在一个单独的JVM时，都需要单独启动JVM，分配资源（内存、CPU），需要时间；多个JVM中的任务再在各自的JVM中并行运行\n\n    - 若将所有任务在appMaster的JVM中顺序执行的话，更高效，那么appMaster就会这么做 ，任务作为uber任务运行\n\n    - 小作业判断依据：①小于10个map任务；②只有一个reduce任务；③MR输入大小小于一个HDFS块大小\n\n    - 如何开启uber?设置属性 mapreduce.job.ubertask.enable 值为true\n\n      ```java\n      configuration.set(\"mapreduce.job.ubertask.enable\", \"true\");\n      ```\n\n    - 在运行任何task之前，appMaster调用setupJob()方法，创建OutputCommitter，创建作业的最终输出目录（一般为HDFS上的目录）及任务输出的临时目录（如map任务的中间结果输出目录）\n\n  - ⑧若作业不以uber任务方式运行，那么appMaster会为作业中的每一个任务（map任务、reduce任务）向RM请求container\n\n    - 由于reduce任务在进入排序阶段之前，所有的map任务必须执行完成；所以，为map任务申请容器要优先于为reduce任务申请容器\n    - 5%的map任务执行完成后，才开始为reduce任务申请容器\n    - 为map任务申请容器时，遵循数据本地化，调度器尽量将容器调度在map任务的输入分片所在的节点上（移动计算，不移动数据）\n\n    - reduce任务能在集群任意计算节点运行\n    - 默认情况下，为每个map任务、reduce任务分配1G内存、1个虚拟内核，由属性决定mapreduce.map.memory.mb、mapreduce.reduce.memory.mb、mapreduce.map.cpu.vcores、mapreduce.reduce.reduce.cpu.vcores\n\n- **Task 任务执行**\n\n  - 当调度器为当前任务分配了一个NodeManager（暂且称之为NM01）的容器，并将此信息传递给appMaster后；appMaster与NM01通信，告知NM01启动一个容器，并此容器占据特定的资源量（内存、CPU）\n  - NM01收到消息后，启动容器，此容器占据指定的资源量\n  - 容器中运行YarnChild，由YarnChild运行当前任务（map、reduce）\n  - ⑩在容器中运行任务之前，先将运行任务需要的资源拉取到本地，如作业的JAR文件、配置文件、分布式缓存中的文件\n\n- **作业运行进度与状态更新**\n\n  - 作业job以及它的每个task都有状态（running、successfully completed、failed），当前任务的运行进度、作业计数器\n  - 任务在运行期间，每隔3秒向appMaster汇报执行进度、状态（包括计数器）\n  - appMaster汇总目前运行的所有任务的上报的结果\n  - 客户端每个1秒，轮询访问appMaster获得作业执行的最新状态，若有改变，则在控制台打印出来\n\n- 完成作业\n\n  - appMaster收到最后一个任务完成的报告后，将作业状态设置为成功\n  - 客户端轮询appMaster查询进度时，发现作业执行成功，程序从waitForCompletion()退出\n  - 作业的所有统计信息打印在控制台\n  - appMaster及运行任务的容器，清理中间的输出结果\n  - 作业信息被历史服务器保存，留待以后用户查询\n\n  \n\n\n### 3.3 yarn应用生命周期\n\n- RM: Resource Manager\n- AM: Application Master\n- NM: Node Manager\n\n1. Client向RM提交应用，包括AM程序及启动AM的命令。\n\n2. RM为AM分配第一个容器，并与对应的NM通信，令其在容器上启动应用的AM。\n\n3. AM启动时向RM注册，允许Client向RM获取AM信息然后直接和AM通信。\n\n4. AM通过资源请求协议，为应用协商容器资源。\n\n5. 如容器分配成功，AM要求NM在容器中启动应用，应用启动后可以和AM独立通信。\n\n6. 应用程序在容器中执行，并向AM汇报。\n\n7. 在应用执行期间，Client和AM通信获取应用状态。\n\n8. 应用执行完成，AM向RM注销并关闭，释放资源。\n\n   **申请资源->启动appMaster->申请运行任务的container->分发Task->运行Task->Task结束->回收container**\n\n\n\n## 4. 如何使用YARN\n\n### 4.1 配置文件\n\n```xml\n<!-- $HADOOP_HOME/etc/hadoop/mapred-site.xml -->\n<configuration>\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n</configuration>\n```\n\n```xml\n<!-- $HADOOP_HOME/etc/hadoop/yarn-site.xml -->\n<configuration>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n</configuration>\n```\n\n### 4.2 YARN启动停止\n\n- 启动 ResourceManager 和 NodeManager （以下分别简称RM、NM）\n\n```shell\n#主节点运行命令\n$HADOOP_HOME/sbin/start-yarn.sh\n```\n\n- 停止 RM 和 NM \n\n```shell\n#主节点运行命令\n$HADOOP_HOME/sbin/stop-yarn.sh\n```\n\n- 若RM没有启动起来，可以单独启动\n\n```shell\n#若RM没有启动，在主节点运行命令\n$HADOOP_HOME/sbin/yarn-daemon.sh start resouremanager\n#相反，可单独关闭\n$HADOOP_HOME/sbin/yarn-daemon.sh stop resouremanager\n\n```\n\n- 若NM没有启动起来，可以单独启动\n\n```shell\n#若NM没有启动，在相应节点运行命令\n$HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager\n#相反，可单独关闭\n$HADOOP_HOME/sbin/yarn-daemon.sh stop nodemanager\n\n```\n\n### 4.3 YARN常用命令\n\n**4.3.1 YARN命令列表**\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201907162219.png)\n\n**4.3.2 yarn application命令**\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201907162224.png)\n\n```shell\n#1.查看正在运行的任务\nyarn application -list\n\n```\n\n```shell\n#2.杀掉正在运行任务\nyarn application -kill 任务id\n\n```\n\n```shell\n#3.查看节点列表\nyarn node -list\n\n```\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201907162252.png)\n\n```shell\n#4.查看节点状况；所有端口号与上图中端口号要一致（随机分配）\nyarn node -status node-03:45568\n\n```\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201907171511.png)\n\n```shell\n#5.查看yarn依赖jar的环境变量\nyarn classpath\n\n```\n\n\n\n## 5. YARN调度器\n\n- 试想一下，你现在所在的公司有**一个**hadoop的集群。但是A项目组经常做一些定时的BI报表，B项目组则经常使用一些软件做一些临时需求。那么他们肯定会遇到同时提交任务的场景，这个时候到底如何分配资源满足这两个任务呢？是先执行A的任务，再执行B的任务，还是同时跑两个？\n\n- 在Yarn框架中，调度器是一块很重要的内容。有了合适的调度规则，就可以保证多个应用可以在同一时间有条不紊的工作。最原始的调度规则就是FIFO，即按照用户提交任务的时间来决定哪个任务先执行，先提交的先执行。但是这样很可能一个大任务独占资源，其他的资源需要不断的等待。也可能一堆小任务占用资源，大任务一直无法得到适当的资源，造成饥饿。所以FIFO虽然很简单，但是并不能满足我们的需求。\n\n- 理想情况下，yarn应用发出的资源请求应该立刻给予满足。然而现实中的资源有限，在一个繁忙的集群上，一个应用经常需要等待才能得到所需的资源。yarn调度器的工作就是根据既定的策略为应用分配资源。调度通常是一个难题，并且**没有一个所谓的“最好”的策略**，这也是为什么yarn提供了多重调度器和可配置策略供我们选择的原因。\n\n**yarn分为一级调度管理和二级调度管理**\n一级调度管理(更近底层,更接近于操作资源, 更偏向于应用层和底层结合)\n    计算资源管理(cpu,内存等,计算复杂消耗的cpu多)\n    App生命周期管理\n二级调度管理(自己代码的算法等, 更偏向于应用层)\n    App内部的计算模型管理\n    多样化的计算模型\n\n### 5.1 调度器\n\n- 在YARN中有三种调度器可以选择：FIFO Scheduler ，Capacity Scheduler，FairS cheduler\n\n![三种调度模型](/Users/dingchuangshi/Downloads/20191021-YARN/assets/20180912140209122.png)\n\n### 5.2 FIFO Scheduler\n\n- FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。\n\n- FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。在共享集群中，更适合采用Capacity Scheduler或Fair Scheduler，这两个调度器都允许大任务和小任务在提交的同时获得一定的系统资源。\n\n- 上图展示了这几个调度器的区别，从图中可以看出，在FIFO 调度器中，小任务会被大任务阻塞。\n\n### 5.3 Capacity Scheduler\n\n- CDH版本默认使用Fair Scheduler公平调度器\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201909241610.png)\n\n- 观察yarn web界面；使用的是fair scheduler\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201909241637.png)\n\n- 若要使用capacity scheduler，需要修改yarn-site.xml文件；node01上\n\n  ```xml\n  <property>\n  \t<name>yarn.resourcemanager.scheduler.class</name>\n  \t<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>\n  </property>\n  \n  ```\n\n- 并分发到各节点\n\n  ```shell\n  [hadoop@node01 hadoop]$ pwd\n  /kkb/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop\n  [hadoop@node01 hadoop]$ scp yarn-site.xml node02:$PWD\n  [hadoop@node01 hadoop]$ scp yarn-site.xml node03:$PWD\n  \n  ```\n\n- 重启yarn\n\n  ```shell\n  [hadoop@node01 hadoop]$ stop-yarn.sh\n  [hadoop@node01 hadoop]$ start-yarn.sh\n  \n  ```\n\n- 而对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间\n\n- 如何配置容量调度器\n\n  - 队列层级结构如下\n\n    ```\n    root \n    ├── prod \n    └── dev \n    \t├── spark \n    \t└── hdp\n    \n    ```\n\n  - 主节点上，将$HADOOP_HOME/etc/hadoop/中的对应capacity-scheduler.xml配置文件备份到其它目录\n\n  - 目录$HADOOP_HOME/etc/hadoop/中建立一个新的capacity-scheduler.xml；内容如下\n\n    ```xml\n    <?xml version=\"1.0\" encoding=\"utf-8\"?>\n    \n    <configuration> \n      <property> \n        <name>yarn.scheduler.capacity.root.queues</name>  \n        <value>prod,dev</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.queues</name>  \n        <value>hdp,spark</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.prod.capacity</name>  \n        <value>40</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.capacity</name>  \n        <value>60</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.maximum-capacity</name>  \n        <value>75</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.hdp.capacity</name>  \n        <value>50</value> \n      </property>  \n      <property> \n        <name>yarn.scheduler.capacity.root.dev.spark.capacity</name>  \n        <value>50</value> \n      </property> \n    </configuration>\n    \n    ```\n\n  - 将此xml文件，远程拷贝到相同目录下\n\n  - 将应用放置在哪个队列中，取决于应用本身。\n\n    例如MR，可以通过设置属性**mapreduce.job.queuename**指定相应队列。以WordCount为例，如下\n\n    如果指定的队列不存在，则发生错误。如果不指定，默认使用\"default\"队列，如下图\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201907171118.png)\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201907171138.png)\n\n- 动态更新配置：容量调度器的配置在运行时，可以随时重新加载，调整资源分配参数；你需要编辑conf/capacity-scheduler.xml 并在yarn主节点运行命令让配置文件生效\n  - 另外，除非重启resourcemanager，否则队列只能添加不能删除；但允许关闭\n\n```shell\n[hadoop@node01 hadoop]$ yarn rmadmin -refreshQueues\n\n```\n\n- 程序打包，提交集群运行\n\n```shell\n[hadoop@node01 target]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.wordcount.WordCountMain /README.txt /w24\n\n```\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/capacity scheduler.gif)\n\n\n\n### 5.4 Fair Scheduler\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201907171437 (38).png)\n\n- Apache Hadoop默认使用Capacity Scheduler容量调度器\n\n- CDH版本默认使用Fair Scheduler公平调度器\n\n\n\n- 若要用Fair Scheduler的话，需要配置yarn-site.xml，将属性\"yarn.resourcemanager.scheduler.class\"的值修改成\"org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler\"，如下\n\n```xml\n<property>\n\t<name>yarn.resourcemanager.scheduler.class</name>\n\t<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>\n</property>\n\n```\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201907171501.png)\n\n- 注意：同样，集群中所有yarn-site.xml文件要同步更新\n\n- 在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如下图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。\n\n- 需要注意的是，在下图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成.\n\n- 支持资源抢占\n\n在yarn-site.xml中设置yarn.scheduler.fair.preemption为true\n\n- 可通过一个名为fair-scheduler.xml文件对公平调度器进行配置\n- 此文件可放置在${HADOOP_HOME}/etc/hadoop/目录下\n- 当没有设置此配置文件时，每个应用放置在以当前用户名命名的队列中\n- 队列是用户提交第一个应用时动态创建的\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201909161716.png)\n\n\n\n## 6. YARN应用状态\n\n我们在yarn 的web ui上能够看到yarn 应用程序分为如下几个状态:\n\n- NEW -----新建状态\n- NEW_SAVING-----新建保存状态\n- SUBMITTED-----提交状态\n- ACCEPTED-----接受状态\n- RUNNING-----运行状态\n- FINISHED-----完成状态\n- FAILED-----失败状态\n- KILLED-----杀掉状态\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/1558703612265.png)\n\n\n\n# 五、拓展点、未来计划、行业趋势（5分钟）\n\n1. [查看官网capacity scheduler内容](<https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html>)\n2. [capacity scheduler参考资料](<https://blog.csdn.net/u014589856/article/details/78119504>)\n3. [官网查看fair scheduler内容](<https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/FairScheduler.html>)\n4. 《Hadoop权威指南 第4版》\n   - 4.3 YARN中的调度\n   - 7.1 剖析MapReduce运行机制\n\n\n\n# 六、总结（5分钟）\n\n![](/Users/dingchuangshi/Downloads/20191021-YARN/assets/Image201907172014.png)\n\n1. 介绍了yarn的应用场景\n2. yarn的核心组件\n3. yarn应用调度过程\n4. yarn的典型应用\n\n# 七、作业\n\n\n\n# 八、互动问答\n\n\n\n# 九、题库 - 本堂课知识点\n\n","tags":["hadoop","Yarn"]},{"title":"MapReduce编程（三）","url":"/2019/10/21/hadoop/MapReduce编程（三）/","content":"\n# MapReduce编程模型\n\n### 1. 自定义OutputFormat\n\n#### 1.1 需求\n\n- 现在有一些订单的评论数据，要将订单的好评与其它级别的评论（中评、差评）进行区分开来，将最终的数据分开到不同的文件夹下面去\n\n- 数据第九个字段表示评分等级：0 好评，1 中评，2 差评\n\n  ![](assets/Image201909111129.png)\n\n#### 1.2 逻辑分析\n\n- 程序的关键点是在一个mapreduce程序中，根据数据的不同(好评的评级不同)，输出两类结果到不同**目录**\n- 这类灵活的输出，需求通过自定义OutputFormat来实现\n\n#### 1.3 实现要点\n\n- 在mapreduce中访问外部资源\n- 自定义OutputFormat类，覆写getRecordWriter()方法\n- 自定义RecordWriter类，覆写具体输出数据的方法write()\n\n#### 1.4 MR代码\n\n- 自定义OutputFormat\n\n```java\npackage com.kaikeba.hadoop.outputformat;\n\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.RecordWriter;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n *\n * 本例使用框架默认的Reducer，它将Mapper输入的kv对，原样输出；所以reduce输出的kv类型分别是Text, NullWritable\n * 自定义OutputFormat的类，泛型表示reduce输出的键值对类型；要保持一致;\n * map--(kv)-->reduce--(kv)-->OutputFormat\n */\npublic class MyOutPutFormat extends FileOutputFormat<Text, NullWritable> {\n\n    /**\n     * 两个输出文件;\n     * good用于保存好评文件；其它评级保存到bad中\n     * 根据实际情况修改path;node01及端口号8020\n     */\n    String bad = \"hdfs://node01:8020/outputformat/bad/r.txt\";\n    String good = \"hdfs://node01:8020/outputformat/good/r.txt\";\n\n    /**\n     *\n     * @param context\n     * @return\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    public RecordWriter<Text, NullWritable> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException {\n        //获得文件系统对象\n        FileSystem fs = FileSystem.get(context.getConfiguration());\n        //两个输出文件路径\n        Path badPath = new Path(bad);\n        Path goodPath = new Path(good);\n        FSDataOutputStream badOut = fs.create(badPath);\n        FSDataOutputStream goodOut = fs.create(goodPath);\n        return new MyRecordWriter(badOut,goodOut);\n    }\n\n    /**\n     * 泛型表示reduce输出的键值对类型；要保持一致\n     */\n    static class MyRecordWriter extends RecordWriter<Text, NullWritable>{\n\n        FSDataOutputStream badOut = null;\n        FSDataOutputStream goodOut = null;\n\n        public MyRecordWriter(FSDataOutputStream badOut, FSDataOutputStream goodOut) {\n            this.badOut = badOut;\n            this.goodOut = goodOut;\n        }\n\n        /**\n         * 自定义输出kv对逻辑\n         * @param key\n         * @param value\n         * @throws IOException\n         * @throws InterruptedException\n         */\n        @Override\n        public void write(Text key, NullWritable value) throws IOException, InterruptedException {\n            if (key.toString().split(\"\\t\")[9].equals(\"0\")){//好评\n                goodOut.write(key.toString().getBytes());\n                goodOut.write(\"\\r\\n\".getBytes());\n            }else{//其它评级\n                badOut.write(key.toString().getBytes());\n                badOut.write(\"\\r\\n\".getBytes());\n            }\n        }\n\n        /**\n         * 关闭流\n         * @param context\n         * @throws IOException\n         * @throws InterruptedException\n         */\n        @Override\n        public void close(TaskAttemptContext context) throws IOException, InterruptedException {\n            if(goodOut !=null){\n                goodOut.close();\n            }\n            if(badOut !=null){\n                badOut.close();\n            }\n        }\n    }\n}\n```\n\n- main方法\n\n```java\npackage com.kaikeba.hadoop.outputformat;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\nimport java.io.IOException;\n\npublic class MyOwnOutputFormatMain extends Configured implements Tool {\n\n    public int run(String[] args) throws Exception {\n        Configuration conf = super.getConf();\n        Job job = Job.getInstance(conf, MyOwnOutputFormatMain.class.getSimpleName());\n        job.setJarByClass(MyOwnOutputFormatMain.class);\n\n        //默认项，可以省略或者写出也可以\n        //job.setInputFormatClass(TextInputFormat.class);\n        //设置输入文件\n        TextInputFormat.addInputPath(job, new Path(args[0]));\n        job.setMapperClass(MyMapper.class);\n        //job.setMapOutputKeyClass(Text.class);\n        //job.setMapOutputValueClass(NullWritable.class);\n\n        //设置自定义的输出类\n        job.setOutputFormatClass(MyOutPutFormat.class);\n        //设置一个输出目录，这个目录会输出一个success的成功标志的文件\n        MyOutPutFormat.setOutputPath(job, new Path(args[1]));\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        //默认项，即默认有一个reduce任务，所以可以省略\n//        job.setNumReduceTasks(1);\n//        //Reducer将输入的键值对原样输出\n//        job.setReducerClass(Reducer.class);\n\n        boolean b = job.waitForCompletion(true);\n        return b ? 0: 1;\n    }\n\n    /**\n     *\n     * Mapper输出的key、value类型\n     * 文件每行的内容作为输出的key，对应Text类型\n     * 输出的value为null，对应NullWritable\n     */\n    public static class MyMapper extends Mapper<LongWritable, Text, Text, NullWritable> {\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            //把当前行内容作为key输出；value为null\n            context.write(value, NullWritable.get());\n        }\n    }\n\n    /**\n     *\n     * @param args /ordercomment.csv /ofo\n     * @throws Exception\n     */\n    public static void main(String[] args) throws Exception {\n        Configuration configuration = new Configuration();\n        ToolRunner.run(configuration, new MyOwnOutputFormatMain(), args);\n    }\n}\n```\n\n#### 1.5 总结\n\n- 自定义outputformat\n  - 泛型与reduce输出的键值对类型保持一致\n  - 覆写getRecordWriter()方法\n- 自定义RecordWriter\n  - 泛型与reduce输出的键值对类型保持一致\n  - 覆写具体输出数据的方法write()、close()\n\n- main方法\n  - job.setOutputFormatClass使用自定义在输出类\n\n\n\n### 2. 二次排序\n\n#### 2.1 需求\n\n- 数据：有一个简单的关于员工工资的记录文件salary.txt\n\n  - 每条记录如下，有3个字段，分别表示name、age、salary\n\n  - nancy\t22\t8000\n\n    ![](assets/Image201910181039.png)\n\n- 使用MR处理记录，实现结果中\n\n  - 按照工资从高到低的降序排序\n  - 若工资相同，则按年龄升序排序\n\n#### 2.2 逻辑分析\n\n- 利用MR中key具有可比较的特点\n\n- MapReduce中，根据key进行分区、排序、分组\n\n- 有些MR的输出的key可以直接使用hadoop框架的可序列化可比较类型表示，如Text、IntWritable等等，而这些类型本身是可比较的；如IntWritable默认升序排序\n\n  ![](assets/Image201909111209.png)\n\n- 但有时，使用MR编程，输出的key，若使用hadoop自带的key类型无法满足需求\n\n  - 此时，需要自定义的key类型（包含的是非单一信息，如此例包含工资、年龄）；\n  - 并且也得是**<font color='red'>可序列化、可比较的</font>**\n\n- 需要自定义key，定义排序规则\n\n  - 实现：按照人的salary降序排序，若相同，则再按age升序排序；若salary、age相同，则放入同一组\n\n#### 2.3 MR代码\n\n- 详见工程代码\n- 自定义key类型Person类\n\n```java\npackage com.kaikeba.hadoop.secondarysort;\n\nimport org.apache.hadoop.io.WritableComparable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n//根据输入文件格式，定义JavaBean，作为MR时，Map的输出key类型；要求此类可序列化、可比较\npublic class Person implements WritableComparable<Person> {\n    private String name;\n    private int age;\n    private int salary;\n\n    public Person() {\n    }\n\n    public Person(String name, int age, int salary) {\n        //super();\n        this.name = name;\n        this.age = age;\n        this.salary = salary;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    public void setAge(int age) {\n        this.age = age;\n    }\n\n    public int getSalary() {\n        return salary;\n    }\n\n    public void setSalary(int salary) {\n        this.salary = salary;\n    }\n\n    @Override\n    public String toString() {\n        return this.salary + \"  \" + this.age + \"    \" + this.name;\n    }\n\n    //两个Person对象的比较规则：①先比较salary，高的排序在前；②若相同，age小的在前\n    public int compareTo(Person other) {\n        int compareResult= this.salary - other.salary;\n        if(compareResult != 0) {//若两个人工资不同\n            //工资降序排序；即工资高的排在前边\n            return -compareResult;\n        } else {//若工资相同\n            //年龄升序排序；即年龄小的排在前边\n            return this.age - other.age;\n        }\n    }\n\n    //序列化，将NewKey转化成使用流传送的二进制\n    public void write(DataOutput dataOutput) throws IOException {\n        //注意：①使用正确的write方法；②记住此时的序列化的顺序，name、age、salary\n        dataOutput.writeUTF(name);\n        dataOutput.writeInt(age);\n        dataOutput.writeInt(salary);\n    }\n\n    //使用in读字段的顺序，要与write方法中写的顺序保持一致：name、age、salary\n    public void readFields(DataInput dataInput) throws IOException {\n        //read string\n        //注意：①使用正确的read方法；②读取顺序与write()中序列化的顺序保持一致\n        this.name = dataInput.readUTF();\n        this.age = dataInput.readInt();\n        this.salary = dataInput.readInt();\n    }\n}\n```\n\n- main类、mapper、reducer\n\n```java\npackage com.kaikeba.hadoop.secondarysort;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\nimport java.net.URI;\n\npublic class SecondarySort {\n\n\t/**\n\t *\n\t * @param args /salary.txt /secondarysort\n\t * @throws Exception\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\t\tConfiguration configuration = new Configuration();\n\t\t//configuration.set(\"mapreduce.job.jar\",\"/home/bruce/project/kkbhdp01/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n\t\tJob job = Job.getInstance(configuration, SecondarySort.class.getSimpleName());\n\n\t\tFileSystem fileSystem = FileSystem.get(URI.create(args[1]), configuration);\n\t\t//生产中慎用\n\t\tif (fileSystem.exists(new Path(args[1]))) {\n\t\t\tfileSystem.delete(new Path(args[1]), true);\n\t\t}\n\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tjob.setMapperClass(MyMap.class);\n\t\t//由于mapper与reducer输出的kv类型分别相同，所以，下两行可以省略\n//\t\tjob.setMapOutputKeyClass(Person.class);\n//\t\tjob.setMapOutputValueClass(NullWritable.class);\n\t\t\n\t\t//设置reduce的个数;默认为1\n\t\t//job.setNumReduceTasks(1);\n\n\t\tjob.setReducerClass(MyReduce.class);\n\t\tjob.setOutputKeyClass(Person.class);\n\t\tjob.setOutputValueClass(NullWritable.class);\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n\t\tjob.waitForCompletion(true);\n\n\t}\n\n\t//MyMap的输出key用自定义的Person类型；输出的value为null\n\tpublic static class MyMap extends Mapper<LongWritable, Text, Person, NullWritable> {\n\t\t@Override\n\t\tprotected void map(LongWritable key, Text value, Context context)\n\t\t\t\tthrows IOException, InterruptedException {\n\n\t\t\tString[] fields = value.toString().split(\"\\t\");\n\t\t\tString name = fields[0];\n\t\t\tint age = Integer.parseInt(fields[1]);\n\t\t\tint salary = Integer.parseInt(fields[2]);\n\t\t\t//在自定义类中进行比较\n\t\t\tPerson person = new Person(name, age, salary);\n\t\t\t//person对象作为输出的key\n\t\t\tcontext.write(person, NullWritable.get());\n\t\t}\n\t}\n\n\tpublic static class MyReduce extends Reducer<Person, NullWritable, Person, NullWritable> {\n\t\t@Override\n\t\tprotected void reduce(Person key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {\n\t\t\t//输入的kv对，原样输出\n\t\t\tcontext.write(key, NullWritable.get());\n\t\t}\n\t}\n}\n```\n\n#### 2.4 总结\n\n- 如果MR时，key的排序规则比较复杂，比如需要根据字段1排序，若字段1相同，则需要根据字段2排序...，此时，可以使用自定义key实现\n- 将自定义的key作为MR中，map输出的key的类型（reduce输入的类型）\n- 自定义的key\n  - 实现WritableComparable接口\n  - 实现compareTo比较方法\n  - 实现write序列化方法\n  - 实现readFields反序列化方法\n\n\n\n### 3. 自定义分组求topN\n\n#### 3.1 需求\n\n- 现有一个淘宝用户订单历史记录文件；每条记录有6个字段，分别表示\n\n  - userid、datetime、title商品标题、unitPrice商品单价、purchaseNum购买量、productId商品ID\n\n  ![](assets/Image201909111241.png)\n\n- 现使用MR编程，求出每个用户、每个月消费金额最多的两笔订单，花了多少钱\n\n  - 所以得相同用户、同一个年月的数据，分到同一组\n\n#### 3.2 逻辑分析\n\n- 根据文件格式，自定义JavaBean类OrderBean\n  - 实现WritableComparable接口\n  - 包含6个字段分别对应文件中的6个字段\n  - 重点实现compareTo方法\n    - 先比较userid是否相等；若不相等，则userid升序排序\n    - 若相等，比较两个Bean的日期是否相等；若不相等，则日期升序排序\n    - 若相等，再比较总开销，降序排序\n  - 实现序列化方法write()\n  - 实现反序列化方法readFields()\n- 自定义分区类\n  - 继承Partitioner类\n  - getPartiton()实现，userid相同的，处于同一个分区\n- 自定义Mapper类\n  - 输出key是当前记录对应的Bean对象\n  - 输出的value对应当前下单的总开销\n- 自定义分组类\n  - 决定userid相同、日期（年月）相同的记录，分到同一组中，调用一次reduce()\n- 自定义Reduce类\n  - reduce()中求出当前一组数据中，开销头两笔的信息\n- main方法\n  - job.setMapperClass\n  - job.setPartitionerClass\n  - job.setReducerClass\n  - job.setGroupingComparatorClass\n\n#### 3.3 MR代码\n\n> 详细代码见代码工程\n\n- OrderBean\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.WritableComparable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n//实现WritableComparable接口\npublic class OrderBean implements WritableComparable<OrderBean> {\n\n    //用户ID\n    private String userid;\n    //年月\n    //year+month -> 201408\n    private String datetime;\n    //标题\n    private String title;\n    //单价\n    private double unitPrice;\n    //购买量\n    private int purchaseNum;\n    //商品ID\n    private String produceId;\n\n    public OrderBean() {\n    }\n\n    public OrderBean(String userid, String datetime, String title, double unitPrice, int purchaseNum, String produceId) {\n        super();\n        this.userid = userid;\n        this.datetime = datetime;\n        this.title = title;\n        this.unitPrice = unitPrice;\n        this.purchaseNum = purchaseNum;\n        this.produceId = produceId;\n    }\n\n    //key的比较规则\n    public int compareTo(OrderBean other) {\n        //OrderBean作为MR中的key；如果对象中的userid相同，即ret1为0；就表示两个对象是同一个用户\n        int ret1 = this.userid.compareTo(other.userid);\n\n        if (ret1 == 0) {\n            //如果userid相同，比较年月\n            String thisYearMonth = this.getDatetime();\n            String otherYearMonth = other.getDatetime();\n            int ret2 = thisYearMonth.compareTo(otherYearMonth);\n\n            if(ret2 == 0) {//若datetime相同\n                //如果userid、年月都相同，比较单笔订单的总开销\n                Double thisTotalPrice = this.getPurchaseNum()*this.getUnitPrice();\n                Double oTotalPrice = other.getPurchaseNum()*other.getUnitPrice();\n                //总花销降序排序；即总花销高的排在前边\n                return -thisTotalPrice.compareTo(oTotalPrice);\n            } else {\n                //若datatime不同，按照datetime升序排序\n                return ret2;\n            }\n        } else {\n            //按照userid升序排序\n            return ret1;\n        }\n    }\n\n    /**\n     * 序列化\n     * @param dataOutput\n     * @throws IOException\n     */\n    public void write(DataOutput dataOutput) throws IOException {\n        dataOutput.writeUTF(userid);\n        dataOutput.writeUTF(datetime);\n        dataOutput.writeUTF(title);\n        dataOutput.writeDouble(unitPrice);\n        dataOutput.writeInt(purchaseNum);\n        dataOutput.writeUTF(produceId);\n    }\n\n    /**\n     * 反序列化\n     * @param dataInput\n     * @throws IOException\n     */\n    public void readFields(DataInput dataInput) throws IOException {\n        this.userid = dataInput.readUTF();\n        this.datetime = dataInput.readUTF();\n        this.title = dataInput.readUTF();\n        this.unitPrice = dataInput.readDouble();\n        this.purchaseNum = dataInput.readInt();\n        this.produceId = dataInput.readUTF();\n    }\n\n    /**\n     * 使用默认分区器，那么userid相同的，落入同一分区；\n     * 另外一个方案：此处不覆写hashCode方法，而是自定义分区器，getPartition方法中，对OrderBean的userid求hashCode值%reduce任务数\n     * @return\n     */\n//    @Override\n//    public int hashCode() {\n//        return this.userid.hashCode();\n//    }\n\n    @Override\n    public String toString() {\n        return \"OrderBean{\" +\n                \"userid='\" + userid + '\\'' +\n                \", datetime='\" + datetime + '\\'' +\n                \", title='\" + title + '\\'' +\n                \", unitPrice=\" + unitPrice +\n                \", purchaseNum=\" + purchaseNum +\n                \", produceId='\" + produceId + '\\'' +\n                '}';\n    }\n\n    public String getUserid() {\n        return userid;\n    }\n\n    public void setUserid(String userid) {\n        this.userid = userid;\n    }\n\n    public String getDatetime() {\n        return datetime;\n    }\n\n    public void setDatetime(String datetime) {\n        this.datetime = datetime;\n    }\n\n    public String getTitle() {\n        return title;\n    }\n\n    public void setTitle(String title) {\n        this.title = title;\n    }\n\n    public double getUnitPrice() {\n        return unitPrice;\n    }\n\n    public void setUnitPrice(double unitPrice) {\n        this.unitPrice = unitPrice;\n    }\n\n    public int getPurchaseNum() {\n        return purchaseNum;\n    }\n\n    public void setPurchaseNum(int purchaseNum) {\n        this.purchaseNum = purchaseNum;\n    }\n\n    public String getProduceId() {\n        return produceId;\n    }\n\n    public void setProduceId(String produceId) {\n        this.produceId = produceId;\n    }\n}\n\n```\n\n- MyPartitioner\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\n//mapper的输出key类型是自定义的key类型OrderBean；输出value类型是单笔订单的总开销double -> DoubleWritable\npublic class MyPartitioner extends Partitioner<OrderBean, DoubleWritable> {\n    @Override\n    public int getPartition(OrderBean orderBean, DoubleWritable doubleWritable, int numReduceTasks) {\n        //userid相同的，落入同一分区\n        return (orderBean.getUserid().hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n}\n\n```\n\n- MyMapper\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * 输出kv，分别是OrderBean、用户每次下单的总开销\n */\npublic class MyMapper extends Mapper<LongWritable, Text, OrderBean, DoubleWritable> {\n    DoubleWritable valueOut = new DoubleWritable();\n    DateUtils dateUtils = new DateUtils();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        //13764633023     2014-12-01 02:20:42.000 全视目Allseelook 原宿风暴显色美瞳彩色隐形艺术眼镜1片 拍2包邮    33.6    2       18067781305\n        String record = value.toString();\n        String[] fields = record.split(\"\\t\");\n        if(fields.length == 6) {\n            String userid = fields[0];\n            String datetime = fields[1];\n            String yearMonth = dateUtils.getYearMonthString(datetime);\n            String title = fields[2];\n            double unitPrice = Double.parseDouble(fields[3]);\n            int purchaseNum = Integer.parseInt(fields[4]);\n            String produceId = fields[5];\n\n            //生成OrderBean对象\n            OrderBean orderBean = new OrderBean(userid, yearMonth, title, unitPrice, purchaseNum, produceId);\n\n            //此订单的总开销\n            double totalPrice = unitPrice * purchaseNum;\n            valueOut.set(totalPrice);\n\n            context.write(orderBean, valueOut);\n        }\n    }\n}\n\n```\n\n- MyReducer\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\n//输出的key为userid拼接上年月的字符串，对应Text；输出的value对应单笔订单的金额\npublic class MyReducer extends Reducer<OrderBean, DoubleWritable, Text, DoubleWritable> {\n    /**\n     * ①由于自定义分组逻辑，相同用户、相同年月的订单是一组，调用一次reduce()；\n     * ②由于自定义的key类OrderBean中，比较规则compareTo规定，相同用户、相同年月的订单，按总金额降序排序\n     * 所以取出头两笔，就实现需求\n     * @param key\n     * @param values\n     * @param context\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    protected void reduce(OrderBean key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException {\n        //求每个用户、每个月、消费金额最多的两笔多少钱\n        int num = 0;\n        for(DoubleWritable value: values) {\n            if(num < 2) {\n                String keyOut = key.getUserid() + \"  \" + key.getDatetime();\n                context.write(new Text(keyOut), value);\n                num++;\n            } else {\n                break;\n            }\n        }\n\n    }\n}\n```\n\n- MyGroup\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\n\n//自定义分组类：reduce端调用reduce()前，对数据做分组；每组数据调用一次reduce()\npublic class MyGroup extends WritableComparator {\n\n    public MyGroup() {\n        //第一个参数表示key class\n        super(OrderBean.class, true);\n    }\n\n  \t// 注意： 分组实现的方法是这个\n  \t// compare（Object a,Object b） 这个方法不可以\n    //分组逻辑\n    @Override\n    public int compare(WritableComparable a, WritableComparable b) {\n        //userid相同，且同一月的分成一组\n        OrderBean aOrderBean = (OrderBean)a;\n        OrderBean bOrderBean = (OrderBean)b;\n\n        String aUserId = aOrderBean.getUserid();\n        String bUserId = bOrderBean.getUserid();\n\n        //userid、年、月相同的，作为一组\n        int ret1 = aUserId.compareTo(bUserId);\n        if(ret1 == 0) {//同一用户\n            //年月也相同返回0，在同一组；\n            return aOrderBean.getDatetime().compareTo(bOrderBean.getDatetime());\n        } else {\n            return ret1;\n        }\n    }\n}\n\n```\n\n- CustomGroupingMain\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport com.kaikeba.hadoop.wordcount.WordCountMain;\nimport com.kaikeba.hadoop.wordcount.WordCountMap;\nimport com.kaikeba.hadoop.wordcount.WordCountReduce;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.DoubleWritable;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\nimport java.io.IOException;\n\npublic class CustomGroupingMain extends Configured implements Tool {\n\n    ///tmall-201412-test.csv /cgo\n    public static void main(String[] args) throws Exception {\n        int exitCode = ToolRunner.run(new CustomGroupingMain(), args);\n        System.exit(exitCode);\n    }\n\n    @Override\n    public int run(String[] args) throws Exception {\n        //判断以下，输入参数是否是两个，分别表示输入路径、输出路径\n        if (args.length != 2 || args == null) {\n            System.out.println(\"please input Path!\");\n            System.exit(0);\n        }\n\n        Configuration configuration = new Configuration();\n        //告诉程序，要运行的jar包在哪\n        //configuration.set(\"mapreduce.job.jar\",\"/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n\n        //调用getInstance方法，生成job实例\n        Job job = Job.getInstance(configuration, CustomGroupingMain.class.getSimpleName());\n\n        //设置jar包，参数是包含main方法的类\n        job.setJarByClass(CustomGroupingMain.class);\n\n        //通过job设置输入/输出格式\n        //MR的默认输入格式是TextInputFormat，所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n        //设置输入/输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        //设置处理Map阶段的自定义的类\n        job.setMapperClass(MyMapper.class);\n        //设置map combine类，减少网路传出量\n        //job.setCombinerClass(MyReducer.class);\n        job.setPartitionerClass(MyPartitioner.class);\n        //设置处理Reduce阶段的自定义的类\n        job.setReducerClass(MyReducer.class);\n        job.setGroupingComparatorClass(MyGroup.class);\n\n        //如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n        job.setMapOutputKeyClass(OrderBean.class);\n        job.setMapOutputValueClass(DoubleWritable.class);\n\n        //设置reduce task最终输出key/value的类型\n        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(DoubleWritable.class);\n\n        // 提交作业\n        return job.waitForCompletion(true) ? 0 : 1;\n    }\n}\n\n```\n\n- DateUtils\n\n```java\npackage com.kaikeba.hadoop.grouping;\n\nimport java.time.LocalDateTime;\nimport java.time.format.DateTimeFormatter;\n\npublic class DateUtils {\n\n    public static void main(String[] args) {\n        //test1\n//        String str1 = \"13764633024  2014-10-01 02:20:42.000\";\n//        String str2 = \"13764633023  2014-11-01 02:20:42.000\";\n//        System.out.println(str1.compareTo(str2));\n\n        //test2\n//        String datetime = \"2014-12-01 02:20:42.000\";\n//        LocalDateTime localDateTime = parseDateTime(datetime);\n//        int year = localDateTime.getYear();\n//        int month = localDateTime.getMonthValue();\n//        int day = localDateTime.getDayOfMonth();\n//        System.out.println(\"year-> \" + year + \"; month -> \" + month + \"; day -> \" + day);\n\n        //test3\n//        String datetime = \"2014-12-01 02:20:42.000\";\n//        System.out.println(getYearMonthString(datetime));\n    }\n\n    public LocalDateTime parseDateTime(String dateTime) {\n        DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\");\n        LocalDateTime localDateTime = LocalDateTime.parse(dateTime, formatter);\n        return localDateTime;\n    }\n\n    //日期格式转换工具类：将2014-12-14 20:42:14.000转换成201412\n    public String getYearMonthString(String dateTime) {\n        DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\");\n        LocalDateTime localDateTime = LocalDateTime.parse(dateTime, formatter);\n        int year = localDateTime.getYear();\n        int month = localDateTime.getMonthValue();\n        return year + \"\" + month;\n    }\n\n\n\n}\n\n```\n\n\n\n#### 3.4 总结\n\n- 要实现自定义分组逻辑\n  - 一般会自定义JavaBean，作为map输出的key\n    - 实现其中的compareTo方法，设置key的比较逻辑\n    - 实现序列化方法write()\n    - 实现反序列化方法readFields()\n  - 自定义mapper类、reducer类\n  - 自定义partition类，getPartition方法，决定哪些key落入哪些分区\n  - 自定义group分组类，决定reduce阶段，哪些kv对，落入同一组，调用一次reduce()\n  - 写main方法，设置自定义的类\n    - job.setMapperClass\n    - job.setPartitionerClass\n    - job.setReducerClass\n    - job.setGroupingComparatorClass\n\n\n\n### 4. MapReduce数据倾斜(20分钟)\n\n- 什么是数据倾斜？\n  - 数据中不可避免地会出现离群值（outlier），并导致数据倾斜。这些离群值会显著地拖慢MapReduce的执行。\n- 常见的数据倾斜有以下几类：\n  - 数据频率倾斜——某一个区域的数据量要远远大于其他区域。比如某一个key对应的键值对远远大于其他键的键值对。\n  - 数据大小倾斜——部分记录的大小远远大于平均值。\n\n- 在map端和reduce端都有可能发生数据倾斜\n  - 在map端的数据倾斜可以考虑使用combine\n  - 在reduce端的数据倾斜常常来源于MapReduce的默认分区器\n\n- 数据倾斜会导致map和reduce的任务执行时间大为延长，也会让需要缓存数据集的操作消耗更多的内存资源\n\n#### 4.1 如何诊断是否存在数据倾斜（10分钟）\n\n2. 如何诊断哪些键存在数据倾斜？\n   - 发现倾斜数据之后，有必要诊断造成数据倾斜的那些键。有一个简便方法就是在代码里实现追踪每个键的**最大值**。\n   - 为了减少追踪量，可以设置数据量阀值，只追踪那些数据量大于阀值的键，并输出到日志中。实现代码如下\n   - 运行作业后就可以从日志中判断发生倾斜的键以及倾斜程度；跟踪倾斜数据是了解数据的重要一步，也是设计MapReduce作业的重要基础\n   \n```java\n   package com.kaikeba.hadoop.dataskew;\n   \n   import org.apache.hadoop.io.IntWritable;\n   import org.apache.hadoop.io.Text;\n   import org.apache.hadoop.mapreduce.Reducer;\n   import org.apache.log4j.Logger;\n   \n   import java.io.IOException;\n   \n   public class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {\n   \n       private int maxValueThreshold;\n   \n       //日志类\n       private static final Logger LOGGER = Logger.getLogger(WordCountReduce.class);\n   \n       @Override\n       protected void setup(Context context) throws IOException, InterruptedException {\n   \n           //一个键达到多少后，会做数据倾斜记录\n           maxValueThreshold = 10000;\n       }\n   \n       /*\n               (hello, 1)\n               (hello, 1)\n               (hello, 1)\n               ...\n               (spark, 1)\n   \n               key: hello\n               value: List(1, 1, 1)\n           */\n       public void reduce(Text key, Iterable<IntWritable> values,\n                             Context context) throws IOException, InterruptedException {\n           int sum = 0;\n           //用于记录键出现的次数\n           int i = 0;\n   \n           for (IntWritable count : values) {\n               sum += count.get();\n               i++;\n           }\n   \n           //如果当前键超过10000个，则打印日志\n           if(i > maxValueThreshold) {\n               LOGGER.info(\"Received \" + i + \" values for key \" + key);\n           }\n   \n           context.write(key, new IntWritable(sum));// 输出最终结果\n       };\n   }\n```\n\n\n\n\n#### 4.2 减缓数据倾斜\n\n- Reduce数据倾斜一般是指map的输出数据中存在数据频率倾斜的状况，即部分输出键的数据量远远大于其它的输出键\n\n- 如何减小reduce端数据倾斜的性能损失？常用方式有：\n  - 一、自定义分区\n\n    - 基于输出键的背景知识进行自定义分区。\n\n    - 例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的一部分reduce实例。而将其他的都发送给剩余的reduce实例。\n\n  - 二、Combine\n\n    - 使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。\n    - combine的目的就是聚合并精简数据。\n\n  - 三、抽样和范围分区\n\n    - Hadoop默认的分区器是HashPartitioner，基于map输出键的哈希值分区。这仅在数据分布比较均匀时比较好。**在有数据倾斜时就很有问题**。\n\n    - 使用分区器需要首先了解数据的特性。**TotalOrderPartitioner**中，可以通过对原始数据进行抽样得到的结果集来**预设分区边界值**。\n    - TotalOrderPartitioner中的范围分区器可以通过预设的分区边界值进行分区。因此它也可以很好地用在矫正数据中的部分键的数据倾斜问题。\n\n  - 四、数据大小倾斜的自定义策略\n\n    - 在map端或reduce端的数据大小倾斜都会对缓存造成较大的影响，乃至导致OutOfMemoryError异常。处理这种情况并不容易。可以参考以下方法。\n\n    - 设置mapreduce.input.linerecordreader.line.maxlength来限制RecordReader读取的最大长度。\n    - RecordReader在TextInputFormat和KeyValueTextInputFormat类中使用。默认长度没有上限。\n\n\n\n### 5. MR调优\n\n- 见后续文章\n\n\n\n\n\n### 6. 抽样、范围分区\n\n#### 6.1 数据\n\n   - 数据：气象站气象数据，来源美国国家气候数据中心（NCDC）（1900-2000年数据，每年一个文件）\n\n        - 气候数据record的格式如下\n\n\n![](assets/Image201907151554.png)\n\n#### 6.2 需求\n\n- 对气象数据，按照气温进行排序（气温符合正太分布）\n\n#### 6.3 实现方案\n\n- 三种实现思路\n\n  - 方案一：\n    - 设置一个分区，即一个reduce任务；在一个reduce中对结果进行排序；\n    - 失去了MR框架并行计算的优势\n  - 方案二：\n    - 自定义分区，人为指定各温度区间的记录，落入哪个分区；如分区温度边界值分别是-15、0、20，共4个分区\n    - 但由于对整个数据集的气温分布不了解，可能某些分区的数据量大，其它的分区小，数据倾斜\n  - 方案三：\n    - 通过对键空间采样\n    - 只查看一小部分键，获得键的近似分布（好温度的近似分布）\n    - 进而据此结果创建分区，实现尽可能的均匀的划分数据集；\n    - Hadoop内置了采样器；InputSampler\n\n#### 6.4 MR代码\n\n> 分两大步\n\n- 一、先将数据按气温对天气数据集排序。结果存储为sequencefile文件，气温作为输出键，数据行作为输出值\n\n- 代码\n\n  > 此代码处理原始日志文件\n  >\n  > 结果用SequenceFile格式存储；\n  >\n  > 温度作为SequenceFile的key；记录作为value\n\n``` java\npackage com.kaikeba.hadoop.totalorder;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n * 此代码处理原始日志文件 1901\n * 结果用SequenceFile格式存储；\n * 温度作为SequenceFile的key；记录作为value\n */\npublic class SortDataPreprocessor {\n\n  //输出的key\\value分别是气温、记录\n  static class CleanerMapper extends Mapper<LongWritable, Text, IntWritable, Text> {\n  \n    private NcdcRecordParser parser = new NcdcRecordParser();\n    private IntWritable temperature = new IntWritable();\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context)\n        throws IOException, InterruptedException {\n      //0029029070999991901010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n      parser.parse(value);\n      if (parser.isValidTemperature()) {//是否是有效的记录\n        temperature.set(parser.getAirTemperature());\n        context.write(temperature, value);\n      }\n    }\n  }\n\n\n  //两个参数：/ncdc/input /ncdc/sfoutput\n  public static void main(String[] args) throws Exception {\n\n    if (args.length != 2) {\n      System.out.println(\"<input> <output>\");\n    }\n\n    Configuration conf = new Configuration();\n\n    Job job = Job.getInstance(conf, SortDataPreprocessor.class.getSimpleName());\n    job.setJarByClass(SortDataPreprocessor.class);\n    //\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n    job.setMapperClass(CleanerMapper.class);\n    //最终输出的键、值类型\n    job.setOutputKeyClass(IntWritable.class);\n    job.setOutputValueClass(Text.class);\n    //reduce个数为0\n    job.setNumReduceTasks(0);\n    //以sequencefile的格式输出\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    //开启job输出压缩功能\n    //方案一\n    conf.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n    conf.set(\"mapreduce.output.fileoutputformat.compress.type\",\"RECORD\");\n    //指定job输出使用的压缩算法\n    conf.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n\n    //方案二\n    //设置sequencefile的压缩、压缩算法、sequencefile文件压缩格式block\n    //SequenceFileOutputFormat.setCompressOutput(job, true);\n    //SequenceFileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);\n    //SequenceFileOutputFormat.setOutputCompressorClass(job, SnappyCodec.class);\n    //SequenceFileOutputFormat.setOutputCompressionType(job, SequenceFile.CompressionType.BLOCK);\n\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n```\n\n- 二、全局排序\n\n  > 使用全排序分区器TotalOrderPartitioner\n  >\n\n```java\npackage com.kaikeba.hadoop.totalorder;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.filecache.DistributedCache;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.partition.InputSampler;\nimport org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner;\n\nimport java.net.URI;\n\n/**\n * 使用TotalOrderPartitioner全局排序一个SequenceFile文件的内容；\n * 此文件是SortDataPreprocessor的输出文件；\n * key是IntWritble，气象记录中的温度\n */\npublic class SortByTemperatureUsingTotalOrderPartitioner{\n\n  /**\n   * 两个参数：/ncdc/sfoutput /ncdc/totalorder\n   * 第一个参数是SortDataPreprocessor的输出文件\n   */\n  public static void main(String[] args) throws Exception {\n    if (args.length != 2) {\n      System.out.println(\"<input> <output>\");\n    }\n\n    Configuration conf = new Configuration();\n\n    Job job = Job.getInstance(conf, SortByTemperatureUsingTotalOrderPartitioner.class.getSimpleName());\n    job.setJarByClass(SortByTemperatureUsingTotalOrderPartitioner.class);\n\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n    //输入文件是SequenceFile\n    job.setInputFormatClass(SequenceFileInputFormat.class);\n\n    //Hadoop提供的方法来实现全局排序，要求Mapper的输入、输出的key必须保持类型一致\n    job.setOutputKeyClass(IntWritable.class);\n    job.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n    //分区器：全局排序分区器\n    job.setPartitionerClass(TotalOrderPartitioner.class);\n\n    //分了3个区；且分区i-1中的key小于i分区中所有的键\n    job.setNumReduceTasks(3);\n\n    /**\n     * 随机采样器从所有的分片中采样\n     * 每一个参数：采样率；\n     * 第二个参数：总的采样数\n     * 第三个参数：采样的最大分区数；\n     * 只要numSamples和maxSplitSampled（第二、第三参数）任一条件满足，则停止采样\n     */\n    InputSampler.Sampler<IntWritable, Text> sampler =\n            new InputSampler.RandomSampler<IntWritable, Text>(0.1, 5000, 10);\n//    TotalOrderPartitioner.setPartitionFile();\n    /**\n     * 存储定义分区的键；即整个数据集中温度的大致分布情况；\n     * 由TotalOrderPartitioner读取，作为全排序的分区依据，让每个分区中的数据量近似\n     */\n    InputSampler.writePartitionFile(job, sampler);\n\n    //根据上边的SequenceFile文件（包含键的近似分布情况），创建分区\n    String partitionFile = TotalOrderPartitioner.getPartitionFile(job.getConfiguration());\n    URI partitionUri = new URI(partitionFile);\n\n//    JobConf jobConf = new JobConf();\n\n    //与所有map任务共享此文件，添加到分布式缓存中\n    DistributedCache.addCacheFile(partitionUri, job.getConfiguration());\n//    job.addCacheFile(partitionUri);\n\n    //方案一：输出的文件RECORD级别，使用BZip2Codec进行压缩\n    conf.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n    conf.set(\"mapreduce.output.fileoutputformat.compress.type\",\"RECORD\");\n    //指定job输出使用的压缩算法\n    conf.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n\n    //方案二\n    //SequenceFileOutputFormat.setCompressOutput(job, true);\n    //SequenceFileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);\n    //SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);\n\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n\n```\n\n#### 6.5 总结\n\n- 对大量数据进行全局排序\n\n  - 先使用InputSampler.Sampler采样器，对整个key空间进行采样，得到key的近似分布\n\n  - 保存到key分布情况文件中\n\n  - 使用TotalOrderPartitioner，利用上边的key分布情况文件，进行分区；每个分区的数据量近似，从而防止数据倾斜\n\n\n\n## 注意\n\n1. 描述MR的shuffle全流程（面试）\n2. 搭建MAVEN工程，统计词频，并提交集群运行，查看结果\n3. 利用搜狗数据，找出所有独立的uid并写入HDFS\n4. 利用搜狗数据，找出所有独立的uid出现次数，并写入HDFS，并要求使用Map端的Combine操作\n5. 谈谈什么是数据倾斜，什么情况会造成数据倾斜？（面试）\n6. 对MR数据倾斜，如何解决？（面试）\n  ","tags":["hadoop","MapReduce"]},{"title":"MapReduce编程（二）","url":"/2019/10/20/hadoop/MapReduce编程（二）/","content":"\n# MapReduce编程模型（二）\n\n### 1. 自定义分区\n\n#### 1.1 分区原理\n\n- 根据之前讲的shuffle，我们知道在map任务中，从环形缓冲区溢出写磁盘时，会先对kv对数据进行分区操作\n\n- 分区操作是由MR中的分区器负责的\n\n- MapReduce有自带的默认分区器\n\n  - **HashPartitioner**\n  - 关键方法getPartition返回当前键值对的**分区索引**(partition index)\n\n  ```java\n  public class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n  \n    public void configure(JobConf job) {}\n  \n    /** Use {@link Object#hashCode()} to partition. */\n    public int getPartition(K2 key, V2 value, int numReduceTasks) {\n      return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n  }\n  ```\n\n- 环形缓冲区溢出写磁盘前，将每个kv对，作为getPartition()的参数传入；\n\n- 先对键值对中的key求hash值（int类型），与MAX_VALUE按位与；再模上reduce task个数，假设reduce task个数设置为4（可在程序中使用job.setNumReduceTasks(4)指定reduce task个数为4）\n\n  - 那么map任务溢出文件有**4个分区**，分区index分别是0、1、2、3\n  - getPartition()结果有四种：0、1、2、3\n  - 根据计算结果，决定当前kv对，落入哪个分区，如结果是0，则当前kv对落入溢出文件的0分区中\n  - 最终被相应的reduce task通过http获得\n\n![](assets/Image201906280826.png)\n\n![](assets/Image201906272145.png)\n\n- 若是MR默认分区器，不满足需求；可根据业务逻辑，设计自定义分区器，比如实现图上的功能\n\n#### 1.2 默认分区\n\n> 程序执行略\n>\n> 代码详见工程com.kaikeba.hadoop.partitioner包\n\n- MR读取三个文件part1.txt、part2.txt、part3.txt；三个文件放到HDFS目录：/customParttitioner中\n\n  ![](assets/Image201909061640.png)\n  \n- part1.txt内容如下：\n\n  ```\n  Dear Bear River\n  Dear Car\n  ```\n  \n- part2.txt内容如下：\n\n  ```\n  Car Car River\n  Dear Bear\n  ```\n\n- part3.txt内容如下：\n\n  ```\n  Dear Car Bear\n  Car Car\n  ```\n\n- 默认HashPartitioner分区时，查看结果（看代码）\n\n![](assets/Image201906272204.png)\n\n- 运行参数：\n\n```shell\n/customParttitioner /cp01\n```\n\n- 打jar包运行，结果如下：\n\n![](assets/Image201906272210.png)\n\n> 只有part-r-00001、part-r-00003有数据；另外两个没有数据\n>\n> HashPartitioner将Bear分到index=1的分区；将Car|Dear|River分到index=3分区\n\n#### 1.3 自定义分区\n\n**1.3.1** 需求\n\n- 自定义分区，使得文件中，分别以Dear、Bear、River、Car为键的键值对，分别落到index是0、1、2、3的分区中\n\n**1.3.2** 逻辑分析\n\n- 若要实现以上的分区策略，需要自定义分区类\n  - 此类实现Partitioner接口\n  - 在getPartition()中实现分区逻辑\n- main方法中\n  - **设定reduce个数**为4\n  - 设置自定义的分区类，调用job.setPartitionerClass方法\n\n**1.3.3** MR代码\n\n> 完整代码见代码工程\n\n- 自定义分区类如下\n\n```java\npackage com.kaikeba.hadoop.partitioner;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\nimport java.util.HashMap;\n\npublic class CustomPartitioner extends Partitioner<Text, IntWritable> {\n    public static HashMap<String, Integer> dict = new HashMap<String, Integer>();\n\n    //定义每个键对应的分区index，使用map数据结构完成\n    static{\n        dict.put(\"Dear\", 0);\n        dict.put(\"Bear\", 1);\n        dict.put(\"River\", 2);\n        dict.put(\"Car\", 3);\n    }\n\n    public int getPartition(Text text, IntWritable intWritable, int i) {\n        //\n        int partitionIndex = dict.get(text.toString());\n        return partitionIndex;\n    }\n}\n```\n\n\n\n![](assets/Image201906272213.png)\n\n- 运行结果\n\n![](assets/Image201906272217.png)\n\n> 结果满足需求\n\n#### 1.4 总结\n\n- 如果默认分区器不满足业务需求，可以自定义分区器\n  - 自定义分区器的类继承Partitioner类\n  - 覆写getPartition()，在方法中，定义自己的分区策略\n  - 在main()方法中调用job.setPartitionerClass()\n  - main()中设置reduce任务数\n\n\n\n### 2. 自定义Combiner\n\n#### 2.1 需求\n\n- 普通的MR是reduce通过http，取得map任务的分区结果；具体的聚合出结果是在reduce端进行的；\n\n- 以单词计数为例：\n  - 下图中的第一个map任务(map1)，本地磁盘中的结果有5个键值对：(Dear, 1)、(Bear, 1)、(River, 1)、(Dear, 1)、(Car, 1)\n  - 其中，map1中的两个相同的键值对(Dear, 1)、(Dear, 1)，会被第一个reduce任务(reduce1)通过网络拉取到reduce1端\n  - 那么假设map1中(Dear, 1)有1亿个呢？按原思路，map1端需要存储1亿个(Dear, 1)，再将1亿个(Dear, 1)通过网络被reduce1获得，然后再在reduce1端汇总\n  - 这样做map端本地磁盘IO、数据从map端到reduce端传输的网络IO比较大\n  - 那么想，能不能在reduce1从map1拉取1亿个(Dear, 1)之前，在map端就提前先做下reduce汇总，得到结果(Dear, 100000000)，然后再将这个结果（一个键值对）传输到reduce1呢？\n  - 答案是可以的\n  - 我们称之为combine操作\n  \n- map端combine本地聚合（**本质是reduce**）\n\n  ![](assets/Image201906280906.png)\n\n#### 2.2 逻辑分析\n\n- **<font color='red'>注意：</font>**\n\n  - **不论运行多少次Combine操作，都不能影响最终的结果**\n\n  - **并非**所有的mr都适合combine操作，比如求平均值 \n\n    **参考：《并非所有MR都适合combine.txt》**\n\n- 原理图\n\n  > 看原图\n\n![](assets/Image201909091014.png)\n\n- 当每个map任务的环形缓冲区添满80%，开始溢写磁盘文件\n\n- 此过程会分区、每个分区内按键排序、再combine操作（若设置了combine的话）、若设置map输出压缩的话则再压缩\n\n  - 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作；\n  - 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销）\n\n- combine本质上也是reduce；因为自定义的combine类继承自Reducer父类\n\n- map: (K1, V1) -> list(K2, V2)\n\n- combiner: (K2, list(V2)) -> (K2, V2)\n\n- reduce: (K2, list(V2)) -> (K3, V3)\n\n  - reduce函数与combine函数通常是一样的\n  - K3与K2类型相同；\n  - V3与V2类型相同\n  - 即reduce的输入的kv类型分别与输出的kv类型相同\n  \n  \n\n#### 2.3 MR代码\n\n> 对原词频统计代码做修改；\n>\n> 详细代码见代码工程\n\n- WordCountMap、WordCountReduce代码保持不变\n- 唯一需要做的修改是在WordCountMain中，增加job.**setCombinerClass**(WordCountReduce.class);\n- 修改如下：\n\n![](assets/Image201906272006.png)\n\n#### 2.4 小结\n\n- 使用combine时，首先考虑当前MR是否适合combine\n- 总原则是不论使不使用combine不能影响最终的结果\n- 在MR时，发生数据倾斜，且可以使用combine时，可以使用combine缓解数据倾斜\n\n\n\n### 3. MR压缩\n\n#### 3.1 需求\n\n- 作用：在MR中，为了减少磁盘IO及网络IO，可考虑在map端、reduce端设置压缩功能\n- 给“MapReduce编程：用户搜索次数”代码，增加压缩功能\n\n#### 3.2 逻辑分析\n\n- 那么如何设置压缩功能呢？只需在main方法中，给Configuration对象增加如下设置即可\n\n\n```java\n//开启map输出进行压缩的功能\nconfiguration.set(\"mapreduce.map.output.compress\", \"true\");\n//设置map输出的压缩算法是：BZip2Codec，它是hadoop默认支持的压缩算法，且支持切分\nconfiguration.set(\"mapreduce.map.output.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n//开启job输出压缩功能\nconfiguration.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n//指定job输出使用的压缩算法\nconfiguration.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n```\n\n#### 3.3 MR代码\n\n- 给“MapReduce编程：用户搜索次数”代码，增加压缩功能，代码如下\n\n  > 如何打jar包，已演示过，此处不再赘述\n\n```java\npackage com.kaikeba.hadoop.mrcompress;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n * 本MR示例，用于统计每个用户搜索并查看URL链接的次数\n */\npublic class UserSearchCount {\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        //判断以下，输入参数是否是两个，分别表示输入路径、输出路径\n        if (args.length != 2 || args == null) {\n            System.out.println(\"please input Path!\");\n            System.exit(0);\n        }\n\n        Configuration configuration = new Configuration();\n        //configuration.set(\"mapreduce.job.jar\",\"/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n        //开启map输出进行压缩的功能\n        configuration.set(\"mapreduce.map.output.compress\", \"true\");\n        //设置map输出的压缩算法是：BZip2Codec，它是hadoop默认支持的压缩算法，且支持切分\n        configuration.set(\"mapreduce.map.output.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n        //开启job输出压缩功能\n        configuration.set(\"mapreduce.output.fileoutputformat.compress\", \"true\");\n        //指定job输出使用的压缩算法\n        configuration.set(\"mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.BZip2Codec\");\n\n        //调用getInstance方法，生成job实例\n        Job job = Job.getInstance(configuration, UserSearchCount.class.getSimpleName());\n\n        //设置jar包，参数是包含main方法的类\n        job.setJarByClass(UserSearchCount.class);\n\n        //通过job设置输入/输出格式\n        //MR的默认输入格式是TextInputFormat，所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n        //设置输入/输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n//        FileOutputFormat.setCompressOutput(job, true);\n//        FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);\n\n        //设置处理Map阶段的自定义的类\n        job.setMapperClass(SearchCountMapper.class);\n        //设置map combine类，减少网路传出量\n        //job.setCombinerClass(WordCountReduce.class);\n        //设置处理Reduce阶段的自定义的类\n        job.setReducerClass(SearchCountReducer.class);\n\n        //如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n//        job.setMapOutputKeyClass(Text.class);\n//        job.setMapOutputValueClass(IntWritable.class);\n\n        //设置reduce task最终输出key/value的类型\n        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // 提交作业\n        job.waitForCompletion(true);\n    }\n\n    public static class SearchCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n        //定义共用的对象，减少GC压力\n        Text userIdKOut = new Text();\n        IntWritable vOut = new IntWritable(1);\n\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            //获得当前行的数据\n            //样例数据：20111230111645  169796ae819ae8b32668662bb99b6c2d        塘承高速公路规划线路图  1       1       http://auto.ifeng.com/roll/20111212/729164.shtml\n            String line = value.toString();\n\n            //切分，获得各字段组成的数组\n            String[] fields = line.split(\"\\t\");\n\n            //因为要统计每个user搜索并查看URL的次数，所以将userid放到输出key的位置\n            //注意：MR编程中，根据业务需求设计key是很重要的能力\n            String userid = fields[1];\n\n            //设置输出的key的值\n            userIdKOut.set(userid);\n            //输出结果\n            context.write(userIdKOut, vOut);\n        }\n    }\n\n    public static class SearchCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n        //定义共用的对象，减少GC压力\n        IntWritable totalNumVOut = new IntWritable();\n\n        @Override\n        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n            int sum = 0;\n\n            for(IntWritable value: values) {\n                sum += value.get();\n            }\n\n            //设置当前user搜索并查看总次数\n            totalNumVOut.set(sum);\n            context.write(key, totalNumVOut);\n        }\n    }\n}\n```\n\n- 生成jar包，并运行jar包\n\n```shell\n[hadoop@node01 target]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.mrcompress.UserSearchCount /sogou.2w.utf8 /compressed\n```\n\n- 查看结果\n\n  > 可增加数据量，查看使用压缩算法前后的系统各计数器的数据量变化\n\n```shell\n[hadoop@node01 target]$ hadoop fs -ls -h /compressed\n```\n\n![](assets/Image201908241707.png)\n\n#### 3.4 总结\n\n- MR过程中使用压缩可减少数据量，进而减少磁盘IO、网络IO数据量\n- 可设置map端输出的压缩\n- 可设置job最终结果的压缩\n- 通过相应的配置项即可实现\n\n\n\n### 4. 自定义InputFormat\n\n#### 4.1 MapReduce执行过程\n\n![](assets/Image201905211621.png)\n\n- 上图也描述了mapreduce的一个完整的过程；我们主要看map任务是如何从hdfs读取分片数据的部分\n\n  - 涉及3个关键的类\n\n  - ①InputFormat输入格式类\n    \n    ②InputSplit输入分片类：getSplits()\n    \n    - InputFormat输入格式类将输入文件分成一个个分片InputSplit\n    - 每个Map任务对应一个split分片\n    \n    ③RecordReader记录读取器类：createRecordReader()\n    \n    - RecordReader（记录读取器）读取分片数据，一行记录生成一个键值对\n    - 传入map任务的map()方法，调用map()\n    \n    ![](assets/Image201910161117.png)\n    \n    \n\n- 所以，如果需要根据自己的业务情况，自定义输入的话，需要自定义两个类：\n  - InputFormat类\n  - RecordReader类\n\n- 详细流程：\n\n  - 客户端调用InputFormat的**getSplits()**方法，获得输入文件的分片信息\n\n    ![](assets/Image201909111008.png)\n\n  - 针对每个MR job会生成一个相应的app master，负责map\\reduce任务的调度及监控执行情况\n\n  - 将分片信息传递给MR job的app master\n\n  - app master根据分片信息，尽量将map任务尽量调度在split分片数据所在节点（**移动计算不移动数据**）\n\n    ![](assets/Image201909111013.png)\n\n  - 有几个分片，就生成几个map任务\n  \n  - 每个map任务将split分片传递给createRecordReader()方法，生成此分片对应的RecordReader\n  \n  - RecordReader用来读取分片的数据，生成记录的键值对\n  \n    - nextKeyValue()判断是否有下一个键值对，如果有，返回true；否则，返回false\n    - 如果返回true，调用getCurrentKey()获得当前的键\n    - 调用getCurrentValue()获得当前的值\n  \n  - map任务运行过程\n  \n    ![](assets/Image201909111022.png)\n  \n    - map任务运行时，会调用run()\n  \n    - 首先运行一次setup()方法；只在map任务启动时，运行一次；一些初始化的工作可以在setup方法中完成；如要连接数据库之类的操作\n  \n    - while循环，调用context.nextKeyValue()；会委托给RecordRecord的nextKeyValue()，判断是否有下一个键值对\n  \n    - 如果有下一个键值对，调用context.getCurrentKey()、context.getCurrentValue()获得当前的键、值的值（也是调用RecordReader的同名方法）\n  \n      ![](assets/Image201909111045.png)\n  \n    - 作为参数传入map(key, value, context)，调用一次map()\n  \n    - 当读取分片尾，context.nextKeyValue()返回false；退出循环\n  \n    - 调用cleanup()方法，只在map任务结束之前，调用一次；所以，一些回收资源的工作可在此方法中实现，如关闭数据库连接\n\n#### 4.2 需求\n\n- 无论hdfs还是mapreduce，处理小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案\n\n#### 4.3 逻辑分析\n\n- 小文件的优化无非以下几种方式：\n  - 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS(SequenceFile方案)\n  - 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并；可使用**自定义InputFormat**实现\n  - 在mapreduce处理时，可采用**CombineFileInputFormat**提高效率\n- 本例使用第二种方案，自定义输入格式\n\n#### 4.4 MR代码\n\n- 自定义InputFormat\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.JobContext;\n  import org.apache.hadoop.mapreduce.RecordReader;\n  import org.apache.hadoop.mapreduce.TaskAttemptContext;\n  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n  \n  import java.io.IOException;\n  \n  /**\n   * 自定义InputFormat类；\n   * 泛型：\n   *  键：因为不需要使用键，所以设置为NullWritable\n   *  值：值用于保存小文件的内容，此处使用BytesWritable\n   */\n  public class WholeFileInputFormat extends FileInputFormat<NullWritable, BytesWritable> {\n  \n      /**\n       *\n       * 返回false，表示输入文件不可切割\n       * @param context\n       * @param file\n       * @return\n       */\n      @Override\n      protected boolean isSplitable(JobContext context, Path file) {\n          return false;\n      }\n  \n      /**\n       * 生成读取分片split的RecordReader\n       * @param split\n       * @param context\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public RecordReader<NullWritable, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException,InterruptedException {\n          //使用自定义的RecordReader类\n          WholeFileRecordReader reader = new WholeFileRecordReader();\n          //初始化RecordReader\n          reader.initialize(split, context);\n          return reader;\n      }\n  }\n  ```\n\n- 自定义RecordReader\n\n  实现6个相关方法\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.conf.Configuration;\n  import org.apache.hadoop.fs.FSDataInputStream;\n  import org.apache.hadoop.fs.FileSystem;\n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.IOUtils;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.RecordReader;\n  import org.apache.hadoop.mapreduce.TaskAttemptContext;\n  import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n  \n  import java.io.IOException;\n  \n  /**\n   *\n   * RecordReader的核心工作逻辑：\n   * 通过nextKeyValue()方法去读取数据构造将返回的key   value\n   * 通过getCurrentKey 和 getCurrentValue来返回上面构造好的key和value\n   *\n   * @author\n   */\n  public class WholeFileRecordReader extends RecordReader<NullWritable, BytesWritable> {\n  \n      //要读取的分片\n      private FileSplit fileSplit;\n      private Configuration conf;\n  \n      //读取的value数据\n      private BytesWritable value = new BytesWritable();\n      /**\n       *\n       * 标识变量，分片是否已被读取过；因为小文件设置成了不可切分，所以一个小文件只有一个分片；\n       * 而这一个分片的数据，只读取一次，一次读完所有数据\n       * 所以设置此标识\n       */\n      private boolean processed = false;\n  \n      /**\n       * 初始化\n       * @param split\n       * @param context\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public void initialize(InputSplit split, TaskAttemptContext context)\n              throws IOException, InterruptedException {\n          this.fileSplit = (FileSplit) split;\n          this.conf = context.getConfiguration();\n      }\n  \n      /**\n       * 判断是否有下一个键值对。若有，则读取分片中的所有的数据\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public boolean nextKeyValue() throws IOException, InterruptedException {\n          if (!processed) {\n              byte[] contents = new byte[(int) fileSplit.getLength()];\n              Path file = fileSplit.getPath();\n              FileSystem fs = file.getFileSystem(conf);\n              FSDataInputStream in = null;\n              try {\n                  in = fs.open(file);\n                  IOUtils.readFully(in, contents, 0, contents.length);\n                  value.set(contents, 0, contents.length);\n              } finally {\n                  IOUtils.closeStream(in);\n              }\n              processed = true;\n              return true;\n          }\n          return false;\n      }\n  \n      /**\n       * 获得当前的key\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public NullWritable getCurrentKey() throws IOException,\n              InterruptedException {\n          return NullWritable.get();\n      }\n  \n      /**\n       * 获得当前的value\n       * @return\n       * @throws IOException\n       * @throws InterruptedException\n       */\n      @Override\n      public BytesWritable getCurrentValue() throws IOException,\n              InterruptedException {\n          return value;\n      }\n  \n      /**\n       * 获得分片读取的百分比；因为如果读取分片数据的话，会一次性的读取完；所以进度要么是1，要么是0\n       * @return\n       * @throws IOException\n       */\n      @Override\n      public float getProgress() throws IOException {\n          //因为一个文件作为一个整体处理，所以，如果processed为true，表示已经处理过了，进度为1；否则为0\n          return processed ? 1.0f : 0.0f;\n      }\n  \n      @Override\n      public void close() throws IOException {\n      }\n  }\n  ```\n\n- main方法\n\n  ```java\n  package com.kaikeba.hadoop.inputformat;\n  \n  import org.apache.hadoop.conf.Configuration;\n  import org.apache.hadoop.conf.Configured;\n  import org.apache.hadoop.fs.Path;\n  import org.apache.hadoop.io.BytesWritable;\n  import org.apache.hadoop.io.NullWritable;\n  import org.apache.hadoop.io.Text;\n  import org.apache.hadoop.mapreduce.InputSplit;\n  import org.apache.hadoop.mapreduce.Job;\n  import org.apache.hadoop.mapreduce.Mapper;\n  import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n  import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\n  import org.apache.hadoop.util.Tool;\n  import org.apache.hadoop.util.ToolRunner;\n  \n  import java.io.IOException;\n  \n  /**\n   * 让主类继承Configured类，实现Tool接口\n   * 实现run()方法\n   * 将以前main()方法中的逻辑，放到run()中\n   * 在main()中，调用ToolRunner.run()方法，第一个参数是当前对象；第二个参数是输入、输出\n   */\n  public class SmallFiles2SequenceFile extends Configured implements Tool {\n  \n      /**\n       * 自定义Mapper类\n       * mapper类的输入键值对类型，与自定义InputFormat的输入键值对保持一致\n       * mapper类的输出的键值对类型，分别是文件名、文件内容\n       */\n      static class SequenceFileMapper extends\n              Mapper<NullWritable, BytesWritable, Text, BytesWritable> {\n  \n          private Text filenameKey;\n  \n          /**\n           * 取得文件名\n           * @param context\n           * @throws IOException\n           * @throws InterruptedException\n           */\n          @Override\n          protected void setup(Context context) throws IOException,\n                  InterruptedException {\n              InputSplit split = context.getInputSplit();\n              //获得当前文件路径\n              Path path = ((FileSplit) split).getPath();\n              filenameKey = new Text(path.toString());\n          }\n  \n          @Override\n          protected void map(NullWritable key, BytesWritable value,\n                             Context context) throws IOException, InterruptedException {\n              context.write(filenameKey, value);\n          }\n      }\n  \n      public int run(String[] args) throws Exception {\n          Configuration conf = new Configuration();\n          Job job = Job.getInstance(conf,\"combine small files to sequencefile\");\n          job.setJarByClass(SmallFiles2SequenceFile.class);\n  \n          //设置自定义输入格式\n          job.setInputFormatClass(WholeFileInputFormat.class);\n  \n          WholeFileInputFormat.addInputPath(job,new Path(args[0]));\n          //设置输出格式SequenceFileOutputFormat及输出路径\n          job.setOutputFormatClass(SequenceFileOutputFormat.class);\n          SequenceFileOutputFormat.setOutputPath(job,new Path(args[1]));\n  \n          job.setOutputKeyClass(Text.class);\n          job.setOutputValueClass(BytesWritable.class);\n          job.setMapperClass(SequenceFileMapper.class);\n          return job.waitForCompletion(true) ? 0 : 1;\n      }\n  \n      public static void main(String[] args) throws Exception {\n          int exitCode = ToolRunner.run(new SmallFiles2SequenceFile(),\n                  args);\n          System.exit(exitCode);\n  \n      }\n  }\n  ```\n\n#### 4.5 总结\n\n- 若要自定义InputFormat的话\n  - 需要自定义InputFormat类，并覆写getRecordReader()方法\n  - 自定义RecordReader类，实现方法\n    - initialize()\n    - nextKeyValue()\n    - getCurrentKey()\n    - getCurrentValue()\n    - getProgress()\n    - close()\n\n\n\n## 5、拓展点、未来计划、行业趋势\n\n1. MR中还有一些自带的输入格式，扩展阅读：《Hadoop权威指南 第4版》8.2 输入格式\n\n   ![](assets/Image201909091251.png)\n\n   \n","tags":["hadoop","MapReduce"]},{"title":"MapRedecer编程（一）","url":"/2019/10/15/hadoop/MapRedecer编程（一）/","content":"\n# MapReduce编程模型\n\n\n## 一、知识要点\n\n### 1. MapReduce编程模型\n\n- Hadoop架构图\n\n  Hadoop由HDFS分布式存储、**MapReduce分布式计算**、Yarn资源调度三部分组成\n\n![](assets/Image201906191834-1562922704761.png)\n\n- MapReduce是采用一种**分而治之**的思想设计出来的分布式计算框架\n- MapReduce由两个阶段组成：\n  - Map阶段（切分成一个个小的任务）\n  - Reduce阶段（汇总小任务的结果）\n- 那什么是分而治之呢？\n  - 比如一复杂、计算量大、耗时长的的任务，暂且称为“大任务”；\n  - 此时使用单台服务器无法计算或较短时间内计算出结果时，可将此大任务切分成一个个小的任务，小任务分别在不同的服务器上**并行**的执行\n  - 最终再汇总每个小任务的结果\n\n![](assets/Image201906251747.png)\n\n#### 1.1 Map阶段\n\n- map阶段有一个关键的map()函数；\n- 此函数的输入是**键值对**\n- 输出是一系列**键值对**，输出写入**本地磁盘**。\n\n#### 1.2 Reduce阶段\n\n- reduce阶段有一个关键的函数reduce()函数\n\n- 此函数的输入也是键值对（即map的输出（kv对））\n\n- 输出也是一系列键值对，结果最终写入HDFS\n\n#### 1.3 Map&Reduce\n\n![](assets/Image201906251807.png)\n\n\n\n### 2. MapReduce编程示例\n\n- 以**MapReduce的词频统计**为例：统计一批英文文章当中，每个单词出现的总次数\n\n#### 2.1 MapReduce原理图\n\n![](assets/Image201906271715.png)\n\n- Map阶段\n  - 假设MR的输入文件“**Gone With The Wind**”有三个block；block1、block2、block3 \n  - MR编程时，每个block对应一个分片split\n  - 每一个split对应一个map任务（map task）\n  - 如图共3个map任务（map1、map2、map3）；这3个任务的逻辑一样，所以以第一个map任务（map1）为例分析 \n  - map1读取block1的数据；一次读取block1的一行数据；\n    - 产生键值对(key/value)，作为map()的参数传入，调用map()；\n    - 假设当前所读行是第一行\n    - 将当前所读行的行首相对于当前block开始处的字节偏移量作为key（0）\n    - 当前行的内容作为value（Dear Bear River）\n  - map()内\n    - (按需求，写业务代码)，将value当前行内容按空格切分，得到三个单词Dear | Bear | River\n    - 将每个单词变成键值对，输出出去(Dear, 1) | (Bear, 1) | (River, 1)；最终结果写入map任务所在节点的本地磁盘中（内里还有细节，讲到shuffle时，再细细展开）\n    - block的第一行的数据被处理完后，接着处理第二行；逻辑同上\n    - 当map任务将当前block中所有的数据全部处理完后，此map任务即运行结束\n  - 其它的每一个map任务都是如上逻辑，不再赘述\n- Reduce阶段\n  - reduce任务（reduce task）的个数由自己写的程序编程指定，main()内的job.setNumReduceTasks(4)指定reduce任务是4个（reduce1、reduce2、reduce3、reduce4）\n  - 每一个reduce任务的逻辑一下，所以以第一个reduce任务（reduce1）为例分析\n  - map1任务完成后，reduce1通过网络，连接到map1，将map1输出结果中属于reduce1的分区的数据，通过网络获取到reduce1端（拷贝阶段）\n  - 同样也如此连接到map2、map3获取结果\n  - 最终reduce1端获得4个(Dear, 1)键值对；由于key键相同，它们分到同一组；\n  - 4个(Dear, 1)键值对，转换成[Dear, Iterable(1, 1, 1, )]，作为两个参数传入reduce()\n  - 在reduce()内部，计算Dear的总数为4，并将(Dear, 4)作为键值对输出\n  - 每个reduce任务最终输出文件（内里还有细节，讲到shuffle时，再细细展开），文件写入到HDFS\n\n#### 2.2 MR中key的作用\n\n- <font color='red'>**MapReduce编程中，key有特殊的作用**</font>\n\n  - **①数据中，若要针对某个值进行分组、聚合时，需将此值作为MR中的reduce的输入的key**\n\n  - **如当前的词频统计例子，按单词进行分组，每组中对出现次数做聚合（计算总和）；所以需要将每个单词作为reduce输入的key，MapReduce框架自动按照单词分组，进而求出每组即每个单词的总次数**\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201910141101.png)\n\n  - **②另外，key还具有可排序的特性，因为MR中的key类需要实现WritableComparable接口；而此接口又继承Comparable接口（可查看源码）**\n\n  - **MR编程时，要充分利用以上两点；结合实际业务需求，设置合适的key**\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201908221717.png)\n\n    ![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第九章MapReduce课件/20191014-MR-第一次/assets/Image201908221718.png)\n\n\n\n#### 2.4 MR参考代码\n\n**2.4.1 Mapper代码**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * 类Mapper<LongWritable, Text, Text, IntWritable>的四个泛型分别表示\n * map方法的输入的键的类型kin、值的类型vin；输出的键的类型kout、输出的值的类型vout\n * kin指的是当前所读行行首相对于split分片开头的字节偏移量,所以是long类型，对应序列化类型LongWritable\n * vin指的是当前所读行，类型是String，对应序列化类型Text\n * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text\n * vout根据需求，输出值指的是单词的个数，1，类型是int，对应序列化类型是IntWritable\n *\n */\npublic class WordCountMap extends Mapper<LongWritable, Text, Text, IntWritable> {\n\n    /**\n     * 处理分片split中的每一行的数据；针对每行数据，会调用一次map方法\n     * 在一次map方法调用时，从一行数据中，获得一个个单词word，再将每个单词word变成键值对形式(word, 1)输出出去\n     * 输出的值最终写到本地磁盘中\n     * @param key 当前所读行行首相对于split分片开头的字节偏移量\n     * @param value  当前所读行\n     * @param context\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public void map(LongWritable key, Text value, Context context)\n            throws IOException, InterruptedException {\n        //当前行的示例数据(单词间空格分割)：Dear Bear River\n        //取得当前行的数据\n        String line = value.toString();\n        //按照\\t进行分割，得到当前行所有单词\n        String[] words = line.split(\"\\t\");\n\n        for (String word : words) {\n            //将每个单词word变成键值对形式(word, 1)输出出去\n            //同样，输出前，要将kout, vout包装成对应的可序列化类型，如String对应Text，int对应IntWritable\n            context.write(new Text(word), new IntWritable(1));\n        }\n    }\n}\n\n```\n\n**2.4.2 Reducer代码**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\n/**\n *\n * Reducer<Text, IntWritable, Text, IntWritable>的四个泛型分别表示\n * reduce方法的输入的键的类型kin、输入值的类型vin；输出的键的类型kout、输出的值的类型vout\n * 注意：因为map的输出作为reduce的输入，所以此处的kin、vin类型分别与map的输出的键类型、值类型相同\n * kout根据需求，输出键指的是单词，类型是String，对应序列化类型是Text\n * vout根据需求，输出值指的是每个单词的总个数，类型是int，对应序列化类型是IntWritable\n *\n */\npublic class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {\n    /**\n     *\n     * key相同的一组kv对，会调用一次reduce方法\n     * 如reduce task汇聚了众多的键值对，有key是hello的键值对，也有key是spark的键值对，如下\n     * (hello, 1)\n     * (hello, 1)\n     * (hello, 1)\n     * (hello, 1)\n     * ...\n     * (spark, 1)\n     * (spark, 1)\n     * (spark, 1)\n     *\n     * 其中，key是hello的键值对被分成一组；merge成[hello, Iterable(1,1,1,1)]，调用一次reduce方法\n     * 同样，key是spark的键值对被分成一组；merge成[spark, Iterable(1,1,1)]，再调用一次reduce方法\n     *\n     * @param key 当前组的key\n     * @param values 当前组中，所有value组成的可迭代集和\n     * @param context reduce上下文环境对象\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public void reduce(Text key, Iterable<IntWritable> values,\n                          Context context) throws IOException, InterruptedException {\n        //定义变量，用于累计当前单词出现的次数\n        int sum = 0;\n\n        for (IntWritable count : values) {\n            //从count中获得值，累加到sum中\n            sum += count.get();\n        }\n\n        //将单词、单词次数，分别作为键值对，输出\n        context.write(key, new IntWritable(sum));// 输出最终结果\n    };\n}\n```\n\n**2.4.3 Main程序入口**\n\n```java\npackage com.kaikeba.hadoop.wordcount;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport java.io.IOException;\n\n/**\n *\n * MapReduce程序入口\n * 注意：\n *  导包时，不要导错了；\n *  另外，map\\reduce相关的类，使用mapreduce包下的，是新API，如org.apache.hadoop.mapreduce.Job;；\n */\npublic class WordCountMain {\n    //若在IDEA中本地执行MR程序，需要将mapred-site.xml中的mapreduce.framework.name值修改成local\n    //参数 c:/test/README.txt c:/test/wc\n    public static void main(String[] args) throws IOException,\n            ClassNotFoundException, InterruptedException {\n\n        //判断一下，输入参数是否是两个，分别表示输入路径、输出路径\n       if (args.length != 2 || args == null) {\n            System.out.println(\"please input Path!\");\n            System.exit(0);\n        }\n\n        Configuration configuration = new Configuration();\n        //configuration.set(\"mapreduce.framework.name\",\"local\");\n\n\n        //告诉程序，要运行的jar包在哪\n        //configuration.set(\"mapreduce.job.jar\",\"/home/hadoop/IdeaProjects/Hadoop/target/com.kaikeba.hadoop-1.0-SNAPSHOT.jar\");\n\n        //调用getInstance方法，生成job实例\n        Job job = Job.getInstance(configuration, WordCountMain.class.getSimpleName());\n\n        //设置job的jar包，如果参数指定的类包含在一个jar包中，则此jar包作为job的jar包； 参数class跟主类在一个工程即可；一般设置成主类\n//        job.setJarByClass(WordCountMain.class);\n        job.setJarByClass(WordCountMain.class);\n\n        //通过job设置输入/输出格式\n        //MR的默认输入格式是TextInputFormat，输出格式是TextOutputFormat；所以下两行可以注释掉\n//        job.setInputFormatClass(TextInputFormat.class);\n//        job.setOutputFormatClass(TextOutputFormat.class);\n\n        //设置输入/输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        //设置处理Map阶段的自定义的类\n        job.setMapperClass(WordCountMap.class);\n        //设置map combine类，减少网路传出量\n        job.setCombinerClass(WordCountReduce.class);\n        //设置处理Reduce阶段的自定义的类\n        job.setReducerClass(WordCountReduce.class);\n        //注意：如果map、reduce的输出的kv对类型一致，直接设置reduce的输出的kv对就行；如果不一样，需要分别设置map, reduce的输出的kv类型\n        //注意：此处设置的map输出的key/value类型，一定要与自定义map类输出的kv对类型一致；否则程序运行报错\n//        job.setMapOutputKeyClass(Text.class);\n//        job.setMapOutputValueClass(IntWritable.class);\n\n        //设置reduce task最终输出key/value的类型\n        //注意：此处设置的reduce输出的key/value类型，一定要与自定义reduce类输出的kv对类型一致；否则程序运行报错\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        // 提交作业\n        job.waitForCompletion(true);\n\n    }\n}\n```\n\n> 程序运行有两种方式，分别是windows本地运行、集群运行，依次演示\n\n\n#### 2.5 集群运行\n\n- 用maven插件打jar包；①点击Maven，②双击package打包\n\n```shell\n[hadoop@node01 ~]$ hadoop jar com.kaikeba.hadoop-1.0-SNAPSHOT.jar com.kaikeba.hadoop.wordcount.WordCountMain /README.txt /wordcount01\n```\n\n> 说明：\n>\n> com.kaikeba.hadoop-1.0-SNAPSHOT.jar是jar包名\n>\n> com.kaikeba.hadoop.wordcount.WordCountMain是包含main方法的类的全限定名\n>\n> /NOTICE.txt和/wordcount是main方法的两个参数，表示输入路径、输出路径\n\n![](assets/hadoop jar.gif)\n\n- 确认结果\n\n```shell\n[hadoop@node01 ~]$ hadoop fs -ls /wordcount01\n```\n\n![](assets/Image201908221620.png)\n\n#### 2.6 总结\n\n- MR分为两个阶段：map阶段、reduce阶段\n- MR输入的文件有几个block，就会生成几个map任务\n- MR的reduce任务的个数，由程序中编程指定：job.setNumReduceTasks(4)\n- map任务\n  - map任务中map()一次读取block的一行数据，以kv对的形式输入map()\n  - map()的输出作为reduce()的输入\n- reduce任务\n  - reduce任务通过网络将各执行完成的map任务输出结果中，属于自己的数据取过来\n  - key相同的键值对作为一组，调用一次reduce()\n  - reduce任务生成一个结果文件\n  - 文件写入HDFS\n\n\n\n### 3. WEB UI查看结果\n\n#### 3.1 Yarn\n\n> node01是resourcemanager所在节点主机名，根据自己的实际情况修改主机名\n\n浏览器访问url地址：http://node01:8088\n\n![](assets/Image201908221638.png)\n\n#### 3.2 HDFS结果\n\n浏览器输入URL：http://node01:50070\n\n①点击下拉框；②浏览文件系统；③输入根目录，查看hdfs根路径中的内容\n\n![](assets/Image201908221639.png)\n\n\n\n### 4. Shuffle\n\n- shuffle主要指的是map端的输出作为reduce端输入的过程\n\n#### 4.1 shuffle简图\n\n![](assets/Image201905231409.png)\n\n#### 4.2 shuffle细节图\n\n![](assets/Image201906280906.png)\n\n- 分区用到了分区器，默认分区器是HashPartitioner\n\n  源码：\n\n  ```java\n  public class HashPartitioner<K2, V2> implements Partitioner<K2, V2> {\n  \n    public void configure(JobConf job) {}\n  \n    /** Use {@link Object#hashCode()} to partition. */\n    public int getPartition(K2 key, V2 value,\n                            int numReduceTasks) {\n      return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n    }\n  \n  }\n  ```\n\n#### 4.3 map端\n\n  - 每个map任务都有一个对应的环形内存缓冲区；输出是kv对，先写入到环形缓冲区（默认大小100M），当内容占据80%缓冲区空间后，由一个后台线程将缓冲区中的数据溢出写到一个磁盘文件\n  - 在溢出写的过程中，map任务可以继续向环形缓冲区写入数据；但是若写入速度大于溢出写的速度，最终造成100m占满后，map任务会暂停向环形缓冲区中写数据的过程；只执行溢出写的过程；直到环形缓冲区的数据全部溢出写到磁盘，才恢复向缓冲区写入\n  - 后台线程溢写磁盘过程，有以下几个步骤：\n    - 先对每个溢写的kv对做分区；分区的个数由MR程序的reduce任务数决定；默认使用HashPartitioner计算当前kv对属于哪个分区；计算公式：(key.hashCode() & Integer.MAX_VALUE) % numReduceTasks\n    - 每个分区中，根据kv对的key做内存中排序；\n    - 若设置了map端本地聚合combiner，则对每个分区中，排好序的数据做combine操作；\n    - 若设置了对map输出压缩的功能，会对溢写数据压缩\n  - 随着不断的向环形缓冲区中写入数据，会多次触发溢写（每当环形缓冲区写满100m），本地磁盘最终会生成多个溢出文件\n  - 合并溢写文件：在map task完成之前，所有溢出文件会被合并成一个大的溢出文件；且是已分区、已排序的输出文件\n  - 小细节：\n    - 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作；\n    - 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销）\n\n#### 4.4 reduce端\n\n- reduce task会在每个map task运行完成后，通过HTTP获得map task输出中，属于自己的分区数据（许多kv对）\n\n- 如果map输出数据比较小，先保存在reduce的jvm内存中，否则直接写入reduce磁盘\n\n- 一旦内存缓冲区达到阈值（默认0.66）或map输出数的阈值（默认1000），则触发**归并merge**，结果写到本地磁盘\n\n- 若MR编程指定了combine，在归并过程中会执行combine操作\n\n- 随着溢出写的文件的增多，后台线程会将它们合并大的、排好序的文件\n\n- reduce task将所有map task复制完后，将合并磁盘上所有的溢出文件\n\n- 默认一次合并10个\n\n- 最后一批合并，部分数据来自内存，部分来自磁盘上的文件\n\n- 进入“归并、排序、分组阶段”\n\n- 每组数据调用一次reduce方法\n\n- 参考文件《**reduce端merge 排序 分组.txt**》\n\n\n\n#### 4.5 总结\n\n- map端\n  - map()输出结果先写入环形缓冲区\n  - 缓冲区100M；写满80M后，开始溢出写磁盘文件\n  - 此过程中，会进行分区、排序、combine（可选）、压缩（可选）\n  - map任务完成前，会将多个小的溢出文件，合并成一个大的溢出文件（已分区、排序）\n- reduce端\n  - 拷贝阶段：reduce任务通过http将map任务属于自己的分区数据拉取过来\n  - 开始merge及溢出写磁盘文件\n  - 所有map任务的分区全部拷贝过来后，进行阶段合并、排序、分组阶段\n  - 每组数据调用一次reduce()\n  - 结果写入HDFS\n","tags":["-hadoop -mapreduce"]},{"title":"HDFS文件系统","url":"/2019/10/14/hadoop/HDFS文件系统/","content":"\n#  HDFS分布式文件系统\n\n### 1. HDFS读写流程\n\n#### 1.1 数据写流程\n\n![1557999856839](img/1557999856839.png)\n\n![HDFS写入文件流程](img/HDFS写入文件流程.png)\n\n**1.1 详细流程**\n\n- 创建文件：\t\n\n  - HDFS客户端向HDFS写数据，先调用DistributedFileSystem.create()方法，在HDFS创建新的空文件\n  - RPC（ClientProtocol.create()）远程过程调用NameNode（NameNodeRpcServer）的create()，首先在HDFS目录树指定路径添加新文件\n  - 然后将创建新文件的操作记录在editslog中\n  - NameNode.create方法执行完后，DistributedFileSystem.create()返回FSDataOutputStream，它本质是封装了一个DFSOutputStream对象\n\n- 建立数据流管道：\n\n  - 客户端调用DFSOutputStream.write()写数据\n  - DFSOutputStream调用ClientProtocol.addBlock()，首先向NameNode申请一个空的数据块\n  - addBlock()返回LocatedBlock对象，对象包含当前数据块的所有datanode的位置信息\n  - 根据位置信息，建立数据流管道\n\n- 向数据流管道pipeline中写当前块的数据：\n\n  - 客户端向流管道中写数据，先将数据写入一个检验块chunk中，大小512Byte，写满后，计算chunk的检验和checksum值（4Byte）\n  - 然后将chunk数据本身加上checksum，形成一个带checksum值的chunk（516Byte）\n  - 保存到一个更大一些的结构**packet数据包**中，packet为64kB大小\n- packet写满后，先被写入一个**dataQueue**队列中\n  - packet被从队列中取出，向pipeline中写入，先写入datanode1，再从datanoe1传到datanode2，再从datanode2传到datanode3中\n- 一个packet数据取完后，后被放入到**ackQueue**中等待pipeline关于该packet的ack的反馈\n  - 每个packet都会有ack确认包，逆pipeline（dn3 -> dn2 -> dn1）传回输出流\n- 若packet的ack是SUCCESS成功的，则从ackQueue中，将packet删除；否则，将packet从ackQueue中取出，重新放入dataQueue，重新发送\n  - 如果当前块写完后，文件还有其它块要写，那么再调用addBlock方法（**流程同上**）\n- 文件最后一个block块数据写完后，会再发送一个空的packet，表示当前block写完了，然后关闭pipeline\n  - 所有块写完，close()关闭流\n- ClientProtocol.complete()通知namenode当前文件所有块写完了\n\n**6.1.2 容错**\n\n- 在写的过程中，pipeline中的datanode出现故障（如网络不通），输出流如何恢复\n  - 输出流中ackQueue缓存的所有packet会被重新加入dataQueue\n  - 输出流调用ClientProtocol.updateBlockForPipeline()，为block申请一个新的时间戳，namenode会记录新时间戳\n  - 确保故障datanode即使恢复，但由于其上的block时间戳与namenode记录的新的时间戳不一致，故障datanode上的block进而被删除\n  - 故障的datanode从pipeline中删除\n  - 输出流调用ClientProtocol.getAdditionalDatanode()通知namenode分配新的datanode到数据流pipeline中，并使用新的时间戳建立pipeline\n  - 新添加到pipeline中的datanode，目前还没有存储这个新的block，HDFS客户端通过DataTransferProtocol通知pipeline中的一个datanode复制这个block到新的datanode中\n  - pipeline重建后，输出流调用ClientProtocol.updatePipeline()，更新namenode中的元数据\n  - 故障恢复完毕，完成后续的写入流程\n\n#### 1.2 数据读流程\n\n**1.2.1 基本流程**\n\n![HDFS文件读取流程](img/HDFS文件读取流程.png)\n\n- 1、client端读取HDFS文件，client调用文件系统对象DistributedFileSystem的open方法\n- 2、返回FSDataInputStream对象（对DFSInputStream的包装）\n- 3、构造DFSInputStream对象时，调用namenode的getBlockLocations方法，获得file的开始若干block（如blk1, blk2, blk3, blk4）的存储datanode（以下简称dn）列表；针对每个block的dn列表，会根据网络拓扑做排序，离client近的排在前；\n- 4、调用DFSInputStream的read方法，先读取blk1的数据，与client最近的datanode建立连接，读取数据\n- 5、读取完后，关闭与dn建立的流\n- 6、读取下一个block，如blk2的数据（重复步骤4、5、6）\n- 7、这一批block读取完后，再读取下一批block的数据（重复3、4、5、6、7）\n- 8、完成文件数据读取后，调用FSDataInputStream的close方法\n\n**1.2.2 容错**\n\n- 情况一：读取block过程中，client与datanode通信中断\n\n  - client与存储此block的第二个datandoe建立连接，读取数据\n  - 记录此有问题的datanode，不会再从它上读取数据\n\n- 情况二：client读取block，发现block数据有问题\n  -  client读取block数据时，同时会读取到block的校验和，若client针对读取过来的block数据，计算检验和，其值与读取过来的校验和不一样，说明block数据损坏\n  -  client从存储此block副本的其它datanode上读取block数据（也会计算校验和）\n  -  同时，client会告知namenode此情况；\n\n\n\n\n\n### 2. Hadoop HA高可用\n\n#### 2.1 HDFS高可用原理\n\n![](img/Image201905211519.png)\n\n- 对于HDFS ，NN存储元数据在内存中，并负责管理文件系统的命名空间和客户端对HDFS的读写请求。但是，如果只存在一个NN，一旦发生“单点故障”，会使整个系统失效。\n- 虽然有个SNN，但是它并不是NN的热备份\n- 因为SNN无法提供“热备份”功能，在NN故障时，无法立即切换到SNN对外提供服务，即HDFS处于停服状态。\n- HDFS2.x采用了HA（High Availability高可用）架构。\n  - 在HA集群中，可设置两个NN，一个处于“活跃（Active）”状态，另一个处于“待命（Standby）”状态。\n  - 由zookeeper确保一主一备（讲zookeeper时具体展开）\n  - 处于Active状态的NN负责响应所有客户端的请求，处于Standby状态的NN作为热备份节点，保证与active的NN的元数据同步\n  - Active节点发生故障时，zookeeper集群会发现此情况，通知Standby节点立即切换到活跃状态对外提供服务\n  - 确保集群一直处于可用状态\n- 如何热备份元数据：\n  - Standby NN是Active NN的“热备份”，因此Active NN的状态信息必须实时同步到StandbyNN。\n  - 可借助一个共享存储系统来实现状态同步，如NFS(NetworkFile System)、QJM(Quorum Journal Manager)或者Zookeeper。\n  - Active NN将更新数据写入到共享存储系统，Standby NN一直监听该系统，一旦发现有新的数据写入，就立即从公共存储系统中读取这些数据并加载到Standby NN自己内存中，从而保证元数据与Active NN状态一致。\n- 块报告：\n  - NN保存了数据块到实际存储位置的映射信息，为了实现故障时的快速切换，必须保证StandbyNN中也包含最新的块映射信息\n  - 因此需要给所有DN配置Active和Standby两个NN的地址，把块的位置和心跳信息同时发送到两个NN上。\n\n### 3. Hadoop联邦\n\n#### 3.1 为什么需要联邦\n\n- 虽然HDFS HA解决了“单点故障”问题，但HDFS在扩展性、整体性能和隔离性方面仍有问题\n  - 系统扩展性方面，元数据存储在NN内存中，受限于内存上限（每个文件、目录、block占用约150字节）\n  - 整体性能方面，吞吐量受单个NN的影响\n  - 隔离性方面，一个程序可能会影响其他程序的运行，如果一个程序消耗过多资源会导致其他程序无法顺利运行\n  - HDFS HA本质上还是单名称节点\n\n#### 3.2 联邦\n\n![](img/Image201909041239.png)\n\n\n- HDFS联邦可以解决以上三个问题\n  - HDFS联邦中，设计了多个命名空间；每个命名空间有一个NN或一主一备两个NN，使得HDFS的命名服务能够水平扩展\n  - 这些NN分别进行各自命名空间namespace和块的管理，相互独立，不需要彼此协调\n  - 每个DN要向集群中所有的NN注册，并周期性的向所有NN发送心跳信息和块信息，报告自己的状态\n  - HDFS联邦每个相互独立的NN对应一个独立的命名空间\n  - 每一个命名空间管理属于自己的一组块，这些属于同一命名空间的块对应一个“块池”的概念。\n  - 每个DN会为所有块池提供块的存储，块池中的各个块实际上是存储在不同DN中的\n\n#### 3.3 扩展\n\n[联邦-官网](<https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/Federation.html>)\n\n\n\n\n\n### 4. 文件压缩\n\n#### 4.1 压缩算法\n\n- 文件压缩好处：\n\n  - 减少数据所占用的磁盘空间\n  - 加快数据在磁盘、网络上的IO\n\n- 常用压缩格式\n\n  | 压缩格式 | UNIX工具 | 算      法 | 文件扩展名 | 可分割 |\n  | -------- | -------- | ---------- | ---------- | ------ |\n  | DEFLATE  | 无       | DEFLATE    | .deflate   | No     |\n  | gzip     | gzip     | DEFLATE    | .gz        | No     |\n  | zip      | zip      | DEFLATE    | .zip       | YES    |\n  | bzip     | bzip2    | bzip2      | .bz2       | YES    |\n  | LZO      | lzop     | LZO        | .lzo       | No     |\n  | Snappy   | 无       | Snappy     | .snappy    | No     |\n\n- Hadoop的压缩实现类；均实现CompressionCodec接口\n\n  | 压缩格式 | 对应的编码/解码器                          |\n  | -------- | ------------------------------------------ |\n  | DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |\n  | gzip     | org.apache.hadoop.io.compress.GzipCodec    |\n  | bzip2    | org.apache.hadoop.io.compress.BZip2Codec   |\n  | LZO      | com.hadoop.compression.lzo.LzopCodec       |\n  | Snappy   | org.apache.hadoop.io.compress.SnappyCodec  |\n\n- 查看集群是否支持本地压缩（所有节点都要确认）\n\n  ```\n  [hadoop@node01 ~]$ hadoop checknative\n  ```\n\n  ![](img/Image201910111114.png)\n\n#### 4.2 编程实践\n\n- 编程：上传压缩过的文件到HDFS\n\n\n```java\n\n    /**\n     * 上传压缩文件到服务器\n     *  传递参数\n     *  args[0] 本地文件路径\n     *  args[1] hdoop文件系统 路径\n     */\n    public static void uploadFileZipToFileSystem(String source,String targetUrl){\n        System.out.println(\"文件地址：\" + source);\n        System.out.println(\"目标服务器：\" + targetUrl);\n        InputStream inputStreamSourceFile = null;\n\n        try {\n            // 获取文件输入流\n            inputStreamSourceFile = new BufferedInputStream(new FileInputStream(source));\n            // HDFS 读写配置文件\n            Configuration configuration = new Configuration();\n            // 压缩类型\n            BZip2Codec codec = new BZip2Codec();\n            codec.setConf(configuration);\n            // 通过url 返回文件系统实例\n            FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);\n            //调用Filesystem的create方法返回的是FSDataOutputStream对象\n            //该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n            // 获取文件系用的输出流\n            OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n            // 对输出流进行压缩\n            CompressionOutputStream compressionOut = codec.createOutputStream(outputStreamTarget);\n            // 将文件输入流，写入输入流\n            IOUtils.copyBytes(inputStreamSourceFile,compressionOut,4069,true);\n            System.out.println(\"上传成功\");\n        } catch (FileNotFoundException e) {\n            System.err.println(e.getMessage());\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n```\n\n- 扩展阅读\n  - 《Hadoop权威指南》 5.2章节 压缩\n  - [HDFS文件压缩](<https://blog.csdn.net/qq_38262266/article/details/79171524>)\n\n\n\n\n\n### 5. 小文件治理\n\n#### 5.1 有没有问题\n\n- NameNode存储着文件系统的元数据，每个文件、目录、块大概有150字节的元数据；\n- 因此文件数量的限制也由NN内存大小决定，如果小文件过多则会造成NN的压力过大\n- 且HDFS能存储的数据总量也会变小\n\n#### 5.2 HAR文件方案（10分钟）\n\n- 本质启动mr程序，所以需要启动yarn\n\n![1558004541101](img/1558004541101.png)\n\n用法：\n\n```sh\narchive -archiveName <NAME>.har -p <parent path> [-r <replication factor>]<src>* <dest>\n```\n\n![](img/Image201909041408.png)\n\n![alt](img/Image201906210960.png)\n\n```shell\n# 创建archive文件；/testhar有两个子目录th1、th2；两个子目录中有若干文件\nhadoop archive -archiveName test.har -p /testhar -r 3 th1 th2 /outhar # 原文件还存在，需手动删除\n\n# 查看archive文件\nhdfs dfs -ls -R har:///outhar/test.har\n\n# 解压archive文件\n# 方式一\nhdfs dfs -cp har:///outhar/test.har/th1 hdfs:/unarchivef # 顺序\nhadoop fs -ls /unarchivef\t\n# 方式二\nhadoop distcp har:///outhar/test.har/th1 hdfs:/unarchivef2 # 并行，启动MR\n```\n\n#### 5.3 Sequence Files方案（*）\n\n- SequenceFile文件，主要由一条条record记录组成；每个record是键值对形式的\n- SequenceFile文件可以作为小文件的存储容器；\n  - 每条record保存一个小文件的内容\n  - 小文件名作为当前record的键；\n  - 小文件的内容作为当前record的值；\n  - 如10000个100KB的小文件，可以编写程序将这些文件放到一个SequenceFile文件。\n- 一个SequenceFile是**可分割**的，所以MapReduce可将文件切分成块，每一块独立操作。\n- 具体结构（如下图）：\n  - 一个SequenceFile首先有一个4字节的header（文件版本号）\n  - 接着是若干record记录\n  - 记录间会随机的插入一些同步点sync marker，用于方便定位到记录边界\n- 不像HAR，SequenceFile**支持压缩**。记录的结构取决于是否启动压缩\n  - 支持两类压缩：\n    - 不压缩NONE，如下图\n    - 压缩RECORD，如下图\n    - 压缩BLOCK，①一次性压缩多条记录；②每一个新块Block开始处都需要插入同步点；如下图\n  - 在大多数情况下，以block（注意：指的是SequenceFile中的block）为单位进行压缩是最好的选择\n  - 因为一个block包含多条记录，利用record间的相似性进行压缩，压缩效率更高\n  - 把已有的数据转存为SequenceFile比较慢。比起先写小文件，再将小文件写入SequenceFile，一个更好的选择是直接将数据写入一个SequenceFile文件，省去小文件作为中间媒介.\n\n![](img/Image201907101934.png)\n\n\n\n![](img/Image201907101935.png)\n\n- 向SequenceFile写入数据\n\n```java\npackage com.kaikeba.hadoop.sequencefile;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.SequenceFile;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.io.compress.BZip2Codec;\n\nimport java.io.IOException;\nimport java.net.URI;\n\npublic class SequenceFileWriteNewVersion {\n\n    //模拟数据源\n    private static final String[] DATA = {\n            \"The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.\",\n            \"It is designed to scale up from single servers to thousands of machines, each offering local computation and storage.\",\n            \"Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer\",\n            \"o delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.\",\n            \"Hadoop Common: The common utilities that support the other Hadoop modules.\"\n    };\n\n    public static void main(String[] args) throws IOException {\n        //输出路径：要生成的SequenceFile文件名\n        String uri = \"hdfs://node01:9000/writeSequenceFile\";\n\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(URI.create(uri), conf);\n        //向HDFS上的此SequenceFile文件写数据\n        Path path = new Path(uri);\n\n        //因为SequenceFile每个record是键值对的\n        //指定key类型\n        IntWritable key = new IntWritable();\n        //指定value类型\n        Text value = new Text();\n//\n//            FileContext fileContext = FileContext.getFileContext(URI.create(uri));\n//            Class<?> codecClass = Class.forName(\"org.apache.hadoop.io.compress.SnappyCodec\");\n//            CompressionCodec SnappyCodec = (CompressionCodec)ReflectionUtils.newInstance(codecClass, conf);\n//            SequenceFile.Metadata metadata = new SequenceFile.Metadata();\n//            //writer = SequenceFile.createWriter(fs, conf, path, key.getClass(), value.getClass());\n//            writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(path), SequenceFile.Writer.keyClass(IntWritable.class),\n//                                        SequenceFile.Writer.valueClass(Text.class));\n\n        //创建向SequenceFile文件写入数据时的一些选项\n        //要写入的SequenceFile的路径\n        SequenceFile.Writer.Option pathOption       = SequenceFile.Writer.file(path);\n        //record的key类型选项\n        SequenceFile.Writer.Option keyOption        = SequenceFile.Writer.keyClass(IntWritable.class);\n        //record的value类型选项\n        SequenceFile.Writer.Option valueOption      = SequenceFile.Writer.valueClass(Text.class);\n        //SequenceFile压缩方式：NONE | RECORD | BLOCK三选一\n        //方案一：RECORD、不指定压缩算法\n        SequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD);\n        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);\n\n\n        //方案二：BLOCK、不指定压缩算法\n//        SequenceFile.Writer.Option compressOption   = SequenceFile.Writer.compression(SequenceFile.CompressionType.BLOCK);\n//        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressOption);\n\n\n\n        //方案三：使用BLOCK、压缩算法BZip2Codec；压缩耗时间\n        //再加压缩算法\n//        BZip2Codec codec = new BZip2Codec();\n//        codec.setConf(conf);\n//        SequenceFile.Writer.Option compressAlgorithm = SequenceFile.Writer.compression(SequenceFile.CompressionType.RECORD, codec);\n//        //创建写数据的Writer实例\n//        SequenceFile.Writer writer = SequenceFile.createWriter(conf, pathOption, keyOption, valueOption, compressAlgorithm);\n\n\n\n        for (int i = 0; i < 100000; i++) {\n            //分别设置key、value值\n            key.set(100 - i);\n            value.set(DATA[i % DATA.length]);\n            System.out.printf(\"[%s]\\t%s\\t%s\\n\", writer.getLength(), key, value);\n            //在SequenceFile末尾追加内容\n            writer.append(key, value);\n        }\n        //关闭流\n        IOUtils.closeStream(writer);\n    }\n}\n```\n\n- 命令查看SequenceFile内容\n\n```shell\n hadoop fs -text /writeSequenceFile\n```\n\n- 读取SequenceFile文件\n\n```java\npackage com.kaikeba.hadoop.sequencefile;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\nimport org.apache.hadoop.io.SequenceFile;\nimport org.apache.hadoop.io.Writable;\nimport org.apache.hadoop.util.ReflectionUtils;\n\nimport java.io.IOException;\n\npublic class SequenceFileReadNewVersion {\n\n    public static void main(String[] args) throws IOException {\n        //要读的SequenceFile\n        String uri = \"hdfs://node01:9000/writeSequenceFile\";\n        Configuration conf = new Configuration();\n        Path path = new Path(uri);\n\n        //Reader对象\n        SequenceFile.Reader reader = null;\n        try {\n            //读取SequenceFile的Reader的路径选项\n            SequenceFile.Reader.Option pathOption = SequenceFile.Reader.file(path);\n\n            //实例化Reader对象\n            reader = new SequenceFile.Reader(conf, pathOption);\n\n            //根据反射，求出key类型\n            Writable key = (Writable)\n                    ReflectionUtils.newInstance(reader.getKeyClass(), conf);\n            //根据反射，求出value类型\n            Writable value = (Writable)\n                    ReflectionUtils.newInstance(reader.getValueClass(), conf);\n\n            long position = reader.getPosition();\n            System.out.println(position);\n\n            while (reader.next(key, value)) {\n                String syncSeen = reader.syncSeen() ? \"*\" : \"\";\n                System.out.printf(\"[%s%s]\\t%s\\t%s\\n\", position, syncSeen, key, value);\n                position = reader.getPosition(); // beginning of next record\n            }\n        } finally {\n            IOUtils.closeStream(reader);\n        }\n    }\n}\n```\n\n\n\n###  6. 文件快照\n\n####  6.1 什么是快照\n\n- 快照比较常见的应用场景是数据备份，以防一些用户错误或灾难恢复\n- 快照snapshots是HDFS文件系统的，只读的、某时间点的拷贝\n- 可以针对某个目录，或者整个文件系统做快照\n- 创建快照时，block块并不会被拷贝。快照文件中只是记录了block列表和文件大小，**不会做任何数据拷贝**\n\n####  6.2 快照操作\n\n- 允许快照\n\n  允许一个快照目录被创建。如果这个操作成功完成，这个目录就变成snapshottable\n\n  用法：hdfs dfsadmin -allowSnapshot <snapshotDir>\n\n  ```shell\n  hdfs dfsadmin -allowSnapshot /wordcount\n  ```\n\n- 禁用快照\n\n  用法：hdfs dfsadmin -disallowSnapshot <snapshotDir>\n\n  ```shell\n  hdfs dfsadmin -disallowSnapshot /wordcount\n  ```\n\n- 创建快照\n\n  用法：hdfs dfs -createSnapshot <snapshotDir> [<snapshotName>]\n\n  ```shell\n  #注意：先将/wordcount目录变成允许快照的\n  hdfs dfs -createSnapshot /wordcount wcSnapshot\n  ```\n\n- 查看快照\n\n  ```shell\n  hdfs dfs -ls /wordcount/.snapshot\n  \n  ```\n\n  ![](img/Image201909041346.png)\n\n- 重命名快照\n\n  这个操作需要拥有snapshottabl目录所有者权限\n\n  用法：hdfs dfs -renameSnapshot <snapshotDir> <oldName> <newName>\n\n  ```shell\n  hdfs dfs -renameSnapshot /wordcount wcSnapshot newWCSnapshot\n  \n  ```\n\n- 用快照恢复误删除数据\n\n  HFDS的/wordcount目录，文件列表如下\n\n  ![](img/Image201909041356.png)\n\n  误删除/wordcount/edit.xml文件\n\n  ```shell\n  hadoop fs -rm /wordcount/edit.xml\n  \n  ```\n\n  ![](img/Image201909041400.png)\n\n  恢复数据\n\n  ```shell\n  hadoop fs -cp /wordcount/.snapshot/newWCSnapshot/edit.xml /wordcount\n  \n  ```\n\n- 删除快照\n\n  这个操作需要拥有snapshottabl目录所有者权限\n\n  用法：hdfs dfs -deleteSnapshot <snapshotDir> <snapshotName>\n\n  ```shell\n  hdfs dfs -deleteSnapshot /wordcount newWCSnapshot\n  \n  ```\n\n\n\n\n\n\n##  7、拓展点、未来计划、行业趋势\n\n1. HDFS存储地位\n\n2. **block块为什么设置的比较大**\n\n- [磁盘基础知识](<https://www.cnblogs.com/jswang/p/9071847.html>)\t\n\n  - 盘片platter、磁头head、磁道track、扇区sector、柱面cylinder\n  - 为了最小化寻址开销；从磁盘传输数据的时间明显大于定位这个块开始位置所需的时间\n\n- 问：块的大小是不是设置的越大越好呢？\n\n  1、 不是，寻址的时间大概是 100ms，设计一般设置为寻址时间占用十分之一，也就是一秒。 硬盘的传输速录大概是100m/s 一秒大概为100M，最接近100的大小为128M。 \n\n![](img/Image201906211143.png)\n","tags":["hadoop","hafs read / write","文件快照"]},{"title":"大数据概论-HDFS理论基础","url":"/2019/10/10/hadoop/Java大数据基础概论/","content":"## 大数据概论\n\n> 概念： 大数据（big data）是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产\n\n| 数据单位 | B    | KB   | MB   | GB   | PE   | PB   | EB   | ZB   | YB   |\n| -------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| 基数     |      | 2    | 2    | 2    | 2    | 2    | 2    | 10   | 10   |\n| 次方     | 0    | 10   | 20   | 30   | 40   | 50   | 60   | 21   | 24   |\n\n### 一、大数据特性\n\n1. 数据量大（Volume） \n2. 类型繁多（Variety） \n3. 价值密度低（Value） \n4. 速度快时效高（Velocity）\n\n### 二、大数据的挑战\n\n1. 存储： 每天几TB、GB的数据增量，并且还在持续的增长中。\n2. 分析： 如何从巨大的数据中挖掘出隐藏的商业价值。\n3. 管理： 如何快速构建并且保证系统的安全、简单可用。\n\n### 三、传统的大数据项目流程\n\n```flow\nst=>start: 开始\ndataCollect=>operation: 数据收集 ： Flume、Kafaka、Scribe\ndataStore=>operation: 数据存储 ： HDFS、HBase、Cassadra\ndataCaculate=>operation: 数据计算 : Mapreduce、Strom、Impala、Spark、Spark Streaming...\n数据计算三大类：\n1、离线处理平台： Spark、Spark Core\n2、交互式处理平台： Spark SQL、Hive 、Impala\n3、流处理平台 ： Strom、Spring Stoeaming 、 Flink\ndataAnalyse=>operation: 分析与挖掘 ： Mahour、R语言、Hive、Pig\ndataEtl=>operation: ETL ： sqoop、DataX\ndataView=>operation: 可视化 ： Echarts.js 、 E3.js、 数据报表系统\ndataActual=>operation: 项目实战\ne=>end: 结束\n\nst->dataCollect->dataStore->dataCaculate->dataAnalyse->dataEtl->dataView->dataActual->e\n\n```\n\n## 分布式文件系统\n\n### 一、Hadoop简介\n\n1. Hadoop架构\n\n   ![Image201906191834](img/Image201906191834.png)\n\n2. Hadoop历史\n\n   ![Image201906202055](img/Image201906202055.png)\n\n### 二、HDFS\n\n- HDFS是Hadoop中的一个存储子模块\n- HDFS (全称Hadoop Distributed File System)，即hadoop的分布式文件系统\n- File System**文件系统**：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读出、修改、转储、删除文件等\n- 当数据集大小超出一台计算机的存储能力时，就有必要将它拆分成若干部分，然后分散到不同的计算机中存储。管理网络中跨多台计算机存储的文件系统称之为**分布式文件系统**（distributed filesystem）\n\n#### 2.1 HDFS特点\n\n**2.1.1 优点：**\n\n- 适合存储大文件，能用来存储管理PB级的数据；不适合存储小文件\n- 处理非结构化数据\n- 流式的访问数据，一次写入、多次读写\n- 运行于廉价的商用机器集群上，成本低\n- 高容错：故障时能继续运行且不让用户察觉到明显的中断\n- 可扩展\n\n![](/Users/dingchuangshi/Documents/Java大数据课件/第八章HDFS课件/20191009-HDFS-第一次/assets/Image201907081216.png)\n\n**2.1.2 局限性**\n\n- 不适合处理低延迟数据访问\n  - DFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的\n  - 对于低延时的访问需求，HBase是更好的选择\n- 无法高效存储大量的小文件\n  - 小文件会给Hadoop的扩展性和性能带来严重问题\n  - 利用SequenceFile、MapFile等方式归档小文件\n- 不支持多用户写入及任意修改文件\n  - 文件有一个写入者，只能执行追加操作\n  - 不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改\n\n#### 2.2 HDFS常用命令\n\n> HDFS两种命令风格，两种命令效果等同\n>\n> hadoop fs / hdfs dfs\n\n![image-20191010155353956](/Users/dingchuangshi/Library/Application Support/typora-user-images/image-20191010155353956.png)\n\n\n\n1. 如何查看hdfs或hadoop子命令的**帮助信息**，如ls子命令\n\n   ```shell\n   hdfs dfs -help ls\n   hadoop fs -help ls\t#两个命令等价\n   ```\n\n2. **查看**hdfs文件系统中已经存在的文件。对比linux命令ls\n\n   ```shell\n   hdfs dfs -ls /\n   hadoop fs -ls /\n   ```\n\n3. 在hdfs文件系统中创建文件\n\n   ```shell\n   hdfs dfs -touchz /edits.txt\n   ```\n\n4. 向HDFS文件中追加内容\n\n    ```shell\n    hadoop fs -appendToFile edit1.xml /edits.txt #将本地磁盘当前目录的edit1.xml内容追加到HDFS根目录 的edits.txt文件\n    ```\n\n5. 查看HDFS文件内容\n\n    ```shell\n    hdfs dfs -cat /edits.txt\n    ```\n\n6. **从本地路径上传文件至HDFS**\n\n    ````` shell\n    #用法：hdfs dfs -put /本地路径 /hdfs路径\n    hdfs dfs -put hadoop-2.7.3.tar.gz /\n    hdfs dfs -copyFromLocal hadoop-2.7.3.tar.gz /  #根put作用一样\n    hdfs dfs -moveFromLocal hadoop-2.7.3.tar.gz /  #根put作用一样，只不过，源文件被拷贝成功后，会被删除\n    `````\n\n7. **在hdfs文件系统中下载文件**\n\n     ```shell\n     hdfs dfs -get /hdfs路径 /本地路径\n     hdfs dfs -copyToLocal /hdfs路径 /本地路径  #根get作用一样\n     ```\n\n8. 在hdfs文件系统中**创建目录**\n\n     ```shell\n     hdfs dfs -mkdir /shell\n     ```\n\n9. 在hdfs文件系统中**删除**文件\n\n     ```shell\n     hdfs dfs -rm /edits.txt\n     hdfs dfs -rm -r /shell\n     ```\n\n10. 在hdfs文件系统中**修改文件名称**（也可以用来**移动**文件到目录）\n\n     ```shell\n     hdfs dfs -mv /xcall.sh /call.sh\n     hdfs dfs -mv /call.sh /shell\n     ```\n\n11. 在hdfs中拷贝文件到目录\n\n      ```shell\n      hdfs dfs -cp /xrsync.sh /shell\n      ```\n\n12. 递归删除目录\n\n      ```shell\n      hdfs dfs -rmr /shell\n      ```\n\n13. 列出本地文件的内容（默认是hdfs文件系统）\n\n      ```shell\n      hdfs dfs -ls file:///home/bruce/\n      ```\n\n14. 查找文件\n\n      ```shell\n      # linux find命令\n      find . -name 'edit*'\n      \n      # HDFS find命令\n      hadoop fs -find / -name part-r-00000 # 在HDFS根目录中，查找part-r-00000文件\n      ```\n\n\n> 还有许多其他命令，大家可以自己探索一下   \n\n##### 2.2.1 命令行小结\n\n- 输入hadoop fs 或hdfs dfs，回车，查看所有的HDFS命令\n\n- 许多命令与linux命令有很大的相似性，学会举一反三\n- 有用的help，如查看ls命令的使用说明：hadoop fs -help ls\n\n##### 2.2.2 hdfs与getconf结合使用\n\n1. 获取NameNode的节点名称（可能有多个）\n\n      ``````shell\n      hdfs getconf -namenodes\n      ``````\n\n2. 获取hdfs最小块信息\n\n      ``````shell\n      hdfs getconf -confKey dfs.namenode.fs-limits.min-block-size\n      ``````\n\n3. 查找hdfs的NameNode的RPC地址\n\n\t``````shell\n\thdfs getconf -nnRpcAddresses\n\t``````\n\t\n\t\n\n##### 2.2.3 hdfs与dfsadmin结合使用\n\n1. 同样要学会借助帮助信息\n\n      ```shell\n      hdfs dfsadmin -help safemode\n      ```\n\n2. 查看hdfs dfsadmin的帮助信息\n\n      ``````shell\n      hdfs dfsadmin\n      ``````\n\n3. 查看当前的模式\n\n      ``````shell\n      hdfs dfsadmin -safemode get\n      ``````\n\n4. 进入安全模式\n\n  ``````shell\n  hdfs dfsadmin -safemode enter\n  ``````\n\n  \n\n##### 2.2.4 hdfs与fsck结合使用\n\n1. fsck指令**显示HDFS块信息**\n\n\t``````shell\n\thdfs fsck /02-041-0029.mp4 -files -blocks -locations # 查看文件02-041-0029.mp4的块信息\n\t``````\n\t\n\t\n\n##### 2.2.5 其他命令\n\n1. 检查压缩库本地安装情况\n\n      ``````shell\n      hadoop checknative\n      ``````\n\n2. 格式化名称节点（**慎用**，一般只在初次搭建集群，使用一次；格式化成功后，不要再使用）\n\n      ``````shell\n      hadoop namenode -format\n      ``````\n\n3. 执行自定义jar包\n\n   ``````shell\n   hadoop jar YinzhengjieMapReduce-1.0-SNAPSHOT.jar com.kaikeba.hadoop.WordCount /world.txt /out\n   ``````\n\n#### 2.3 HDFS编程\n\n\n- 1.向hdfs中,上传一个文本文件\n\n  ```java\n   /**\n       * 上传文件到服务器\n       *  传递参数\n       *  args[0] 本地文件路径\n       *  args[1] hdoop文件系统 路径\n       */\n      public static void uploadFileToFileSystem(String source,String targetUrl){\n          System.out.println(\"文件地址：\" + source);\n          System.out.println(\"目标服务器：\" + targetUrl);\n          InputStream inputStreamSourceFile = null;\n          try {\n              // 获取文件输入流\n              inputStreamSourceFile = new BufferedInputStream(new FileInputStream(source));\n              // HDFS 读写配置文件\n              Configuration configuration = new Configuration();\n              // 通过url 返回文件系统实例\n              FileSystem fileSystem = FileSystem.get(URI.create(targetUrl),configuration);\n              //调用Filesystem的create方法返回的是FSDataOutputStream对象\n              //该对象不允许在文件中定位，因为HDFS只允许一个已打开的文件顺序写入或追加\n              // 获取文件系用的输出流\n              OutputStream outputStreamTarget = fileSystem.create(new Path(targetUrl));\n              // 将文件输入流，写入输入流\n              IOUtils.copyBytes(inputStreamSourceFile,outputStreamTarget,4069,true);\n              System.out.println(\"上传成功\");\n          } catch (FileNotFoundException e) {\n              System.err.println(e.getMessage());\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n      }\n  \n  ```\n\n- 2.读取hdfs上的文件\n\n```java\n\n    /**\n     * 从文件系统中读取文件\n     * @param source 需要读取的文件\n     * @return 读取文件内容\n     */\n    public static String readFileFromFileSystem(String source){\n        String result = null;\n        try {\n            // HDFS 读写文件配置\n            Configuration configuration = new Configuration();\n            // HDFS文件系统\n            FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n            // 文件输入流，用于读取文件\n            InputStream inputStream = fileSystem.open(new Path(source));\n            BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream));\n            result = readBufferReader(bufferedReader).toString();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return result;\n    }\n    \n    \n    /**\n     * 获取内容\n     * @param br\n     * @return\n     */\n    private static StringBuffer readBufferReader(BufferedReader br) throws IOException {\n        StringBuffer stringBuffer = new StringBuffer();\n        String temp = \"\";\n        while ((temp = br.readLine()) != null){\n            stringBuffer.append(temp);\n        }\n        return stringBuffer;\n    }\n```\n\n\n\n- 3.列出某一个文件夹下的所有文件\n\n```java\n\n    /**\n     * 列出当前目录下所有字文件名称\n     * @param source\n     * @return\n     */\n    public static String listAllFileChildren(String source){\n        StringBuffer stringBuffer = new StringBuffer();\n        try {\n            // HDFS 读写文件配置\n            Configuration configuration = new Configuration();\n            // HDFS文件系统\n            FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n            // recursive 继续深入遍历\n            RemoteIterator<LocatedFileStatus> iterator = fileSystem.listFiles(new Path(source),true);\n            while (iterator.hasNext()){\n                LocatedFileStatus fileStatus = iterator.next();\n                stringBuffer.append(fileStatus.getPath() + \"\\n\");\n            }\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        return stringBuffer.toString();\n    }\n\n```\n\n\n\n- 4.列出多级目录名称和目录下的文件名称\n\n  ```java\n  \n      /**\n       * 递归列出当前目录下所有目录和文件名称\n       * @param source\n       * @return\n       */\n      public static String listAllChildren(String source){\n          StringBuffer stringBuffer = new StringBuffer();\n          try {\n              // HDFS 读写文件配置\n              Configuration configuration = new Configuration();\n              // HDFS文件系统\n              FileSystem fileSystem = FileSystem.get(URI.create(source),configuration);\n              list(stringBuffer,fileSystem,new Path(source));\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n          return stringBuffer.toString();\n      }\n  \n      /**\n       * 递归目录和文件\n       * @param stringBuffer  文件目录名称集合\n       * @param fileSystem  hdfs 文件系统\n       * @param source path 路径\n       * @throws IOException\n       */\n      private static void list(StringBuffer stringBuffer,FileSystem fileSystem, Path source) throws IOException {\n          FileStatus[] iterator = fileSystem.listStatus(source);\n          for (FileStatus status:iterator) {\n              stringBuffer.append(status.getPath() + \"\\n\");\n              if(status.isDirectory()){\n                  list(stringBuffer,fileSystem,status.getPath());\n              }\n  \n          }\n      }\n  \n  \n  ```\n\n  \n\n#### 2.4  HDFS架构\n\n![](img/1558073557041.png)\n\n- 大多数分布式框架都是主从架构\n- HDFS也是主从架构Master|Slave或称为管理节点|工作节点\n\n##### 1、NameNode\n\n**1.1 文件系统**\n\n- file system文件系统：操作系统中负责管理和存储文件信息的软件；具体地说，它负责为用户创建文件，存入、读取、修改、转储、删除文件等\n- 读文件 =>>找到文件 =>> 在哪 + 叫啥？\n- 元数据\n  - 关于文件或目录的描述信息，如文件所在路径、文件名称、文件类型等等，这些信息称为文件的元数据metadata\n- 命名空间\n  - 文件系统中，为了便于管理存储介质上的，给每个目录、目录中的文件、子目录都起了名字，这样形成的层级结构，称之为命名空间\n  - 同一个目录中，不能有同名的文件或目录\n  - 这样通过目录+文件名称的方式能够唯一的定位一个文件\n\n![](img/Image201906211418.png)\n\n**5.1.2 HDFS-NameNode**\n\n- HDFS本质上也是文件系统filesystem，所以它也有元数据metadata；\n- 元数据metadata保存在NameNode**内存**中\n- NameNode作用\n  - HDFS的主节点，负责管理文件系统的命名空间，将HDFS的元数据存储在NameNode节点的内存中\n  - 负责响应客户端对文件的读写请求\n- HDFS元数据\n  - 文件目录树、所有的文件（目录）名称、文件属性（生成时间、副本、权限）、每个文件的块列表、每个block块所在的datanode列表\n\n![](img/Image201909031504.png)\n\n  - 每个文件、目录、block占用大概**150Byte字节的元数据**；所以HDFS适合存储大文件，不适合存储小文件\n\n  - HDFS元数据信息以两种形式保存：①编辑日志**edits log**②命名空间镜像文件**fsimage**\n    - edits log：HDFS编辑日志文件 ，保存客户端对HDFS的所有更改记录，如增、删、重命名文件（目录），这些操作会修改HDFS目录树；NameNode会在编辑日志edit日志中记录下来；\n    - fsimage：HDFS元数据镜像文件 ，即将namenode内存中的数据落入磁盘生成的文件；保存了文件系统目录树信息以及文件、块、datanode的映射关系，如下图\n\n\n![](img/Image201910091133.png)\n\n> 说明：\n>\n> ①为hdfs-site.xml中属性dfs.namenode.edits.dir的值决定；用于namenode保存edits.log文件\n>\n> ②为hdfs-site.xml中属性dfs.namenode.name.dir的值决定；用于namenode保存fsimage文件\n\n##### 2、DataNode\n\n- DataNode数据节点的作用\n  - 存储block以及block元数据到datanode本地磁盘；此处的元数据包括数据块的长度、块数据的校验和、时间戳\n\n##### 3 SeconddaryNameNode   \n\n- 为什么引入SecondaryNameNode\n\n  - 为什么元数据存储在NameNode在内存中？\n\n  - 这样做有什么问题？如何解决？\n\n  - HDFS编辑日志文件 editlog：在NameNode节点中的编辑日志editlog中，记录下来客户端对HDFS的所有更改的记录，如增、删、重命名文件（目录）；\n\n  - 作用：一旦系统出故障，可以从editlog进行恢复；\n\n  - 但editlog日志大小会随着时间变在越来越大，导致系统重启根据日志恢复的时候会越来越长；\n\n  - 为了避免这种情况，引入**检查点机制checkpoint**，命名空间镜像fsimage就是HDFS元数据的持久性检查点，即将内存中的元数据落磁盘生成的文件；\n\n  - 此时，namenode如果重启，可以将磁盘中的fsimage文件读入内容，将元数据恢复到某一个检查点，然后再执行检查点之后记录的编辑日志，最后完全恢复元数据。\n\n  - 但是依然，随着时间的推移，editlog记录的日志会变多，那么当namenode重启，恢复元数据过程中，会花越来越长的时间执行editlog中的每一个日志；而在namenode元数据恢复期间，HDFS不可用。\n\n  - 为了解决此问题，引入secondarynamenode辅助namenode，用来合并fsimage及editlog\n\n\n\n![](img/Image201906211525.png)\n\n- SecondaryNameNode定期做checkpoint检查点操作\n\n  - 创建检查点checkpoint的两大条件：\n    - SecondaryNameNode每隔1小时创建一个检查点\n    - 另外，Secondary NameNode每1分钟检查一次，从上一检查点开始，edits日志文件中是否已包括100万个事务，如果是，也会创建检查点\n  - Secondary NameNode首先请求原NameNode进行edits的滚动，这样新的编辑操作就能够进入新的文件中\n  - Secondary NameNode通过HTTP GET方式读取原NameNode中的fsimage及edits\n  - Secondary NameNode读取fsimage到内存中，然后执行edits中的每个操作，并创建一个新的统一的fsimage文件\n  - Secondary NameNode通过HTTP PUT方式将新的fsimage发送到原NameNode\n  - 原NameNode用新的fsimage替换旧的fsimage，同时系统会更新fsimage文件到记录检查点的时间。 \n  - 这个过程结束后，NameNode就有了最新的fsimage文件和更小的edits文件\n\n- SecondaryNameNode一般部署在另外一台节点上\n\n  - 因为它需要占用大量的CPU时间\n  - 并需要与namenode一样多的内存，来执行合并操作\n\n- 如何查看edits日志文件\n\n  ```shell\n  hdfs oev -i edits_0000000000000000256-0000000000000000363 -o /home/hadoop/edit1.xml\n  ```\n\n- 如何查看fsimage文件\n\n  ```shell\n  hdfs oiv -p XML -i fsimage_0000000000000092691 -o fsimage.xml  \n  ```\n\n- checkpoint相关属性\n\n  | 属性                                 | 值              | 解释                                                         |\n  | ------------------------------------ | --------------- | ------------------------------------------------------------ |\n  | dfs.namenode.checkpoint.period       | 3600秒(即1小时) | The number of seconds between two periodic checkpoints.      |\n  | dfs.namenode.checkpoint.txns         | 1000000         | The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless of whether 'dfs.namenode.checkpoint.period' has expired. |\n  | dfs.namenode.checkpoint.check.period | 60(1分钟)       | The SecondaryNameNode and CheckpointNode will poll the NameNode every 'dfs.namenode.checkpoint.check.period' seconds to query the number of uncheckpointed transactions. |\n\n  \n\n##### 4 心跳机制\n\n![](img/Image201906211518.png)\n\n**工作原理：**\n\n1. NameNode启动的时候，会开一个ipc server在那里\n2. DataNode启动后向NameNode注册，每隔**3秒钟**向NameNode发送一个“**心跳heartbeat**”\n3. 心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一DataNode，或删除某个数据块\n4. 如果超过**10分钟**NameNode没有收到某个DataNode 的心跳，则认为该DataNode节点不可用\n5. DataNode周期性（**6小时**）的向NameNode上报当前DataNode上的块状态报告BlockReport；块状态报告包含了一个该 Datanode上所有数据块的列表\n\n**心跳的作用：**\n\n1. 通过周期心跳，NameNode可以向DataNode返回指令\n\n2. 可以判断DataNode是否在线\n\n3. 通过BlockReport，NameNode能够知道各DataNode的存储情况，如磁盘利用率、块列表；跟**负载均衡**有关\n\n4. hadoop集群刚开始启动时，99.9%的block没有达到最小副本数(dfs.namenode.replication.min默认值为1)，集群处于**安全模式**，涉及BlockReport；\n\n**心跳相关配置**\n\n- [hdfs-default.xml](<https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml>)\n- 心跳间隔\n\n| 属性                   | 值   | 解释                                               |\n| ---------------------- | ---- | -------------------------------------------------- |\n| dfs.heartbeat.interval | 3    | Determines datanode heartbeat interval in seconds. |\n\n- **block report**\n\n| More Actions属性             | 值               | 解释                                                 |\n| ---------------------------- | ---------------- | ---------------------------------------------------- |\n| dfs.blockreport.intervalMsec | 21600000 (6小时) | Determines block reporting interval in milliseconds. |\n\n- 查看hdfs-default.xml默认配置文件\n\n![](/Users/dingchuangshi/Documents/Java大数据课件/三期课件/第八章HDFS课件/20191009-HDFS-第一次/assets/Image201907311730.png)\n\n##### 5 负载均衡\n\n- 什么原因会有可能造成不均衡？\n  - 机器与机器之间磁盘利用率不平衡是HDFS集群非常容易出现的情况\n  - 尤其是在DataNode节点出现故障或在现有的集群上增添新的DataNode的时候\n\n- 为什么需要均衡？\n  - 提升集群存储资源利用率\n  - 从存储与计算两方面提高集群性能\n\n- 如何手动负载均衡？\n\n```shell\n$HADOOP_HOME/sbin/start-balancer.sh -t 5%\t# 磁盘利用率最高的节点若比最少的节点，大于5%，触发均衡\n```\n\n##### 6 小结\n\n- NameNode负责存储元数据，存在内存中\n- DataNode负责存储block块及块的元数据\n- SecondaryNameNode主要负责对HDFS元数据做checkpoint操作\n- 集群的心跳机制，让集群中各节点形成一个整体；主节点知道从节点的死活\n- 节点的上下线，导致存储的不均衡，可以手动触发负载均衡\n","tags":["hadoop"]}]